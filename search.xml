<?xml version="1.0" encoding="utf-8" ?>
<search>
    <entry>
    	<title><![CDATA[LaTeX 黑魔法（三）：定义参数变长的命令 z]]></title>
    	<url>/tech/2017/07/30/define-a-new-command-with-different-amount-of-parameters-in-latex/</url>
		<content type="text"><![CDATA[[原文地址：https://liam0205.me/2017/07/30/define-a-new-command-with-different-amount-of-parameters-in-LaTeX/ 在 C&#43;&#43; 中，我们可以为同一个函数赋予不同的执行内容，这种行为称之为「函数重载」。具体重载的函数，共享同一个函数名，但是接收的函数参数在数量、类型上不同。LaTeX 是宏语言，没有一般意义上的参数类型的说法。但是，有没有办法在 LaTeX 中「重载」一个宏，根据输入的参数数量不同，而产生不同的效果呢？ 本文给出解决方案。 在 TeX 和 LaTeX2e 中定义新命令 TeX 中，定义新命令的标准方法是使用 TeX 原语 \def。它有几个变种，记录如下。 \def：局部定义，定义时不展开； \edef：局部定义，定义时完全展开； \gdef：相当于 \global\def； \xdef：相当于 \global\edef。 建立在 TeX 之上的各种格式，其提供的定义新命令的方案，都是通过这四个 \def 来实现的。LaTeX2e 中定义新命令的标准方法是使用 \newcommand。它也有几个变种，记录如下。 \newcommand：新定义一个命令，如果该命令已有定义，则报错； \renewcommand：重定义一个命令，如果该命令未定义，则报错； \providecommand：如果该命令未定义，则定义一个新的命令；否则，啥也不干。 当然，在 LaTeX2e 中，也有 \DeclareRobustCommand 一系列命令，可以用来定义新的命令。这一系列命令，是 LaTeX2e 针对「脆弱命令」问题，提供的一些保护机制。此处不表。 在标准的方法中，不论是 TeX 还是 LaTeX2e，都没有提供「参数变长」的实现方法。也就是说，如果不引入奇怪的技巧，我们在普通的 LaTeX 文稿中，是无法重载命令的。 \@ifnextchar \@ifnextchar 是一个 LaTeX 内部宏。它的作用，是「预读」输入列表的下一个字符，然后判断预读的字符是否与作者期待的字符一致，执行不同的分支。 例如，我们知道，LaTeX 命令的可选参数，默认是放在所有必选参数之前。那么，我们是否有可能让可选参数放在必选参数之后呢？答案当然是肯定的，利用 \@ifnextchar 就可以做到。 \documentclass{article} \makeatletter \newcommand{\foo@helper@i}[1]{One parameter: #1{}.} \def\foo@helper@ii #1[#2]{Two parameters: #1{}, #2{}.} \newcommand{\foo}[1]{% \@ifnextchar[% {\foo@helper@ii{#1}}% {\foo@helper@i{#1}}% } \makeatother \begin{document} \foo{hello} \foo{hello}[world] \end{document} 我们来看 \foo 的定义。它接收一个标准的 LaTeX 参数。因此不管是 \foo{hello} 还是 \foo{hello}[world]，LaTeX 都会把其中的 \foo{hello} 先「吃下去」。接下来，LaTeX 会判断下一个字符是否为 [。对于 \foo{hello} 这种用法，下一个字符是换行符，因此判定失败，执行 \foo@helper@i。而对于 \foo{hello}[world] 这种用法，吃下去 \foo{hello} 之后，输入流中剩下了 [world]...，下一个字符正是 [，因此执行 \foo@helper@ii。 对于 \foo@helper@ii，它是使用 TeX 的原语 \def 定义的命令。参数列表 #1[#2] 表示该命令接受两个参数。第一个参数是标准的 TeX 参数——用分组包括起来。因此，上一步执行的 \foo@helper@ii 将第一个参数喂给了 \foo@helper@ii。接下来，\foo@helper@ii 还要吃下去第二个参数。按照定义，第二个参数被方括号 [] 所包围。因此 [world] 中的 world 被吃掉，作为第二个参数。 最终输出如图。 \bgroup 上面的 \foo 命令，基本已经达成了我们的目标。只不过，第二个参数必须是用方括号表达的。当然这不是不可以，但强迫症选手们可能会希望第二个参数也能用花括号来界定。于是，强迫症们尝试把 \@ifnextchar[ 尝试换成了 \@ifnextchar{。于是它们得到了报错 File ended while scanning use of... 这是因为，TeX 遇到 { 时，会将其解释为一个分组。因此，这种写法会造成 TeX 读入的分组不匹配。这样一来，我们就必须用 \bgroup 来代替花括号。它的定义是 \let\bgroup={。 \documentclass{article} \makeatletter \newcommand{\foo@helper@i}[1]{One parameter: #1{}.} \newcommand{\foo@helper@ii}[2]{Two parameters: #1{}, #2{}.} \newcommand{\foo}[1]{% \@ifnextchar\bgroup% {\foo@helper@ii{#1}}% {\foo@helper@i{#1}}% } \makeatother \begin{document} \foo{hello} \foo{hello}{world} \end{document} 这样一来，我们就实现了一个 \foo 命令，在参数不同的情况下，具有不同的行为。 xparse 宏包 基于 LaTeX3 的 xparse 宏包给了我们新的选项。它提供的 \NewDocumentCommand 命令1，允许用户使用新的接口定义 LaTeX 命令。其形式为 \NewDocumentCommand{&amp;lt;command&amp;gt;}{&amp;lt;parameter specificers&amp;gt;}{&amp;lt;replacement text&amp;gt;} 比如，以下两个定义，效果是一致的。 \usepackage{xparse} \newcommand{\baz}[1]{I eat #1{}.} \NewDocumentCommand{\bar}{m}{I eat #1{}.} 其中，参数标识符 m 表示 \bar 接收一个标准的 LaTeX 参数。除去 m 之外，xparse 宏包还提供了许多额外的参数标识符（具体参照其手册）。其中，g 表示该参数是一个可选参数，并且以花括号界定其范围。当参数未给出时，参数值为 -NoValue-；否则是实际的参数内容。此时我们可以用 \IfNoValueTF 命令来做分支判断。 于是，上述 \foo 命令可以按如下方式实现。 \documentclass{article} \usepackage{xparse} \NewDocumentCommand{\foo}{mg}{% \IfNoValueTF{#2}% {One parameter: #1{}.}% {Two parameters: #1{}, #2{}.}% } \begin{document} \foo{hello} \foo{hello}{world} \end{document} 这样的实现方式，相对在 LaTeX2e 里用 \@ifnextchar\bgroup 判断就简单清晰多了。 可以从这里学习一个关于 \NewDocumentCommand 命令的更有意思的示例。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[用 minted 宏包实现语法高亮]]></title>
    	<url>/tech/2017/07/20/minted/</url>
		<content type="text"><![CDATA[[listings 的麻烦 在 LaTeX 中实现语法高亮的老牌方案是 listings，然而这种方案用起来非常麻烦： 默认的字体效果很糟糕，居然是 Serif 系列的； 定义一种新语言需要先耗费大量时间了解帮助手册，还可能需要了解很多奇怪的规则； 代码跨页时需要一些额外的工作； 代码中的单引号，双引号问题； 中文支持问题1。 甚至对 LaTeX 的语法高亮支持也需要做许多额外工作。 当然，不是说 listings 包的功能不够强大，从《给语法高亮内容中的部分内容进行高亮标识》这个方案可以看到，利用 escape character 除了可以解决中文的问题外，还可以实现更多自定义的效果，比如《在代码中添加公式》。如果是英文文档，定义一个类似于 \verb|.| 的命令，实现 inline code 式的命令也很容易2。 因此，虽然 listings 包理论上可以满足我们的需求，但是要解决特定的问题往往需要耗费相当的精力和时间，对初学者并不算太友好。 minted 宏包需要的支持 LaTeX 中的 minted 宏包实际上是调用 Python 的 pygments 来完成语法高亮的，高亮效果好，支持的语法充分，可以实现代码内的公式3，通过 \mintinline 命令实现行内代码的同时4可以指定语言的名称5，需要的额外配置少，唯一目前要做就是在安装完 Python 后，通过下面的命令安装 pygments： pip install pygments 之后编译时用下面的命令： xelatex -shell-escape demo.tex 或 lualatex -shell-escape demo.tex 对于使用 WinEdt 或者 Sublime Text 界面接口的用户，可能选项 -shell-escape 会比较麻烦一些，但由于自己是直接调用 bat 文件来处理编译问题，因此这个选项完全不是问题。 现在 knitr 转成 pdf 的语法高亮应该也是由 pygments 完成的，并且完全不需要用户干预（不确定在 knitr 时 pygments 是否需要提前安装。）。 可能的麻烦 提醒：尝试在 CTeX 2.9.2 下面直接将最新版的 minted 相关文件复制到相应的目录，但结果出错，说找不到 fvextra 宏包，然而添加了该宏包之后仍然会有其它错误，估计是相关的依赖包没有更新的原因，因此推荐使用最新版的 TeXLive 或者将整个 CTeX 套装的宏包都进行更新来解决这个问题，但是在线更新 MikTeX 之后，再更新相关宏包时，同样会出现各种各样的奇怪问题，像 picins 这要的宏包甚至会认为是过时的而直接删除，导致其它很多麻烦，最终放弃更新 CTeX。 好像更新新的 ctex 宏包后，ctexsetup 命令也不再推荐使用。 简单例子 这个例子来自 https://zhuanlan.zhihu.com/p/27996164。 \documentclass[a4paper]{article} \usepackage{minted} \usepackage{xcolor} \definecolor{bg}{rgb}{0.95,0.95,0.95} \usepackage[margin=2.5cm]{geometry} \begin{document} \begin{minted}[bgcolor=bg]{rust} fn foo(v1: Vec&amp;lt;i32&amp;gt;, v2: Vec&amp;lt;i32&amp;gt;) -&amp;gt; (Vec&amp;lt;i32&amp;gt;, Vec&amp;lt;i32&amp;gt;, i32) { // Do stuff with `v1` and `v2`. // Hand back ownership, and the result of our function. (v1, v2, 42) } let v1 = vec![1, 2, 3]; let v2 = vec![1, 2, 3]; let (v1, v2, answer) = foo(v1, v2); \end{minted} \begin{minted}[bgcolor=bg]{go} import &amp;quot;math&amp;quot; type Shape interface { Area() float64 } type Square struct { // Note: no &amp;quot;implements&amp;quot; declaration side float64 } func (sq Square) Area() float64 { return sq.side * sq.side } type Circle struct { // No &amp;quot;implements&amp;quot; declaration here either radius float64 } func (c Circle) Area() float64 { return math.Pi * math.Pow(c.radius, 2) } \end{minted} \end{document} 下面是实际的高亮效果： 中文支持后期已经基本解决，参考 CTeX #76123 和 CTeX #77046。 ↩ 中文里面还有 bug，参见 GitHub #282 以及《语法高亮：在 lstinline 中给行内代码添加背景颜色 z》。 ↩ 参见帮助文档。 ↩ 可进一步参考 Stack Exchange #45756。 ↩ 参考 Stack Exchange #45756，这个功能需要 minted 2.0 以上版本，CTeX 2.9.2 套装默认的版本不行。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[在 RStudio 中使用 GitHub]]></title>
    	<url>/tech/2017/07/14/github-in-rstudio/</url>
		<content type="text"><![CDATA[[1 基本知识 1.1 查看配置 2 生成、配置密钥 2.1 生成密钥 2.2 配置密钥 2.3 测试连接 3 在 RStudio 中使用 GitHub 3.1 发布到 GitHub Pages 3.2 连接 GitHub 可能遇到的问题 1 基本知识 Git 共有三个级别的 config 文件，分别是 system、global 和 local。在当前环境中，分别对应 %GitPath%\mingw64\etc\gitconfig 文件、$home\.gitconfig 文件和 %RepoPath%\.git\config 文件。其中 %GitPath% 为 Git 的安装路径，%RepoPath% 为某仓库的本地路径。所以 system 配置整个系统只有一个，global 配置每个账户只有一个，而 local 配置和 git 仓库的数目相同，并且只有在仓库目录才能看到该配置。 1.1 查看配置 参考资料：http://www.jianshu.com/p/0ad3d88c51f4 由于之前已经成功安装 Git 并登陆到过 GitHub，因此查看当前配置时，最后两行会显示用户名及用户密码。 $ git config --list | grep user user.name=xxxxxx user.email=xxxxxx@xxxx.com 这里的 user.name 和 user.email 就是 Author 信息。此时，上面提到的三个级别都配置了 user 信息。当 git commit 时，Author 信息依次读取 local、global 和 system 的配置，如果找到则不再继续读取。其他配置的读取顺序也是如此。 GitHub 支持 https 和 ssh 协议连接。下面以较为安全的 ssh 方式作为示例进行说明。 2 生成、配置密钥 2.1 生成密钥 由于之前已经生成过密钥并成功登陆过 GitHub，这里暂时不考虑多账号切换登陆的做法，因此直接 cd ~/.ssh 切换到 SSH 所在目录，将其中的 id_rsa 和 id_rsa.pub 备份好后删除掉。 输入 git config --global user.name &amp;quot;haopen&amp;quot; 和 git config --global user.email haopeng.yn@gmail.com 切换到新的账号，还可以用 git config --global user.name 和 git config --global user.email 看账号是否添加成功。 $ git config --global user.name &amp;quot;haopen&amp;quot; $ git config --global user.email haopeng.yn@gmail.com $ git config --global user.name haopen $ git config --global user.email haopeng.yn@gmail.com $ git config --list ...... user.name=haopen user.email=haopeng.yn@gmail.com 接下来输入 ssh-keygen -t rsa -C &amp;quot;haopeng.yn@gmail.com&amp;quot; 生成新的密钥。根据这篇博文的例子，在下面生成密钥文件时一路回车，不指定任何文件名称，直接使用系统默认的名称，到最后一步时密钥生成成功！ $ ssh-keygen -t rsa -C &amp;quot;haopeng.yn@gmail.com&amp;quot; Generating public/private rsa key pair. Enter file in which to save the key (/c/Users/haopeng/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /c/Users/haopeng/.ssh/id_rsa. Your public key has been saved in /c/Users/haopeng/.ssh/id_rsa.pub. The key fingerprint is: SHA256:ZH9aXjek6jUT7DWIe7SBrlFGuy50SjseMTtg5HSZLJw haopeng.yn@gmail.com The key&amp;#39;s randomart image is: &#43;---[RSA 2048]----&#43; | | | . o o | | Eo= . . | | &#43;oo.. = &#43; | | &#43;So.*oB.&#43;.| | . .oB**.=.o| | o===.B | | =B o o | | .ooo | &#43;----[SHA256]-----&#43; 2.2 配置密钥 复制前一步中生成的 id_rsa.pub 中的内容，之后登陆到 GitHub，在右上角的头像上点击 Settings - SSH and GPG keys，点击右边的 New SSH Key，然后将复制好的内容粘贴进去，标题自己随意取一个，比如 haopen&#39;s key，这样就完成了远程端的配置。 2.3 测试连接 问题：在 Git Bash Shell 中输入如下命令后连接失败！ ssh -T git@github.com Warning: Permanently added the RSA host key for IP address ‘192.30.255.112’ to the list of known hosts. Permission denied (publickey). 在网上查阅资料后，感觉有类似经历的人的解决方法似乎都与自己面临的情况不完全吻合，无意中看到 ~/.ssh 目录中有一个名称为 known_hosts 的文件，尝试将其删除后再重新进行了前面的第 1 步和第 2 步，这次再测试时就成功了。 $ ssh -T git@github.com The authenticity of host &amp;#39;github.com (192.30.255.113)&amp;#39; can&amp;#39;t be established. RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &amp;#39;github.com,192.30.255.113&amp;#39; (RSA) to the list of known hosts. Hi haopen! You&amp;#39;ve successfully authenticated, but GitHub does not provide shell access. 3 在 RStudio 中使用 GitHub 到 GitHub 的网站上新添加一个代码仓库（包含 readme.md），然后打开 RStudio，File - New Project - Version Control - Git，输入 Repository URL（要输入的内容可以在 GitHub 代码仓库右方点 Clone or download 找到），其中 Project directory name 用于设定项目所在文件夹的名称，Create project as subdirectory of 用于指定该文件夹所属的父目录，此外勾选 Open in new session，接下来 RStudio 会将该仓库中的内容 Pull 到本地。 创建一个 Git Repository：Step 1 创建一个 Git Repository：Step 2 创建一个 Git Repository：Step 3 成功之后显示如下内容： C:/Program Files/.../git.exe clone --progress git@github.com:haopen/Blogdown.git Blogdown 尝试添加文件并保存到，点击右边 Git 面板中的 Commit（直接在 Git 面板中某个文件上点右键，选择 ignore 可以指定应忽略跳过的文件），可以在打开的窗口中勾选需要 Commit 的文件，之后在右半边窗口中填写简单的描述文字，之后可以点击下方的 Commit。 提交一个 Commit 问题：Commit 完成之后，刚开始右上角并没有 Pull 向上箭头，并且尝试提交时会出现 unable to access... The requested URL returned error: 403 类的错误，根据这个讨论，在本地目录中找到隐藏的 .git/config，将其中的 url = https://haopen@github.com/derekerdmann/lunch_call.git 修改成 url = ssh://git@github.com/haopen/testGit.git 之后再次提交成功，并且上图右上角出现 Pull 向上箭头。当然，更好的做法是在点 Clone or download 时，直接点击 Use SSH，然后复制 git@github.com:haopen/testGit.git 即可。 3.1 发布到 GitHub Pages 先到 GitHub 创建一个名称为 haopen.github.io 的代码仓库； 之后 Clone 到本地1：参考前面在 RStudio 中 Clone Repository 的步骤，也可以先进入到 D:\GitHub，再点击右键，选择 Git GUI Here，在打开的窗口中选择 Clone Existing Repository，参考下图填写内容即可： Git 客户端 RStudio 中的 Git 客户端 其中 haopen-public 是在 Hugo 中的 .Rprofile 中指定的内容发布目录，对应的绝对地址是 D:\GitHub\haopen-public。 做一个 Commit，之后再 Push，等 Push 完成就可以在 https://haopen.github.io/ 观察到发布后的页面效果。 3.2 连接 GitHub 可能遇到的问题 问题：Push 的时候可能会遇到如下的错误： Git push origin ssh: Could not resolve hostname ssh.github.com: Name or service not known fatal: Could not read from remote repository. 参考 http://blog.csdn.net/qq_17335153/article/details/51701700 的说明，打开 ~/.ssh，在里面新建一个名称为 config 的文件，注意没有扩展名，在其中添加内容： Host github.com User haopeng.yn@gmail.com Hostname ssh.github.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa Port 443 但是通过 ssh -T git@github.com 测试仍然报错： ssh: Could not resolve hostname github.com: Name or service not known fatal: The remote end hung up unexpectedly 参考 http://blog.sina.com.cn/s/blog_a3fb1fe30102wieq.html 先通过 ping github.com 获得 ip 地址 192.30.255.113，之后在 /etc/hosts 中添加如下内容2： 192.30.255.113 github.com 再测试时就没有问题3： $ ssh -T git@github.com The authenticity of host &amp;#39;[ssh.github.com]:443 ([192.30.253.123]:443)&amp;#39; can&amp;#39;t be established. RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &amp;#39;[ssh.github.com]:443,[192.30.253.123]:443&amp;#39; (RSA) to the list of known hosts. Hi haopen! You&amp;#39;ve successfully authenticated, but GitHub does not provide shell access. 技巧：在 RStudio 第一次准备将本地大量文件 Commit 时，右侧 Git 面板中的 Staged 对应的 Checkbox 经常点不动，参考 https://pages.github.com/ 后此时可以考虑从齿轮那里打开 RStudio 带的 Git shell，然后在里面输入4： git add --all ~$git commit -m &amp;quot;Initial commit&amp;quot; ~$git push -u origin master 这种命令行的方式此时可能会更加方便一些。 可以考虑使用直接使用 Git GUI Here 客户端，也可以考虑使用 RStudio 提供的界面，通常后者可能会更人性化一些。↩ 修改 C:\Windows\System32\drivers\etc\hosts 这个文件时，经常提示有其它程序占用，因此转而修改 C:\Program Files\Git\etc\hosts，效果也是一样的。↩ 但连接速度真的很慢。↩ 要在这个自带的 Shell 窗口中粘贴内容，只能在窗口的标题栏点击右击，再选择“编辑”，之后选择“粘贴”。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[添加 lightGallery 功能到 Hugo]]></title>
    	<url>/tech/2017/07/10/hugo-light-gallery/</url>
		<content type="text"><![CDATA[[Hexo 中用的 jQuery FancyBox 本来效果也不错，但是在移植这部分功能的时候并不顺利，又在搜索解决方案的时候看到了 lightGallery，配置和使用感觉都非常好 lightGallery 的图片浏览功能移植过来。 下载相关文件并复制到恰当的目录； 在模板文件中的适当位置，主要是 header 相关部分添加样式文件，在 footer 相关部分添加 js 相关的文件； 一个比较麻烦的地方是，这个插件要求全部图片按比较规范的格式放到一个有 id 的 html 标记容器之中，思考尝试再三，最后通过 lgGallery_Prepare.js 文件大致达到了相应的目的； 主要思路是先通过 js 遍历页面中的全部 img 元素，提取 src 属性并在一个隐藏的 div 中按 lightGallery 要求的格式再动态生成一遍。之后再通过 js 给原有的每张图片添加一个 onclick 属性，用于监听点击事件并通过 js 模拟点击隐藏 div 中对应的图片文件，最后由 lightGallery 监听到隐藏 div 中的点击事件并最终达到浏览图片的目的； 不管是用 setAttribute 还是用 createAttributes 创建属性，最后除了 onclick 属性被传递出去外，其它的自定义名称的属性都不成功； 要想在后面的代码中通过 document.getElementById() 的方式获取，就不能用 innerHTML 的方式，而要用 createElement 的方式； 没有通过 addEventListener 给对象添加匿名函数的方式来处理，因为这时还没有完成 div 中内容的生成，这会导致后续的监听处理程序找不到动态生成的对象； 对 onclick 属性，同时传递自身与事件的代码为 javascript: lg_click(event, this);，注意这里的 event 和 this 的顺序不能反，否则后面 lg_click() 中引用时会找不到对象，因此基本可以认为是固定用法。当然，只传递 event 或者 this 在语法上也是可行的； 在 lg_click() 中要记得禁止点击事件向上冒泡； 最后，完整的配置文档可以从这里找到。根据这个示例配置文件以及官方示例文件的说明，其实可以用 js 生成一个指定格式的数据对象，最后交由 lightGallery 来使用，但由于监听事件是与某个具体 id 的容器联系的，因此这种动态的方法与本站需要的点击某张图片打开浏览窗口的需求并不一致。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[添加本地搜索功能到 Hugo]]></title>
    	<url>/tech/2017/07/10/hugo-local-search/</url>
		<content type="text"><![CDATA[[Yihui 修改过的主题并不提供本地搜索功能，但 Hexo 的 NexT 主题却有一个插件（JS、html、css）可以实现该功能，可以考虑将这个功能移植过来。 模板文件 按 Hugo 的规模将 js, html, css 文件复制到对应的目录，并在模板文件中添加对应的引用； 修改 css 的定义，实现与现有主题的风格搭配； js 文件中找到关键的三处配置，一是 .xml 数据文件的路径，二是每条 Post 中匹配关键词的文本片断的数量，三是搜索的触发方式是 onkeypress 还是 enter 式，对应于 auto 和 manual 两个选择； $(&#39;.popup&#39;).detach().appendTo(&#39;...&#39;) 这里的 appendTo 实际上是随意指定的，要更换成本主题的某个 html 标记容器，这个地方的错误查了将近 7 个小时； 搜索引擎 js 文件可以异步加载，但是 jQuery.min.js 文件却最好直接加载，以避免引起不必要的麻烦； 数据文件 xml 数据文件在 Hugo 中的生成有些特殊，可按下面的步骤进行： 添加如下代码到 \contents\search-index.md 文件： --- date: &amp;quot;2017-03-05T21:10:52&#43;01:00&amp;quot; type: &amp;quot;search&amp;quot; url: &amp;quot;search.xml&amp;quot; --- 添加模板文件到 /layouts/search/single.html。这个文件中的 search 路径由第 1 点中的 type 取舍指定，而 single.html 是 Hugo 的模板优先级规则确定的； 说明：根据模板文件生成 xml 文件在 Hugo 中并不方便，原因是 Hugo 会将模板文件中的 &amp;lt; 强制转换成 &amp;amp;lt;，而 https://github.com/gohugoio/hugo/issues/1740 提供的 hack 手段比较好的解决了这个问题，具体的实现当然还需要根据实际情况做相应调整，可以参见这个模板文件的内容： {{ `&amp;lt;?xml` | safeHTML }} version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot; ?&amp;gt; &amp;lt;!-- Hugo 在转换模板时，&amp;lt; 这个符号总是被强制转换为 &amp;amp;lt; --&amp;gt; &amp;lt;!-- 根据 https://github.com/gohugoio/hugo/issues/1740 使用如下的 Hack 可以解决 --&amp;gt; &amp;lt;!-- part1 和 part2 的内容在 &amp;quot;/cn/&amp;quot; 这样的情形中，前者是 cn，后者是 /cn/ --&amp;gt; &amp;lt;!-- 第一个 if：是不是只有一级路径 /cn/ --&amp;gt; &amp;lt;!-- 第二个 if：是不是特殊的二级路径 --&amp;gt; &amp;lt;!-- 第三个 if：是不是特殊的一级路径 --&amp;gt; &amp;lt;!-- 第四个 if：search.xml 和 主页 --&amp;gt; &amp;lt;search&amp;gt; {{ range $index, $page := .Site.Pages }} {{ $.Scratch.Set &amp;quot;Part1&amp;quot; (replaceRE &amp;quot;^/([^/]&#43;)/.*&amp;quot; &amp;quot;$1&amp;quot; $page.RelPermalink) }} {{ $.Scratch.Set &amp;quot;Part2&amp;quot; (replaceRE &amp;quot;^/([^/]&#43;)/([^/]&#43;)/.*&amp;quot; &amp;quot;$2&amp;quot; $page.RelPermalink) }} {{ $.Scratch.Set &amp;quot;Part2&amp;quot; (replace Part2 &amp;quot;/&amp;quot; &amp;quot;&amp;quot; ) }} &amp;lt;!-- 上面两行在真实代码中被合并成了一行 --&amp;gt; {{ if not (eq ($.Scratch.Get &amp;quot;Part1&amp;quot;) ($.Scratch.Get &amp;quot;Part2&amp;quot;)) }} {{ if not (in (split &amp;quot;tags,categories,vitae,about&amp;quot; &amp;quot;,&amp;quot;) ($.Scratch.Get &amp;quot;Part2&amp;quot;)) }} {{ if not (in (split &amp;quot;tags,categories&amp;quot; &amp;quot;,&amp;quot;) ($.Scratch.Get &amp;quot;Part1&amp;quot;)) }} {{ if not (or (eq $page.Title &amp;quot;&amp;quot;) (eq $page.RelPermalink &amp;quot;/&amp;quot;)) }} &amp;lt;entry&amp;gt; {{ `&amp;lt;title&amp;gt;&amp;lt;!` | safeHTML }}[CDATA[{{ $page.Title }}]]&amp;gt;&amp;lt;/title&amp;gt; &amp;lt;url&amp;gt;{{ $page.RelPermalink }}&amp;lt;/url&amp;gt; {{ `&amp;lt;content type=&amp;quot;text&amp;quot;&amp;gt;&amp;lt;!` |safeHTML }}[CDATA[{{ $page.PlainWords }}]]&amp;gt;&amp;lt;/content&amp;gt; &amp;lt;/entry&amp;gt; {{ end }}{{ end }}{{ end }}{{ end }}{{ end }} &amp;lt;/search&amp;gt;]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[添加标签云页面到 Hugo]]></title>
    	<url>/tech/2017/07/10/tags-cloud/</url>
		<content type="text"><![CDATA[[Yihui 修改过的主题并不提供分类、标签云这样的分类功能，因此需要自己想办法解决。 由于标签和分类在 Hugo 都属于 Taxonomies 的一种实现，因此两种类别的内容的 Render 都由 list.html 来实现，要使用标签云就需要再单独定义与标签云匹配的模板文件（分为主站模板与 Section 下的模板）； 整个站点的标签云由 /layouts/taxonomy/tag.terms.html 负责，按这种目录和文件名结构，Huxo 就能自动根据该文件中的信息生成 tags/index.html； 调整 /js/tags.js 中的 begin 和 end，用于设定最小字体、最大字体以及每种标签对应的显示颜色； 然而仅仅这样做还不够，因为还还需要生成每个 Section 下的标签云页面：在 /contents/ 创建 section-arts.md 文件，并在其中指定 type 和 layout 信息，并且 slug 将文件映射到指定的路径，之后在 /layouts/customterms/tagslist.html 中指定文件的渲染模板，注意这里的 customterms 与前面在 /contents/ 中的 tags-*.md 对应； 由于 Hugo 目前无法获取指定 Section 下的 Categories 和 Tags，因此只好用了一些 Hack 的手法，给全部 Post 的 tags 和 categories 信息添加了一个特定的前缀编号（如 4-tagName），之后模板文件利用这些信息再配合 config.yaml 中的信息筛选出当前 Section 下的全部 tags； 一些特殊的代码用于保证在标题中显示的 Section 信息以及 tag 的名称信息是正确的； Categories 列表的处理这标签基本类似，只是站点下的分类页面使用了默认的 list.html 文件，而 Section 下的分类页面使用了 /layouts/customterms/categorieslist.html 文件；]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Hugo 模板优先级解读]]></title>
    	<url>/tech/2017/07/08/hugo-template-priority/</url>
		<content type="text"><![CDATA[[功能解读 \layouts\ 用于放置模板文件，并且文件在该目录中的名称、文件层次决定了模板应用的优先级。 \archetypes\ 是内容文件 .md 的模板； https://gohugo.i https://gohugo.io/templates/variables/：模板可操作的变量和对象列表； $section 是通过赋值得到的一个可使用的页面级变量，而 .Section 是一个 Hugo 自带的变量； 自定义功能实现 目前来说 list.html 可以遍历全部的 Section，但 Taxonomy Terms 只有一个 tags 目录和一个 categories 目录，并不能遍历全部的 Section。 https://discourse.gohugo.io/t/create-section-taxonomies/343 想为每个 Section 都创建 tags 和 categories 页面，但后续的讨论应该是没有成功，个人觉得如果能够返回某个 Section 下的所有 tags 对象和 categories 对象时，这个功能才有可能实现。 根据 https://discourse.gohugo.io/t/create-page-with-type-or-layout-template-set-in-frontmatter/5265 和 https://github.com/gohugoio/hugo/issues/386，虽然可以通过给单独文件定义 type 和 layout 来做一些事情，但由于无法返回指定 Section 下的 Taxonomy Terms 集合，因此为每个 Section 都创建 tags 和 categories 页面的想法仍然无法实现。 文章列表视图 根据 https://gohugo.io/templates/views/，除了 single 类型的模板外，每个 Section 都可以利用 list templates 生成各种不同的视图，https://gohugo.io/templates/homepage/ 中给出了一个使用摘要视图的例子： {{ range first 10 .Data.Pages }} {{ .Render &amp;quot;summary&amp;quot;}} {{ end }} 其中的 summary 还可以是 li 类型，相关的文件目录结构为： ▾ layouts/ ▾ arts/ li.html single.html summary.html ▾ tech/ li.html single.html summary.html 其中 li.html 和 summary.html 实际上在 \layouts\_default\list.html 这样的 list templates 类模板文件中使用，起到循环的作用。list.html 以及这两种元视图文件的示例参考 https://gohugo.io/templates/views/。 tags 页面 经试验，\contents\tags\ 目录（及子目录）下使用 _index.md 不是使用 terms.html 相关的模板（比如 terms.html 或者 taxonomy/tag.terms.html），而是使用了 list.html 作为模板，换句话讲，要想让不同的 Section 有自己的标签云页面，在 \contents\ 下创建如下目录结构的想法行不通： └── content ├── tags | ├── _index.md | ├── tech | | └── _index.md | └── arts └── tech ├── first.md └── second.md 模板部件 页面类型 变量一节中 Page variables 下的 .Kind 指出，页面的类型有：page, home, section, taxonomy, taxonomyTerm, RSS, sitemap, robotsTXT 以及 404，但是附带的说明还没有看懂1。 Home Page 根据 https://gohugo.io/templates/homepage/，Home page 的模板优先级如下： /layouts/index.html /layouts/_default/list.html /layouts/_default/single.html 在 Home page 的模板中，可以访问全部 page variables 和 site variables 中的变量，除此之外，Home page 可以通过 .Data.Pages 访问全部页面对象，具体方法可以参考 Lists Template 部分的介绍。 single 根据 https://gohugo.io/templates/content/，每一篇 post，甚至全部的 _index.md 都可以视为一个单页（但是 list 和 terms.list 类的页面不是）的模板优先级如下： /layouts/TYPE/LAYOUT.html /layouts/SECTION/LAYOUT.html /layouts/TYPE/single.html /layouts/SECTION/single.html /layouts/_default/single.html 其中的 TYPE 和 LAYOUT 可以在单个的 post 之中，以 type 和 layout 属性对的形式出现在 front-matter 之中；SECTION 则是依据 md 文件在 \contents\ 中对应的文件夹而确定。 but these will only available during rendering of that kind of page, and not available in any of the Pages collections. ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Hugo 博客平台应用笔记]]></title>
    	<url>/tech/2017/07/09/hugo/</url>
		<content type="text"><![CDATA[[1 使用心得 2 安装与运行 2.1 RStudio 安装 2.2 Hugo 安装 2.3 博客搭建 2.3.1 方法 1 2.3.2 方法 2 2.3.3 方法 3 2.3.4 方法 4（推荐） 3 站点配置与功能订制 3.1 站点配置 3.2 功能订制 3.3 待实现 4 主 4.1 页面宽度 4.2 字体 5 功能解读 1 使用心得 在 yml 头中加入 subtitle: A subtitle 可以为 post 添加一个副标题； 这里是一个 &amp;lt;kbd&amp;gt;...&amp;lt;/kbd&amp;gt; 的例子：Ctrl； slug: &amp;quot;cn/about&amp;quot; 中的 slug 是 Hugo 特有的一种机制，这里对应的文件实际上只是 \content\cn_about.md，但最终这个文件在 URL 中显示的却是 /cn/about/，所以 Slug 可以同时设定所属的博客子分类以及在 URL 中的文件名称，相当于强制重定向到了某一指定的路径。 要实现 $...$ 这样的效果，需要使用 &amp;lt;code&amp;gt; 标记，具体参考此处的源代码； 用 R Markdown 时，生成的 toc 如果是全中文的条目，可能链接的下划线无法用 css hack 很好的消除，但给对应的标题一个 {#idName} 时，这个问题自动消失，详细参考本文档上方的目录； R Markdown 的说明文档中说在 R Markdown 中不能直接使用 Shortcode，而要用下面的语法1： ```{r echo=FALSE} blogdown::shortcode(&amp;#39;tweet&amp;#39;, &amp;#39;852205086956818432&amp;#39;) ``` 在 md 格式下输入以下内容可以自动将最后一行转换成右对齐的格式（普通的 Rmd 下 knitr 时不支持，但 Blogdown 下的 Rmd 可以）： # Quotes &amp;gt; Let us change our traditional attitude to the construction of programs: &amp;gt; Instead of imagining that our main task is to instruct a computer what to do, &amp;gt; let us concentrate rather on explaining to humans what we want the computer to do. &amp;gt; --- Donald E. Knuth, Literate Programming, 1984 提示：下面的内容，没有改模板的需求时，基本不用理会。 2 安装与运行 2.1 RStudio 安装 暂时不能直接用官方网站的正式版安装，而要用 https://dailies.rstudio.com/ 提供的开发测试版，否则无法直接通过 RStudio 的新建 Project 向导创建一个 Hugo 的博客站点。 2.2 Hugo 安装 Hugo 基于 Go 语言编写，与 Jekyll 和 Hexo 相比，虽然 Hugo 目前主题和插件还不如 Hexo 丰富，但它运行速度极快，并且安装非常容易，用户只需要下载主程序 Hugo.exe 并复制到指定目录即可2： C:\Users\Howard\AppData\Roaming\Hugo 用户可以参考官方文档（中文）进一步了解如何使用 Hugo，还可以到 GitHub 了解存在的问题以及目前的开发进度，到 Community 与其他用户讨论使用过程中遇到的各种问题。 2.3 博客搭建 2.3.1 方法 1 首先，切换到放置博客文件的目录，比如 D:\GitHub，接下来右键并选择 GUI Bash Here，之后在弹出的命令对话框中输入如下命令： git clone --recursive https://github.com/yihui/blogdown-yihui-template.git 这个命令从谢益辉的 Github 上提取建立博客需要的全部文件到本地 D:\GitHub\blogdown-yihui-template\ 目录之中，这是谢根据 hugo-lithium-theme 主题修改后又从自已的站点抽取出来的一个框架，这个目录名称可以自行修改，比如修改成 blogdown_Haopeng，觉得麻烦的话，也可以参考这里通过 Git 的图形客户端完成整个过程。 其次，打开 RStudio，输入如下命令： setwd(&amp;#39;D:/GitHub/blogdown_Haopeng&amp;#39;) blogdown::serve_site() 当控制台显示 Serving the directory D:\GitHub\blogdown_Haopeng at http://127.0.0.1:4321 类似的文字时，就可以在浏览器中输入 http://127.0.0.1:4321 进行预览，因为具有 Hugo 实时预览功能，因此对目录中相关文件进行修改后，甚至不需要刷新浏览器就可以看到更新后的结果。 2.3.2 方法 2 如果没有安装 git 客户端，也可以跳过上面的两个步骤，直接在 RStudio 中输入如下命令： setwd(&amp;#39;D:/GitHub/blogdown_Haopeng&amp;#39;) blogdown::new_site(theme = &amp;#39;yihui/hugo-lithium-theme&amp;#39;) blogdown::serve_site() 这样生成的博客站点和上面的非常类似，但细节和相关功能上和上面的那个代码仓库相比，可能会有比较多的差别。 2.3.3 方法 3 先复制一份打包好的 blogdown_Haopeng 目录到指定位置，然后参考方法 1 第二步进行操作。 2.3.4 方法 4（推荐） 2017.07.14 所有一切怪异的源头来自 R/*.*，这里面的文件配合站点下的自定义的 .Rprofile 用于自定义整个站点的渲染过程，相当于一个自定义版本的 blogdown::servesite()，虽然现在已经弄清楚中间发生了什么。这种自定义的方式其实是先将 Rmd 文件通过 knitr 转换成 md 文件，之后再交由 Hugo 的 Markdown 引擎 Blackfriday 来处理，对于一般的文档，这种自定义的方法的最大好处是可以控制更多细节，比较 Rmd 文件中绘制图形的保存路径等，但最大的问题是： Blackfriday 处理数学公式的能力远不如 Pandoc； 也无法做到为各级标题自动添加编号； 不能自动给源文档中不连续的有序列表添加连续的编号； 还有一个比较难受的点是，生成的临时 md 文件与原始的 md 混在一起，非常难区分； 因此最终还是选择用这里的第 4 种方法：先参考第 2 种方法下载最精简版本的文件，之后再从第 1 种方法下载的文件中提取需要用到的各种文件，再整合到精简版的内容之中。 实际操作过程中，在模板整合完成并调试结束后，可以把其中的内容全部复制出来，再参考这里将提取出来的内容放到一个 GitHub 的本地代码仓库之中，之后再 Push 到远程仓库。 方法 1 和 方法 3 的思路会遇到一个比较奇怪的事情，在 https://github.com/rstudio/blogdown/issues/140 反映了此问题：就是在 Rmd 中无法成功通过如下的设置来生成文章内的目录，并给每个小标题添加编号， output: blogdown::html_page: toc: true number_sections: true 根据试验的结果来看，直接在 RStudio 中按 https://bookdown.org/yihui/blogdown/rstudio-ide.html 给出的说明： 点击 File -&amp;gt; New Project -&amp;gt; New Directory，然后按照下面两张图的设置进行站点初始化： Create a new website project in RStudio. Create a website project based on blogdown. 将 yihui/blogdown-yihui-template.git 下面的 \layouts\* 和 \static\* 复制到 Blogdown 中； 复制 config.yaml 并删除默认的 config.toml； 在 Rmd 文件头中添加上面给出的 output 设置； 不复制 blogdown-yihui-template\R 中的 build_one.R 和 build.R； 在 RStudio 中运行 blogdown::serve_site()。 这样又可以成功得到 Pandoc 风格的 toc 和 编号后的标题。但目前不清楚为什么用方法 1 和方法 3 不能成功，也不确定方法 2 和这里的方法 4 的完整区别（至少方法 4 中的指定 Create project as subdirectory of 是方法 2 默认没有的）。 注意：如 https://bookdown.org/yihui/blogdown/output-format.html 最下方指出的，方法 4 可能会导致语法高亮等失效，可以通过 highlight.js 的方法来修正，在实际运行过程中，IE 下在线加载该语法高亮工具的 CSS 文件可能会比较慢，而在 Chrome 下这个过程很迅速。另外，这个页面中说使用 R Markdown 时，公式需要通过 `$...$` 才可以正常工作，但实际上模板已经可以直接用 $...$ 来完成公式的输入。 https://support.rbind.io/2017/04/25/yihui-website/ 是谢益辉对自己网站模板的比较详细的思路介绍，值得多读一下，了解更多的细节。 3 站点配置与功能订制 3.1 站点配置 站点配置主要在 config.yaml 文件中进行，直接参考谢益辉的 config.yaml 文件进行修改即可，注意目前暂时不添加 googleAnalytics 相关的信息，因此这一部分的取值为空。 说明：Hugo 的配置包含两部分，一部分位于主题目录之中，比如 themes\hugo-lithium-theme\ 下的 static\ 或 layout\，另一部分位于站点根目录之中，比如 \layout\ 或 \static\，其中后者的优先级要高于前者。在后续的内容中，涉及到相关目录时，如果没有提及“主题中的”相关字眼，通常指的是站点根目录中的相关目录或文件。 3.2 功能订制 功能订制主要是在谢益辉已提供功能的基础上进行修正和添加，首先需要从谢益辉的 \static\ 和 \layout\ 中复制全部文件到博客的根目录中，接下来3， 公式：默认的模板公式做的不够好，因此参考 Hexo 的模板加入了公式编号功能，在 /layouts/paritals/footer_mathjax.html 加入了下面的内容： &amp;lt;script type=&amp;quot;text/x-mathjax-config&amp;quot;&amp;gt; MathJax.Hub.Config({ TeX: {equationNumbers: {autoNumber: [&amp;quot;AMS&amp;quot;], useLabelIds: true}}, &amp;quot;HTML-CSS&amp;quot;: {linebreaks: {automatic: true}}, SVG: {linebreaks: {automatic: true}} }); &amp;lt;/script&amp;gt; logo：在 \static\images\ 中放置已经做好的头像文件，如果觉得不满意，可以在 Photoshop 中参考已经做好的头像文件自行调整； disqus：将 \layouts\partials\disqus.html 文件中的： var disqus_js = &amp;#39;//{{ .Site.DisqusShortname }}.disqus.com/embed.js&amp;#39;; 替换成： var disqus_js = &amp;#39;https://{{ .Site.DisqusShortname }}.disqus.com/embed.js&amp;#39;; OK：关于异步加载字体功能，将 \static\js\load-typekit.js 文件中的 kitId: &#39;kwz5xar&#39; 修改成 kitId: &#39;oqz0fck&#39;，这里的 kitId 需要注册获得4，目前看来，似乎之前这个账号在新的地址中也可以继续使用； 从 \layouts\partials\head_custom.html 可以看出： OK：FontAwesome 相关的字体也是异步在线加载的，现在已迁移到本地，否则 Ad block 有可能产生不好的影响，导致图标无法显示； OK：只有 cn/ 目录下的文章会异步加载思源宋体，已修改，参考 head_custom.html； OK：谢益辉设定 2017 年以前的文章会异步加载 pangu 插件，这里将 {{ if lt (.Date.Year) 2017 }} 修改成 {{ if lt (.Date.Year) 2005 }}，相当于不再加载该插件； OK：时间格式正文页中的时间格式显示比较怪，在没有了解 Hugo 的语法前，将 \layouts\partials\article_meta.html 文件中的 {{ .Date.Format (default &amp;quot;January 2, 2006&amp;quot; .Site.Params.dateFormat ) }} 修改成 {{ .Date.Format &amp;quot;2006/01/02&amp;quot; }}； OK：GitHub Edit 在 Windows 上，文件路径中使用 \，而 \layouts\partials\article_meta.html 中没有考虑到这一点，导致生成的链接中包含 %5c 字样，这正是 \ 对应的 html 转义码，因此将文件中的{{ $.Scratch.Set &amp;quot;filePath&amp;quot; $.File.Path }} 修改成 {{ $.Scratch.Set &amp;quot;filePath&amp;quot; (replace $.File.Path &amp;quot;\\&amp;quot; &amp;quot;/&amp;quot;) }}，注意不是替换 %5c，而是 \\； OK：关于页面标题居中通过修改 \layouts\partials\article_meta.html 将“关于”页面的标题剧中，具体的，需要将如下代码 &amp;lt;h1 class=&amp;quot;article-title&amp;quot;&amp;gt;{{ .Title }} 修改为： {{if findRE &amp;quot;-about$&amp;quot; $.File.BaseFileName }} &amp;lt;h1 class=&amp;quot;article-title&amp;quot; style=&amp;quot;text-align: center;&amp;quot;&amp;gt;{{ .Title }} {{ else }} &amp;lt;h1 class=&amp;quot;article-title&amp;quot;&amp;gt;{{ .Title }} {{ end }} 3.3 待实现 已解决：R Markdown 下面设定超链接在新窗口中打开，Plain Markdown 中在 _config.yaml 的 blackfriday 设置即可； 已解决：标签和分类相关的问题： 标签和分类中的空格会被自动忽略，怎么禁止(做不到，但可以在生成页面时再用 Hugo 的指令替换回来)； 标签和分类为中文时，Blogdown 对应的当前分类（标签）条目汇总页无法显示，但是 https://blog.coderzh.com/categories/读书笔记/ 是正常工作的，这说明有可能是 blogdown 或者模板的问题(已得到 Yihui 修正，是 Servr 包的问题)； 另一个问题是当前分类（标签）汇总页的标题不能是 .Title，否则取值为 年份，这里由于 tags 和 categories 都位于站点根目录下，因此唯一的可能是获取用户进到本页面之前的前一个页面对应的 section，然后根据该 section 展示一个与 section 有关的标签页或分类页，然而这样做的基础是可以在不同的 section 下面生成各自的 tags 和 categories 页(Terms 页本来也不怎么需要分布，而 Section 的 list pages 分页并不受影响)； 已解决：标签云功能的实现，默认的分类和标签都是同一类对象，而现在需要将两类对象分离出来，查看 https://github.com/kakawait/hugo-tranquilpeak-theme 中 kakawait-tranquilpeak 的主题效果时，发现里面有一个 \layouts\taxonomy 目录，其中包含了 archive.terms.html, category.html, category.terms.html, tag.html, tag.terms.html，再根据 https://gohugo.io/templates/terms/ 的文档说明，显然存档页、分类页、标签页都可以分开定义模板，其中 tag.terms.html 是标签汇总页（标签云），那么 tag.html 就应该是某个标签下的 post 的展示页，暂时不清楚 archive.terms.html 的作用。在 \layouts\taxonomy 中添加了一个自定义的 tag.terms.html，之后就可以生成标签汇总页。具体生成标签汇总页时，发现 https://www.josephearl.co.uk/tags 的标签云效果很不错，于是： (1) 到 https://github.com/josephearl/website 下载了对应的源代码（主题的其它功能可以到 https://github.com/nodejh/hugo-theme-cactus-plus 查阅），提取其中的 themes\cactus-plus\static\js\js.tags 到自己博客的 \static\js\，(2) 参考这个模板中的\section.html文件的内容，同时参考 &amp;lt;https://github.com/kakawait/hugo-tranquilpeak-theme&amp;gt; 的目录结构，在中添加了一个自定义的tag.terms.html，(3) 修改tags.js中的var tagcloudOptions={size:{start:12,end:30,unit:“pt”},color:{start:“#bbbbbb”,end:“#2dbe60”}}，将其中的两个start,end` 修改成现在的取值，用于指定标签云字体的大小和颜色。 https://www.josephearl.co.uk/tags 是一个非常好的标签云的效果； 已解决：标签：正文中的标签，标签汇总页（已解决），单个标签对应文章列表（分页，文章多时再解决，这是 Hugo 自动做的事情，完全不是问题）； 已解决：分类：正文中显示所属分类，分类目录页（分页），单个分类中的文章列表（分页，文章多时再解决，这是 Hugo 自动做的事情，完全不是问题）； https://tranquilpeak.kakawait.com/categories/：这个分类页面显示了标签，还显示了全部 post，不同分类下的 post，值得参考； 已解决：除了 EN 下外，其它地方的 tags list/paper list 的标题都用中文，这些地方的标题都居中(技巧是给每个分类以及标签一个 Section 代码前缀)； 已解决：能不能每个 section 中包含一个 tags 和 categories 页面？ 已解决：存档：按年份倒序的存档页（分页），根据 https://gohugo.io/templates/terms/，可以用 len .Data.Terms 和 len .Data.Pages 得到标签数和页数； 已解决：Hexo 中的 &amp;lt;ol&amp;gt; 标记默认可以添加一个 start 属性，但是本主题中还不支持，用 R Markdown 可以通过 Pandoc 的能力来实现； 已解决：FontAwesome 相关的字体也是异步在线加载的，可以考虑将来迁移到本地； 已解决：只有 cn/ 目录下的文章会异步加载思源宋体，需要修改； 已解决：页面摘要：用 &amp;lt;!--more--&amp;gt; 生成的目录页和存档页不同，这种类型的页面中包含一部分正文内容，并且需要支持分页，对应于模板的 .Summary 和 .li 视图，具体参见这里； 已解决：搜索功能（实现过程参考这里）： 可以用 lunr.js 或者其增强版 elasticlunr.js 来实现，其中 lunr.js 的比较好的主题示例可以参考 DocDock； 另一种方案是 Hexo 的 NexT 主题使用的 instantsearch.js，https://community.algolia.com/instantsearch.js/v2/getting-started.html 有一个比较详细的使用步骤介绍，https://codepen.io/Algolia/pen/ZOyoJg 是一个非常好的演示例子。 https://www.josephearl.co.uk/ 的在线搜索感觉非常轻量，也还不错，一些小细节的 CSS 问题还可以参考 https://www.josephearl.co.uk/post/css-overlay/。这个博客用的是 https://github.com/nodejh/hugo-theme-cactus-plus，从 https://github.com/josephearl/website 下载的源代码中的 \themes\cactus-plus\static\js 目录中可以发现，同时有 search.js 和 lunr.js，因此感觉应该是一个 lunr.js 的包装，不过这个站点的搜索好像只针对标题和标签，虽然快但是没有全文搜索的能力； Hexo 的 NexT 主题实际上本人现在用的是一种本地搜索的实现方式，themes\next\layout\_third-party\search 中的 localsearch.swig 以及 next\source\css\_common\components\third-party 中的 localsearch.styl 是两个最为重要的文件，还有一个 next\layout\_partials\search 中的 localsearch.swig 可以参考，这是目前最满意的方案，可惜需要用到 db.json，这个文件目前还没有找到好的方案来生成，但感觉上与 Hugo 的 index.xml 文件的生成有关联。 Search Index .json-file for Lunr.js：没看明白； Save data to “data/pages.json” #144：没有解决方案； Add generator for a search index #1853：没有解决方案； 生成全站的 json 文件（未解决）： https://bitquabit.com/post/json-feed-with-hugo/：已试验不成功； https://github.com/bep/bepsays.com/commit/1d7bf7fd6e7c637f28a04f93a97ebfed084c27ff：未试验，但感觉是一个 post 生成一个文件，这个地址来自 Simple JSON site index in Hugo； 已解决：图片浏览器不再用 fancybox，使用方便程度和整合难度都不太理想，改成用 lightGallery； OK：图片保存的路径，在 \static\images\ 中保存是比较好的选择，Rmd 文件生成的图像在 static\figures\之中，不需要人工干预； 已解决：文章内目录：\layouts\_default\single.html 显示目前的主题是支持 toc 功能的，通过 show_toc: true 开启，如果是 Rmd 文件，开启方法参考 Bookdown 即可： output: blogdown::html_page: toc: true number_sections: true 已解决：文章内的子标题编号，由于 Hugo 的 Markdown 引擎 blackfriday 还不支持 Auto numbering，因此这个功能暂时不好实现，Rmd 提供了接口，开启方法见上面的示例代码或本文档的源文件； 可解决：视频文件的特殊显示效果 lightGallery 就可以解决，参考 Shortcodes 可以了解更多关于短代码的说明，里面有 Youtube 的支持，但是感觉这会导致不同的系统不兼容，另外还可以参考主题 Castanet 的实现方法；需要进一步了解自己怎样定义好的短代码； 已解决：图片的标题是用 *...* 还是直接用一个 &amp;lt;p class=&amp;quot;text-align: center&amp;quot;&amp;gt;...&amp;lt;/p&amp;gt;，最终决定定义一个 imgCaption 和 tabCaption，这里使用了 Pandoc 的转换功能，将 Markdown 代码中的可选文本自动转换成 Caption； 可解决：如果将幻灯片等和本博客集成到一起，需不需要集成，做不了，也没必要，直接再做一个仓库保存幻灯片即可； OK：试验添加 Rmd 文件，并在 RStudio 中查看运行效果，原生的 Rmd 格式的 YAML 头和 blogdown 中的并不完全相同，各走各即可，目前使用过程中除 toc 功能外大部分是相同的； 可解决：不准备做，反正 EN 已经基本的外观。像 https://github.com/bep/bepsays.com/blob/master/config.toml 一样，提供不同语言界面的支持，或者像 https://github.com/kakawait/hugo-tranquilpeak-theme 中的 hugo-tranquilpeak-theme-master\i18n 那样用不同 YAML 提供语言支持的做法也非常值得参考； 可解决：多主题：https://yulinling.net/ 中 blog 部分是卡片列表，而相册部分是另外一种主题，不知道是不是可以在不同的 Section 中用不同的主题，还是压根是两个站点，是一个站点，只需要给指定的 Section 一个特殊的 list.html 即可，参考这里，再移植其它模板的代码即可； 别解决：如何通过 Hugo 删除一些不想要的 post 在 public\ 中的 html 存档，html 文档类似于缓存，删除之后下次 Render 时候又要再做一次，所以不要删除； 4 主题调整 4.1 页面宽度 在 static/css/fonts/ 中修改 custom.css，相关内容参考该文件中的注释。 4.2 字体 在 static/css/fonts/ 中修改 fonts.css，调整字体相关的设置为如下内容： body { font-family: &amp;#39;Alegreya&amp;#39;, &amp;#39;Palatino Linotype&amp;#39;, &amp;#39;Book Antiqua&amp;#39;, Palatino, &amp;#39;source-han-serif-sc&amp;#39;, &amp;#39;Source Han Serif SC&amp;#39;, &amp;#39;Source Han Serif CN&amp;#39;, &amp;#39;Source Han Serif TC&amp;#39;, &amp;#39;Source Han Serif TW&amp;#39;, &amp;#39;Source Han Serif&amp;#39;, &amp;#39;Songti SC&amp;#39;, &amp;#39;仿宋&amp;#39;, &amp;#39;FangSong&amp;#39;, &amp;#39;NSimSun&amp;#39;, &amp;#39;Microsoft YaHei&amp;#39;, serif; } blockquote { font-family: &amp;#39;Source Sans Pro&amp;#39;, Tahoma, Geneva, &amp;#39;STKaiti&amp;#39;, &amp;#39;KaiTi&amp;#39;, &amp;#39;楷体&amp;#39;, &amp;#39;SimKai&amp;#39;, &amp;#39;DFKai-SB&amp;#39;, &amp;#39;NSimSun&amp;#39;, serif; } .cn blockquote { font-family: &amp;#39;Alegreya&amp;#39;, &amp;#39;Palatino Linotype&amp;#39;, &amp;#39;Book Antiqua&amp;#39;, Palatino, &amp;#39;STKaiti&amp;#39;, &amp;#39;KaiTi&amp;#39;, &amp;#39;楷体&amp;#39;, &amp;#39;SimKai&amp;#39;, &amp;#39;DFKai-SB&amp;#39;, &amp;#39;NSimSun&amp;#39;, serif; } code { font-family: &amp;#39;PT Mono&amp;#39;, &amp;#39;STKaiti&amp;#39;, &amp;#39;KaiTi&amp;#39;, &amp;#39;SimKai&amp;#39;, monospace; font-size: 85%; } p code, li code { font-size: 90%; } strong { color: #2dbe60; } 5 功能解读 Hugo 中的相关统计数据，如字数(.WordCount)、估计阅读时间(.ReadingTime)、页数、文章数、分类数等可以参考变量列表，特殊功能实现参考函数列表； .Site.Params 开头的是站点的全局参数，可以自定义； .Params 开头的是当前文章中的局部参数，在 yml 头中定义，但暂时不清楚是否可以自定义，当然即使可以支持，为了保持和其它系统或 Markdown 的良好兼容，不推荐在文章内用自定义的参数； 下面的代码实现了一个循环： {{ range .Site.Params.customJS }} &amp;lt;script async src=&amp;quot;{{ . | relURL }}&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; {{ end }} {{ with FOO }}{{ . }}{{ end }} 是 {{if FOO }}{{ FOO }}{{ end }} 的另一种写法，可以不用写两次 FOO； .Data is dynamic, and its value changes according to the specific list you want to generate. For example, the list page https://xmin.yihui.name/post/ only contains pages under content/post/, and https://xmin.yihui.name/note/ only contains pages under content/note/； content\ 目录下的 _index.md 对应的是存档页，对应 list.html，用于生成全部 post 的存档列表；分类、标签汇总页对应 terms.html，是自动生成的，single.html 对应一篇 post 的生成；分类、汇总还可以通过 list.html 生成各自的存档页，对分类、标签设置不同的模板参考“待解决问题”； _index.md 中有内容时（YAML 除外），本主题将 list.html 转换成一个类似于网站主页说明的 index.html，但这个转换不是用 single.html 的模板，而是 list.html 的 if 分支的模板，没有内容时，填充当前 section 下的全部文章的 list。但不同位置的 _index.md 的管辖范围不同； 模板文件不能用 Markdown，只能用 html 语法格式； 主题中一段比较有参考价值的代码的解释： {{ if .File.Path }} {{ $Rmd := (print .File.BaseFileName &amp;quot;.Rmd&amp;quot;)) }} {{ if (where (readDir (print &amp;quot;content/&amp;quot; .File.Dir)) &amp;quot;Name&amp;quot; $Rmd) }} {{ $.Scratch.Set &amp;quot;FilePath&amp;quot; (print .File.Dir $Rmd) }} {{ else }} {{ $.Scratch.Set &amp;quot;FilePath&amp;quot; .File.Path }} {{ end }} {{ with .Site.Params.GithubEdit}} &amp;lt;a href=&amp;#39;{{ . }}{{ $.Scratch.Get &amp;quot;FilePath&amp;quot; }}&amp;#39;&amp;gt;Edit this page&amp;lt;/a&amp;gt; {{ end }} {{ end }} The basic logic is that for a file, if the same filename with the extension .Rmd exists, we will point the Edit link to the Rmd file. First, we define a variable $Rmd to be the filename with the .Rmd extension. Then we check if it exists. Unfortunately, there is no function in Hugo like file.exists() in R, so we have to use a hack: list all files under the directory and see if the Rmd file is in the list. $.Scratch is the way to dynamically store and obtain variables in Hugo templates. Most variables in Hugo are read-only, and you have to use $.Scratch when you want to modify a variable. We set a variable FilePath in $.Scratch, whose value is the full path to the Rmd file when the Rmd file exists, and the path to the Markdown source file otherwise. Finally we concatenate a custom option GithubEdit in config.toml with the file path to complete the Edit link &amp;lt;a&amp;gt;. Here is an example of the option in config.toml: .Data.Terms 解读：.Data.Terms stores all terms under a taxonomy, e.g., all category names. The variable $key denotes the term and $value denotes the list of pages associated with this term. The link of the term is passed to the Hugo function relURL via a pipe | to make it relative, $ is required because we are inside a loop, and need to access variables from the outside scope. 注意 $value 中是全部文章对象 &amp;lt;ul class=&amp;quot;terms&amp;quot;&amp;gt; {{ range $key, $value := .Data.Terms }} &amp;lt;li&amp;gt; &amp;lt;a href=&amp;#39;{{ (print &amp;quot;/&amp;quot; $.Data.Plural &amp;quot;/&amp;quot; $key) | relURL }}&amp;#39;&amp;gt; {{ $key }} &amp;lt;/a&amp;gt; ({{ len $value }}) &amp;lt;/li&amp;gt; {{ end }} &amp;lt;/ul&amp;gt; 这段代码的插入需要用比较特殊的语法，与传统的 R Markdow 中使用 backtick 的方法还不相同，具体参考源文件。↩ 这个目录是本人电脑上的目录，可自行根据实际情况修改。↩ 实际的修改过程要考虑的细节和内容要复杂很多，下面只是最早不太熟悉时候的一些记录。↩ 必需通过 VPN 方式才能注册，无法用 Chrome 带的翻墙功能注册。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[用 Python 搭建一个简单的 Web 服务器]]></title>
    	<url>/tech/2017/07/06/python-web-server/</url>
		<content type="text"><![CDATA[[在 RStudio 中运行 Hugo 的站点时，含有中文的地址解析都会出错，就想查出到底是 Hugo 的原因还是 Blogdown 的原因，先是发现 https://blog.coderzh.com/ 里面的中文分类以及中文 URL 地址在 Chrome 中都可以正常 Web 服务器，看看在非 Blogdown 的环境中会有什么样的结果。 参考 http://bbs.chinaunix.net/thread-743286-1-1.html 在 Hugo 的 \public\ 目录下放置一个包含如下内容的 Python 脚本： import SimpleHTTPServer SimpleHTTPServer.test() 按照作者的说明，此时可以双击开启 8000 端口并启动一个简单的 Web 服务，但由于是 Windows，因此根据廖雪峰的 Python 3.X 的讲义，不可能双击执行。于是在 CMD 窗口中尝试用 Python 命令来启动，仍然失败，提示没有 SimpleHTTPServer 包。 参考 http://www.cnblogs.com/harry-xiaojun/p/6739003.html，在 CMD 窗口中输入： python -m http.server 80 这次成功启动。在 Chrome 中浏览发现带有中文 URL 地址的内容在这个简单的 Web 服务器下解析正常，因此大致上可以确定是 Blogdown 在接管 Hugo 提供的 Server 服务时，做了一些额外的事情，或者做的工作不够，导致中文 URL 地址解析出错。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[比较回归系数的四种方法]]></title>
    	<url>/prof/2017/06/27/comparing-coefficients/</url>
		<content type="text"><![CDATA[[1 总结 2 suest 和交乘 3 xtreg 无法使用 suest 4 bdiff - 自举法 by 连玉君 5 Z 统计量 6 不能用 Hausman test - 慧航 1 总结 若干扰项相同，可以考虑用分组变量交乘项回归； 若干扰项不相 test1，但这个命令不支持xtreg； 用自举法，bdiff by 连玉君； 构造一个 Z 统计量： \[z=\frac{\hat{\beta}_m-\hat{\beta}_f}{\sqrt{s^2(\hat{\beta}_m)&#43;s^2(\hat{\beta}_f)}}\] 2 suest 和交乘 参考：http://bbs.pinggu.org/thread-2267159-1-1.html Stata 里suest可以做 Chow test 检验 Example 2: Do coefficients vary between groups? (“Chow test”) . webuse income . regress inc edu exp if male . estimates store Male . regress inc edu exp if !male . estimates store Female . suest Male Female . test [Male_mean = Female_mean] // 注意这里是 test 那如果我要检验两组的edu系数是否相同，是不是可以用命令test [Male_mean]edu =[ Female_mean]edu 另外，我在http://www.stata.com/support/faqs/statistics/computing-chow-statistic/这一网站看到，将male与其他所有解释变量交乘，检验交乘项的系数也可以得到edu系数在两组是否不同。我发现者两种方法的检验统计量不同，但 p 值大致相同。 请问这两种方法有差异么？哪种更好？ 连玉君：SUest假设两组的干扰项具有不同的分布，允许两组的干扰项相关；而采用交乘项的方式，估计时只有一条方程，所以相当于假设两个组的干扰项具有相同的分布。 3 xtreg 无法使用 suest http://bbs.pinggu.org/thread-2862014-1-1.html http://bbs.pinggu.org/thread-3927057-1-1.html 用面板数据进行分组回归（按中西东进行地区划分，分别作回归），如何对回归系数的差异的显著性进行检验？尝试用suest命令做，但是结果显示： .suest region1 region2 region3 xtreg is not supported by suest 即suest不支持xtreg命令。 连玉君：如果估计的是固定效应模型，可以使用reg y x i.id替代xtreg y x, fe命令进行估计，然后就可以进一步使用suest命令执行检验了。 另外还有命令test和ttest，可以进一步补充。 Comparing coefficients across groups using suest and test 4 bdiff - 自举法 by 连玉君 https://www.zhihu.com/question/23642050 5 Z 统计量 见最上面的总结部分。 Client Importance, Institutional Improvements, and Audit Quality in China: An Office and Individual Auditor Level Analysis, Chen &amp;amp; Sun &amp;amp; Wu(2010), TAR Statistical Methods for Comparing Regression Coefficients Between Models, Clogg &amp;amp; Petkova &amp;amp; Haritou(1995), American Journal of Sociology 6 不能用 Hausman test - 慧航 https://www.zhihu.com/question/23642050 Hausman test 的应用场景：存在同一组系数的两个估计b1和b2，满足：在 H0 的条件下，b1和b2均一致，但是b1是最有效的在 H1 的条件下，b1是不一致的，但是b2是一致的，Hausman 证明了，在 H0 的条件下，var(b2-b1)=var(b2)-var(b1)故而可以构造统计量，(b2-b1)(var(b2)-var(b1))^(-1)(b2-b1)~chi2。 比如，检验线性回归是不是有内生性：H0：不具有内生性 H1：具有内生性那么b1就是 ols 回归结果，b2就是 iv 的回归结果。同样，检验固定效应还是随机效应，b1是随机效应结果，b2是固定效应结果。但如果比较的是不是组回归的系数，显然不满足 Hausman test 的前提，所以绝对不能用 Hausman test。 参考 UCLA 上面的《Comparing Regressioni Coefficients Across Groups using Suest》。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[正则]]></title>
    	<url>/prof/2017/06/24/regular/</url>
		<content type="text"><![CDATA[[临界点、正则点、临界值、正则值 出处：经济数学方法与模型, de la Fuente(2000), 朱保华 &amp;amp; 钱晓明(2003), p150-151. 在以后的讨论中，我们需要对函数的雅可比矩阵的秩作 下面介绍一些在后面将用到的概念。令\(f:\mathbb{R}^n\supseteq X\to\mathbb{R}^m\)（\(X\)是开集）是可微函数1。若\(f\)在\(x\in X\)处的微分，即线性映射\(\mathrm{d}f_{x}\in L(\mathbb{R}^n,\mathbb{R}^m)\)是满射，则称\(x\)是\(f\)的正则点（Regular Point）。若\(x\)不是\(f\)的正则点，即若\(\mathrm{d}f_{x}\)不是满射，则称\(x\)是\(f\)的临界点（Critical Point）。若\(y\in\mathbb{R}^m\)是某个临界点的像，则称\(y\)是\(f\)的临界值，否则称\(y\)是\(f\)的正则值。 可以看到，\(\mathrm{d}f_x\)是满射，即\(x\)是\(f\)的正则点的充要条件是导数\(\mathrm{D}f(x)\)的秩为\(m\)。因此，\(f:\mathbb{R}^n\supseteq X\to\mathbb{R}^m\)的临界点的集合为： \[ C_f=\left\{\,x\in X;\,\text{rank}\,[\mathrm{D}f(x)]&amp;lt;m\,\right\} \] \(f\)的临界值的集合为\(f(C_f)\)，正则值的集合为\(f(C_f)\)的补集，即\(\mathbb{R}^m\sim f(C_f)\)。可以看到：若\(y\)不是\(X\)中任意点的像，根据定义，正则值是那些不是临界值的点，则\(y\)是\(f\)的正则值，而\(y\)是临界值的充要条件是\(f^{-1}(y)\)至少包含一个临界点，这样\(f^{-1}(y)\)就不可能是空集。 这个定义将初等微积分中临界点的定义作了推广。若\(f\)是多元实值函数，我们刚才给出的临界点的定义等价于条件“梯度\(\nabla f(x)\)是零向量”，因为只有这种情形\(\nabla f(x)\)不能生成\(\mathbb{R}\)；若\(f\)是一元函数，条件为\(f&amp;#39;(x)=0\)，同样看到：当\(f\)是从\(\mathbb{R}^n\)到\(\mathbb{R}^n\)的函数时，\(\mathrm{D}f(x)\)是方阵，若\(|\mathrm{D}f(x)|=0\)，则\(x\)是临界点。 经济是正则的 出处：经济学拓扑方法, 王则柯(2001), p305. 设\(\pmb{p}\)是经济\(\varepsilon\)的一个均衡价格向量，即\(f(\pmb{p})=\pmb{0}\)，那么当\(\mathrm{Rank}[\mathrm{d}f_{\pmb{p}}]=n-1\)时，称均衡价格向量\(\pmb{p}\)是正则的（Regular）。如果每个均衡价格向量都是正则的，就说该经济\(\varepsilon\)是正则的2。 换言之，如果\(f(\pmb{p})=\pmb{0}\)时必有\(\mathrm{Rank}[\mathrm{d}f_{\pmb{p}}]=n-1\)，就说经济\(\varepsilon\)是正则的。 作为一般经济均衡问题微分方法的初步结果，我们有如下定理3： 均衡价格向量\(\pmb{p}\in\mathbb{S}_&#43;^{n-1}\)正则当且仅当在\((\pmb{p},\pmb{0})\)，流形\(T\mathbb{S}_0^{n-1}\)和流形\(\text{Graph}\,f\)在流形\(T^{n-1}\)中横截。 这里的\(\mathbb{R}^m\)是一个多维的变量，但只要不是集合，就可以认为\(f\)是函数？↩ 这里的秩为\(n-1\)为什么要有一个-1，感觉和下面的\(\pmb{p}\in\mathbb{S}_&#43;^{n-1}\)有关。↩ 没有学过拓扑，看不懂。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python 动态创建类 z]]></title>
    	<url>/tech/2017/06/19/create-class-dynamic/</url>
		<content type="text"><![CDATA[[廖雪峰 Metaclass-ORM 个人理解 默认的类看不出是动态创建的； eval(), getattr 动态创建对象 - 见下面内容； 区分__getattr__()方法与getattr()函数；前者是 廖雪峰的 ORM 的例子中中，Model里面只有非常抽象的save()等方法，在ModelMetaclass里面根据User中的属性动态创建映射关系，然后再实时返回一个创建好的具体的User类； Users中由使用者指定需要映射的具体条目，对于不同的使用者，要映射的条目不相同，比如现在要的是username, email，下次的场景就可能是user, password； 即使要映射的条目相同，使用者定义的Users的属性名称也可能不同，比如有人是username，有人可能是userid； 但我们不希望每次需求不同时，就重新定义一个面向特定需求的类，类中包含需要的各种属性； 分离出来的Model负责实现与数据库的交互，但Model不清楚究竟要与哪张表交互，有哪些字段； ModelMetaclass在 new Users 的时候，就把Users中指定好的条目对（本地条目、数据库表的字段）接管过来，保存好属性-字段关系，将其作为一部分参数重新传递给type()函数重新创建User类，也就是在Users()创建的时候拦截下来，做了修改之后再按规划创建一个规范的User类 User如果不用ModelMetaclass的话，其创建的时候，自身是无法知道接下来会有什么属性和属性名的。 这时User继承自Model的Save()其实在此之前并不知道会有哪些条目，但是经过ModelMetaclass之后，就可以从动态生成的__mappings__中获知这一点； 现在User只管根据需要即时创建好条目对，之后由ModelMetaclass创建修改后的类，创建初始化完成后，参数再传递到继承自Model的Save()方法，最后Save()根据收到的参数：对象的具体参数以及具体需要的条目对完成数据库操作。 User中只有条目对作为属性； ModelMetaclass中只有__new__()； Model中包含具体的初始化以及业务逻辑。 利用 eval() 原文地址：http://blog.chinaunix.net/uid-608135-id-3774614.htmlz 某些时候我们需要创建一个对象的时候，要根据运行环境来确定对象的类型，这个时候就需要一种方法来动态的创建对象，也就是说类的名字是不确定的。 def getObj(name): return eval(name&#43;&#39;()&#39;) 利用 getattr() 函数 - 未理解 比如 modulename = &#39;haha&#39; #模块字符串 然后： m = __import__(mymodule) 下面方法就可以用 Python 动态创建类。如果要取得模块中的一个属性的话：可以用getattr()，比如： c = getattr(m, &#39;myclass&#39;) myobject = c() 但是要注意：如果myclass并不在mymodule的自动导出列表中（__all__），则必须显式地导入，例如： m = __import__(&#39;mymodule&#39;, globals(), locals(), [&#39;myclass&#39;]) c = getattr(m, &#39;myclass&#39;) myobject = c() 简单的可以用globals()[class_name]() def create_obj(cls_name): names = cls_name.split(&amp;quot;.&amp;quot;) cls = globals()[names[0]] for name in names[1:]: cls = getattr(cls, name) if isinstance(cls, type): return cls() else: raise Exception(&amp;quot;no such class&amp;quot;) 如果要使用当前模块： classname = &#39;blabla&#39;; mod = sys.modules[__name__]; dynclass = getattr(mod, classname) object = dynclass(params); 利用 type() 利用 Metaclass &#43; return type() class in function - 不如 eval 彻底 代码出处：https://stackoverflow.com/questions/100003/what-is-a-metaclass-in-python 如同 e-satis 所说的，这里的方案还不够动态，因为仍然需要自己编写整个类的代码。 &amp;gt;&amp;gt;&amp;gt; def choose_class(name): … if name == &#39;foo&#39;: … class Foo(object): … pass … return Foo # 返回的是类，不是类的实例 … else: … class Bar(object): … pass … return Bar … &amp;gt;&amp;gt;&amp;gt; MyClass = choose_class(&#39;foo&#39;) &amp;gt;&amp;gt;&amp;gt; print MyClass # 函数返回的是类，不是类的实例 &amp;lt;class &#39;__main__&#39;.Foo&amp;gt; &amp;gt;&amp;gt;&amp;gt; print MyClass() # 你可以通过这个类创建类实例，也就是对象 &amp;lt;__main__.Foo object at 0x89c6d4c&amp;gt;]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[SAS 中 IF 与 WHERE 的辨析]]></title>
    	<url>/tech/2017/06/18/sas-if-where/</url>
		<content type="text"><![CDATA[[参考资料：《SAS 编程演义》, by 谷鸿秋, p90 在新数据集里，我们可能只要部分观测，比如：只要女生。如何挑出女生的观测呢？通常可以从这三个阶段入手 打开数据集时，直接读取只需要的观测； PDV 里筛选过滤观测； 只写入所需观测进入数据集。 具体执行时，可以借助 IF 或 WHERE 语句选项。 *===第一阶段：通过 WHERE 选项限定读入数据集; data tmp; set sashlep.class(where=(sex=&amp;quot;F&amp;quot;)); run; *===第二阶段：通过 IF 或者 WHERE 语句; *===通过 where 语句; data tmp; set sashlep.class; where sex=&amp;quot;F&amp;quot;; run; *===通过求子集 IF 语句; data tmp; set sashlep.class; if sex=&amp;quot;F&amp;quot;; run; *===第三阶段：通过 WHERE 选项限定输出数据集; data tmp(where=(sex=&amp;quot;F&amp;quot;)); set sashlep.class; run; 在数据集选项里，我们只能用 WHERE，而不能用 IF； 从效率上讲，WHERE 更高效。因为 WHERE 语句在读入 PDV 之前就先行判断，而求子集 IF 语句先读入观测进入 PDV，而后再判断； 从使用范围上讲，WHERE 更广泛。WHREE 语句不仅可以用在 DATA 步，还可以用在 PROC 步中。此外，WHERE 还可以作为数据集选项使用，而 IF 只能作为 DATA 步语句使用； IF 语句对 INPUT 语句创建的观测有效，但是 WHERE 语句只能筛选数据集里的观测； 有 BY 语句时，求子集 IF 语句与 WHERE 语句的结果可能会不同，因为 SAS 创建 BY 组在 WHERE 之后，求子集 IF 语句之前； 求子集 IF 语句可以用在条件 IF 语句中，但 WHERE 语句不行； 当读入多个数据集时，求子集 IF 语句无法针对每个数据集单独筛选，但是 WHERE 选项却可以。 大多数情况下，作者喜欢用数据集选项来筛选观测。因此，在筛选观测时，代码大致如下： *=== WHERE 选项筛选观测; data want(where=(not missing(id))); set raw1(where=(age between 20 and 30)) raw2(where=(sex=&amp;quot;F&amp;quot;)); run;]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[《廖雪峰 Python 教程》笔记 5：面向对象编程2]]></title>
    	<url>/tech/2017/06/18/python-class-2/</url>
		<content type="text"><![CDATA[[原文地址： http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/00143186738532805c392f2cc09446caf3236c34e3f980f000 进一步阅读资料，了解关于 Metaclass 的相关细节： 深刻理解Python中的元类(metaclass)； Python 动态创建类z； Python 是动态语言，因此创建 定制类 slots - 限制实例可以添加的属性和方法 试图绑定不允许的属性将得到AttributeError的错误。但要注意，__slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的，除非在子类中也定义__slots__，这样，子类实例允许定义的属性就是自身的__slots__加上父类的__slots__。 class Student(object): __slots__ = (&#39;name&#39;, &#39;age&#39;) # 用tuple定义允许绑定的属性名称 __str__(), __repr__() - 结果看上去更人性化 __str__()：print()类的实例时结果看上去更人性化； __repr__()：直接访问类实例，不print()时的结果更人性化。 class Student(object): def __init__(self, name): self.name = name def __str__(self): return &#39;Student object (name=%s)&#39; % self.name __repr__ = __str__ __iter__() - 类似 list 或 tuple 那样可以 for&amp;hellip; in class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，故返回自己 def __next__(self): self.a, self.b = self.b, self.a &#43; self.b # 计算下一个值 if self.a &amp;gt; 100000: # 退出循环的条件 raise StopIteration() return self.a # 返回下一个值 __getitem__() - 更像 list 下标、切片访问 class Fib(object): def __getitem__(self, n): a, b = 1, 1 for x in range(n): a, b = b, a &#43; b return a &amp;gt;&amp;gt;&amp;gt; f = Fib() &amp;gt;&amp;gt;&amp;gt; f[0] 1 &amp;gt;&amp;gt;&amp;gt; f[1] 切片访问 class Fib(object): def __getitem__(self, n): if isinstance(n, int): # n是索引 a, b = 1, 1 for x in range(n): a, b = b, a &#43; b return a if isinstance(n, slice): # n是切片 start = n.start stop = n.stop if start is None: start = 0 a, b = 1, 1 L = [] for x in range(stop): if x &amp;gt;= start: L.append(a) a, b = b, a &#43; b return L 切片的step功能，即f[:10:2]中的2，负数处理等都需要额外的代码实现。所以，要正确实现一个__getitem__()还是有很多工作要做的。 此外，如果把对象看成dict，__getitem__()的参数也可能是一个可以作key的object，例如str。 与之对应的是__setitem__()方法，把对象视作list或dict来对集合赋值。最后，还有一个__delitem__()方法，用于删除某个元素。 总之，通过上面的方法，我们自己定义的类表现得和 Python 自带的list、tuple、dict没什么区别，这完全归功于动态语言的“鸭子类型”，不需要强制继承某个接口。 理解：可能直接继承自list()，然后将返回值变成一个list值的对象可能更容易达到目的。 __getattr__() - 动态返回一个不存在的属性或函数 注意与getattr()函数区分开：c = getattr(m, &#39;myclass&#39;)。前者是返回指定类属性的取值，后者是访问不存在的属性时动态返回一个属性或函数。 class Student(object): def __init__(self): self.name = &#39;Michael&#39; def __getattr__(self, attr): if attr==&#39;score&#39;: return 99 &amp;gt;&amp;gt;&amp;gt; s = Student() &amp;gt;&amp;gt;&amp;gt; s.name &#39;Michael&#39; &amp;gt;&amp;gt;&amp;gt; s.score 99 # 返回函数 class Student(object): def __getattr__(self, attr): if attr==&#39;age&#39;: return lambda: 25 只是调用方式要变为： &amp;gt;&amp;gt;&amp;gt; s.age() # 注意调用方法 25 注意到任意调用如s.abc都会返回None，这是因为我们定义的__getattr__()默认返回就是None。要让class只响应特定的几个属性，我们就要按照约定，抛出AttributeError的错误： class Student(object): def __getattr__(self, attr): if attr==&#39;age&#39;: return lambda: 25 raise AttributeError(&#39;\&#39;Student\&#39; object has no attribute \&#39;%s\&#39;&#39; % attr) 理解：理解教程中 REST 的关键就在于，一个 URL 对应一个功能接口，所以在链式方式调用某个功能时，利用__getattr__()可以动态实现一个特定的 URL 字符串，这样就不必要针对每个功能的 URL 专门定义一个方法或属性，而是动态生成即可。 class Chain(object): def __init__(self, path=&#39;GET &#39;): self._path = path def __getattr__(self, path): return Chain(&#39;%s/%s&#39; % (self._path, path)) def __call__(self,path): return Chain(&#39;%s/%s&#39; % (self._path, path)) def __str__(self): return self._path __repr__ = __str__ print(Chain().users(&#39;lidu&#39;).repos) Chain() -&amp;gt; init得到&#39;GET &#39;，.user()没有这个方法，所以走__getattr__()，这时，self._path是GET，path是user，然后在传给Chain时，被连接了起来GET /user，然后又调用了Chain -&amp;gt; init，这时，path为GET /user， 因为Chain()将生成一个实例，而这个实例后面跟着(&#39;lidu&#39;)，所以将调用__call__()，把lidu与_path连接起来，然后__call__()中又调用Chain()，继续走init, 把GET /user/lidu给了_path，后面又跟了一个.repos，没有这个属性，走__getatter__()，就如一开始那样，把repos也连接起来了。 最后，外面是一个print，里面是一个Chain的实例，所以会调用str，类中定义str返回_path，就是上面一连串过程后，生成的字符串：GET /user/lidu/repos。 __call__() - 让实例像函数一样 callable 这部分可以参考Python 中 __init__ 和 __call__ 的区别z。 一个对象实例可以有自己的属性和方法，当我们调用实例方法时，我们用instance.method()来调用。能不能直接在实例本身上调用呢？在 Python 中，答案是肯定的。 任何类，只需要定义一个__call__()方法，就可以直接对实例进行调用。请看示例： class Student(object): def __init__(self, name): self.name = name def __call__(self): print(&#39;My name is %s.&#39; % self.name) 调用方式如下： &amp;gt;&amp;gt;&amp;gt; s = Student(&#39;Michael&#39;) &amp;gt;&amp;gt;&amp;gt; s() # self参数不要传入 My name is Michael. __call__()还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样，所以你完全可以把对象看成函数，把函数看成对象，因为这两者之间本来就没啥根本的区别。 如果你把对象看成函数，那么函数本身其实也可以在运行期动态创建出来，因为类的实例都是运行期创建出来的，这么一来，我们就模糊了对象和函数的界限。 那么，怎么判断一个变量是对象还是函数呢？其实，更多的时候，我们需要判断一个对象是否能被调用，能被调用的对象就是一个 Callable 对象，比如函数和我们上面定义的带有__call__()的类实例： &amp;gt;&amp;gt;&amp;gt; callable(Student()) True &amp;gt;&amp;gt;&amp;gt; callable(max) True &amp;gt;&amp;gt;&amp;gt; callable([1, 2, 3]) False &amp;gt;&amp;gt;&amp;gt; callable(None) False &amp;gt;&amp;gt;&amp;gt; callable(&#39;str&#39;) False 通过callable()函数，我们就可以判断一个对象是否是可调用对象。 @property - 方便的 set(), get() 在绑定属性时，如果我们直接把属性暴露出去，虽然写起来很简单，但是，没办法检查参数。为解决这一缺陷，初级方法中的解决方案是将属性设定为私有类型，然后提供相应的set(), get()方法来访问属性。但是，上面的调用方法又略显复杂，没有直接用属性这么直接简单。有没有既能检查参数，又可以用类似属性这样简单的方式来访问类的变量呢？Python内置的@property装饰器就是负责把一个方法变成属性调用的。 class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError(&#39;score must be an integer!&#39;) if value &amp;lt; 0 or value &amp;gt; 100: raise ValueError(&#39;score must between 0 ~ 100!&#39;) self._score = value @property的实现比较复杂，我们先考察如何使用。 用法：把一个getter()方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter()方法变成属性赋值，于是，我们就拥有一个可控的属性操作： &amp;gt;&amp;gt;&amp;gt; s = Student() &amp;gt;&amp;gt;&amp;gt; s.score = 60 # OK，实际转化为s.set_score(60) &amp;gt;&amp;gt;&amp;gt; s.score # OK，实际转化为s.get_score() 60 &amp;gt;&amp;gt;&amp;gt; s.score = 9999 Traceback (most recent call last): ... ValueError: score must between 0 ~ 100! 注意到这个神奇的@property，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter()和setter()方法来实现的。还可以定义只读属性，只定义getter()方法，不定义setter()方法就是一个只读属性： class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2015 - self._birth 多重继承, MixIn 通过多重继承，一个子类就可以同时获得多个父类的所有功能。 在设计类的继承关系时，通常，主线都是单一继承下来的，例如，Ostrich继承自Bird。但是，如果需要“混入”额外的功能，通过多重继承就可以实现，比如，让Ostrich除了继承自Bird外，再同时继承Runnable。这种设计通常称之为MixIn。 为了更好地看出继承关系，我们把Runnable和Flyable改为RunnableMixIn和FlyableMixIn。类似的，你还可以定义出肉食动物CarnivorousMixIn和植食动物HerbivoresMixIn，让某个动物同时拥有好几个MixIn： class Dog(Mammal, RunnableMixIn, CarnivorousMixIn): pass MixIn 的目的就是给一个类增加多个功能，这样，在设计类的时候，我们优先考虑通过多重继承来组合多个 MixIn 的功能，而不是设计多层次的复杂的继承关系1。 理解：MixIn 本身在语法上和主线上的继承类没有区别，但是在名称上加了MixIn以后，可以更清晰的看到继承的主与次？ Python 自带的很多库也使用了 MixIn。举个例子，Python 自带了TCPServer和UDPServer这两类网络服务，而要同时服务多个用户就必须使用多进程或多线程模型，这两种模型由ForkingMixIn和ThreadingMixIn提供。通过组合，我们就可以创造出合适的服务来。 比如，编写一个多进程模式的 TCP 服务，定义如下： class MyTCPServer(TCPServer, ForkingMixIn): pass 编写一个多线程模式的 UDP 服务，定义如下： class MyUDPServer(UDPServer, ThreadingMixIn): pass 如果你打算搞一个更先进的协程模型，可以编写一个CoroutineMixIn： class MyTCPServer(TCPServer, CoroutineMixIn): pass 这样一来，我们不需要复杂而庞大的继承链，只要选择组合不同的类的功能，就可以快速构造出所需的子类。 枚举 Python提供了Enum类来实现这个功能： from enum import Enum Month = Enum(&#39;Month&#39;, (&#39;Jan&#39;, &#39;Feb&#39;, &#39;Mar&#39;, &#39;Apr&#39;, &#39;May&#39;, &#39;Jun&#39;, &#39;Jul&#39;, &#39;Aug&#39;, &#39;Sep&#39;, &#39;Oct&#39;, &#39;Nov&#39;, &#39;Dec&#39;)) 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，或者枚举它的所有成员： for name, member in Month.__members__.items(): print(name, &#39;=&amp;gt;&#39;, member, &#39;,&#39;, member.value) Jan =&amp;gt; Month.Jan , 1 Feb =&amp;gt; Month.Feb , 2 Mar =&amp;gt; Month.Mar , 3 Apr =&amp;gt; Month.Apr , 4 May =&amp;gt; Month.May , 5 Jun =&amp;gt; Month.Jun , 6 Jul =&amp;gt; Month.Jul , 7 Aug =&amp;gt; Month.Aug , 8 Sep =&amp;gt; Month.Sep , 9 Oct =&amp;gt; Month.Oct , 10 Nov =&amp;gt; Month.Nov , 11 Dec =&amp;gt; Month.Dec , 12 value属性则是自动赋给成员的int常量，默认从1开始计数。 如果需要更精确地控制枚举类型，可以从Enum派生出自定义类： from enum import Enum, unique @unique class Weekday(Enum): Sun = 0 # Sun的value被设定为0 Mon = 1 Tue = 2 Wed = 3 Thu = 4 Fri = 5 Sat = 6 @unique装饰器可以帮助我们检查保证没有重复值。 访问这些枚举类型可以有若干种方法： &amp;gt;&amp;gt;&amp;gt; day1 = Weekday.Mon &amp;gt;&amp;gt;&amp;gt; print(day1) Weekday.Mon &amp;gt;&amp;gt;&amp;gt; print(Weekday.Tue) Weekday.Tue &amp;gt;&amp;gt;&amp;gt; print(Weekday[&#39;Tue&#39;]) Weekday.Tue &amp;gt;&amp;gt;&amp;gt; print(Weekday.Tue.value) 2 &amp;gt;&amp;gt;&amp;gt; print(day1 == Weekday.Mon) True &amp;gt;&amp;gt;&amp;gt; print(day1 == Weekday.Tue) False &amp;gt;&amp;gt;&amp;gt; print(Weekday(1)) Weekday.Mon &amp;gt;&amp;gt;&amp;gt; print(day1 == Weekday(1)) True &amp;gt;&amp;gt;&amp;gt; Weekday(7) Traceback (most recent call last): ... ValueError: 7 is not a valid Weekday &amp;gt;&amp;gt;&amp;gt; for name, member in Weekday.__members__.items(): ... print(name, &#39;=&amp;gt;&#39;, member) ... Sun =&amp;gt; Weekday.Sun Mon =&amp;gt; Weekday.Mon Tue =&amp;gt; Weekday.Tue Wed =&amp;gt; Weekday.Wed Thu =&amp;gt; Weekday.Thu Fri =&amp;gt; Weekday.Fri Sat =&amp;gt; Weekday.Sat 可见，既可以用成员名称引用枚举常量，又可以直接根据value的值获得枚举常量。 元类 type类是一切之祖，是 Python 的内建 Metaclass，type()函数可以查看类型； type()函数既可以返回一个对象的类型，又可以创建出新的类型，比如，我们可以通过type()函数创建出Hello类，而无需通过class Hello(object)...的定义： &amp;gt;&amp;gt;&amp;gt; def fn(self, name=&#39;world&#39;): # 先定义函数 ... print(&#39;Hello, %s.&#39; % name) ... &amp;gt;&amp;gt;&amp;gt; Hello = type(&#39;Hello&#39;, (object,), dict(hello=fn)) # 创建Hello class &amp;gt;&amp;gt;&amp;gt; h = Hello() &amp;gt;&amp;gt;&amp;gt; h.hello() Hello, world. &amp;gt;&amp;gt;&amp;gt; print(type(Hello)) &amp;lt;class &#39;type&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print(type(h)) &amp;lt;class &#39;__main__.Hello&#39;&amp;gt; 要创建一个class对象，type()函数依次传入 3 个参数： class的名称； 继承的父类集合，注意 Python 支持多重继承，如果只有一个父类，别忘了tuple的单元素写法； class的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。 通过type()函数创建的类和直接写 class 是完全一样的，因为 Python 解释器遇到 class 定义时，仅仅是扫描一下 class 定义的语法，然后调用type()函数创建出class。 正常情况下，我们都用class Xxx...来定义类，但是，type()函数也允许我们动态创建出类来，也就是说，动态语言本身支持运行期动态创建类，这和静态语言有非常大的不同，要在静态语言运行期创建类，必须构造源代码字符串再调用编译器，或者借助一些工具生成字节码实现，本质上都是动态编译，会非常复杂。 理解：教程中关于 Metaclass 的内容不好理解，一是例子本身比较深奥，二是讲解也不到位。但需要注意的是普通的 class 不仅是一个类，而且也是一个 object，这是 Python 的一个非常特殊的概念。而所有 class 的祖先就是 type，但 type 的实现又用了一些特殊的技巧，class这个关键词实际上就是告诉 Python 在创建一个 class 的时候，也会创建一个对应的 classObject。 type就是 Python 的内建元类，当然了，你也可以创建自己的元类。 除了一些代码中不能理解的东西外，还不好理解的是为什么 ORM 适合用 Metaclass 来实现，这一部分教程写的非常不够。https://stackoverflow.com/questions/100003/what-is-a-metaclass-in-python 的讲解虽然好（中文版），但这一问题仍然感觉不到位。 对于教程中的代码： # metaclass是类的模板，所以必须从`type`类型派生： class ListMetaclass(type): def __new__(cls, name, bases, attrs): // cls 是固定写法，new 阶段还没有 self，也就无法用 self 为参数 attrs[&#39;add&#39;] = lambda self, value: self.append(value) return type.__new__(cls, name, bases, attrs) 现在难以理解的是这里的ListMetaclass中的name和bases究竟是什么。 Metaclass 除了使用type()动态创建类以外，要控制类的创建行为，还可以使用Metaclass。Metaclass，直译为元类，简单的解释就是： 当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。 但是如果我们想创建出类呢？那就必须根据Metaclass创建出类，所以：先定义 Metaclass，然后创建类。 连接起来就是：先定义 Metaclass，就可以创建类，最后创建实例。 所以，Metaclass允许你创建类或者修改类。换句话说，你可以把类看成是 Metaclass 创建出来的实例，也就是前面总结的，普通的类不仅是一个 class，同时还是一个由 Metaclass 而来的 object，。 Metaclass 是 Python 面向对象里最难理解，也是最难使用的魔术代码。正常情况下，你不会碰到需要使用 Metaclass 的情况，所以，以下内容看不懂也没关系，因为基本上你不会用到。 我们先看一个简单的例子，这个 Metaclass 可以给我们自定义的MyList增加一个add方法： 定义ListMetaclass，按照默认习惯，Metaclass 的类名总是以 Metaclass 结尾，以便清楚地表示这是一个 Metaclass： # Metaclass 是类的模板，所以必须从`type`类型派生： class ListMetaclass(type): def __new__(cls, name, bases, attrs): // new 的时候还没有 object，到 init 时才有，所以不能是 self，只能是 cls attrs[&#39;add&#39;] = lambda self, value: self.append(value) return type.__new__(cls, name, bases, attrs) 有了ListMetaclass，我们在定义类的时候还要指示使用ListMetaclass来定制类，传入关键字参数metaclass： class MyList(list, metaclass=ListMetaclass): pass 当我们传入关键字参数metaclass时，魔术就生效了，它指示 Python 解释器在创建MyList时，要通过ListMetaclass.__new__()来创建，在此，我们可以修改类的定义，比如，加上新的方法，然后，返回修改后的定义。 __new__()方法接收到的参数依次是： 当前准备创建的类的对象； 类的名字； 类继承的父类集合； 类的方法集合。 测试一下MyList是否可以调用add()方法： &amp;gt;&amp;gt;&amp;gt; L = MyList() &amp;gt;&amp;gt;&amp;gt; L.add(1) &amp;gt;&amp;gt; L [1] 而普通的list没有add()方法： &amp;gt;&amp;gt;&amp;gt; L2 = list() &amp;gt;&amp;gt;&amp;gt; L2.add(1) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; AttributeError: &#39;list&#39; object has no attribute &#39;add&#39; 动态修改有什么意义？直接在MyList定义中写上add()方法不是更简单吗？正常情况下，确实应该直接写，通过 Metaclass 修改纯属变态。但是，总会遇到需要通过 Metaclass 修改类定义的。ORM 就是一个典型的例子。 Metaclass - ORM 实例 ORM 全称Object Relational Mapping，即对象-关系映射，就是把关系数据库的一行映射为一个对象2，也就是一个类User-Class对应一个表User-Table，这样，写代码更简单，不用直接操作 SQL 语句。 不理解：要编写一个 ORM 框架，所有的类都只能动态定义，因为只有使用者才能根据表的结构定义出对应的类来。 让我们来尝试编写一个 ORM 框架。 编写底层模块的第一步，就是先把调用接口写出来。比如，使用者如果使用这个 ORM 框架，想定义一个User类来操作对应的数据库表User，我们期待他写出这样的代码： class User(Model): # 定义类的属性到列的映射： id = IntegerField(&#39;id&#39;) name = StringField(&#39;username&#39;) email = StringField(&#39;email&#39;) password = StringField(&#39;password&#39;) // 这部分的语法用于指定 User 的属性，但实际上最终被 Metaclass `pop` 了 # 创建一个实例： u = User(id=12345, name=&#39;Michael&#39;, email=&#39;test@orm.org&#39;, password=&#39;my-pwd&#39;) // User 的 __init__() 可以直接用 Model 的 __init()? # 保存到数据库： u.save() 其中，父类Model和属性类型StringField、IntegerField是由 ORM 框架提供的，剩下的魔术方法比如save()全部由metaclass自动完成。虽然metaclass的编写会比较复杂，但 ORM 的使用者用起来却异常简单。 现在，我们就按上面的接口来实现该 ORM。 首先来定义Field类，它负责保存数据库表的字段名和字段类型： class Field(object): def __init__(self, name, column_type): self.name = name self.column_type = column_type def __str__(self): return &#39;&amp;lt;%s:%s&amp;gt;&#39; % (self.__class__.__name__, self.name) 在Field的基础上，进一步定义各种类型的Field，比如StringField，IntegerField等等： class StringField(Field): def __init__(self, name): super(StringField, self).__init__(name, &#39;varchar(100)&#39;) class IntegerField(Field): def __init__(self, name): super(IntegerField, self).__init__(name, &#39;bigint&#39;) 下一步，就是编写最复杂的ModelMetaclass了： class ModelMetaclass(type): def __new__(cls, name, bases, attrs): # cls 固定；name, bases, attrs 对应为将来准备使用的参数，来自 User，走向新 User # 参考《深刻理解Python中的元类(metaclass)》可知是未来的 if name==&#39;Model&#39;: return type.__new__(cls, name, bases, attrs) print(&#39;Found model: %s&#39; % name) mappings = dict() for k, v in attrs.items(): if isinstance(v, Field): print(&#39;Found mapping: %s ==&amp;gt; %s&#39; % (k, v)) mappings[k] = v for k in mappings.keys(): attrs.pop(k) attrs[&#39;__mappings__&#39;] = mappings # 保存属性和列的映射关系 attrs[&#39;__table__&#39;] = name # 假设表名和类名一致 return type.__new__(cls, name, bases, attrs) 以及基类Model： class Model(dict, metaclass=ModelMetaclass): def __init__(self, **kw): # **kw 在初始化 User 时用到 super(Model, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r&amp;quot;&#39;Model&#39; object has no attribute &#39;%s&#39;&amp;quot; % key) def __setattr__(self, key, value): self[key] = value def save(self): fields = [] params = [] args = [] for k, v in self.__mappings__.items(): fields.append(v.name) params.append(&#39;?&#39;) args.append(getattr(self, k, None)) sql = &#39;insert into %s (%s) values (%s)&#39; % (self.__table__, &#39;,&#39;.join(fields), &#39;,&#39;.join(params)) print(&#39;SQL: %s&#39; % sql) print(&#39;ARGS: %s&#39; % str(args)) 当用户定义一个class User(Model)时，Python 解释器首先在当前类User的定义中查找metaclass，如果没有找到，就继续在父类Model中查找metaclass，找到了，就使用Model中定义的metaclass的ModelMetaclass来创建User类，也就是说，metaclass可以隐式地继承到子类，但子类自己却感觉不到。 在ModelMetaclass中，一共做了几件事情： 排除掉对Model类的修改3； 在当前类（比如User）中查找定义的类的所有属性，如果找到一个Field属性，就把它保存到一个__mappings__的dict中，同时从类属性中删除该Field属性，否则，容易造成运行时错误（实例的属性会遮盖类的同名属性4； 把表名保存到__table__中，这里简化为表名默认为类名。 在Model类中，就可以定义各种操作数据库的方法，比如save()，delete()，find()，update()等等。 我们实现了save()方法，把一个实例保存到数据库中。因为有表名，属性到字段的映射和属性值的集合，就可以构造出INSERT语句。 编写代码试试： u = User(id=12345, name=&#39;Michael&#39;, email=&#39;test@orm.org&#39;, password=&#39;my-pwd&#39;) u.save() 输出如下： Found model: User Found mapping: email ==&amp;gt; &amp;lt;StringField:email&amp;gt; Found mapping: password ==&amp;gt; &amp;lt;StringField:password&amp;gt; Found mapping: id ==&amp;gt; &amp;lt;IntegerField:uid&amp;gt; Found mapping: name ==&amp;gt; &amp;lt;StringField:username&amp;gt; SQL: insert into User (password,email,username,id) values (?,?,?,?) ARGS: [&#39;my-pwd&#39;, &#39;test@orm.org&#39;, &#39;Michael&#39;, 12345] 可以看到，save()方法已经打印出了可执行的 SQL 语句，以及参数列表，只需要真正连接到数据库，执行该 SQL 语句，就可以完成真正的功能。 不到 100 行代码，我们就通过metaclass实现了一个精简的 ORM 框架。 由于 Python 允许使用多重继承，因此，MixIn 就是一种常见的设计。只允许单一继承的语言（如 Java）不能使用 MixIn 的设计。 ↩ 这里对应User。 ↩ 这个对应于if name==&#39;Model&#39;，但不明白含义。 ↩ 没有彻底的理解。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[《廖雪峰 Python 教程》笔记 6：调试]]></title>
    	<url>/tech/2017/06/18/python-debug/</url>
		<content type="text"><![CDATA[[原文地址： http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431915578556ad30ab3933ae4e82a03ee2e9a4f70871000 程序能一次写完并正常运行的概率很小，基本不超过 1%。总会有各种各样的 bug 需要修正。有的 bug 很简单，看看错误信息就知道，有的 bug 很复杂， bug。 print() 第一种方法简单直接粗暴有效，就是用print()把可能有问题的变量打印出来看看： def foo(s): n = int(s) print(&#39;&amp;gt;&amp;gt;&amp;gt; n = %d&#39; % n) return 10 / n def main(): foo(&#39;0&#39;) main() 执行后在输出中查找打印的变量值： $ python3 err.py &amp;gt;&amp;gt;&amp;gt; n = 0 Traceback (most recent call last): ... ZeroDivisionError: integer division or modulo by zero 用print()最大的坏处是将来还得删掉它，想想程序里到处都是print()，运行结果也会包含很多垃圾信息。所以，我们又有第二种方法。 断言 凡是用print()来辅助查看的地方，都可以用断言（assert）来替代： def foo(s): n = int(s) assert n != 0, &#39;n is zero!&#39; return 10 / n def main(): foo(&#39;0&#39;) assert的意思是，表达式n != 0应该是True，否则，根据程序运行的逻辑，后面的代码肯定会出错。 如果断言失败，assert语句本身就会抛出AssertionError： $ python3 err.py Traceback (most recent call last): ... AssertionError: n is zero! 程序中如果到处充斥着assert，和print()相比也好不到哪去。不过，启动 Python 解释器时可以用-O参数来关闭assert： $ python3 -O err.py Traceback (most recent call last): ... ZeroDivisionError: division by zero 关闭后，你可以把所有的assert语句当成pass来看。 logging 把print()替换为logging是第 3 种方式，和assert比，logging不会抛出错误，而且可以输出到文件： import logging s = &#39;0&#39; n = int(s) logging.info(&#39;n = %d&#39; % n) print(10 / n) logging.info() 就可以输出一段文本。运行，发现除了ZeroDivisionError，没有任何信息。怎么回事？ 别急，在import logging之后添加一行配置再试试： import logging logging.basicConfig(level=logging.INFO) 看到输出了： $ python3 err.py INFO:root:n = 0 Traceback (most recent call last): File &amp;quot;err.py&amp;quot;, line 8, in &amp;lt;module&amp;gt; print(10 / n) ZeroDivisionError: division by zero 这就是logging的好处，它允许你指定记录信息的级别，有debug，info，warning，error等几个级别，当我们指定level=INFO时，logging.debug就不起作用了。同理，指定level=WARNING后，debug和info就不起作用了。这样一来，你可以放心地输出不同级别的信息，也不用删除，最后统一控制输出哪个级别的信息。 logging的另一个好处是通过简单的配置，一条语句可以同时输出到不同的地方，比如console和文件。 pdb 第 4 种方式是启动 Python 的调试器pdb，让程序以单步方式运行，可以随时查看运行状态。我们先准备好程序： # err.py s = &#39;0&#39; n = int(s) print(10 / n) 然后启动： $ python3 -m pdb err.py &amp;gt; /Users/michael/Github/learn-python3/samples/debug/err.py(2)&amp;lt;module&amp;gt;() -&amp;gt; s = &#39;0&#39; 以参数-m pdb启动后，pdb定位到下一步要执行的代码-&amp;gt; s = &#39;0&#39;。输入命令l来查看代码： (Pdb) l 1 # err.py 2 -&amp;gt; s = &#39;0&#39; 3 n = int(s) 4 print(10 / n) 输入命令n可以单步执行代码： (Pdb) n &amp;gt; /Users/michael/Github/learn-python3/samples/debug/err.py(3)&amp;lt;module&amp;gt;() -&amp;gt; n = int(s) (Pdb) n &amp;gt; /Users/michael/Github/learn-python3/samples/debug/err.py(4)&amp;lt;module&amp;gt;() -&amp;gt; print(10 / n) 任何时候都可以输入命令p 变量名来查看变量： (Pdb) p s &#39;0&#39; (Pdb) p n 0 输入命令q结束调试，退出程序： (Pdb) q 这种通过pdb在命令行调试的方法理论上是万能的，但实在是太麻烦了，如果有一千行代码，要运行到第 999 行得敲多少命令啊。还好，我们还有另一种调试方法。 pdb.set_trace() 这个方法也是用pdb，但是不需要单步执行，我们只需要import pdb，然后，在可能出错的地方放一个pdb.set_trace()，就可以设置一个断点： # err.py import pdb s = &#39;0&#39; n = int(s) pdb.set_trace() # 运行到这里会自动暂停 print(10 / n) 运行代码，程序会自动在pdb.set_trace()暂停并进入pdb调试环境，可以用命令p查看变量，或者用命令c继续运行： $ python3 err.py &amp;gt; /Users/michael/Github/learn-python3/samples/debug/err.py(7)&amp;lt;module&amp;gt;() -&amp;gt; print(10 / n) (Pdb) p n 0 (Pdb) c Traceback (most recent call last): File &amp;quot;err.py&amp;quot;, line 7, in &amp;lt;module&amp;gt; print(10 / n) ZeroDivisionError: division by zero 这个方式比直接启动pdb单步调试效率要高很多，但也高不到哪去。 IDE 如果要比较爽地设置断点、单步执行，就需要一个支持调试功能的 IDE。目前比较好的 Python IDE 有 PyCharm：http://www.jetbrains.com/pycharm/ 另外，Eclipse 加上pydev插件也可以调试 Python 程序。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[不动点定理]]></title>
    	<url>/prof/2017/06/18/fixed-point/</url>
		<content type="text"><![CDATA[[参考资料：《经济数学方法与模型》, by de la Fuente, p191 简介 Brouwer (布劳维尔)不动点定理给出的是函数存在不动点的充分条件； Kakutani (角谷静夫)不动点定理给出的是有 不动点定理 压缩映射原理(Émile Picard(1890)；Stefan Banach(1922))：设\(X\)是一个完备的度量空间，映射\(f:X\to X\) 把每两点的距离至少压缩\(\lambda\)倍，即\(\mathrm{d}(f(x), f(y))\leqslant\lambda\mathrm{d}(x,y)\)，这里\(\lambda\)是一个小于\(1\)的常数，那么\(f\)必有而且只有一个不动点，而且从\(Χ\)的任何点\(x_0\)出发作出序列\(x_1=f(x_0),x_2=f(x_1),\dotsc,x_n=f(x_{n-1}),\dotsc\)，这序列一定收敛到那个不动点。这条定理是许多种方程的解的存在性、惟一性及迭代解法的理论基础。由于分析学的需要，这定理已被推广到非扩展映射、概率度量空间、映射族、集值映射等许多方面。 Brouwer 不动点定理(1910): 设\(Χ\)是欧氏空间中的紧凸集，那么\(Χ\)到自身的每个连续映射都至少有一个不动点。 用这定理可以证明代数基本定理：复系数的代数方程一定有复数解。把布劳威尔定理中的欧氏空间换成巴拿赫空间，就是绍德尔不动点定理(1930)，常用于偏微分方程理论。这些定理可以从单值映射推广到集值映射，除微分方程理论外还常用于对策论和数理经济学。 Kakutani 不动点定理: 设\(C\)是\(\mathbb{R}^n\)中的紧凸集，\(f\)为从\(C\)到\(C\)的非空凸子集的上半连续的点—集映射，则至少存在一点\(x^\ast\), 使得\(x^\ast\in f(x^\ast)\)。 1941 年，Kakutani 把 Brouwer 不动点定理推广到有限维空间中多值映射的情形。不动点的个数有两种数法。代数上通常说\(n\)次复多项式有\(n\)个复根，是把一个\(k\)重根算作\(k\)个根的。如果不把重数统计在内，根的个数就可以小于\(n\)。推广根的重数概念，可以定义不动点的指数，它是一个整数，可正可负可零，取决于映射在不动点附近的局部几何性质。一个映射的所有不动点的指数的总和，称为这映射的不动点代数个数，以别于不动点的实际个数。 莱夫谢茨不动点定理：设\(Χ\)是紧多面体，\(f:Χ\to Χ\)是映射，那么\(f\)的不动点代数个数等于\(f\)的莱夫谢茨数\(L(f)\)，它是一个容易计算的同伦不变量，可以利用同调群以简单的公式写出。当\(L(f)\neq 0\)时，与\(f\)同伦的每个映射都至少有一个不动点。这个定理既发展了布劳威尔定理，也发展了关于向量场奇点指数和等于流形的欧拉数的庞加莱－霍普夫定理，把它进一步推广到泛函空间而得的勒雷－绍德尔参数延拓原理，早已成为偏微分方程理论的标准的工具。 J.尼尔斯 1927 年发现，一个映射\(f\)的全体不动点可以自然地分成若干个不动点类，每类中诸不动点的指数和都是同伦不变量。指数和不为\(0\)的不动点类的个数，称为这映射的尼尔斯数\(N(f)\)。只要\(Χ\)是维数大于\(2\)的流形，\(N(f)\)恰是与 \(f\)同伦的映射的最少不动点数。这就提供了研究方程的解的实际个数（而不只是代数个数）的一种方法。 莱夫谢茨定理的一个重要发展是关于微分流形上椭圆型算子与椭圆型复形的阿蒂亚－辛格指标定理与阿蒂亚-博特不动点定理。 参考文献 江泽涵，《不动点类理论》，科学出版社，北京，1979。 V. I. Istratescu. Fixed Point Theory (An Introduction)[M]. Reidel, 1981. B.Jiang,Lectures on Nielsen Fixed Point Theory,Amer. Math. Soc., Providence, 1983. M.J.Todd,The Computation of Fixed Points and Applications, Springer-Verlag, New York, 1976. L. E. J. Brouwer. Beweis der Invarianz der Dimensionenzahl[J]. Math. Ann., 1911, (70): 161–165. S. Kakutani. A Generalization of Brouwer Fixed Point Theorem[J]. Duke Math. J., 1941, (8): 457–459. 熊金城. 点集拓扑讲义(第三版)[M]. 高等教育出版社, 2003. Picard–Lindelöf theorem Banach fixed-point theorem https://zhidao.baidu.com/question/1671784868029411107.html]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[算子 (Operator)]]></title>
    	<url>/prof/2017/06/18/operator/</url>
		<content type="text"><![CDATA[[参考资料：《经济数学方法与模型》, by de la Fuente, p119 从\(X\)到\(X\)自身的映射称为算子(Operator)。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[《廖雪峰 Python 教程》笔记 4：面向对象编程1]]></title>
    	<url>/tech/2017/06/17/python-class/</url>
		<content type="text"><![CDATA[[原文地址： http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014318645694388f1f10473d7f416e9291616be8367ab5000 面向对象的设计思想是从自然界中来的，因为在自然界中，类（Class）和实例（Instance）的概念是很自然的。Class 是一种 Class——Student，是指学生这个概念，而实例（Instance）则是一个个具体的 Student，比如，Bart Simpson 和Lisa Simpson 是两个具体的 Student。所以，面向对象的设计思想是抽象出 Class，根据 Class 创建 Instance。 面向对象的抽象程度又比函数要高，因为一个 Class 既包含数据，又包含操作数据的方法。 简单示例 面向函数 std1 = { &#39;name&#39;: &#39;Michael&#39;, &#39;score&#39;: 98 } std2 = { &#39;name&#39;: &#39;Bob&#39;, &#39;score&#39;: 81 } def print_score(std): print(&#39;%s: %s&#39; % (std[&#39;name&#39;], std[&#39;score&#39;])) 面向对象 class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print(&#39;%s: %s&#39; % (self.name, self.score)) bart = Student(&#39;Bart Simpson&#39;, 59) lisa = Student(&#39;Lisa Simpson&#39;, 87) bart.print_score() lisa.print_score() 类和实例 一般模块文件名称小写，类的名称首字母大写，在新式类的定义中，需要明确指定是继承自哪个父类。 class Student(object): pass &amp;gt;&amp;gt;&amp;gt; bart = Student() &amp;gt;&amp;gt;&amp;gt; bart &amp;lt;__main__.Student object at 0x10a67a590&amp;gt; &amp;gt;&amp;gt;&amp;gt; Student &amp;lt;class &#39;__main__.Student&#39;&amp;gt; 类的实例——对象可以自由添加新的属性1： &amp;gt;&amp;gt;&amp;gt; bart.name = &#39;Bart Simpson&#39; &amp;gt;&amp;gt;&amp;gt; bart.name &#39;Bart Simpson&#39; __init__() 类可以起到模板的作用，因此，可以在初始化实例的时候2，把一些我们认为必须绑定的属性强制填写进去。通过定义一个特殊的__init__()方法，在初始化实例的时候，就把name，score等属性绑上去： class Student(object): def __init__(self, name, score): self.name = name self.score = score 总结：__init__()方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__()方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。有了__init__()方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__()方法匹配的参数，但self不需要传，Python 解释器自己会把实例变量传进去： &amp;gt;&amp;gt;&amp;gt; bart = Student(&#39;Bart Simpson&#39;, 59) &amp;gt;&amp;gt;&amp;gt; bart.name &#39;Bart Simpson&#39; &amp;gt;&amp;gt;&amp;gt; bart.score 59 和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数、关键字参数和命名关键字参数。 访问限制 总结： __name式的成员是私有成员，不能外部访问，当然，也可以用._Class__name的方式强制访问； _name式的成员可以外部访问，但习惯上认为没应该从外部访问； __name__式的可以外部访问，但一般有特殊含义，不建议自己定义的成员使用； 私有成员考虑实现get(), set()方法来进行访问，这样可以增加访问限制，从而避免无效参数设定； 不要给类实例增加__name这样的属性，这个名称与实际上内部私有成员的属性名._Class__name并不相同。 公有、私有成员 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线__，在 Python 中，实例的变量名如果以__开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把Student类改一改： class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print(&#39;%s: %s&#39; % (self.__name, self.__score)) 改完后，对于外部代码来说，没什么变动，但是已经无法从外部访问实例变量.__name和实例变量.__score了： &amp;gt;&amp;gt;&amp;gt; bart = Student(&#39;Bart Simpson&#39;, 98) &amp;gt;&amp;gt;&amp;gt; bart.__name Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; AttributeError: &#39;Student&#39; object has no attribute &#39;__name&#39; 这样就确保了外部代码不能随意修改对象内部的状态，这样通过访问限制的保护，代码更加健壮。 get(), set() 但是如果外部代码要获取name和score怎么办？可以给Student类增加get_name()和get_score()这样的方法3： class Student(object): ... def get_name(self): return self.__name def get_score(self): return self.__score 如果又要允许外部代码修改score怎么办？可以再给Student类增加set_score()方法： class Student(object): ... def set_score(self, score): self.__score = score 你也许会问，原先那种直接通过bart.score = 59也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数： class Student(object): ... def set_score(self, score): if 0 &amp;lt;= score &amp;lt;= 100: self.__score = score else: raise ValueError(&#39;bad score&#39;) 强行访问 需要注意的是，在 Python 中，变量名类似__xxx__的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是 private 变量，所以，不能用__name__()、__score__()这样的变量名。 有些时候，你会看到以一个下划线开头的实例变量名，比如_name，这样的实例变量外部是可以访问的，但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。 双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问__name是因为 Python 解释器对外把__name变量改成了_Student__name，所以，仍然可以通过_Student__name来访问__name变量： &amp;gt;&amp;gt;&amp;gt; bart._Student__name &#39;Bart Simpson&#39; 但是强烈建议你不要这么干，因为不同版本的 Python 解释器可能会把__name改成不同的变量名。总的来说就是，Python 本身没有任何机制阻止你干坏事，一切全靠自觉。 最后注意下面的这种错误写法： &amp;gt;&amp;gt;&amp;gt; bart = Student(&#39;Bart Simpson&#39;, 98) &amp;gt;&amp;gt;&amp;gt; bart.get_name() &#39;Bart Simpson&#39; &amp;gt;&amp;gt;&amp;gt; bart.__name = &#39;New Name&#39; # 设置__name变量！ &amp;gt;&amp;gt;&amp;gt; bart.__name &#39;New Name&#39; 表面上看，外部代码“成功”地设置了__name变量，但实际上这个__name变量和 class 内部的__name变量不是一个变量！内部的__name变量已经被 Python 解释器自动改成了_Student__name，而外部代码给bart新增了一个__name变量。不信试试： &amp;gt;&amp;gt;&amp;gt; bart.get_name() # get_name()内部返回self.__name &#39;Bart Simpson&#39; 继承和多态 继承有什么好处？最大的好处是子类获得了父类的全部功能。由于Animial实现了run()方法，因此，Dog和Cat作为它的子类，什么事也没干，就自动拥有了run()方法。 当子类和父类都存在相同的run()方法时，我们说，子类的run()覆盖了父类的run()，在代码运行的时候，总是会调用子类的run()。这样，我们就获得了继承的另一个好处：多态。 对于下面的函数： def run_twice(animal): animal.run() animal.run() 新增一个Animal的子类，不必对run_twice()做任何修改，实际上，任何依赖Animal作为参数的函数或者方法都可以不加修改地正常运行，原因就在于多态。 多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思： 对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种Animal的子类时，只要确保run()方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则： 对扩展开放：允许新增Animal子类； 对修改封闭：不需要修改依赖Animal类型的run_twice()等函数。 静态语言 vs 动态语言 对于静态语言（例如 Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用run()方法。 对于 Python 这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个run()方法就可以了： class Timer(object): def run(self): print(&#39;Start...&#39;) 这就是动态语言的鸭子类型，它并不要求严格的继承体系，一个对象只要看起来像鸭子，走起路来像鸭子，那它就可以被看做是鸭子。 Python 的file-like object就是一种鸭子类型。对真正的文件对象，它有一个read()方法，返回其内容。但是，许多对象，只要有read()方法，都被视为file-like object。许多函数接收的参数就是file-like object，你不一定要传入真正的文件对象，完全可以传入任何实现了read()方法的对象。 获取对象信息 type() - 判断对象类型 &amp;gt;&amp;gt;&amp;gt; type(123)==type(456) True &amp;gt;&amp;gt;&amp;gt; type(123)==int True &amp;gt;&amp;gt;&amp;gt; type(&#39;abc&#39;)==type(&#39;123&#39;) True &amp;gt;&amp;gt;&amp;gt; type(&#39;abc&#39;)==str True &amp;gt;&amp;gt;&amp;gt; type(&#39;abc&#39;)==type(123) False &amp;gt;&amp;gt;&amp;gt; import types &amp;gt;&amp;gt;&amp;gt; def fn(): ... pass ... &amp;gt;&amp;gt;&amp;gt; type(fn)==types.FunctionType True &amp;gt;&amp;gt;&amp;gt; type(abs)==types.BuiltinFunctionType True &amp;gt;&amp;gt;&amp;gt; type(lambda x: x)==types.LambdaType True &amp;gt;&amp;gt;&amp;gt; type((x for x in range(10)))==types.GeneratorType True isinstance() - 有继承关系时比 type() 好用 &amp;gt;&amp;gt;&amp;gt; isinstance(h, Husky) True &amp;gt;&amp;gt;&amp;gt; isinstance(h, Dog) True &amp;gt;&amp;gt;&amp;gt; isinstance(d, Dog) and isinstance(d, Animal) True &amp;gt;&amp;gt;&amp;gt; isinstance(d, Husky) False 能用type()判断的基本类型也可以用isinstance()判断： &amp;gt;&amp;gt;&amp;gt; isinstance(&#39;a&#39;, str) True &amp;gt;&amp;gt;&amp;gt; isinstance(123, int) True &amp;gt;&amp;gt;&amp;gt; isinstance(b&#39;a&#39;, bytes) True 并且还可以判断一个变量是否是某些类型中的一种，比如下面的代码就可以判断是否是list或者tuple： &amp;gt;&amp;gt;&amp;gt; isinstance([1, 2, 3], (list, tuple)) True &amp;gt;&amp;gt;&amp;gt; isinstance((1, 2, 3), (list, tuple)) True dir() - 获得对象的所有属性和方法 dir(&#39;ABC&#39;) 类似__xxx__的属性和方法在 Python 中都是有特殊用途的，剩下的都是普通属性或方法。 getattr()、setattr()、hasattr() 注意这里的getattr()和setattr()与前面的get_name()和set_name()不同，这里的两个方法是针对全体类的一个通过实现，而前面的两个方法是程序作者根据需要自己定义的方法，作者通常会在其中添加自己的更多额外工作，如数据有效验证等。 &amp;gt;&amp;gt;&amp;gt; hasattr(obj, &#39;x&#39;) # 有属性&#39;x&#39;吗？ True &amp;gt;&amp;gt;&amp;gt; obj.x 9 &amp;gt;&amp;gt;&amp;gt; hasattr(obj, &#39;y&#39;) # 有属性&#39;y&#39;吗？ False &amp;gt;&amp;gt;&amp;gt; setattr(obj, &#39;y&#39;, 19) # 设置一个属性&#39;y&#39; &amp;gt;&amp;gt;&amp;gt; hasattr(obj, &#39;y&#39;) # 有属性&#39;y&#39;吗？ True &amp;gt;&amp;gt;&amp;gt; getattr(obj, &#39;y&#39;) # 获取属性&#39;y&#39; 19 &amp;gt;&amp;gt;&amp;gt; obj.y # 获取属性&#39;y&#39; 19 如果试图获取不存在的属性，会抛出AttributeError的错误： &amp;gt;&amp;gt;&amp;gt; getattr(obj, &#39;z&#39;) # 获取属性&#39;z&#39; Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; AttributeError: &#39;MyObject&#39; object has no attribute &#39;z&#39; 可以传入一个default参数，如果属性不存在，就返回默认值： &amp;gt;&amp;gt;&amp;gt; getattr(obj, &#39;z&#39;, 404) # 获取属性&#39;z&#39;，如果不存在，返回默认值404 404 也可以获得对象的方法： &amp;gt;&amp;gt;&amp;gt; hasattr(obj, &#39;power&#39;) # 有属性&#39;power&#39;吗？ True &amp;gt;&amp;gt;&amp;gt; getattr(obj, &#39;power&#39;) # 获取属性&#39;power&#39; &amp;lt;bound method MyObject.power of &amp;lt;__main__.MyObject object at 0x10077a6a0&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; fn = getattr(obj, &#39;power&#39;) # 获取属性&#39;power&#39;并赋值到变量fn &amp;gt;&amp;gt;&amp;gt; fn # fn指向obj.power &amp;lt;bound method MyObject.power of &amp;lt;__main__.MyObject object at 0x10077a6a0&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; fn() # 调用fn()与调用obj.power()是一样的 81 小结 通过内置的一系列函数，我们可以对任意一个 Python 对象进行剖析，拿到其内部的数据。要注意的是，只有在不知道对象信息的时候，我们才会去获取对象信息。如果可以直接写： sum = obj.x &#43; obj.y 就不要写： sum = getattr(obj, &#39;x&#39;) &#43; getattr(obj, &#39;y&#39;) 一个正确的用法的例子如下： def readImage(fp): if hasattr(fp, &#39;read&#39;): return readData(fp) return None 假设我们希望从文件流fp中读取图像，我们首先要判断该fp对象是否存在read()方法，如果存在，则该对象是一个流，如果不存在，则无法读取。hasattr()就派上了用场。 请注意，在 Python 这类动态语言中，根据鸭子类型，有read()方法，不代表该fp对象就是一个文件流，它也可能是网络流，也可能是内存中的一个字节流，但只要read()方法返回的是有效的图像数据，就不影响读取图像的功能。 实例属性和类属性 由于 Python 是动态语言，根据类创建的实例可以任意绑定属性，但这个属性只与实例有关，与类无关。我们定义了一个类属性后，这个属性虽然归类所有，但类的所有实例都可以访问到。编写程序的时候，千万不要把实例属性和类属性使用相同的名字，因为相同名称的实例属性将屏蔽掉类属性，但是删除实例属性后，再使用相同的名称，访问到的将是类属性。 类在一定条件下也可以实时添加新的属性，具体方法参考高级部分内容。 ↩ 教程中说的是创建实例的时候，实际上创建与__new__()对应，要早于__init__()。 ↩ 和后面的getattr()、setattr()不同，那里的两个方法是 Python 实现的适用于全体类的通用方法。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[《廖雪峰 Python 教程》笔记 3：Module, Package]]></title>
    	<url>/tech/2017/06/16/python/</url>
		<content type="text"><![CDATA[[原文地址： http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014318447437605e90206e261744c08630a836851f5183000 概念 模块 在计算机程序的开发过程中，随着程序代码越写越多，在一个文件里代码就会越来越长，越来越不容易维护。为了编写可维护的代码，我 Python 中，一个.py文件就称之为一个模块（Module）。 使用模块有什么好处？ 最大的好处是大大提高了代码的可维护性。其次，编写代码不必从零开始。当一个模块编写完毕，就可以被其他地方引用。我们在编写程序的时候，也经常引用其他模块，包括 Python 内置的模块和来自第三方的模块。 使用模块还可以避免函数名和变量名冲突。相同名字的函数和变量完全可以分别存在不同的模块中，因此，我们自己在编写模块时，不必考虑名字会与其他模块冲突。但是也要注意，尽量不要与内置函数名字冲突。点这里查看 Python 的所有内置函数。 包 你也许还想到，如果不同的人编写的模块名相同怎么办？为了避免模块名冲突，Python 又引入了按目录来组织模块的方法，称为包（Package）。 举个例子，一个abc.py的文件就是一个名字叫abc的模块，一个xyz.py的文件就是一个名字叫xyz的模块。 现在，假设我们的abc和xyz这两个模块名字与其他模块冲突了，于是我们可以通过包来组织模块，避免冲突。方法是选择一个顶层包名，比如mycompany，按照如下目录存放： &#43; mycompany - __init__.py - abc.py - xyz.py 引入了包以后，只要顶层的包名不与别人冲突，那所有模块都不会与别人冲突。现在，abc.py模块的名字就变成了mycompany.abc，类似的，xyz.py的模块名变成了mycompany.xyz。 请注意，每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python 就把这个目录当成普通目录，而不是一个包。__init__.py可以是空文件，也可以有 Python 代码，因为__init__.py本身就是一个模块，而它的模块名就是mycompany。 类似的，可以有多级目录，组成多级层次的包结构。比如如下的目录结构： &#43; mycompany &#43; web - __init__.py - utils.py - www.py - __init__.py - abc.py - utils.py - xyz.py 文件www.py的模块名就是mycompany.web.www，两个文件utils.py的模块名分别是mycompany.utils和mycompany.web.utils。 自己创建模块时要注意命名，不能和 Python 自带的模块名称冲突。例如，系统自带了sys模块，自己的模块就不可命名为sys.py，否则将无法导入系统自带的sys模块。 mycompany.web也是一个模块，请指出该模块对应的.py文件。 使用模块：完整示例 Python 本身就内置了很多非常有用的模块，只要安装完毕，这些模块就可以立刻使用。 我们以内建的sys模块为例，编写一个hello的模块： #!/usr/bin/env python3 # -*- coding: utf-8 -*- &#39; a test module &#39; __author__ = &#39;Michael Liao&#39; import sys def test(): args = sys.argv if len(args)==1: print(&#39;Hello, world!&#39;) elif len(args)==2: print(&#39;Hello, %s!&#39; % args[1]) else: print(&#39;Too many arguments!&#39;) if __name__==&#39;__main__&#39;: test() 第 1 行和第 2 行是标准注释，第 1 行注释可以让这个hello.py文件直接在Unix/Linux/Mac上运行，第 2 行注释表示.py文件本身使用标准 UTF-8 编码； 第 4 行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释； 第 6 行使用__author__变量把作者写进去，这样当你公开源代码后别人就可以瞻仰你的大名； 以上就是 Python 模块的标准文件模板，当然也可以全部删掉不写，但是，按标准办事肯定没错。后面开始就是真正的代码部分。 你可能注意到了，使用sys模块的第一步，就是导入该模块： import sys 导入sys模块后，我们就有了变量sys指向该模块，利用sys这个变量，就可以访问sys模块的所有功能。 sys模块有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称，例如： 运行python3 hello.py获得的sys.argv就是[&#39;hello.py&#39;]； 运行python3 hello.py Michael获得的sys.argv就是[&#39;hello.py&#39;, &#39;Michael]。 最后，注意到这两行代码： if __name__==&#39;__main__&#39;: test() 当我们在命令行运行hello模块文件时，Python 解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。 我们可以用命令行运行hello.py看看效果： $ python3 hello.py Hello, world! $ python hello.py Michael Hello, Michael! 如果启动Python交互环境，再导入hello模块： $ python3 Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 23 2015, 02:52:03) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information. &amp;gt;&amp;gt;&amp;gt; import hello &amp;gt;&amp;gt;&amp;gt; 导入时，没有打印Hello, word!，因为没有执行test()函数。 调用hello.test()时，才能打印出Hello, word!： &amp;gt;&amp;gt;&amp;gt; hello.test() Hello, world! 作用域 在一个模块中，我们可能会定义很多函数和变量，但有的函数和变量我们希望给别人使用，有的函数和变量我们希望仅仅在模块内部使用。在 Python 中，是通过_前缀来实现的。 正常的函数和变量名是公开的（public），可以被直接引用，比如：abc，x123，PI等； 类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如上面的__author__，__name__就是特殊变量，hello模块定义的文档注释也可以用特殊变量__doc__访问，我们自己的变量一般不要用这种变量名； 类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，__abc等； 之所以我们说，private 函数和变量不应该被直接引用，而不是不能被直接引用，是因为 Python 并没有一种方法可以完全限制访问 private 函数或变量，但是，从编程习惯上不应该引用 private 函数或变量。 private 函数或变量不应该被别人引用，那它们有什么用呢？请看例子： def _private_1(name): return &#39;Hello, %s&#39; % name def _private_2(name): return &#39;Hi, %s&#39; % name def greeting(name): if len(name) &amp;gt; 3: return _private_1(name) else: return _private_2(name) 我们在模块里公开greeting()函数，而把内部逻辑用 private 函数隐藏起来了，这样，调用greeting()函数不用关心内部的 private 函数细节，这也是一种非常有用的代码封装和抽象的方法，即： 外部不需要引用的函数全部定义成 private，只有外部需要引用的函数才定义为 public。 第三方模块 在 Python 中，安装第三方模块，是通过包管理工具pip完成的。 如果你正在使用 Mac 或 Linux，安装pip本身这个步骤就可以跳过了。 如果你正在使用 Windows，请参考安装 Python 一节的内容，确保安装时勾选了pip和Add python.exe to Path。 在命令提示符窗口下尝试运行pip，如果 Windows 提示未找到命令，可以重新运行安装程序添加pip。 注意：Mac 或 Linux 上有可能并存 Python 3.x 和 Python 2.x，因此对应的 pip 命令是pip3。 现在，让我们来安装一个第三方库——Python Imaging Library，这是 Python 下非常强大的处理图像的工具库。不过，PIL目前只支持到 Python 2.7，并且有年头没有更新了，因此，基于PIL的Pillow项目开发非常活跃，并且支持最新的 Python 3。 一般来说，第三方库都会在 Python 官方的 网站注册，要安装一个第三方库，必须先知道该库的名称，可以在官网或者pypi上搜索，比如 Pillow 的名称叫Pillow，因此，安装Pillow的命令就是： pip install Pillow 耐心等待下载并安装后，就可以使用Pillow了。 有了Pillow，处理图片易如反掌。随便找个图片生成缩略图： &amp;gt;&amp;gt;&amp;gt; from PIL import Image &amp;gt;&amp;gt;&amp;gt; im = Image.open(&#39;test.png&#39;) &amp;gt;&amp;gt;&amp;gt; print(im.format, im.size, im.mode) PNG (400, 300) RGB &amp;gt;&amp;gt;&amp;gt; im.thumbnail((200, 100)) &amp;gt;&amp;gt;&amp;gt; im.save(&#39;thumb.jpg&#39;, &#39;JPEG&#39;) 其他常用的第三方库还有 MySQL 的驱动：mysql-connector-python，用于科学计算的 NumPy 库：numpy，用于生成文本的模板工具Jinja2，等等。 模块搜索路径 当我们试图加载一个模块时，Python 会在指定的路径下搜索对应的.py文件，如果找不到，就会报错： &amp;gt;&amp;gt;&amp;gt; import mymodule Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; ImportError: No module named mymodule 默认情况下，Python 解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中： &amp;gt;&amp;gt;&amp;gt; import sys &amp;gt;&amp;gt;&amp;gt; sys.path [&#39;&#39;, &#39;/Library/Frameworks/Python.framework/Versions/3.4/lib/python34.zip&#39;, &#39;/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4&#39;, &#39;/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/plat-darwin&#39;, &#39;/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/lib-dynload&#39;, &#39;/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages&#39;] 如果我们要添加自己的搜索目录，有两种方法： 直接修改sys.path，添加要搜索的目录： &amp;gt;&amp;gt;&amp;gt; import sys &amp;gt;&amp;gt;&amp;gt; sys.path.append(&#39;/Users/michael/my_py_scripts&#39;) 这种方法是在运行时修改，运行结束后失效。 设置环境变量PYTHONPATH，该环境变量的内容会被自动添加到模块搜索路径中。设置方式与设置Path环境变量类似。注意只需要添加你自己的搜索路径，Python 自己本身的搜索路径不受影响。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[RMarkdown 应用笔记]]></title>
    	<url>/tech/2017/06/15/rmarkdown-notes/</url>
		<content type="text"><![CDATA[[给 Chunk 添加新的 Class Name 普通代码高亮 根据https://stackoverflow.com/questions/37944197/add-a-css 的fenced_code_attributes默认在 RMarkdown 中已经打开，因此不需要再添加md_extensions: &#43;fenced_code_attributes语句到 YAML 的html_document之下；又根据 markdown-in-pandoc 中 6.2.3 的说明可知，在fenced_code_attributes扩展启用的前提下，可以将下面的代码 ```html 这是一个普通段落。 &amp;lt;table&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;Foo&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/table&amp;gt; 这是另一个普通段落。 ``` 修改成 ```{.html .gray} ...... ``` 这样就可以达到添加新 Class 名称的目的。 R Chunk 中的代码高亮 推荐做法 根据https://github.com/yihui/knitr-examples/第 116 号示例以及 knitr 133 号问题的讨论，可以给 R Chunk 一个class.output属性，如,class.output=&#39;myClass&#39;的方式添加一个输出样式类名称，如果需要多个，可以用,class.output=c(&amp;quot;myclass1&amp;quot;, &amp;quot;myclass2&amp;quot;)达到目的。 ```{r df-drop-ok, class.source=&#39;bg-success&#39;, class.output=&#39;bg-success&#39;} mtcars[, &amp;quot;mpg&amp;quot;, drop=FALSE] ``` 不推荐做法 可以参考https://stackoverflow.com/questions/37944197/add-a-css-class-to-single-code-chunks-in-rmarkdown，这种做法一是不如谢给出的官方做法适用，另一个原因是只能针对 R 起作用，换言之，想要高亮的代码是 html 时，需要将其中的钩子函数中的.r改成.html，一旦这样，又会只能对 html 代码起作用，所以不推荐。 属性解释 不理解 class.source和class.output同时出现在第 116 号示例中，但目前不清楚其作用。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[《廖雪峰 Python 教程》笔记 2：函数式编程]]></title>
    	<url>/tech/2017/06/14/python/</url>
		<content type="text"><![CDATA[[原文地址： http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014317848428125ae6aa24068b4c50a7e71501ab275d52000 函数式编程 函数式编程是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的， 函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！ Python 对函数式编程提供部分支持。由于 Python 允许使用变量，因此，Python 不是纯函数式编程语言。 高阶函数 变量可以指向函数 &amp;gt;&amp;gt;&amp;gt; abs(-10) 10 &amp;gt;&amp;gt;&amp;gt; abs &amp;lt;built-in function abs&amp;gt; &amp;gt;&amp;gt;&amp;gt; f = abs &amp;gt;&amp;gt;&amp;gt; f &amp;lt;built-in function abs&amp;gt; &amp;gt;&amp;gt;&amp;gt; f = abs &amp;gt;&amp;gt;&amp;gt; f(-10) 10 从上面的代码运行结果可以看到，函数本身也可以赋值给变量，即：变量可以指向函数。如果一个变量指向了一个函数，那么可以通过该变量来调用这个函数。 函数名也是变量 函数名其实就是指向函数的变量！对于abs()这个函数，完全可以把函数名abs看成变量，它指向一个可以计算绝对值的函数！如果把abs指向其他对象，例如 &amp;gt;&amp;gt;&amp;gt; abs = 10 &amp;gt;&amp;gt;&amp;gt; abs(-10) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; TypeError: &#39;int&#39; object is not callable 把abs指向10后，就无法通过abs(-10)调用该函数了！因为abs这个变量已经不指向求绝对值函数而是指向一个整数10！当然实际代码绝对不能这么写，这里是为了说明函数名也是变量。要恢复abs函数，请重启 Python 交互环境。 注：由于abs函数实际上是定义在import builtins模块中的，所以要让修改abs变量的指向在其它模块也生效，要用import builtins; builtins.abs = 10。 高阶函数：变量名作为参数 变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。编写高阶函数，就是让函数的参数能够接收别的函数，函数式编程就是指这种高度抽象的编程范式。 def add(x, y, f): return f(x) &#43; f(y) x = -5 y = 6 f = abs f(x) &#43; f(y) ==&amp;gt; abs(-5) &#43; abs(6) ==&amp;gt; 11 return 11 map, reduce, filter, sorted 读过论文 MapReduce: Simplified Data Processing on Large Clusters，就能大概明白map/reduce的概念： map类似于 Matlab 或者 R 中的apply系数函数，将函数f作用于list中的每个元素； reduce类似于 R 中的cumsum，将函数f累积作用于序列中的每个元素； filter作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素； sorted函数可以指定排序的依据，默认时数值按大小、字符串按 ASCII 编号，但可将排序改为按绝对值大小、忽略字符大小写、按升序还是降序等； map() map()函数接收两个参数，一个是函数名，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为一个Iterator型返回。 &amp;gt;&amp;gt;&amp;gt; def f(x): ... return x * x ... &amp;gt;&amp;gt;&amp;gt; r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9]) &amp;gt;&amp;gt;&amp;gt; list(r) [1, 4, 9, 16, 25, 36, 49, 64, 81] 上面结果r是一个 Iterator，Iterator 是惰性序列，因此通过list()函数让它把整个序列都计算出来并返回一个list。map()作为高阶函数，事实上它把运算规则抽象了，因此，我们不但可以计算简单的$f(x)=x^2$，还可以计算任意复杂的函数，比如，把这个list中的所有数字转为字符串： &amp;gt;&amp;gt;&amp;gt; list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9])) [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] reduce() reduce()把一个函数作用在一个序列[x1, x2, x3, ...]上，这个函数必须接收两个参数，reduce()把结果继续和序列的下一个元素做累积计算，其效果就是： reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) 比方说对一个序列求和，就可以用reduce()实现： &amp;gt;&amp;gt;&amp;gt; from functools import reduce &amp;gt;&amp;gt;&amp;gt; def add(x, y): ... return x &#43; y ... &amp;gt;&amp;gt;&amp;gt; reduce(add, [1, 3, 5, 7, 9]) 25 当然求和运算可以直接用 Python 内建函数sum()，没必要动用reduce()。但是如果要把序列[1, 3, 5, 7, 9]变换成整数13579，reduce()就可以派上用场： &amp;gt;&amp;gt;&amp;gt; from functools import reduce &amp;gt;&amp;gt;&amp;gt; def fn(x, y): ... return x * 10 &#43; y ... &amp;gt;&amp;gt;&amp;gt; reduce(fn, [1, 3, 5, 7, 9]) 13579 这个例子本身没多大用处，但是，如果考虑到字符串str也是一个序列，对上面的例子稍加改动，配合map()，我们就可以写出把str转换为int的函数： &amp;gt;&amp;gt;&amp;gt; from functools import reduce &amp;gt;&amp;gt;&amp;gt; def fn(x, y): ... return x * 10 &#43; y ... &amp;gt;&amp;gt;&amp;gt; def char2num(s): ... return {&#39;0&#39;: 0, &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, &#39;4&#39;: 4, &#39;5&#39;: 5, &#39;6&#39;: 6, &#39;7&#39;: 7, &#39;8&#39;: 8, &#39;9&#39;: 9}[s] ... &amp;gt;&amp;gt;&amp;gt; reduce(fn, map(char2num, &#39;13579&#39;)) 13579 上面的函数中，先用map()将字符串13579转换成一个整数序列[1, 3, 5, 7, 9]，再用reduce()将整数序列转换成一个整数，这个过程整理成一个str2int()函数就是： from functools import reduce def str2int(s): def fn(x, y): return x * 10 &#43; y def char2num(s): return {&#39;0&#39;: 0, &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, &#39;4&#39;: 4, &#39;5&#39;: 5, &#39;6&#39;: 6, &#39;7&#39;: 7, &#39;8&#39;: 8, &#39;9&#39;: 9}[s] return reduce(fn, map(char2num, s)) 还可以用lambda函数进一步简化成： from functools import reduce def char2num(s): return {&#39;0&#39;: 0, &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, &#39;4&#39;: 4, &#39;5&#39;: 5, &#39;6&#39;: 6, &#39;7&#39;: 7, &#39;8&#39;: 8, &#39;9&#39;: 9}[s] def str2int(s): return reduce(lambda x, y: x * 10 &#43; y, map(char2num, s)) 也就是说，假设 Python 没有提供int()函数，你完全可以自己写一个把字符串转化为整数的函数，而且只需要几行代码！lambda函数的用法在后面介绍。 filter() filter()也接收一个函数和一个序列。和map()不同的是，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 def is_odd(n): return n % 2 == 1 list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15])) # 结果: [1, 5, 9, 15] // 删除序列中的空字符串 def not_empty(s): return s and s.strip() list(filter(not_empty, [&#39;A&#39;, &#39;&#39;, &#39;B&#39;, None, &#39;C&#39;, &#39; &#39;])) # 结果: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;] 用filter()这个高阶函数，关键在于正确实现一个筛选函数。注意到filter()函数返回的是一个 Iterator，也就是一个惰性序列，所以要强迫filter()完成计算结果，需要用list()函数获得所有结果并返回list。 讲义中用埃氏筛法筛选中全部素数的方法，首先是利用了filter()函数，其次还用到了 Iterator 型变量惰性计算特征下可以表示出全体自然数的优点，应该说是 Python 语言特征的一次典型应用，实际上在利用 Python 的这两个特殊技巧操作数据也是非常方便的。 sorted() &amp;gt;&amp;gt;&amp;gt; sorted([36, 5, -12, 9, -21]) [-21, -12, 5, 9, 36] &amp;gt;&amp;gt;&amp;gt; sorted([36, 5, -12, 9, -21], key=abs) [5, 9, -12, -21, 36] &amp;gt;&amp;gt;&amp;gt; sorted([&#39;bob&#39;, &#39;about&#39;, &#39;Zoo&#39;, &#39;Credit&#39;]) [&#39;Credit&#39;, &#39;Zoo&#39;, &#39;about&#39;, &#39;bob&#39;] &amp;gt;&amp;gt;&amp;gt; sorted([&#39;bob&#39;, &#39;about&#39;, &#39;Zoo&#39;, &#39;Credit&#39;], key=str.lower) [&#39;about&#39;, &#39;bob&#39;, &#39;Credit&#39;, &#39;Zoo&#39;] &amp;gt;&amp;gt;&amp;gt; sorted([&#39;bob&#39;, &#39;about&#39;, &#39;Zoo&#39;, &#39;Credit&#39;], key=str.lower, reverse=True) [&#39;Zoo&#39;, &#39;Credit&#39;, &#39;bob&#39;, &#39;about&#39;] 返回值为函数、闭包（难度较高） 高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。一个可变参数的求和通常情况下是这样定义的： def calc_sum(*args): ax = 0 for n in args: ax = ax &#43; n return ax 但是，如果不需要立刻求和，而是在后面的代码中，根据需要再计算怎么办？可以不返回求和的结果，而是返回求和的函数： def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax &#43; n return ax return sum 当我们调用lazy_sum()时，返回的并不是求和结果，而是求和函数： &amp;gt;&amp;gt;&amp;gt; f = lazy_sum(1, 3, 5, 7, 9) &amp;gt;&amp;gt;&amp;gt; f &amp;lt;function lazy_sum.&amp;lt;locals&amp;gt;.sum at 0x101c6ed90&amp;gt; &amp;gt;&amp;gt;&amp;gt; f() // 调用函数 f 时，才真正计算求和的结果： 25 在这个例子中，我们在函数lazy_sum中又定义了函数sum，并且，内部函数sum可以引用外部函数lazy_sum的参数和局部变量，当lazy_sum返回函数sum时，相关参数和变量都保存在返回的函数中1，这种称为闭包（Closure）的程序结构拥有极大的威力。 注意：调用lazy_sum()时，每次调用都会返回一个新的函数，即使传入相同的参数： &amp;gt;&amp;gt;&amp;gt; f1 = lazy_sum(1, 3, 5, 7, 9) &amp;gt;&amp;gt;&amp;gt; f2 = lazy_sum(1, 3, 5, 7, 9) &amp;gt;&amp;gt;&amp;gt; f1==f2 False f1()和f2()的调用结果互不影响。 注意到返回的函数在其定义内部引用了局部变量args2，所以，当一个函数返回了一个函数后，其内部的局部变量还被新函数引用，所以，闭包用起来简单，实现起来可不容易。疑惑的是这里其指的是什么、新函数是什么没有弄明白。 注意：返回的函数并没有立刻执行，而是直到调用了f()才执行 def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fs f1, f2, f3 = count() 在上面的例子中，range(1,4)对应[1, 2, 3]，每次循环都创建了一个新的函数，然后，把创建的 3 个函数都返回了。可能会认为调用f1()，f2()和f3()结果应该是1，4，9，但实际结果是： &amp;gt;&amp;gt;&amp;gt; f1() 9 &amp;gt;&amp;gt;&amp;gt; f2() 9 &amp;gt;&amp;gt;&amp;gt; f3() 9 全部都是9！原因就在于返回的函数引用了变量i，但它并非立刻执行。等到 3 个函数都返回时，它们所引用的变量i已经变成了3，因此最终结果为9。返回闭包时牢记的一点就是：返回函数不要引用任何循环变量，或者后续会发生变化的变量。如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变： 疑惑：什么时候f1()、f2()和f3()会有参数，该怎么操作？ def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fs &amp;gt;&amp;gt;&amp;gt; f1, f2, f3 = count() &amp;gt;&amp;gt;&amp;gt; f1() 1 &amp;gt;&amp;gt;&amp;gt; f2() 4 &amp;gt;&amp;gt;&amp;gt; f3() 9 这种做法的缺点是代码较长，可利用lambda()函数缩短代码。 匿名函数(lambda) 有些时候，不需要显式地定义函数，直接传入匿名函数更方便。在 Python 中，对匿名函数提供了有限支持。还是以map()函数为例，计算$f(x)=x^2$时，除了定义一个$f(x)$的函数外，还可以直接传入匿名函数： &amp;gt;&amp;gt;&amp;gt; list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9])) [1, 4, 9, 16, 25, 36, 49, 64, 81] 通过对比可以看出，匿名函数lambda x: x * x实际上就是： def f(x): return x * x 关键字lambda表示匿名函数，冒号前面的x表示函数参数。 匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数： &amp;gt;&amp;gt;&amp;gt; f = lambda x: x * x &amp;gt;&amp;gt;&amp;gt; f &amp;lt;function &amp;lt;lambda&amp;gt; at 0x101c6ef28&amp;gt; &amp;gt;&amp;gt;&amp;gt; f(5) 25 同样，也可以把匿名函数作为返回值返回，比如： def build(x, y): return lambda: x * x &#43; y * y 装饰器(decorator) 在面向对象（OOP）的设计模式中，decorator 被称为装饰模式。OOP 的装饰模式需要通过继承和组合来实现，而 Python 除了能支持 OOP 的 decorator 外，直接从语法层次支持 decorator。Python 的 decorator 可以用函数实现，也可以用类实现。decorator 可以增强函数的功能，定义起来虽然有点复杂，但使用起来非常灵活和方便。 由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。 &amp;gt;&amp;gt;&amp;gt; def now(): ... print(&#39;2015-3-25&#39;) ... &amp;gt;&amp;gt;&amp;gt; f = now &amp;gt;&amp;gt;&amp;gt; f() 2015-3-25 函数对象有一个__name__属性，可以得到函数的名字： &amp;gt;&amp;gt;&amp;gt; now.__name__ &#39;now&#39; &amp;gt;&amp;gt;&amp;gt; f.__name__ &#39;now&#39; 现在，假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为装饰器（Decorator）。 本质上，decorator 就是一个返回函数的高阶函数，其参数为指定的函数名称，返回值为一个函数，目的是在不改变输入函数功能的前提下对原始函数进行功能增加。所以，我们要定义一个能打印日志的 decorator，可以定义如下： def log(func): def wrapper(*args, **kw): print(&#39;call %s():&#39; % func.__name__) return func(*args, **kw) return wrapper 观察上面的log()，因为它是一个 decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助 Python 的@语法，把 decorator 置于函数的定义处： @log def now(): print(&#39;2015-3-25&#39;) 调用now()函数，不仅会运行now()函数本身，还会在运行now()函数前打印一行日志： &amp;gt;&amp;gt;&amp;gt; now() call now(): 2015-3-25 把@log放到now()函数的定义处，相当于执行了语句： now = log(now) 由于log()是一个 decorator，返回一个函数，所以，原来的now()函数仍然存在，只是现在同名的now变量指向了新的函数，于是调用now()将执行新函数，即在log()函数中返回的wrapper()函数。 wrapper()函数的参数定义是(*args, **kw)，因此，wrapper()函数可以接受任意参数的调用。在wrapper()函数内，首先打印日志，再紧接着调用原始函数。 如果 decorator 本身需要传入参数，那就需要编写一个返回 decorator 的高阶函数，写出来会更复杂。比如，要自定义log()的文本： def log(text): def decorator(func): def wrapper(*args, **kw): print(&#39;%s %s():&#39; % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 这个 3 层嵌套的 decorator 用法如下： @log(&#39;execute&#39;) def now(): print(&#39;2015-3-25&#39;) 执行结果如下： &amp;gt;&amp;gt;&amp;gt; now() execute now(): 2015-3-25 和两层嵌套的 decorator 相比，3 层嵌套的效果是这样的： &amp;gt;&amp;gt;&amp;gt; now = log(&#39;execute&#39;)(now) 我们来剖析上面的语句，首先执行log(&#39;execute&#39;)，返回的是decorator()函数，再调用返回的函数，参数是now()函数，返回值最终是wrapper()函数。 提醒：以上两种 decorator 的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有__name__等属性，但你去看经过 decorator 装饰之后的函数，它们的__name__已经从原来的&#39;now&#39;变成了&#39;wrapper&#39;： &amp;gt;&amp;gt;&amp;gt; now.__name__ &#39;wrapper&#39; 因为返回的那个wrapper()函数名字就是&#39;wrapper&#39;，所以，需要把原始函数的__name__等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错。 不需要编写wrapper.__name__ = func.__name__这样的代码，Python 内置的functools.wraps就是干这个事的，所以，一个完整的 decorator 的写法如下： import functools def log(func): @functools.wraps(func) def wrapper(*args, **kw): print(&#39;call %s():&#39; % func.__name__) return func(*args, **kw) return wrapper 或者针对带参数的 decorator： import functools def log(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print(&#39;%s %s():&#39; % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator import functools是导入functools模块。模块的概念稍候讲解。现在，只需记住在定义wrapper()的前面加上@functools.wraps(func)即可。 偏函数 当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。 Python 的functools模块提供了很多有用的功能，其中一个就是偏函数（Partial function）。要注意，这里的偏函数和数学意义上的偏函数不一样。 在介绍函数参数的时候，我们讲到，通过设定参数的默认值，可以降低函数调用的难度。而偏函数也可以做到这一点。举例如下： int()函数可以把字符串转换为整数，当仅传入字符串时，int()函数默认按十进制转换： &amp;gt;&amp;gt;&amp;gt; int(&#39;12345&#39;) 12345 但int()函数还提供额外的base参数，默认值为10。如果传入base参数，就可以做N进制的转换： &amp;gt;&amp;gt;&amp;gt; int(&#39;12345&#39;, base=8) 5349 &amp;gt;&amp;gt;&amp;gt; int(&#39;12345&#39;, 16) 74565 假设要转换大量的二进制字符串，每次都传入int(x, base=2)非常麻烦，于是，我们想到，可以定义一个int2()的函数，默认把base=2传进去： def int2(x, base=2): return int(x, base) 这样，我们转换二进制就非常方便了： &amp;gt;&amp;gt;&amp;gt; int2(&#39;1000000&#39;) 64 &amp;gt;&amp;gt;&amp;gt; int2(&#39;1010101&#39;) 85 functools.partial就是帮助我们创建一个偏函数的，不需要我们自己定义int2()，可以直接使用下面的代码创建一个新的函数int2： &amp;gt;&amp;gt;&amp;gt; import functools &amp;gt;&amp;gt;&amp;gt; int2 = functools.partial(int, base=2) &amp;gt;&amp;gt;&amp;gt; int2(&#39;1000000&#39;) 64 &amp;gt;&amp;gt;&amp;gt; int2(&#39;1010101&#39;) 85 所以，简单总结functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 注意到上面的新的int2()函数，仅仅是把base参数重新设定默认值为2，但也可以在函数调用时传入其他值： &amp;gt;&amp;gt;&amp;gt; int2(&#39;1000000&#39;, base=10) 1000000 提醒：最后，创建偏函数时，实际上可以接收函数对象、*args和**kw这3个参数，当传入： int2 = functools.partial(int, base=2) 实际上固定了int()函数的关键字参数base，也就是： int2(&#39;10010&#39;) 相当于： kw = { &#39;base&#39;: 2 } int(&#39;10010&#39;, **kw) 当传入： max2 = functools.partial(max, 10) 实际上会把10作为*args的一部分自动加到左边，也就是： max2(5, 6, 7) 相当于： args = (10, 5, 6, 7) max(*args) 结果为10。 这里的相关参数和变量指什么？ ↩ 指sum函数中用到了args？ ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[《廖雪峰 Python 教程》笔记 1：基础语法、函数]]></title>
    	<url>/tech/2017/06/13/python/</url>
		<content type="text"><![CDATA[[原文地址：http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949d Python 简介 Python解释器 CPython: 官方版本的解释器。这个解释器是用 C 语言开发的，所以叫 CPython。在命令行下运行 Python 就是启动 CPython 解释器。 IPython: CPython 之上的一个交互式解释器，也就是说，IPython 只是在交互方式上有所增强，但是执行 Python 代码的功能和 CPython 是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了 IE。 CPython 用 &amp;gt;&amp;gt;&amp;gt; 作为提示符，而 IPython 用 In [序号]: 作为提示符。 Jython: 运行在 Java 平台上的 Python 解释器，可以直接把 Python 代码编译成 Java 字节码执行。 IronPython: 和 Jython 类似，只不过 IronPython 是运行在微软 .Net 平台上的 Python 解释器，可以直接把 Python 代码编译成 .Net 的字节码。 Jupyter Notebook: 此前被称为IPython notebook，是一个交互式笔记本，支持运行 40 多种编程语言。本质是一个 Web 应用程序，便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和 markdown。 用途包括数据清理和转换、数值模拟、统计建模、机器学习等等。 第一个 Python 程序 exit()退出 Python。 python hello.py执行程序。 提醒：能不能像.exe文件那样直接运行.py文件呢？在 Windows 上是不行的，但是，在 Mac 和 Linux 上是可以的，方法是在.py文件的第一行加上一个特殊的注释： #!/usr/bin/env python3 print(&#39;hello, world&#39;) 然后，通过命令给hello.py以执行权限： $ chmod a&#43;x hello.py Python代码运行助手 print()会依次打印每个字符串，遇到逗号,会输出一个空格。 name = input() name = input(&#39;please enter your name: &#39;) print(&#39;hello,&#39;, name) input()返回的数据类型是str，str不能直接和整数比较，必须先把str转换成整数。Python 提供了int()函数来完成这件事情。 Python 基础语法 预备知识 注释：用#开头，行内最后的注释也有用//的； Python 大小写敏感； 代码段由缩进控制，因此复制可能导致意外发生，为了尽量减少意外，可能需要少用多重缩进，并且习惯上用将 1 个tab对应为 4 个空格； 注意if和else后面的:，这表明接下来是一个block； 转义：\用于字符的转义，而%%则用于占位符情形下%的转义；r&#39;&#39;用于简化转义；如果字符串内部有很多换行，用\n写在一行里不好阅读，因此可以用&#39;&#39;&#39;...&#39;&#39;&#39;定义多行文本，类似的，用r&#39;&#39;&#39;...&#39;&#39;&#39;定义不必转义的多行文本，关于字符串中的&#39;和&amp;quot;，如果只有一种，则用另一种引括即可，如果两种都有，就需要用\&#39;和\&amp;quot;进行转义； 占位符：%d对应整数，%f对应浮点数，%s对应字符串，%x对应十六进制数，其中%s是万金油，而%f还可以指定显示格式，如%.1f对应只显示 1 位小数的浮点数； 基础语法 常见运算符与常量：&#43;, -, *, **, /, //, True, False, and, or, not, &amp;amp;, |, !=, &amp;gt;=, &amp;lt;=, None, PI(常量经常用大写), /, //(地板除), %(余数)； Python 的整数没有大小限制，而某些语言的整数根据其存储长度是有大小限制的，例如 Java 对 32 位整数的范围限制在-2147483648-2147483647。 Python 的浮点数也没有大小限制，但是超出一定范围就直接表示为inf（无限大）。 数据类型 基础的类型有字符串、整数、浮点数、布尔值； 复杂的类型有list, tuple，其中的list类似于 Matlab 中的cell类型，用[]进行定义，tuple中的内容一旦定义，不能再修改，用()定义1； 其它还有dict, set，其中dict对应key-value对，如d = {&#39;Michael&#39;: 95, &#39;Bob&#39;: 75, &#39;Tracy&#39;: 85}所示，用{:,}的方式定义，包含get()与pop()方法，如&#39;Thomas&#39; in d所示，还可以用in这一操作；set和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key，用set([,])方式定义，可以使用add()和remove()方法，参考下面的例子： &amp;gt;&amp;gt;&amp;gt; s = set([1, 1, 2, 2, 3, 3]) &amp;gt;&amp;gt;&amp;gt; s {1, 2, 3} 字符串 对于字符串而言，在内存中统一为 Unicode，但是具体到终端时，可能是各种类型的编码； 对于单个字符的编码，Python 提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符； 如果知道字符的整数编码，还可以用十六进制这么写 str：&#39;\u4e2d\u6587&#39;，这其实对应于字符串中文； 由于 Python 的字符串类型是str，在内存中以 Unicode 表示，一个字符对应若干个字节（英文字母是 1 个字节，汉字是 3 个字节）。如果要在网络上传输，或者保存到磁盘上，就需要把str变为以字节为单位的bytes。Python 对bytes类型的数据用带b前缀的单引号或双引号表示：x = b&#39;ABC&#39;，要注意区分&#39;ABC&#39;和b&#39;ABC&#39;，前者是str，后者虽然内容显示得和前者一样，但bytes的每个字符都只占用一个字节2。 以 Unicode 表示的str通过encode()方法可以编码为指定的bytes，例如： &amp;gt;&amp;gt;&amp;gt; &#39;ABC&#39;.encode(&#39;ascii&#39;) b&#39;ABC&#39; &amp;gt;&amp;gt;&amp;gt; &#39;中文&#39;.encode(&#39;utf-8&#39;) b&#39;\xe4\xb8\xad\xe6\x96\x87&#39; &amp;gt;&amp;gt;&amp;gt; &#39;中文&#39;.encode(&#39;ascii&#39;) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode characters in position 0-1: ordinal not in range(128) 纯英文的str可以用 ASCII 编码为bytes，内容是一样的，含有中文的str可以用 UTF-8 编码为bytes。含有中文的str无法用 ASCII 编码，因为中文编码的范围超过了 ASCII 编码的范围，Python 会报错。 在bytes中，无法显示为 ASCII 字符的字节，用\x##显示。 反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是bytes。要把bytes变为str，就需要用decode()方法： &amp;gt;&amp;gt;&amp;gt; b&#39;ABC&#39;.decode(&#39;ascii&#39;) &#39;ABC&#39; &amp;gt;&amp;gt;&amp;gt; b&#39;\xe4\xb8\xad\xe6\x96\x87&#39;.decode(&#39;utf-8&#39;) &#39;中文&#39; 要计算 str 包含多少个字符，可以用len()函数： &amp;gt;&amp;gt;&amp;gt; len(&#39;ABC&#39;) 3 &amp;gt;&amp;gt;&amp;gt; len(&#39;中文&#39;) 2 len()函数计算的是str的字符数，如果换成bytes，len()函数就计算字节数： &amp;gt;&amp;gt;&amp;gt; len(b&#39;ABC&#39;) 3 &amp;gt;&amp;gt;&amp;gt; len(b&#39;\xe4\xb8\xad\xe6\x96\x87&#39;) 6 &amp;gt;&amp;gt;&amp;gt; len(&#39;中文&#39;.encode(&#39;utf-8&#39;)) 6 可见，1 个中文字符经过 UTF-8 编码后通常会占用 3 个字节，而 1 个英文字符只占用 1 个字节。 在操作字符串时，我们经常遇到str和bytes的互相转换。为了避免乱码问题，应当始终坚持使用 UTF-8 编码对str和bytes进行转换。 List 和 tuple list：是一种有序的集合，可以随时添加[append, insert(i, value)]和删除[pop, pop(i)]其中的元素，list中的元素从0号开始，其中-1表示最后一个元素，索引越界时，会抛出IndexError错误；list可以作为另一个list的元素，此时的访问方法是myList[i][j]，初始的赋值用classmates = [&#39;Michael&#39;, &#39;Bob&#39;, &#39;Tracy&#39;]完成，注意其中用的是[]，之后操纵元素时用的是函数的()； tuple：和list非常类似，但是tuple一旦初始化就不能修改，初始的赋值用classmates = (&#39;Michael&#39;, &#39;Bob&#39;, &#39;Tracy&#39;)完成，注意其中用的是()，并且之后元素的不可改变3。tuple所谓的不变是说，tuple的每个元素，指向永远不变。即指向&#39;a&#39;，就不能改成指向&#39;b&#39;，指向一个list，就不能改成指向其他对象，但指向的这个list本身是可变的！ dict 和 list 注意dict内部存放的顺序和key放入的顺序没有关系。和list比较，dict有以下几个特点： 查找和插入的速度极快，不会随着key的增加而变慢； 需要占用大量的内存，内存浪费多。 而list相反，所以，dict是用空间来换取时间的一种方法。 dict可以用在需要高速查找的很多地方，在 Python 代码中几乎无处不在，正确使用dict非常重要，需要牢记的第一条就是dict的key必须是不可变对象。 循环、判断 if 语句 if &amp;lt;条件判断1&amp;gt;: &amp;lt;执行1&amp;gt; elif &amp;lt;条件判断2&amp;gt;: &amp;lt;执行2&amp;gt; elif &amp;lt;条件判断3&amp;gt;: &amp;lt;执行3&amp;gt; else: &amp;lt;执行4&amp;gt; 循环语句：for 和 while for...in循环，依次把list或tuple中的每个元素迭代出来 sum = 0 for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: sum = sum &#43; x print(sum) 如果要计算 1-100 的整数之和，从 1 写到 100 有点困难，幸好 Python 提供一个range()函数，可以生成一个整数序列，再通过list()函数可以转换为list。比如range(5)生成的序列是从 0 开始小于 5 的整数： &amp;gt;&amp;gt;&amp;gt; list(range(5)) [0, 1, 2, 3, 4] range(101)就可以生成 0-100 的整数序列，计算如下： sum = 0 for x in range(101): sum = sum &#43; x print(sum) 从上面的例子看业，range()生成的序列，即使不转换成list，也可以循环操作。 while循环，只要条件满足，就不断循环，条件不满足时退出循环。比如我们要计算 100 以内所有奇数之和，可以用 while 循环实现： sum = 0 n = 99 while n &amp;gt; 0: sum = sum &#43; n n = n - 2 print(sum) 可以用break以及continue跳出循环，有些时候，如果代码写得有问题，会让程序陷入死循环，也就是永远循环下去。这时可以用Ctrl&#43;C退出程序，或者强制结束 Python 进程。 函数 内置函数列表：https://docs.python.org/3/library/functions.html 类型转换：int(), float(), str(), bool(), hex(), enumerate()4, iter()5, list()6； 类型检查：isinstance(x, str)； 是否包含关键字：in； 定义函数 在 Python 中，定义一个函数要使用def语句，依次写出函数名、括号、括号中的参数和冒号:，然后，在缩进块中编写函数体，函数的返回值用return语句返回。如果没有return语句，函数执行完毕后也会返回结果，只是结果为None。return None可以简写为return。 def my_abs(x): if x &amp;gt;= 0: return x else: return -x 如果已经把my_abs()的函数定义保存为abstest.py文件了，那么，可以在该文件的当前目录下启动 Python 解释器，用from abstest import my_abs来导入my_abs()函数，注意abstest是文件名（不含.py扩展名）。 更稳健的函数 空函数：如果想定义一个什么事也不做的空函数，可以用pass语句。 def nop(): pass 参数检查：调用函数时，如果参数个数不对，Python 解释器会自动检查出来，并抛出TypeError，但是如果参数类型不对，Python 解释器就无法帮我们检查。数据类型检查可以用内置函数isinstance()实现。 def my_abs(x): if not isinstance(x, (int, float)): raise TypeError(&#39;bad operand type&#39;) if x &amp;gt;= 0: return x else: return -x 返回多个值：返回值是一个tuple！但是，在语法上，返回一个tuple可以省略括号，而多个变量可以同时接收一个tuple，按位置赋给对应的值，所以，Python 的函数返回多值其实就是返回一个tuple，但写起来更方便。 import math def move(x, y, step, angle=0): nx = x &#43; step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny &amp;gt;&amp;gt;&amp;gt; x, y = move(100, 100, 60, math.pi / 6) &amp;gt;&amp;gt;&amp;gt; print(x, y) 151.96152422706632 70.0 &amp;gt;&amp;gt;&amp;gt; r = move(100, 100, 60, math.pi / 6) &amp;gt;&amp;gt;&amp;gt; print(r) (151.96152422706632, 70.0) 函数的参数 Python 的函数定义非常简单，但灵活度却非常大。除了正常定义的必选参数外，还可以使用默认参数、可变参数、关键字参数和命名关键字参数，使得函数定义出来的接口，不但能处理复杂的参数，还可以简化调用者的代码。下面是关于函数参数的一些重要规则的总结： 默认参数必须指向不变对象； 可变参数内部是()-tuple，关键字参数内部是{}-dict； 可变参数在语法上是*，关键字参数是**，命名关键字参数是单独的 *，如果有可变参数，则命名关键字参数不再需要单独的使用特殊分隔符*； 可变参数本身调用时，参数可以是list-()或tuple-[]； 对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的，但是这种做法容易引起混淆，不推荐； 参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 默认参数(必须指向不变对象) def power(x, n=2): s = 1 while n &amp;gt; 0: n = n - 1 s = s * x return s 必选参数在前，默认参数在后，否则 Python 的解释器会报错；当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。 默认参数降低了函数调用的难度，而一旦需要更复杂的调用时，又可以传递更多的参数来实现。无论是简单调用还是复杂调用，函数只需要定义一个。有多个默认参数时，调用的时候，既可以按顺序提供默认参数，比如调用enroll(&#39;Bob&#39;, &#39;M&#39;, 7)，意思是，除了name，gender这两个参数外，最后 1 个参数应用在参数age上，city参数由于没有提供，仍然使用默认值。也可以不按顺序提供部分默认参数。当不按顺序提供部分默认参数时，需要把参数名写上。比如调用enroll(&#39;Adam&#39;, &#39;M&#39;, city=&#39;Tianjin&#39;)，意思是，city参数用传进去的值，其他默认参数继续使用默认值。 警告：默认参数很有用，但使用不当，会有很大的麻烦。 下面的例子先定义一个函数，传入一个list，添加一个&#39;END&#39;再返回： def add_end(L=[]): L.append(&#39;END&#39;) return L 正常调用时，结果似乎不错： &amp;gt;&amp;gt;&amp;gt; add_end([1, 2, 3]) [1, 2, 3, &#39;END&#39;] &amp;gt;&amp;gt;&amp;gt; add_end([&#39;x&#39;, &#39;y&#39;, &#39;z&#39;]) [&#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;END&#39;] 使用默认参数调用时，一开始结果也是对的： &amp;gt;&amp;gt;&amp;gt; add_end() [&#39;END&#39;] 但是，再次调用add_end()时，结果就不对了： &amp;gt;&amp;gt;&amp;gt; add_end() [&#39;END&#39;, &#39;END&#39;] &amp;gt;&amp;gt;&amp;gt; add_end() [&#39;END&#39;, &#39;END&#39;, &#39;END&#39;] 默认参数是[]，但是函数似乎每次都记住了上次添加了&#39;END&#39;后的list。 原因是 Python 函数在定义的时候，默认参数L的值就被计算出来了，即[]，因为默认参数L也是一个变量，它指向对象[]，每次调用该函数，如果改变了L的内容，则下次调用时，默认参数的内容就变了，不再是函数定义时的[]了。 提醒：默认参数必须指向不变对象！ 要修改上面的例子，我们可以用None这个不变对象来实现： def add_end(L=None): if L is None: L = [] L.append(&#39;END&#39;) return L 现在，无论调用多少次，都不会有问题： &amp;gt;&amp;gt;&amp;gt; add_end() [&#39;END&#39;] &amp;gt;&amp;gt;&amp;gt; add_end() [&#39;END&#39;] 为什么要设计str、None这样的不变对象呢？因为不变对象一旦创建，对象内部的数据就不能修改，这样就减少了由于修改数据导致的错误。此外，由于对象不变，多任务环境下同时读取对象不需要加锁，同时读一点问题都没有。在编写程序时，如果可以设计一个不变对象，那就尽量设计成不变对象。 可变参数(函数内部为 tuple，定义和引用都可以用 *) 要定义出这种函数，必须确定输入的参数。由于参数个数不确定，首先想到可以把a，b，c……作为一个list或tuple传进来，但是调用的时候，需要先组装出一个list或tuple，如果利用可变参数，调用函数的方式可以简化。 def calc(*numbers): sum = 0 for n in numbers: sum = sum &#43; n * n return sum 定义可变参数和定义一个list或tuple参数相比，仅仅在参数前面加了一个*号。在函数内部，参数numbers接收到的是一个tuple，因此，函数代码完全不变。但是，调用该函数时，可以传入任意个参数，包括 0 个参数。如果已经有一个list或者tuple，可以将其中的元素一个一个传递给可变参数函数，但是太繁琐，所以 Python 允许在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去，这种写法相当有用，而且很常见。 &amp;gt;&amp;gt;&amp;gt; nums = [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; calc(*nums) 14 关键字参数(函数内部为 dict，定义和引用都可以用 **) 可变参数允许你传入 0 个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入 0 个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例： def person(name, age, **kw): print(&#39;name:&#39;, name, &#39;age:&#39;, age, &#39;other:&#39;, kw) 函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数： &amp;gt;&amp;gt;&amp;gt; person(&#39;Michael&#39;, 30) name: Michael age: 30 other: {} 也可以传入任意个数的关键字参数： &amp;gt;&amp;gt;&amp;gt; person(&#39;Bob&#39;, 35, city=&#39;Beijing&#39;) name: Bob age: 35 other: {&#39;city&#39;: &#39;Beijing&#39;} &amp;gt;&amp;gt;&amp;gt; person(&#39;Adam&#39;, 45, gender=&#39;M&#39;, job=&#39;Engineer&#39;) name: Adam age: 45 other: {&#39;gender&#39;: &#39;M&#39;, &#39;job&#39;: &#39;Engineer&#39;} 关键字参数有什么用？它可以扩展函数的功能。比如，在person函数里，我们保证能接收到name和age这两个参数，但是，如果调用者愿意提供更多的参数，我们也能收到。试想你正在做一个用户注册的功能，除了用户名和年龄是必填项外，其他都是可选项，利用关键字参数来定义这个函数就能满足注册的需求。 和可变参数类似，也可以先组装出一个dict，然后，把该dict转换为关键字参数传进去： &amp;gt;&amp;gt;&amp;gt; extra = {&#39;city&#39;: &#39;Beijing&#39;, &#39;job&#39;: &#39;Engineer&#39;} &amp;gt;&amp;gt;&amp;gt; person(&#39;Jack&#39;, 24, city=extra[&#39;city&#39;], job=extra[&#39;job&#39;]) name: Jack age: 24 other: {&#39;city&#39;: &#39;Beijing&#39;, &#39;job&#39;: &#39;Engineer&#39;} 当然，上面复杂的调用可以用简化的写法7： &amp;gt;&amp;gt;&amp;gt; extra = {&#39;city&#39;: &#39;Beijing&#39;, &#39;job&#39;: &#39;Engineer&#39;} &amp;gt;&amp;gt;&amp;gt; person(&#39;Jack&#39;, 24, **extra) name: Jack age: 24 other: {&#39;city&#39;: &#39;Beijing&#39;, &#39;job&#39;: &#39;Engineer&#39;} **extra表示把extra这个dict的所有key-value用关键字参数传入到函数的**kw参数，kw将获得一个dict，注意kw获得的dict是extra的一份拷贝，对kw的改动不会影响到函数外的extra。 命名关键字参数(限制参数名称，单独的 *) 对于关键字参数，函数的调用者可以传入任意不受限制的关键字参数。至于到底传入了哪些，就需要在函数内部通过kw检查。 仍以person()函数为例，我们希望检查是否有city和job参数： // 这种做法更灵活 def person(name, age, **kw): if &#39;city&#39; in kw: # 有city参数 pass if &#39;job&#39; in kw: # 有job参数 pass print(&#39;name:&#39;, name, &#39;age:&#39;, age, &#39;other:&#39;, kw) 但是调用者仍可以传入不受限制的关键字参数： &amp;gt;&amp;gt;&amp;gt; person(&#39;Jack&#39;, 24, city=&#39;Beijing&#39;, addr=&#39;Chaoyang&#39;, zipcode=123456) 如果要限制关键字参数的名字，就可以用命名关键字参数，例如，只接收city和job作为关键字参数。这种方式定义的函数如下： // 这种做法更严谨 def person(name, age, *, city, job): print(name, age, city, job) 定义：和关键字参数**kw不同，命名关键字参数需要一个特殊分隔符*，*后面的参数被视为命名关键字参数。 调用方式如下： &amp;gt;&amp;gt;&amp;gt; person(&#39;Jack&#39;, 24, city=&#39;Beijing&#39;, job=&#39;Engineer&#39;) Jack 24 Beijing Engineer 提示：如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符*了： def person(name, age, *args, city, job): print(name, age, args, city, job) 警告：命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错： &amp;gt;&amp;gt;&amp;gt; person(&#39;Jack&#39;, 24, &#39;Beijing&#39;, &#39;Engineer&#39;) Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; TypeError: person() takes 2 positional arguments but 4 were given 由于调用时缺少参数名city和job，Python 解释器把这 4 个参数均视为位置参数，但person()函数仅接受 2 个位置参数。 命名关键字参数可以有缺省值，从而简化调用： def person(name, age, *, city=&#39;Beijing&#39;, job): print(name, age, city, job) 由于命名关键字参数city具有默认值，调用时，可不传入city参数： &amp;gt;&amp;gt;&amp;gt; person(&#39;Jack&#39;, 24, job=&#39;Engineer&#39;) Jack 24 Beijing Engineer 使用命名关键字参数时，要特别注意，如果没有可变参数，就必须加一个*作为特殊分隔符。如果缺少*，Python 解释器将无法识别位置参数和命名关键字参数： def person(name, age, city, job): # 缺少 *，city和job被视为位置参数 pass ```` ### 不同类型参数的优先级 &amp;gt; **提示**：在 Python 中定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这 5 种参数都可以组合使用。但是请注意，参数定义的顺序必须是：`必选参数`、`默认参数`、`可变参数`、`命名关键字参数`和`关键字参数`。 比如定义一个函数，包含上述若干种参数： ```python def f1(a, b, c=0, *args, **kw): print(&#39;a =&#39;, a, &#39;b =&#39;, b, &#39;c =&#39;, c, &#39;args =&#39;, args, &#39;kw =&#39;, kw) def f2(a, b, c=0, *, d, **kw): print(&#39;a =&#39;, a, &#39;b =&#39;, b, &#39;c =&#39;, c, &#39;d =&#39;, d, &#39;kw =&#39;, kw) 在函数调用的时候，Python 解释器自动按照参数位置和参数名把对应的参数传进去。 &amp;gt;&amp;gt;&amp;gt; f1(1, 2) a = 1 b = 2 c = 0 args = () kw = {} &amp;gt;&amp;gt;&amp;gt; f1(1, 2, c=3) a = 1 b = 2 c = 3 args = () kw = {} &amp;gt;&amp;gt;&amp;gt; f1(1, 2, 3, &#39;a&#39;, &#39;b&#39;) a = 1 b = 2 c = 3 args = (&#39;a&#39;, &#39;b&#39;) kw = {} // 这里是直接一个一个参数的传递 &amp;gt;&amp;gt;&amp;gt; f1(1, 2, 3, &#39;a&#39;, &#39;b&#39;, x=99) a = 1 b = 2 c = 3 args = (&#39;a&#39;, &#39;b&#39;) kw = {&#39;x&#39;: 99} &amp;gt;&amp;gt;&amp;gt; f2(1, 2, d=99, ext=None) a = 1 b = 2 c = 0 d = 99 kw = {&#39;ext&#39;: None} // c 用默认值，d 对应可变参数, ext 对应关键字参数 最神奇的是通过一个tuple和dict，你也可以调用上述函数： &amp;gt;&amp;gt;&amp;gt; args = (1, 2, 3, 4) &amp;gt;&amp;gt;&amp;gt; kw = {&#39;d&#39;: 99, &#39;x&#39;: &#39;#&#39;} &amp;gt;&amp;gt;&amp;gt; f1(*args, **kw) a = 1 b = 2 c = 3 args = (4,) kw = {&#39;d&#39;: 99, &#39;x&#39;: &#39;#&#39;} // 直接用 list 或 tuple 作为参数 // 注意 args = (4,) 后面有一个逗号，(4) 对应一个数 4，而 (4, ) 才对应只有一个元素 4 的 tuple &amp;gt;&amp;gt;&amp;gt; args = (1, 2, 3) &amp;gt;&amp;gt;&amp;gt; kw = {&#39;d&#39;: 88, &#39;x&#39;: &#39;#&#39;} &amp;gt;&amp;gt;&amp;gt; f2(*args, **kw) a = 1 b = 2 c = 3 d = 88 kw = {&#39;x&#39;: &#39;#&#39;} // 这里的 args 和 kw 不是直接对应 * 和 ** // 而是要先满足必选参数的 1,2,3，之后再用 kw 的第一项满足可变参数 // 最后用 kw 的第二项满足关键字参数 所以，对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的8。 递归 教程上说用尾递归的方法可以避免栈溢出，但 Python 也没有提供针对尾递归的优化，考虑到平时用的很少，并且尾递归代码的写法比较麻烦9，所以干脆不要使用。 高级特性 切片 &amp;gt;&amp;gt;&amp;gt; L[0:3] &amp;gt;&amp;gt;&amp;gt; L[:3] // 取前 3 个元素 &amp;gt;&amp;gt;&amp;gt; L[1:3] // 取 1, 2 号元素，0 号不取 &amp;gt;&amp;gt;&amp;gt; L[:10:2] // 前 10 个元素，每两个取 1 个 &amp;gt;&amp;gt;&amp;gt; L[::5] // 所有数，每5个取一个 &amp;gt;&amp;gt;&amp;gt; [:] // 复制 &amp;gt;&amp;gt;&amp;gt; L[-1] // 取倒数第一个元素 &amp;gt;&amp;gt;&amp;gt; L[-2:-1] // 取最后两个元素 &amp;gt;&amp;gt;&amp;gt; L[-10:] // 取最后 10 个元素 &amp;gt;&amp;gt;&amp;gt; &#39;ABCDEFG&#39;[:3] // 字符串也可以视作一个 list &#39;ABC&#39; &amp;gt;&amp;gt;&amp;gt; &#39;ABCDEFG&#39;[::2] &#39;ACEG&#39; 迭代 通常用for ... in完成迭代； 默认情况下，dict迭代的是key。如果要迭代value，可以用for value in d.values()，如果要同时迭代key和value，可以用for k, v in d.items()； 字符串也是可迭代对象，因此，也可以作用于for循环； 通过collections模块的Iterable类型判断一个对象是可迭代对象； &amp;gt;&amp;gt;&amp;gt; from collections import Iterable &amp;gt;&amp;gt;&amp;gt; isinstance(&#39;abc&#39;, Iterable) # str是否可迭代 True &amp;gt;&amp;gt;&amp;gt; isinstance([1,2,3], Iterable) # list是否可迭代 True &amp;gt;&amp;gt;&amp;gt; isinstance(123, Iterable) # 整数是否可迭代 False 可以同时引用了两个变量； &amp;gt;&amp;gt;&amp;gt; for x, y in [(1, 1), (2, 4), (3, 9)]: ... print(x, y) ... 1 1 2 4 3 9 要对list实现类似 Java 那样的下标循环，Python 内置的enumerate()函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身； &amp;gt;&amp;gt;&amp;gt; for i, value in enumerate([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]): ... print(i, value) ... 0 A 1 B 2 C list 生成器 静态 list &amp;gt;&amp;gt;&amp;gt; [x * x for x in range(1, 11)] [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] &amp;gt;&amp;gt;&amp;gt; [x * x for x in range(1, 11) if x % 2 == 0] // 用 if 后，只输出偶数的平方 [4, 16, 36, 64, 100] &amp;gt;&amp;gt;&amp;gt; [m &#43; n for m in &#39;ABC&#39; for n in &#39;XYZ&#39;] [&#39;AX&#39;, &#39;AY&#39;, &#39;AZ&#39;, &#39;BX&#39;, &#39;BY&#39;, &#39;BZ&#39;, &#39;CX&#39;, &#39;CY&#39;, &#39;CZ&#39;] // 使用两层循环，可以生成全排列 &amp;gt;&amp;gt;&amp;gt; import os // 导入 os 模块，模块的概念后面讲到 &amp;gt;&amp;gt;&amp;gt; [d for d in os.listdir(&#39;.&#39;)] // os.listdir 可以列出文件和目录 [&#39;.emacs.d&#39;, &#39;.ssh&#39;, &#39;.Trash&#39;, &#39;Desktop&#39;, &#39;Documents&#39;, &#39;Downloads&#39;, &#39;Library&#39;, &#39;VirtualBox VMs&#39;] &amp;gt;&amp;gt;&amp;gt; d = {&#39;x&#39;: &#39;A&#39;, &#39;y&#39;: &#39;B&#39;, &#39;z&#39;: &#39;C&#39; } // 使用两个变量来生成 list &amp;gt;&amp;gt;&amp;gt; [k &#43; &#39;=&#39; &#43; v for k, v in d.items()] [&#39;y=B&#39;, &#39;x=A&#39;, &#39;z=C&#39;] &amp;gt;&amp;gt;&amp;gt; L = [&#39;Hello&#39;, &#39;World&#39;, &#39;IBM&#39;, &#39;Apple&#39;] // 把一个 list 中所有的字符串变成小写 &amp;gt;&amp;gt;&amp;gt; [s.lower() for s in L] [&#39;hello&#39;, &#39;world&#39;, &#39;ibm&#39;, &#39;apple&#39;] &amp;gt;&amp;gt;&amp;gt; L = [&#39;Hello&#39;, &#39;World&#39;, 18, &#39;Apple&#39;, None] // 非字符串类型没有 lower() 方法，列表生成式会报错 &amp;gt;&amp;gt;&amp;gt; [s.lower() for s in L] Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;listcomp&amp;gt; AttributeError: &#39;int&#39; object has no attribute &#39;lower&#39; &amp;gt;&amp;gt;&amp;gt; x = &#39;abc&#39; // 用内建 isinstance 函数可判断变量是否为字符串 &amp;gt;&amp;gt;&amp;gt; y = 123 &amp;gt;&amp;gt;&amp;gt; isinstance(x, str) True &amp;gt;&amp;gt;&amp;gt; isinstance(y, str) False 动态 list：generator 通过静态 list 生成式，我们可以直接创建一个list。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含 100 万个元素的列表，会占用很大的存储空间，如果仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。 所以，如果list中的元素可以按照某种算法推算出来，就不必创建完整的list，从而节省大量的空间。在 Python 中，这种一边循环一边计算的机制，称为生成器：generator。 方法1(())：要创建一个generator，把一个列表生成式的[]改成()，就创建了一个generator。 &amp;gt;&amp;gt;&amp;gt; L = [x * x for x in range(10)] // list &amp;gt;&amp;gt;&amp;gt; L [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] &amp;gt;&amp;gt;&amp;gt; g = (x * x for x in range(10)) // generator &amp;gt;&amp;gt;&amp;gt; g // 元素无法直接输出 &amp;lt;generator object &amp;lt;genexpr&amp;gt; at 0x1022ef630&amp;gt; &amp;gt;&amp;gt;&amp;gt; next(g) // 使用 next() 才能依次输出元素 0 &amp;gt;&amp;gt;&amp;gt; next(g) 1 不断调用next(g)并不可行，正确的方法是使用for循环，因为generator也是可迭代对象。 &amp;gt;&amp;gt;&amp;gt; g = (x * x for x in range(10)) &amp;gt;&amp;gt;&amp;gt; for n in g: ... print(n) ... 0 1 4 方法2(yield)：如果推算的算法比较复杂，用类似列表生成式的for循环无法实现的时候，可以用函数来实现。 def fib(max): n, a, b = 0, 0, 1 while n &amp;lt; max: print(b) a, b = b, a &#43; b n = n &#43; 1 return &#39;done&#39; 注意其中的a, b = b, a &#43; b相当于： t = (b, a &#43; b) # t 是一个 tuple a = t[0] b = t[1] 但不必显式写出临时变量t就可以赋值，另外这里t只是临时的，所以应该不涉及 tuple 不能修改的问题。可以看出，fib()函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。也就是说，上面的函数和generator仅一步之遥。把fib()函数变成generator，只需要把print(b)改为yield b就可以了： def fib(max): n, a, b = 0, 0, 1 while n &amp;lt; max: yield b a, b = b, a &#43; b n = n &#43; 1 return &#39;done&#39; 简单来讲，如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator。 警告：用for循环调用generator时，得不到generator的return语句的返回值。如果想要得到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中。 &amp;gt;&amp;gt;&amp;gt; g = fib(6) &amp;gt;&amp;gt;&amp;gt; while True: ... try: ... x = next(g) ... print(&#39;g:&#39;, x) ... except StopIteration as e: ... print(&#39;Generator return value:&#39;, e.value) ... break ... g: 1 g: 1 g: 2 g: 3 g: 5 g: 8 Generator return value: done 迭代器 Iterable：可以直接用for循环的对象统称为可迭代对象，list、tuple、dict、set、str等都可以用for遍历； Iterator：可以被next()函数调用并不断返回下一个值的对象称为迭代器，方法 1 和方法 2 中用generator生成的都可以用next()操作； 可以使用isinstance()判断一个对象是否是Iterable对象或者Iterator对象。 list、dict、str虽然Iterable，却不是Iterator，把list、dict、str等Iterable变成Iterator可以使用iter()函数： &amp;gt;&amp;gt;&amp;gt; isinstance(iter([]), Iterator) True &amp;gt;&amp;gt;&amp;gt; isinstance(iter(&#39;abc&#39;), Iterator) True Python 的 Iterator 对象表示的是一个数据流，Iterator 对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以 Iterator 的计算是惰性的，只有在需要返回下一个数据时它才会计算。Iterator 甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。 空tuple的定义在语法上要用myTuple(1,)这样的方式，否则用myTuple(1)则认为是 一个元素为1的一维tuple，具体可参考教程说明。 ↩ 这地方稍有歧义，汉字是 3 个字节，但对应于一个字符，因此这种解释只适用于 ASCII 范围内的符号。 ↩ 虽然tuple指向的元素不可改变，但如果其中一个元素是可以改变的list，那么可以通过改变list中元素的内容来达到修改tuple内容的目的，具体参考使用list和tuple。 ↩ 把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身。 ↩ 将iterable转换成iterator。 ↩ 将一个惰性的iterator转换成一个确定的iterable型的list。 ↩ 虽然将一个dict作为参数传递给函数也是可行的，但这样做在函数内部就需要再将dict一个一个单独进行额外处理，而**的方法可以省略这个步骤。 ↩ 这种做法容易引起混淆，个人不太推荐。 ↩ 尾递归是指，在函数返回的时候，调用自身本身，并且，return语句不能包含表达式。这样，编译器或者解释器就可以把尾递归做优化，使递归本身无论调用多少次，都只占用一个栈帧，不会出现栈溢出的情况。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[LaTeX 如何在文档的侧面插入图片实现“绕排” z]]></title>
    	<url>/tech/2017/06/12/wrap-figure/</url>
		<content type="text"><![CDATA[[参考资料： https://www.zhihu.com/question/26837705 https://tex.stackexchange.com/questions/56176/handling-of-wrapfig-pictures-in-latex 买了我书的读者，请看 5.3.5 节「文字绕排」。 本质上绕排功能都是通过在 TeX 中使用 \parshape 命令控制段落形状，挖出一个空洞出来，然后把图表内容填进 排版 TAOCP 就都是手工完成这种绕排工作的。不过手工完成这种工作非常繁琐，在 LaTeX 中通常还是使用现成的宏包工具。 主要宏包 实现文字绕排工具很多，效果各有千秋。按当前版本的时间顺序排是： picins：1992 年 3.0 版，早于 LaTeX2e，因许可证问题现已从 TeXLive 中删除，MiKTeX 仍可用； picinpar：1993 年 1.2a 版，早于 LaTeX2e； floatflt：1997 年 1.31 版，最早版本发布于 1994 年，LaTeX2e 代码； wrapfig：2003 年 3.6 版，最早版本发布于 1991 年，现为 LaTeX2e 代码； cutwin：2010 年 0.1 版，LaTeX2e 代码。 为什么会有这么多宏包？因为绕排问题复杂，要求繁多，并没有哪一个宏包能在各方面完美地解决文字绕排的问题。所以多少需要选着用。 各个宏包的其本用法可以看各自的手册，但注意 picins 的手册是一个文本文件，而 picinpar 的手册要看德文版 pdf（看例子）配合源代码里面的注释。其中 picins、picinpar、wrapfig 的一个集中（但有些过时）的介绍可以看《LaTeX2e 插图指南》的一节：30. 图文混排。 功能差异 我们忽略掉在 TeX Live 下无法使用的老宏包 picins，下面主要考虑一些功能上的差别： wrapfig 代码较新，语法与标准的 figure 环境接近，基本功能比较全面，可以指定宽度高度也可以自动计算，可以设置伸出版心。此外在页面放不下时还有浮动到下一段的功能（这块儿我的书写错了）。但位置限定较多，一般只用在段落开头，图片出现在段落（包括连续几段）的左上角或右上角。 picinpar 位置灵活，水平方向和垂直方向都可以放在一段的中间。但语法比较怪异，也不能直接指定内容大小。 floatflt 功能和语法都与 wrapfig 接近，但可以控制的参数更少。它的特色是对列表 \item 项有特殊处理，并且对奇偶页面可以有不同的位置处理。 cutwin 的特色是对位置的控制最强，不仅可以把图片放在段落的各种不对称位置，而且还支持特殊形状的挖洞，可以自己定义三角形、圆形之类的空洞来放置特殊的图片。但 cutwin 语法麻烦，在其他方面的功能也较弱，比如 cutwin 就没有提供 \caption 功能，只提供了基本的挖洞功能。所以如果有图表标题又不想自己实现，就不要选 cutwin 宏包。而 picins、picinpar、floatflt、wrapfig 都支持图表标题，并且有 caption 宏包支持。 选用小结 在需求简单的情况下，使用 wrapfig 或 floatflt 就可以满足要求，优先选择功能多的 wrapfig； 如果对插入图表的位置有特殊要求，用 picinpar； 如果对位置或形状有较高要求，用 cutwin。 示例 \documentclass{article} \usepackage{graphicx} \usepackage{wrapfig} \usepackage{picinpar} \usepackage{cutwin} \begin{document} \section{wrapfig} \begin{wrapfigure}{l}[1cm]{0pt} \includegraphics[width=3cm]{example-image-a} \caption{wrapfig} \end{wrapfigure} text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text \section{picinpar} \begin{figwindow}[2,c, {\includegraphics[width=3cm]{example-image-b}}, picinpar] text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text \end{figwindow} \section{cutwin} \renewcommand{\windowpagestuff}{% \quad\includegraphics[width=3cm]{example-image-c}} \begin{cutout}{2}{2.2cm}{6.2cm}{6} text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text text \end{cutout} \end{document}]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[随机数生成及其在统计模拟中的应用 z]]></title>
    	<url>/prof/2017/05/26/random-number-generation/</url>
		<content type="text"><![CDATA[[原文地址：https://cosx.org/2017/05/random-number-generation/ 揭秘统计软件如 R，Octave 等使用的随机数发生器，然后做一些统计检验，再将其应用到独立随机变量和的模拟中，最后与符号计算得到的精确结果比较。除特别说明外，文中涉及到的随机数都是指伪随机数，发生器都是指随机数发生器。 1 背景 随机数的产生和检验方法是蒙特卡罗方法的重要部分，另外两个是概率分布抽样方法和降低方差提高效率方法。在 20 世纪 40 年代中期，当时为了原子弹的研制，乌拉姆（S.Ulam）、冯诺依曼（J.von Neumann） 和梅特罗波利斯（N. Metropolis） 在美国核武器研究实验室创立蒙特卡罗方法。当时出于保密的需要，与随机模拟相关的技术就代号“蒙特卡罗”。早期取得的成果有产生随机数的平方取中方法，取舍算法和逆变换法等。这两个算法的内容见统计之都王夜笙的文章。 2 随机数生成 讲随机数发生器，不得不提及一个名为 Mersenne Twister（简称 MT）的发生器，它的周期长达$2^{19937}-1$， 现在是 R 、Octave 和 Matlab 等软件（较新版本）的默认随机数发生器1。 Matlab 通过内置的 rng 函数指定不同的发生器，其中包括 1995 年 Matlab 采用 George Marsaglia 在 1991 年提出的借位减（subtract with borrow，简称 SWB）发生器。在 Matlab 中，设置如下命令可指定发生器及其状态，其中 1234 是随机数种子，指定发生器的状态，目的是重复实验结果，v5uniform 是发生器的名字。 rng(1234, &amp;#39;v5uniform&amp;#39;) Octave 通过内置的 rand 函数指定发生器的不同状态，为获取相同的两组随机数，state 参数的设置一样，如 1234（你也可以设置为别的值）。Octave 已经放弃了老版本内置的发生器，找不到命令去指定早期的发生器，这个和 Matlab 不一样。 rand (&amp;#39;state&amp;#39;,1234) rand(1,5) 0.9664535 0.4407326 0.0074915 0.9109760 0.9392690 rand (&amp;#39;state&amp;#39;,1234) rand(1,5) 0.9664535 0.4407326 0.0074915 0.9109760 0.9392690 Python 的 numpy 模块也采用 MT 发生器，类似地，通过如下方式获得相同的两组随机数 import numpy as np a = np.random.RandomState(1234) a.rand(5) array([ 0.19151945, 0.62210877, 0.43772774, 0.78535858, 0.77997581]) a = np.random.RandomState(1234) a.rand(5) array([ 0.19151945, 0.62210877, 0.43772774, 0.78535858, 0.77997581]) R 的默认发生器也是 MT 发生器，除了设置随机数种子的 seed 参数，还可以通过 kind 参数指定其他发生器，normal.kind 参数指定产生正态分布随机数的发生器，下面也使用类似的方式产生两组完全一样的随机数。 set.seed(seed, kind = NULL, normal.kind = NULL) set.seed(1234,kind = &amp;quot;Mersenne-Twister&amp;quot;, normal.kind = &amp;quot;Inversion&amp;quot;) # 默认参数设置 runif(5) [1] 0.1137034 0.6222994 0.6092747 0.6233794 0.8609154 set.seed(1234,kind = &amp;quot;Mersenne-Twister&amp;quot;, normal.kind = &amp;quot;Inversion&amp;quot;) # 默认参数设置 runif(5) [1] 0.1137034 0.6222994 0.6092747 0.6233794 0.8609154 SWB 发生器中“借位相减”步骤是指序列的第 \(i\) 个随机数$z_{i}$要依据如下递推关系产生，$$z_{i}=z_{i&#43;20}-z_{i&#43;5}-b$$ 下标$i,i&#43;20,i&#43;5$都是对 32 取模的结果，$z_{i&#43;20}$与$z_{i&#43;5}$做减法运算，\(b\) 是借位，其取值与前一步有关，当$z_i$是正值时，下一步将 \(b\) 置为 0，如果计算的$z_i$是负值，在保存$z_i$之前，将其值加 1，并在下一步，将 \(b\) 的值设为$2^{-53}$。 下面关于随机数生成的效率和后面的统计检验，都以生成$2^{24}$个为基准，是 1600 多万个，取这么多，一方面为了比较编程语言实现的发生器产生随机数的效率，另一方面是后面的游程检验需要比较大的样本量。 Matlab 内置的发生器及大部分的函数，底层实现都是 C 或者 Fortran，MathWorks 创始人 Cleve B. Moler 是数值分析领域著名的 LINPACK 和 EISPACK 包的作者之一，他当年做了很多优化和封装，进而推出 Matlab，只要是调用内置函数，效率不会比 C 差，自己写的含有循环、判断等语句的代码，性能就因人而异了，对大多数人来说，性能要比 C 低。这里比较 Matlab 内置SWB发生器（就当作是 C 语言实现的好了）和用 Matlab 重写的 SWB 发生器的效率，代码如下： % matlab code tic % 大约几秒 rng(1234, &amp;#39;v5uniform&amp;#39;) % 调用SWB发生器 x = rand(1,2^24); toc % octave code id = tic % 时间耗费大约一小时 randtx(&amp;#39;state&amp;#39;,0) x = randtx(1,2^24); toc (id) randtx不是 Matlab 和 Octave 内置的函数，而是 Cleve B. Moler 基于 Matlab 实现的 SWB 发生器，也是 100 多行包含嵌套循环等语句的 Matlab 代码打包的函数，上面的代码运行时间差异之大也就不难理解了，为了能在 Octave 上跑，我做了少量修改，Octave 软件版本为 4.2.1，安装 Octave 时，Blas 选择 OpenBlas，为了后续检验，在获得随机数后，将其保存到磁盘文件 random_number.mat save -mat random_number.mat x R，Octave，Matlab 和 Python 内置的发生器都是 MT 发生器，与之实现有关的数学库，也是 Blas，虽然有开源和进一步优化的商业版本之分，但是矩阵乘法，向量乘法之类运算的效率也就半斤八两，Julia 语言官网给出了一个标准测试2。 不同语言的性能表现（C 语言在算法中的表现为基准，时间记为 1.0） 这里再给出用C语言实现的 MT 发生器，产生同样多的随机数，只需要 10 秒左右，其中包含编译和写随机数到文件的时间，实际产生随机数的时间应该远小于这个时间。（程序运行环境环境 Dev-C&#43;&#43; 5.11，用 64 位的 GCC 编译）。 3 统计检验 随机数的检验是有一套标准的，如 George Marsaglia 开发的 DieHard 检验程序，检验的内容很丰富。这篇文章只能算初窥门径，R 内产生真随机数的包是 Dirk Eddelbuettel 开发的 random包，它是连接产生真随机数网站的接口。 3.1 相关性检验 先来一个简单的，就用 R 产生的两个独立同均匀分布样本，调用 cor.test 做相关性检验，看看这两组数是不是足够独立同分布，通过眼球验证，随着样本量增大，相关性趋于 0，相关性弱到可以视为独立。如下图所示 library(ggplot2) library(viridisLite) library(viridis) set.seed(1234) corr &amp;lt;- rep(0, 1000) for(i in seq(from = 1000, to = 1000000, by = 1000)) { corr[i/1000] &amp;lt;- cor.test(runif(i, min = 0, max = 1), runif(i, min = 0, max = 1))$estimate} ggplot(data.frame(x = seq(1000), y = corr), aes(x = x, y = y)) &#43; geom_hex(show.legend = FALSE) &#43; scale_fill_viridis(direction = -1) &#43; xlab(&amp;quot;Sample size *10^3&amp;quot;) &#43; ylab(&amp;quot;Correlation&amp;quot;) image 3.2 分布检验 检验产生的随机数是否服从指定的分布：原假设是样本来自指定的分布，计算的 P 值比较大，就不能拒绝原假设。 ks.test(runif(1000), &amp;quot;punif&amp;quot;) # 分布检验 ## ## One-sample Kolmogorov-Smirnov test ## ## data: runif(1000) ## D = 0.022302, p-value = 0.7025 ## alternative hypothesis: two-sided 检验两样本是否来自同一分布：原假设是两样本来自同一分布，计算的P值比较小，就表示两样本不是来自同一分布。 ks.test(runif(1000), runif(1000)) # 同分布检验 ## ## Two-sample Kolmogorov-Smirnov test ## ## data: runif(1000) and runif(1000) ## D = 0.04, p-value = 0.4005 ## alternative hypothesis: two-sided 从结果来看，R内置的随机数发生器通过了检验（嘿嘿，这是肯定的！！）。 3.3 游程检验 游程检验对随机数序列的随机性检验，不对序列做任何分布假设，不同于上面的相关性检验和省略没讲的特征统计量（如均值和方差）的检验。先对随机性略作解释，简单起见，我们考虑 0-1 序列，抛掷均匀的硬币 1000 次，正面向上记为 1，反面向上记为 0，这是一个离散的均匀分布，每一次抛掷硬币都无法准确地判断出现的是正面还是反面，若记录的序列中 0 和 1 相对集中的出现，不是随机，0 和 1 交替出现，呈现周期性也不是随机，除了这两种情况基本就是随机了。 游程检验的原假设是序列满足随机性，序列一旦生成，就是有序的，因为游程检验需要固定位置，再数上升（下降）的游程数，当计算的 P 值比较大时，不能拒绝原假设，即不能否认这个序列是随机的。对上述 0-1 序列进行模拟，然后检验，如下所示 library(tseries) x &amp;lt;- sample(c(0, 1), 1000, replace = TRUE, prob = c(1/2, 1/2)) runs.test(factor(x)) ## ## Runs Test ## ## data: factor(x) ## Standard Normal = 0.45116, p-value = 0.6519 ## alternative hypothesis: two.sided 现在用游程检验比较 SWB 发生器（Octave/Matlab 版）、MT 发生器（R 语言版）和 MT 发生器（C 语言版）。对于一般的实数序列，得先指定一个阈值，记为 delta，然后比较序列中的值和 delta 的大小关系，这里类似数上升或下降的游程长度（runs length），基于这样一个事实：如果序列表现随机，则序列中两个小于 delta 的值，间隔越远出现的次数越少。可以这样理解，还是以上面抛硬币的例子来说，出现了很多次正面，那么下一次抛掷倾向于出现反面，这是一条人人可接受的经验。 为了把这条经验可视化出来，对序列做如下操作：先给定阈值 delta 为 0.01（也可以取别的值），取出序列中的值小于 delta 的位置，位置序列前面添加 0，再差分，然后每个值减 1，得到新序列，新序列中的值为 0，表明原序列连续两个值小于 delta，新序列中的值为1，表明间隔 1 个数小于 delta，新序列中的值为 2，表明间隔 2 个数小于 delta，依次类推…..统计所有的情况，用直方图显示，这就获得游程长度与间隔的关系图（间隔数取到 100 足可示意）。 library(gridExtra) library(R.matlab) # 游程频数直方图 run_test_fun &amp;lt;- function(x, string, delta) { n &amp;lt;- length(x) len &amp;lt;- diff(c(0, which(x &amp;lt; delta), n &#43; 1)) - 1 ggplot(data.frame(x = len[len &amp;lt; 101]), aes(x, fill = ..count..)) &#43; scale_fill_viridis(direction = -1) &#43; geom_histogram(binwidth = 1, show.legend = FALSE) &#43; xlab(string) &#43; ylab(&amp;quot;&amp;quot;) } set.seed(1234) # R默认采用Mersenne Twister发生器 r_data &amp;lt;- runif(2^24, 0, 1); # R内生成均匀分布随机数 matlabv5_data &amp;lt;- readMat(&amp;quot;random_number.mat&amp;quot;) # 读取Octave生成的均匀分布随机数 temp &amp;lt;- read.table(file = &amp;quot;random_number.txt&amp;quot;) # 读取C语言生成的均匀分布随机数 c_data &amp;lt;- c(as.matrix(t(temp))) p1 &amp;lt;- run_test_fun(x = r_data, string = &amp;quot;R&amp;quot;, delta = 0.01) p2 &amp;lt;- run_test_fun(x = matlabv5_data$x, string = &amp;quot;Matlab v5&amp;quot;, delta = 0.01) p3 &amp;lt;- run_test_fun(x = c_data, string = &amp;quot;C&amp;quot;, delta = 0.01) grid.arrange(p1, p2, p3, ncol=3) image 从图中可以看出 MT 发生器通过了检验，SWT 发生器没有通过，在间隔数为 27 的位置，有一条沟，按规律游程长度不应该减少这么多，这是因为 SWB 发生器“借位减”步骤，取模 32 的运算和 5 一起消耗了间隔为 27 的数量（读者可以按借位减的递推关系思考是如何消耗的），导致不符合随机性的要求，该算法细节参见 Cleve B. Moler的书《Numerical Computing with MATLAB》第 9 章第 267 页 。 4 应用 4.1 两个均匀分布的统计模拟 随机变量 $X_{1},X_{2}\stackrel{iid}{\sim}$某分布（比如二项分布，泊松分布，正态分布，指数分布，卡方分布，伽马分布），则$X_{1}&#43;X_{2}$也服从该分布。常见的均匀分布是否具有这样的可加性？具体地说，就是$X_{1},X_{2}\stackrel{iid}{\sim}U(0,1)$ ，$X_{1}&#43;X_{2}$ 是否服从$U(0,2)$ ？ 如果有一台电脑在旁边，我首先想到的就是敲三五行代码，画个散点图、直方图，看图说话。 set.seed(1234) x &amp;lt;- runif(10000, min = 0, max = 1) y &amp;lt;- runif(10000, min = 0, max = 1) z &amp;lt;- x &#43; y plot(z) # 散点图 hist(z) # 直方图 为美观起见，从 viridis 包调用 viridis 调色板，颜色越深的地方，相应的数值越大，不管是此处 geom_hex 绘制的六角形热图，还是 geom_histogram 绘制的直方图，都遵循这个规律。 ggplot(data.frame(x = seq(10000), y = z), aes(x = x, y = y)) &#43; geom_hex(show.legend = FALSE) &#43; scale_fill_viridis(direction = -1) &#43; xlab(&amp;quot;&amp;quot;) &#43; ylab(&amp;quot;&amp;quot;) image 显然这不是均匀分布，在 $z=1$ 处，散点比较集中，看起来有点像正态分布。如果往中心极限定理上靠，将作如下标准化$$Y_{2}^{\star}=\frac{X_1 &#43; X_2 - 2*\frac{1}{2}}{\sqrt{\frac{1}{12}}*\sqrt{2}}=\sqrt{6}(X_1 &#43; X_2 -1)$$ 则$Y_{2}^{\star}$的期望为 0，方差为 1。 p4 &amp;lt;- ggplot(data.frame(x = z), aes(x, fill = ..count..)) &#43; scale_fill_viridis(direction = -1) &#43; geom_histogram(bins=20, show.legend = FALSE) &#43; xlab(&amp;quot;&amp;quot;) &#43; ylab(&amp;quot;&amp;quot;) p5 &amp;lt;- ggplot(data.frame(x = sqrt(6)*(z-1)), aes(x, fill = ..count..)) &#43; scale_fill_viridis(direction = -1) &#43; geom_histogram(bins = 20, show.legend = FALSE) &#43; xlab(&amp;quot;&amp;quot;) &#43; ylab(&amp;quot;&amp;quot;) grid.arrange(p4, p5, ncol=2) image 只是变换后的图像和之前基本一致，那么现在看来眼球检验不好使了，那就上$P$值呗！ ks.test(sqrt(6)*(z-1), &amp;quot;pnorm&amp;quot;) # 分布检验 ## ## One-sample Kolmogorov-Smirnov test ## ## data: sqrt(6) * (z - 1) ## D = 0.025778, p-value = 3.381e-06 ## alternative hypothesis: two-sided 也不是正态分布，既然如此，那就在两个随机变量的情况下，把精确分布推导出来。 4.2 精确分布的推导及计算 课本如《概率论与数理统计教程》 采用卷积的方法求分布函数，这种方法实行起来比较繁琐，也不利于后续编程，下面考虑用特征函数的方法求。我们知道标准均匀分布的特征函数$$\varphi(t)=\frac{e^{it}-1}{it}$$考虑$X_1$和$X_2$相互独立，它们的和用$S_2$表示，则随机变量$S_2$的特征函数为 $$\varphi_2(t)=\varphi(t)*\varphi(t)=(\frac{e^{it}-1}{it})^2=\frac{2(1-\cos(t))e^{it}}{t^2}$$ 只要满足条件 $$\int_{-\infty}^{&#43;\infty}\vert \varphi_2(t) \vert \mathrm{d} t &amp;lt; \infty$$ $S_2$的密度函数就可以表示为 $$p_2(x)=\frac{1}{2 \pi}\int_{-\infty}^{&#43;\infty}\mathrm{e}^{-itx}\varphi_2(t)\mathrm{d}t$$ 经计算 $$\int_{-\infty}^{&#43;\infty}\vert \varphi_2(t) \vert \mathrm{d} t=4\int_{0}^{&#43;\infty}\frac{1-\cos(t)}{t^2}\mathrm{d}t=4\int_{0}^{&#43;\infty}\big(\frac{\sin(x)}{x}\big)^2\mathrm{d}x=2\pi$$ 那么 $$p_2(x)=\frac{1}{2 \pi}\int_{-\infty}^{&#43;\infty}\mathrm{e}^{-itx}\varphi_2(t)\mathrm{d}t=\frac{2}{\pi}\int_{0}^{&#43;\infty}\frac{(1-\cos(t))\cos(t(1-x))}{t^2}\mathrm{d}t=\frac{2}{\pi}\int_{0}^{&#43;\infty}\cos\big(2(1-x)t\big)\big(\frac{\sin(t)}{t}\big)^2\mathrm{d}t$$ 一般地，\(n\) 个独立随机变量的和 $$\varphi_n(t)=\big(\frac{e^{it}-1}{it}\big)^n=\big(\frac{\sin(t/2)\mathrm{e}^{\frac{it}{2}}}{t/2}\big)^n$$ 那么，同理 $$p_n(x)=\frac{2}{\pi}\int_{0}^{&#43;\infty}\cos\big(2(n/2-x)t\big)(\frac{\sin(t)}{t})^n\mathrm{d}t$$ 要说数值计算一个$p(x)$近似值，是一点问题没有！且看 integrate(function(t,x,n) 2/pi*cos((n-2*x)*t)*(sin(t)/t)^n ,x = 1,n = 2, lower = 0,upper = Inf,subdivisions = 1000) ## 0.9999846 with absolute error &amp;lt; 6.6e-05 那如果要把上面的积分积出来，获得一个精确的表达式，在$n=2$的时候还可以手动计算，主要使用分部积分，余弦积化和差公式和一个狄利克雷积分公式$\int_{0}^{&#43;\infty}\frac{\sin(ax)}{x}\mathrm{d}x=\frac{\pi}{2}\mathrm{sgn}(a)$，过程略，最后算得 $$p_2(x)=\frac{1}{2}\big((2-x)\mathrm{sgn}(2-x)-x\mathrm{sgn}(-x)\big)-(1-x)\mathrm{sgn}(1-x)=\frac{1}{2}(\left | x \right |&#43;\left | x-2 \right |)-\left | x-1 \right |,0&amp;lt;x&amp;lt;2$$ $p_2(x)$的密度函数图象如下： fun_p2_1 &amp;lt;- function(x) { 1 / 2 * (abs(x - 2) - 2 * abs(x - 1) &#43; abs(x)) } fun_p2_2 &amp;lt;- function(x) { x &amp;lt;- as.matrix(x) tempfun &amp;lt;- function(x) { integrate(function(t, x, n) 2 / pi * cos((n - 2 * x) * t) * (sin(t) / t) ^ n, x = x, n = 2,lower = 0, upper = Inf, subdivisions = 1000)$value } return( sapply(x,tempfun) ) } ggplot(data.frame(x = c(0, 2)), aes(x = x)) &#43; stat_function(fun = fun_p2_2, geom = &amp;quot;point&amp;quot;, colour = &amp;quot;#2A768EFF&amp;quot;) &#43; stat_function(fun = fun_p2_1, geom = &amp;quot;line&amp;quot;, colour = &amp;quot;#78D152FF&amp;quot;) image 从图中可以看出，两种形式的密度函数在数值计算的结果上很一致，当 $n=100,1000$ 时，含参量积分的表示形式就很方便啦！任意给定一个 \(n\)，符号计算上面的含参量积分，这个时候还是用软件计算比较合适，R 的符号计算仅限于求导，积分运算需要借助 Ryacas，rSymPy，可惜的是，这些包更新缓慢，即使 $\int_{0}^{&#43;\infty}\frac{\sin(at)}{t}\mathrm{d}t$ 也算不出来，果断直接使用 Python 的 sympy 模块 from sympy import * a=symbols(&amp;#39;a&amp;#39;, real = True) t=symbols(&amp;#39;t&amp;#39;, real = True, positive = True) print(integrate(sin(a*t)/t, (t, 0, oo))) ## Piecewise((pi/2, Eq(Abs(periodic_argument(polar_lift(a)**2, oo)), 0)), (Integral(sin(a*t)/t, (t, 0, oo)), True)) 。。。初次见到这样的结果，是不是一脸mb，翻译一下，就是 $$ \begin{equation*} \begin{cases} \frac{\pi}{2} &amp;amp; \text{for}\: \left|{\operatorname{periodic_{argument}}{\left (\operatorname{polar\_lift}^{2}{\left (a \right )},\infty \right )}}\right| = 0 \\ \int\limits_{0}^{\infty} \frac{1}{t} \sin{\left (a t \right )}\, dt &amp;amp; \text{otherwise} \end{cases} \end{equation*} $$ 稍为好点，但是还是有一大块看不懂，那个绝对值里是什么3？还是不要纠结了，路远坑多，慢走不送啊！话说要是计算$p_2(x)$密度函数里的积分， from sympy import * x=symbols(&amp;#39;x&amp;#39;, real=True) t=symbols(&amp;#39;t&amp;#39;, real=True,positive=True) print(integrate(2/pi*cos(2*t*(1-x))*(sin(t)/t)**2,(t,0,oo))) ## Piecewise((Piecewise((2*x, (2*x - 2)**2/4 &amp;lt; 1), (0, 4/(2*x - 2)**2 &amp;lt; 1), (meijerg(((1/2,), (1, 1, 3/2)), ((1/2, 1, 0), (1/2,)), polar_lift(-2*x &#43; 2)**2/4), True))/2, Eq(Abs(periodic_argument(polar_lift(-2*x &#43; 2)**2, oo)), 0)), (Integral(2*sin(t)**2*cos(2*t*(-x &#43; 1))/(pi*t**2), (t, 0, oo)), True)) 那就更长了。。。 $$ \begin{equation*} \begin{cases} \frac{1}{2} \begin{cases} 2 x &amp;amp; \text{for}\: \frac{1}{4} \left(2 x - 2\right)^{2} &amp;lt; 1 \\ 0 &amp;amp; \text{for}\: \frac{4}{\left(2 x - 2\right)^{2}} &amp;lt; 1 \\ {G_{4, 4}^{3, 1}\left(\begin{matrix} \frac{1}{2} &amp;amp; 1, 1, \frac{3}{2} \\\frac{1}{2}, 1, 0 &amp;amp; \frac{1}{2} \end{matrix} \middle| {\frac{1}{4} \operatorname{polar\_lift}^{2}{\left (- 2 x &#43; 2 \right )}} \right)} &amp;amp; \text{otherwise} \end{cases} &amp;amp; \text{for}\: \left|{\operatorname{periodic_{argument}}{\left (\operatorname{polar\_lift}^{2}{\left (- 2 x &#43; 2 \right )},\infty \right )}}\right| = 0 \\ \int\limits_{0}^{\infty} \frac{2}{\pi t^{2}} \sin^{2}{\left (t \right )} \cos{\left (2 t \left(- x &#43; 1\right) \right )}\, dt &amp;amp; \text{otherwise} \end{cases} \end{equation*} $$ sympy 模块还是比较强的，化简可能比较弱，感觉是我的条件声明没有充分利用，要看懂，得知道一些复变函数的知识，这个时候，可以试试 Maple 或者 Mathematica，面对高昂的费用，我们可以使用在线的免费计算 WolframAlpha（http://www.wolframalpha.com/），输入 integrate 2/pi*cos(2*t*(1-x))*(sin(t)/t)^2 ,t ,0,oo 即可得$p_2(x)=\frac{1}{2}(\left | x-2 \right |-2\left | x-1 \right |&#43;\left | x \right |)$，\(n\) 取任意值都是可以算的，由于式子比较复杂，就不展示了。 5 小结 作者的一些经验感悟: 因为看论文的原因（感觉 MCMC 好像哪都有），接着从随机数生成开始自学 MCMC，一次偶然的机会，去年在北京计算科学研究中心听清华喻文健教授的报告，提到均匀分布的随机数检验，中间也出现了这个图，现在已经记不得是喻教授因为时间原因，没细讲背后的原因，还是自己没听懂，总之只觉得挺有意思的（涉及统计中的游程检验，周围基本都是工科学生，我想我听的更明白些），就记下来，在听报告之前，囫囵地看了康崇禄写的《蒙特卡罗方法理论和应用》的前两章（前两章故事比较多因此看完了），这本书没讲那个例子，却把背后的原因讲明白了（后来细看才知道的）。错位相减算法曾出现在 Matlab，自然就去读 Cleve B. Moler 写的《Numerical Computing with MATLAB》(Revised in 2013)，这本书在文中有出现，也介绍了 Matlab 这么多年内置的随机数发生器的变化史。其实还是推荐看康崇禄那本，不仅因为故事多，而且内容全面和透彻，可以挑自己需要和感兴趣的部分读，也不拘泥于 Matlab。 关于应用部分的举例，源于面试，陷于教材，钟于符号计算。这部分涉及一本广为人知的教材《概率论与数理统计教程》（第二版）茆诗松、程依明和濮晓龙著，这本书给了用卷积求独立随机变量和的例子，后面讲特征函数，说它在求独立随机变量和有优势，但是没有举例，所以正好是补充，而且意外地简洁和统一。符号计算获得精确结果是为了和数值计算的结果比较，之前在统计之都的投名状就是符号计算与 R 语言，但是没有提及 Python 的 sympy，这下也正好合体了。 现在，R、Octave 和 Matlab 这些软件没有单纯用借位相减算法来产生随机数，1995 年后，Matlab 使用延迟斐波那契和移位寄存器的组合发生器，直到 2007 年，Matlab 推出 7.4 版本的时候才采用 MT 发生器。↩ 在官网的 High-Performance JIT Compiler 部分↩ Python 的符号计算模块 sympy 功能比较全，但是化简比较弱，导致结果理解起来不是很方便，比如式子的第一行，看似当$0&amp;lt;x&amp;lt;2$时，$p_{2}(x)=x$是错的，正确的范围应该是$0&amp;lt;x&amp;lt;1$，其实 for 后面的函数 polar_lift() 要求参数大于 \(0\)，这样就没问题了，建议多撸一撸sympy 官方文档。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[使 LaTeX 文稿中的 URL 正确换行z]]></title>
    	<url>/tech/2017/05/17/latex-url-linkbreak/</url>
		<content type="text"><![CDATA[[原文地址：https://liam0205.me/2017/05/17/help-the-url-command-from-hyperref 大部分稍有经验的 LaTeX 用户，都知道使用\url命令在 LaTeX 文稿中插入 URL。更资深一些的用户，会使用hyperref宏包，而不是过时的url宏包来处理。 然而，不论是否资深，大多数用户应该都有遇到过 LaTeX 无法正确对 URL 进行折行的问题。此篇介绍一下如何处理。 TeX 对于断行和分页，是有专门的算法处理的。通常而言，如果一个单词（一整个\url可以看做是一个单词），TeX 不知道从何处进行分词，那么 TeX 就不会在这个单词上断行。对于很长的单词，比如一个\url，如果 TeX 不能在此处断行，而它又处于某一行的末尾，就很容易出现overful box。 因此，本质上，这个问题需要让\url命令，知道我们允许在何处断行。 hyperref 宏包提供了两个宏，\UrlBreaks以及\UrlBigBreaks，用于告知 TeX，用户允许在何处截断 URL 以便换行。二者有一些细微的差别，但此处按下不表——大多数读者只需要使用\UrlBreaks即可。 \documentclass{article} \usepackage{hyperref} \makeatletter \def\UrlAlphabet{% \do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j% \do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t% \do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D% \do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N% \do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X% \do\Y\do\Z} \def\UrlDigits{\do\1\do\2\do\3\do\4\do\5\do\6\do\7\do\8\do\9\do\0} \g@addto@macro{\UrlBreaks}{\UrlOrds} \g@addto@macro{\UrlBreaks}{\UrlAlphabet} \g@addto@macro{\UrlBreaks}{\UrlDigits} \makeatother \begin{document} \url{http://foo.bar.com/documentclassarticleusepackagehyperrefbegindocumenturlenddocument} \end{document} 在这里，\UrlOrds里记录了一些特殊符号（例如-和_），而\UrlAlphabet记录了26个英文字母的大小写，\UrlDigits则记录了 10 个阿拉伯数字。 而后，我们使用 LaTeX 内核提供的\g@addto@marco，依次将上述三个宏的内容，续接在\UrlBreaks之后。这就是说，我们允许在上述所有字符处断行。 如此，编译出的结果也是符合预期的。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Bandit 算法与推荐系统 z]]></title>
    	<url>/data/2017/05/04/bandit-and-recommender-systems/</url>
		<content type="text"><![CDATA[[0.导语 1.什么是bandit算法 1.1 为选择而生 1.2 bandit算法与推荐系统 1.3 怎么选择bandit算法？ 1.4 常用bandit算法 Thompson samplin UCB算法 Epsilon-Greedy算法 朴素bandit算法 2. bandit算法与线性回归 2.1 UCB算法 2.2 UCB算法加入特征信息 2.3 详解LinUCB的实现 2.4 怎么构建特征 原始用户特征 原始文章特征 3. bandit算法与协同过滤 3.1 协同过滤背后的哲学 3.2 bandit结合协同过滤 3.3 COFIBA算法 4. 总结 参考文献 原文地址：https://cosx.org/2017/05/bandit-and-recommender-systems/ 注：本文首发于《程序员》杂志 0.导语 推荐系统里面有两个经典问题：EE问题和冷启动问题。前者涉及到平衡准确和多样，后者涉及到产品算法运营等一系列东西。bandit算法是一种简单的在线学习算法，常常用于尝试解决这两个问题，本文为你介绍基础的bandit算法及一系列升级版，以及对推荐系统这两个经典问题的思考。 1.什么是bandit算法 1.1 为选择而生 我们会遇到很多选择的场景。上哪个大学，学什么专业，去哪家公司，中午吃什么，等等。这些事情，都让选择困难症的我们头很大。那么，有算法能够很好地对付这些问题吗？ 当然有！那就是bandit算法！ MAB问题 bandit算法来源于历史悠久的赌博学，它要解决的问题是这样的1： 一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是多臂赌博机问题(Multi-armed bandit problem, K-armed bandit problem, MAB)。 怎么解决这个问题呢？最好的办法是去试一试，不是盲目地试，而是有策略地快速试一试，这些策略就是bandit算法。 这个多臂问题，推荐系统里面很多问题都与他类似： 假设一个用户对不同类别的内容感兴趣程度不同，那么我们的推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？这就是推荐系统的冷启动。 假设我们有若干广告库存，怎么知道该给每个用户展示哪个广告，从而获得最大的点击收益？是每次都挑效果最好那个么？那么新广告如何才有出头之日？ 我们的算法工程师又想出了新的模型，有没有比A/B test更快的方法知道它和旧模型相比谁更靠谱？ 如果只是推荐已知的用户感兴趣的物品，如何才能科学地冒险给他推荐一些新鲜的物品？ 这些问题本质上全都是关乎如何选择。只要是关于选择，都可以简化成一个多臂赌博机问题，毕竟小赌怡情嘛，人生何处不赌博。 1.2 bandit算法与推荐系统 在推荐系统领域里，有两个比较经典的问题常被人提起，一个是EE问题，另一个是用户冷启动问题。 什么是EE问题？又叫exploit－explore问题。exploit就是：对用户比较确定的兴趣，当然要利用开采迎合，好比说已经挣到的钱，当然要花；explore就是：光对着用户已知的兴趣使用，用户很快会腻，所以要不断探索用户新的兴趣才行，这就好比虽然有一点钱可以花了，但是还得继续搬砖挣钱，不然花完了就得喝西北风。 用户冷启动问题，也就是面对新用户时，如何能够通过若干次实验，猜出用户的大致兴趣。 我想，屏幕前的你已经想到了，推荐系统冷启动可以用bandit算法来解决一部分。 这两个问题本质上都是如何选择用户感兴趣的主题进行推荐，比较符合bandit算法背后的MAB问题。 比如，用bandit算法解决冷启动的大致思路如下： 用分类或者Topic来表示每个用户兴趣，也就是MAB问题中的臂（Arm），我们可以通过几次试验，来刻画出新用户心目中对每个topic的感兴趣概率。 这里，如果用户对某个topic感兴趣（提供了显式反馈或隐式反馈），就表示我们得到了收益，如果推给了它不感兴趣的topic，推荐系统就表示很遗憾(regret)了。 如此经历“选择-观察-更新-选择”的循环，理论上是越来越逼近用户真正感兴趣的topic的。 1.3 怎么选择bandit算法？ 现在来介绍一下bandit算法怎么解决这类问题的。bandit算法需要量化一个核心问题：错误的选择到底有多大的遗憾？能不能遗憾少一些？ 王家卫在《一代宗师》里寄出一句台词： 人生要是无憾，那多无趣？ 而我说：算法要是无憾，那应该是过拟合了。 所以说：怎么衡量不同bandit算法在解决多臂问题上的效果？首先介绍一个概念，叫做累积遗憾(regret)2： \begin{align*} R_T &amp;amp; = \sum_{i=1}^T(w_{opt}-w_{B(i)}) \\ &amp;amp;= T_{w^*}-\sum_{i=1}^Tw_{B(i)} \end{align*} 这个公式就是计算bandit算法的累积遗憾，解释一下： 首先，这里我们讨论的每个臂的收益非0即1，也就是伯努利收益。 然后，每次选择后，计算和最佳的选择差了多少，然后把差距累加起来就是总的遗憾。 $w_{B(i)}$是第i次试验时被选中臂的期望收益，$w^*$是所有臂中的最佳那个，如果上帝提前告诉你，我们当然每次试验都选它，问题是上帝不告诉你，所以就有了bandit算法，我们就有了这篇文章。 这个公式可以用来对比不同bandit算法的效果：对同样的多臂问题，用不同的bandit算法试验相同次数，看看谁的regret增长得慢。 那么到底不同的bandit算法有哪些呢？ 1.4 常用bandit算法 Thompson sampling算法 thompson sampling算法简单实用，因为它只有一行代码就可以实现3。简单介绍一下它的原理，要点如下： 假设每个臂是否产生收益，其背后有一个概率分布，产生收益的概率为p。 我们不断地试验，去估计出一个置信度较高的“概率p的概率分布”就能近似解决这个问题了。 怎么能估计“概率p的概率分布”呢？ 答案是假设概率p的概率分布符合beta(wins, lose)分布，它有两个参数: wins, lose。 每个臂都维护一个beta分布的参数。每次试验后，选中一个臂，摇一下，有收益则该臂的wins增加1，否则该臂的lose增加1。 每次选择臂的方式是：用每个臂现有的beta分布产生一个随机数b，选择所有臂产生的随机数中最大的那个臂去摇。 以上就是Thompson采样，用python实现就一行： import numpy as np import pymc #wins 和 trials 是一个N维向量，N是赌博机的臂的个数，每个元素记录了 choice = np.argmax(pymc.rbeta(1 &#43; wins, 1 &#43; trials - wins)) wins[choice] &#43;= 1 trials &#43;= 1 UCB算法 UCB算法全称是Upper Confidence Bound(置信区间上界)，它的算法步骤如下4： 初始化：先对每一个臂都试一遍 按照如下公式计算每个臂的分数，然后选择分数最大的臂作为选择： $$\bar{x}_j(t)&#43;\sqrt{\frac{2\ln{t}}{T_{j,t}}}$$ 观察选择结果，更新t和$T_{jt}$。其中加号前面是这个臂到目前的收益均值，后面的叫做bonus，本质上是均值的标准差，t是目前的试验次数，$T_{jt}$是这个臂被试次数。 这个公式反映一个特点：均值越大，标准差越小，被选中的概率会越来越大，同时哪些被选次数较少的臂也会得到试验机会。 Epsilon-Greedy算法 这是一个朴素的bandit算法，有点类似模拟退火的思想： 选一个(0,1)之间较小的数作为epsilon 每次以概率epsilon做一件事：所有臂中随机选一个 每次以概率1-epsilon 选择截止到当前，平均收益最大的那个臂。 是不是简单粗暴？epsilon的值可以控制对Exploit和Explore的偏好程度。越接近0，越保守，只想花钱不想挣钱。 朴素bandit算法 最朴素的bandit算法就是：先随机试若干次，计算每个臂的平均收益，一直选均值最大那个臂。这个算法是人类在实际中最常采用的，不可否认，它还是比随机乱猜要好。 以上五个算法，我们用10000次模拟试验的方式对比了其效果如图，实验代码来源5： 算法效果对比一目了然：UCB算法和Thompson采样算法显著优秀一些。 至于你实际上要选哪一种bandit算法，你可以选一种bandit算法来选bandit算法。。。 2. bandit算法与线性回归 2.1 UCB算法 UCB算法在做EE(Exploit-Explore)的时候表现不错，但它是上下文无关(context free)的bandit算法，它只管埋头干活，根本不观察一下面对的都是些什么特点的arm，下次遇到相似特点但不一样的arm也帮不上什么忙。 UCB解决Multi-armed bandit问题的思路是：用置信区间。置信区间可以简单地理解为不确定性的程度，区间越宽，越不确定，反之亦反之。 每个item的回报均值都有个置信区间，随着试验次数增加，置信区间会变窄（逐渐确定了到底回报丰厚还是可怜）。 每次选择前，都根据已经试验的结果重新估计每个item的均值及置信区间。 选择置信区间上限最大的那个item。 “选择置信区间上界最大的那个item”这句话反映了几个意思： 如果item置信区间很宽（被选次数很少，还不确定），那么它会倾向于被多次选择，这个是算法冒风险的部分； 如果item置信区间很窄（备选次数很多，比较确定其好坏了），那么均值大的倾向于被多次选择，这个是算法保守稳妥的部分； UCB是一种乐观的算法，选择置信区间上界排序，如果时悲观保守的做法，是选择置信区间下界排序。 2.2 UCB算法加入特征信息 Yahoo!的科学家们在2010年发表了一篇论文6，给UCB引入了特征信息，同时还把改造后的UCB算法用在了Yahoo!的新闻推荐中，算法名叫LinUCB。 刘鹏博士在《计算广告》一书中也有介绍LinUCB在计算广告中的应用7。 应用LinUCB算法的Yahoo!首页 单纯的老虎机回报情况就是老虎机自己内部决定的，而在广告推荐领域，一个选择的回报，是由User和Item一起决定的，如果我们能用feature来刻画User和Item这一对CP，在每次选择item之前，通过feature预估每一个arm（item）的期望回报及置信区间，选择的收益就可以通过feature泛化到不同的item上。 为UCB算法插上了特征的翅膀，这就是LinUCB最大的特色。 LinUCB算法做了一个假设：一个Item被选择后推送给一个User，其回报和相关Feature成线性关系，这里的“相关feature”就是context，也是实际项目中发挥空间最大的部分。 于是试验过程就变成：用User和Item的特征预估回报及其置信区间，选择置信区间上界最大的item推荐，观察回报后更新线性关系的参数，以此达到试验学习的目的。 LinUCB基本算法描述如下： LinUCB算法描述 对照每一行解释一下(编号从1开始)： 设定一个参数$\alpha$，这个参数决定了我们Explore的程度 开始试验迭代 获取每一个arm的特征向量$x_{a,t}$ 开始计算每一个arm的预估回报及其置信区间 如果arm还从没有被试验过，那么： 用单位矩阵初始化$A_a$ 用0向量初始化$b_a$， 处理完没被试验过的arm 计算线性参数$\theta$ 用$\theta$和特征向量$x_{a,t}$计算预估回报, 同时加上置信区间宽度 处理完每一个arm 选择第10步中最大值对应的arm，观察真实的回报$r_t$ 更新$A_{at}$ 更新$b_{at}$ 算法结束 注意到上面的第4步，给特征矩阵加了一个单位矩阵，这就是岭回归（ridge regression），岭回归主要用于当样本数小于特征数时，对回归参数进行修正8。 对于加了特征的bandit问题，正符合这个特点：试验次数（样本）少于特征数。 每一次观察真实回报之后，要更新的不止是岭回归参数，还有每个arm的回报向量$b_a$。 2.3 详解LinUCB的实现 根据论文给出的算法描述，其实很好写出LinUCB的代码9，麻烦的只是构建特征。 代码如下，一些必要的注释说明已经写在代码中。 class LinUCB: def __init__(self): self.alpha = 0.25 self.r1 = 1 # if worse -&amp;gt; 0.7, 0.8 self.r0 = 0 # if worse, -19, -21 # dimension of user features = d self.d = 6 # Aa : collection of matrix to compute disjoint part for each article a, d*d self.Aa = {} # AaI : store the inverse of all Aa matrix self.AaI = {} # ba : collection of vectors to compute disjoin part, d*1 self.ba = {} self.a_max = 0 self.theta = {} self.x = None self.xT = None # linUCB def set_articles(self, art): # init collection of matrix/vector Aa, Ba, ba for key in art: self.Aa[key] = np.identity(self.d) self.ba[key] = np.zeros((self.d, 1)) self.AaI[key] = np.identity(self.d) self.theta[key] = np.zeros((self.d, 1)) &amp;quot;&amp;quot;&amp;quot; 这里更新参数时没有传入更新哪个arm，因为在上一次recommend的时候缓存了被选的那个arm，所以此处不用传入 另外，update操作不用阻塞recommend，可以异步执行 &amp;quot;&amp;quot;&amp;quot; def update(self, reward): if reward == -1: pass elif reward == 1 or reward == 0: if reward == 1: r = self.r1 else: r = self.r0 self.Aa[self.a_max] &#43;= np.dot(self.x, self.xT) self.ba[self.a_max] &#43;= r * self.x self.AaI[self.a_max] = linalg.solve(self.Aa[self.a_max], np.identity(self.d)) self.theta[self.a_max] = np.dot(self.AaI[self.a_max], self.ba[self.a_max]) else: # error pass &amp;quot;&amp;quot;&amp;quot; 预估每个arm的回报期望及置信区间 &amp;quot;&amp;quot;&amp;quot; def recommend(self, timestamp, user_features, articles): xaT = np.array([user_features]) xa = np.transpose(xaT) art_max = -1 old_pa = 0 # 获取在update阶段已经更新过的AaI(求逆结果) AaI_tmp = np.array([self.AaI[article] for article in articles]) theta_tmp = np.array([self.theta[article] for article in articles]) art_max = articles[np.argmax(np.dot(xaT, theta_tmp) &#43; self.alpha * np.sqrt(np.dot(np.dot(xaT, AaI_tmp), xa)))] # 缓存选择结果，用于update self.x = xa self.xT = xaT # article index with largest UCB self.a_max = art_max return self.a_max 2.4 怎么构建特征 LinUCB算法有一个很重要的步骤，就是给User和Item构建特征，也就是刻画context。在原始论文里，Item是文章，其中专门介绍了它们怎么构建特征的，也甚是精妙。容我慢慢表来。 原始用户特征 人口统计学：性别特征（2类），年龄特征（离散成10个区间） 地域信息：遍布全球的大都市，美国各个州 行为类别：代表用户历史行为的1000个类别取值 原始文章特征 URL类别：根据文章来源分成了几十个类别 编辑打标签：编辑人工给内容从几十个话题标签中挑选出来的 原始特征向量都要归一化成单位向量。 还要对原始特征降维，以及模型要能刻画一些非线性的关系。 用Logistic Regression去拟合用户对文章的点击历史，其中的线性回归部分为： $$\phi_u^\mathrm{T}W\phi_a$$ 拟合得到参数矩阵W，可以将原始用户特征（1000多维）投射到文章的原始特征空间（80多维），投射计算方式： $$\psi_u\stackrel{def}{=}\phi_u^\mathrm{T}W$$ 这是第一次降维，把原始1000多维降到80多维。 然后，用投射后的80多维特征对用户聚类，得到5个类簇，文章页同样聚类成5个簇，再加上常数1，用户和文章各自被表示成6维向量。 Yahoo!的科学家们之所以选定为6维，因为数据表明它的效果最好10，并且这大大降低了计算复杂度和存储空间。 我们实际上可以考虑三类特征：U（用户），A（广告或文章），C（所在页面的一些信息）。 前面说了，特征构建很有发挥空间，算法工程师们尽情去挥洒汗水吧。 总结一下LinUCB算法，有以下优点： 由于加入了特征，所以收敛比UCB更快（论文有证明）； 特征构建是效果的关键，也是工程上最麻烦和值的发挥的地方； 由于参与计算的是特征，所以可以处理动态的推荐候选池，编辑可以增删文章； 特征降维很有必要，关系到计算效率。 3. bandit算法与协同过滤 3.1 协同过滤背后的哲学 推荐系统里面，传统经典的算法肯定离不开协同过滤。协同过滤背后的思想简单深刻，在万物互联的今天，协同过滤的威力更加强大。协同过滤看上去是一种算法，不如说是一种方法论，不是机器在给你推荐，而是“集体智慧”在给你推荐。 它的基本假设就是“物以类聚，人以群分”，你的圈子决定了你能见到的物品。这个假设很靠谱，却隐藏了一些重要的问题：作为用户的我们还可能看到新的东西吗？还可能有惊喜吗？还可能有圈子之间的更迭流动吗？这些问题的背后其实就是在前面提到过的EE问题（Exploit &amp;amp; Explore）。我们关注推荐的准确率，但是我们也应该关注推荐系统的演进发展，因为“推荐系统不止眼前的Exploit，还有远方的Explore”。 做Explore的方法有很多，bandit算法是其中的一种流派。前面也介绍过几种bandit算法，基本上就是估计置信区间的做法，然后按照置信区间的上界来进行推荐，以UCB,LinUCB为代表。 作为要寻找诗和远方的bandit浪漫派算法，能不能和协同过滤这种正统算法结合起来呢？事实上已经有人这么尝试过了，叫做COFIBA算法，具体在题目为Collaborative Filtering Bandits11和Online Clustering of Bandits12）的两篇文章中有详细的描述，它就是bandit和协同过滤的结合算法，两篇文章的区别是后者只对用户聚类（即只考虑了User-based的协同过滤），而前者采用了协同聚类（co-clustering，可以理解为item-based和user-based两种协同方式在同时进行），后者是前者的一个特殊情况。下面详细介绍一下这种结合算法。 3.2 bandit结合协同过滤 很多推荐场景中都有这两个规律： 相似的用户对同一个物品的反馈可能是一样的。也就是对一个聚类用户群体推荐同一个item，他们可能都喜欢，也可能都不喜欢，同样地，同一个用户会对相似的物品反馈相同。这是属于协同过滤可以解决的问题； 在使用推荐系统过程中，用户的决策是动态进行的，尤其是新用户。这就导致无法提前为用户准备好推荐候选，只能“走一步看一步”，是一个动态的推荐过程。 每一个推荐候选item，都可以根据用户对其偏好不同（payoff不同）将用户聚类成不同的群体，一个群体来集体预测这个item的可能的收益，这就有了协同的效果，然后再实时观察真实反馈回来更新用户的个人参数，这就有了bandit的思想在里面。 举个例子，如果你父母给你安排了很多相亲对象，要不要见面去相一下？那需要提前看看每一个相亲对象的资料，每次大家都分成好几派，有说好的，有说再看看的，也有说不行的；你自己也会是其中一派的一员，每次都是你所属的那一派给你集体打分，因为他们是和你“三观一致的人”，“诚不欺我”；这样从一堆资料中挑出分数最高的那个人，你出去见TA，回来后把实际感觉说给大家听，同时自己心里的标准也有些调整，重新给剩下的其它对象打分，打完分再去见，周而复始…… 以上就是协同过滤和bandit结合的思想。 另外，如果要推荐的候选item较多，还需要对item进行聚类，这样就不用按照每一个item对user聚类，而是按照每一个item的类簇对user聚类，如此以来，item的类簇数相对于item数要大大减少。 3.3 COFIBA算法 基于这些思想，有人提出了算法COFIBA（读作coffee bar）13，简要描述如下： 在时刻t，用户来访问推荐系统，推荐系统需要从已有的候选池子中挑一个最佳的物品推荐给他，然后观察他的反馈，用观察到的反馈来更新挑选策略。 这里的每个物品都有一个特征向量，所以这里的bandit算法是context相关的。 这里依然是用岭回归去拟合用户的权重向量，用于预测用户对每个物品的可能反馈（payoff），这一点和linUCB算法是一样的。 对比LinUCB算法，COFIBA算法的不同有两个： 基于用户聚类挑选最佳的item（相似用户集体决策的bandit） 基于用户的反馈情况调整user和item的聚类（协同过滤部分） 整体算法过程如下： COFIBA算法描述 核心步骤是，针对某个用户i，在每一轮试验时做以下事情： 首先计算该用户的bandit参数W（和LinUCB相同），但是这个参数并不直接参与到bandit的选择决策中（和LinUCB不同），而是用来更新用户聚类的； 遍历候选item，每一个item表示成一个context向量了。 每一个item都对应一套用户聚类结果，所以遍历到每一个item时判断当前用户在当前item下属于哪个类簇，然后把对应类簇中每个用户的M矩阵(对应LinUCB里面的A矩阵)，b向量（payoff向量，对应linUCB里面的b向量）聚合起来，从而针对这个类簇求解一个岭回归参数（类似LinUCB里面单独针对每个用户所做），同时计算其payoff预测值和置信上边界 每个item都得到一个payoff预测值及置信区间上界，挑出那个上边界最大的item推出去（和LinUCB相同） 观察用户的真实反馈，然后更新用户自己的M矩阵和b向量（更新个人的，对应类簇里其他的不更新） 以上是COFIBA算法的一次决策过程。在收到用户真实反馈之后，还有两个计算过程： 更新user聚类 更新item聚类 如何更新user和item的聚类呢？示意图为： User和Item聚类更新描述 解释一下这个图。 (a.) 这里有6个user，8个item，初始化时，user和item的类簇个数都是1 (b1.) 在某一轮试验时，推荐系统面对的用户是4。推荐过程就是遍历1~8每个item，然后看看对应每个item时，user4在哪个类簇中，把对应类簇中的用户聚合起来为这个item预测payoff和CB。这里假设最终item5胜出，被推荐出去了。 (b2.) 在时刻t，item有3个类簇，需要更新的用户聚类是item5对应的user4所在类簇。更新方式：看看该类簇里面除了user4之外的用户，对item5的payoff是不是和user4相近，如果是，则保持原来的连接边，否则删除原来的连接边。删除边之后重新构建聚类结果。这里假设重新构建后原来user4所在的类簇分裂成了两个类簇：{4,5}和{6} (c.) 更新完用户类簇后，item5对应的类簇也要更新。更新方式是：对于每一个和item5(被推荐出的那个item)还存在连接边的item j，都去构造一个user的近邻集合N，这个集合的用户对item j有相近的payoff，然后看看N是不是和刚刚更新后的user4所在的类簇相同，是的话，保留item5和item j之间的连接边，否则删除。这里假设item 3和item 5之间的连接边被删除。item3独立后给他初始化了一个聚类结果：所有用户还是一个类簇。 简单来说就是这样： User-based协同过滤来选择要推荐的item，选择时用了LinUCB的思想 根据用户的反馈，调整User-based和Item-based的聚类结果 Item-based的聚类变化又改变了User的聚类 不断根据用户实时动态的反馈来划分User-Item矩阵 4. 总结 Exploit-Explore这一对矛盾一直客观存在，bandit算法是公认的一种比较好的解决EE问题的方案。除了bandit算法之外，还有一些其他的explore的办法，比如：在推荐时，随机地去掉一些用户历史行为（特征）。 解决Explore，势必就是要冒险，势必要走向未知，而这显然就是会伤害用户体验的：明知道用户肯定喜欢A，你还偏偏以某个小概率给推荐非A。 实际上，很少有公司会采用这些理性的办法做Explore，反而更愿意用一些盲目主观的方式。究其原因，可能是因为： 互联网产品生命周期短，而Explore又是为了提升长期利益的，所以没有动力做； 用户使用互联网产品时间越来越碎片化，Explore的时间长，难以体现出Explore 的价值； 同质化互联网产品多，用户选择多，稍有不慎，用户用脚投票，分分钟弃你于不顾。 已经成规模的平台，红利杠杠的，其实是没有动力做Explore的； 基于这些，我们如果想在自己的推荐系统中引入Explore机制，需要注意以下几点： 用于Explore的item要保证其本身质量，纵使用户不感兴趣，也不至于引起其反感； Explore本身的产品需要精心设计，让用户有耐心陪你玩儿； 深度思考，这样才不会做出脑残的产品，产品不会早早夭折，才有可能让Explore机制有用武之地。 参考文献 https://en.wikipedia.org/wiki/Multi-armed_bandit↩ http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Chapter6.ipynb#↩ https://en.wikipedia.org/wiki/Thompson_sampling↩ http://hunch.net/~coms-4771/lecture20.pdf↩ https://gist.github.com/anonymous/211b599b7bef958e50af↩ http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf↩ 《计算广告：互联网商业变现的市场与技术》p253, 刘鹏，王超著↩ https://en.wikipedia.org/wiki/Tikhonov_regularization↩ https://github.com/Fengrui/HybridLinUCB-python/blob/master/policy_hybrid.py↩ http://www.gatsby.ucl.ac.uk/~chuwei/paper/isp781-chu.pdf↩ http://arxiv.org/abs/1401.8257↩ http://arxiv.org/abs/1502.03473↩ https://github.com/qw2ky/CoLinUCB_Revised/blob/master/COFIBA.py↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[自定义命令：在 LaTeX 中使用微分算子的正确姿势 z]]></title>
    	<url>/tech/2017/05/01/the-correct-way-to-use-differential-operator/</url>
		<content type="text"><![CDATA[[此篇讨论很小的「一点」——关于如何正确排版微分算子。 所谓微分算子，其英文名字叫做 differential operator，也就是 $\mathrm{d} x$ 中的 $\mathrm{d}$。 d；并且中国人读起来也就是简单粗暴的「滴埃克斯」。所以，在书写 LaTeX 手稿时，很多人简单粗暴地将微分算子写作 dx，例如不定积分 \int f(x)dx。 我们说，数学算式是精美而珍贵的。在排版数学算式时，你不能马虎，应该小心谨慎。这也是为什么 Knuth 在设计 TeX 时，将数学式子都包含在美元符号 $ 之间的原因。对待微分算子，我们也应该有这样的心态。 粗话说：「如果没有标准，那所有的争执都是瞎扯淡」。正如单位制有所谓的 ISO 国际单位制一样，数学符号也有对应的 ISO 标准。数学符号的标准，首先是定义在 ISO 31-11 当中；而后这个标准被 ISO 80000-2:2009 取代。因此，此篇讨论的内容，都是基于 ISO 80000-2:2009 的。 在 ISO 80000-2:2009 中，微分算子被描述为 直立的拉丁字母 d； 一个右边没有间距的操作符。 对于直立的拉丁字母 d，我们可以使用 \mathrm{d} 达成效果。而若要微分算子的左边有间距，而右边没有，这个问题就值得思考了。 最简单的办法，是将微分算子做如下实现 \newcommand*{\dif}{\,\mathrm{d}} 看起来，这样是没有问题的。但是，在某些情况下，就会出现尴尬的问题。比如 关于 $x$ 的微分 $\dif x$ 是指的思考的问题。 因为在 \dif 的定义中，\mathrm{d} 之前有不可省略的铅空 \,。于是，这份代码中 \dif x 与前后正文之间的距离就不一致了。为了解决这个问题，更有经验的人可能会选择这样定义 \newcommand*{\dif}{\mathop{\mathrm{d}}\!} 这份代码，试图利用 \mathop，只在必要的时候于左边插入空白，修复了上面的问题。不过，这样一来也带来了一些副作用——在 \mathop 的作用下，\mathrm{d} 的基线发生了改变，不再与正常的数学变量保持在同一个基线上。这也是不好的。 正确做法 最终解决问题，应该对微分算子有这样的定义 \newcommand*{\dif}{\mathop{}\!\mathrm{d}} 在这个定义中，拉丁字母 d 本身的特点得到了保留（比如基线是正常的）。此外，在 \mathrm{d} 的左边，插入了一个空白的 \mathop{}；其左边的空白保留，而右边与 \mathrm{d} 之间的距离，则由 \! 抑制。这样就达成了我们的目标。 参考：http://www.tug.org/TUGboat/Articles/tb18-1/tb54becc.pdf#page=8]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[回归时交叉项为什么要去平均？ - 慧航 - 专栏]]></title>
    	<url>/prof/2017/04/17/demean/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/26257159 之所以写这篇文章是因为在一个微信群里面的争论。 我想很多人 有的人的解释是消除多重共线性。然而就像我之前写过的一个答案1一样，在计量经济学领域，如果我们关注的是系数，增大样本几乎是解决多重共线性的唯一方法。其他的方法，要么会导致参数的不一致，要么就是自欺欺人的方法。所以期刊中的文章如果意识到了多重共线性的问题，都不会去讨论，更没有办法去解决这一问题。 至于为什么要去平均，我们不妨稍微写一下线性投影（按照伍德里奇书里面的方法）： 如果不去平均，那么 OLS 是一个线性投影： \[L(y|x_1,x_2,x_1\cdot x_2)=\beta_1 x_1&#43;\beta_2\cdot x_2 &#43;\gamma x_1\cdot x_2\] 而如果去平均（疑问：中括号中的部分是什么？2）： \[ \begin{aligned} L(y|\tilde{x}_1,\tilde{x}_2,\tilde{x}_1\cdot \tilde{x}_2) &amp;amp; =b_1 \tilde{x}_1&#43;b_2\cdot \tilde{x}_2 &#43;r\cdot \tilde{x}_1\cdot \tilde{x}_2\\ &amp;amp; =b_1 (x_1-\bar{x}_1)&#43;b_2\cdot (x_2-\bar{x}_2) &#43;r\cdot(x_1-\bar{x}_1)\cdot (x_2-\bar{x}_2)\\ &amp;amp; =b_1x_1-b_1\bar{x}_1&#43;b_2x_2-b_2\bar{x}_2&#43;rx_1x_2-rx_1\bar{x}_2-rx_2\bar{x}_1&#43;r\bar{x}_1\bar{x}_2\\ &amp;amp; =(b_1-r\bar{x}_2)x_1&#43;(b_2-r\bar{x}_1)x_2&#43;rx_1x_2&#43;[r\bar{x}_1\bar{x}_2-b_1\bar{x}_1-b_2\bar{x}_2] \end{aligned} \] 所以其实我们可以得到： \[ \left\{ \begin{aligned} \beta_1 &amp;amp; =b_1-r\bar{x}_2\\ \beta_2 &amp;amp; =b_2-r\bar{x}_1\\ \gamma &amp;amp; =r \end{aligned}\right. \] 所以实际上，这两种方法计算的系数，可以使用上面三个方程相互计算得到。相应的，系数的标准差也可以使用上面三个方程计算出来。 也就是说，如果做了去平均的回归，那么不去平均的回归结果（包括系数和方差）用上面三个方程就可以算出来了；如果做了不去平均的回归，那么去平均的回归结果（包括系数和方差）用上面三个方程也可以算出来。 所以你跟我说这是解决了多重共线性？逗我呢？ 如果再不信，可以做个简单的小模拟： clear set obs 100 gen x1=rnormal()*sqrt(2)&#43;1 // mean 1 gen x2=2*x1&#43;rnormal()&#43;1 // mean 3 gen x12=x1*x2 scalar beta1=-3 scalar beta2=-1 scalar gamma=1 gen y=beta1*x1&#43;beta2*x2&#43;gamma*x12&#43;rchi2(2) // deman egen mean_x1=mean(x1) egen mean_x2=mean(x2) gen demean_x1=x1-mean_x1 gen demean_x2=x2-mean_x2 gen demean_x12=demean_x1*demean_x2 // regression reg y x1 x2 x12 reg y demean_x1 demean_x2 demean_x12 回归结果： 配合着描述性统计： 可以轻易的验证上面的三个方程是成立的。 特别是，注意去不去平均，交叉项的系数和标准误都是一样的，所以如果我们只关注交叉项，比如在 DID 里面，去不去平均都可以。 那么这个变换是不是解决了多重共线性问题呢？有的人是这么 argue 的： 你看，相关系数小了很多哎！难道不是解决了多重共线性？ 要注意，多重共线性是要看偏相关系数的，实际上，如果我们做这样的回归： 你会发现，两个回归的 R-squared 基本上是一样的。 这不是自欺欺人是干嘛？ 也有人说了，去平均之后的确更容易显著啊！难道不是因为多重共线性吗？ 不是的。去平均之后更容易显著是因为，去平均之后\(x_1\)和\(x_2\)前面的系数都不一样了，参考我上面写的三个方程的结论，相当于加上了一个系数，所以更容易显著了。当然，这也不是绝对的，比如在上面的模拟里面，我通过一个特殊的设定展示了，去平均之后也有可能更不显著了。 注意我在这里仅仅是说去平均这一做法不是为了解决多重共线性问题，而没有说去平均这个做法是错的。实际上，去平均才是比较标准的做法。 为什么呢？如果我们算一下偏效应： \[\frac{\partial \mathrm{E}(y|\tilde{x}_1,\tilde{x}_2,\tilde{x}_1\cdot \tilde{x}_2)}{\partial x_1} =b_1 &#43;r\cdot\tilde{x}_2\] 而其期望： \[\mathrm{E}\left\{\frac{\partial \mathrm{E}(y|\tilde{x}_1,\tilde{x}_2,\tilde{x}_1\cdot \tilde{x}_2)}{\partial x_1}\right\} =b_1\] 也就是说，去平均之后的\(x_1\)的系数才是\(x_1\)对\(y\)的平均影响。 如果我们做出了\(x_1\)的系数不显著，但是交叉项的系数是显著的，说明平均而言\(x_1\)对\(y\)是没有影响的，但是\(x_1\)对\(y\)是有影响的，其影响随着\(x_2\)的不同而变化。只是在总体上平均而言，\(x_1\)对\(y\)没有影响。 所以总结一下，其实去不去平均这两种做法是等价的，只不过去平均之后的系数更容易解释，所以我们一般做回归的时候会去平均处理。这个跟多重共线性没啥关系。 此外多说一句，这个例子也可以回答为什么我们需要做描述性统计。我在这个回答3里面提到过： 除以上原因外，还有个非常重要的原因，就是帮助读者和审稿人阅读回归表格。很多人做回归的时候，出于某些目的会对一些变量 scaling 等等，但是读者和审稿人往往希望知道这些变量的「经济显著性」究竟有多强。仅仅得到一个显著的结果往往是不够的，关心的变量\(x\)变动之后对结果\(y\)究竟有多大影响？因为单位的问题，有些时候往往难以比较。所以经常我们可能关心「当\(x\)变动一个标准差之后，\(y\)有多大的影响」，这个时候就需要使用描述性统计和回归表格结合起来一起看了。还有比如上面的age同时有age^2，那么当年龄增加\(1\)，平均而言会对\(y\)有多大影响呢？这个时候可能会需要age的均值，同样需要诉诸于描述性统计。类似此类的问题，没有描述性统计的情况下，读者是没办法计算的。 这里给我们的一个启发就是，当我们阅读回归表格的时候，特别是遇到交叉项的时候，一定要结合着描述性统计去看。比如，有的回归结果去平均是不显著的，那么会报告不去平均的结果，这个时候如果配合着描述性统计，配合上面的三个式子，就能还原回真正的平均偏效应了。经验丰富的学者在阅读文章的时候是不会被这些小的 tricks 给迷惑的。 最后硬广：欢迎来我的小密圈交流： 李远哲：学统计那一套的时候真没仔细想这个解释性问题，以为标准化变量是机器学习那一套的标准流程，为了做 gradient descent 的时候比较数值稳定才这么做的。 慧航：如果要做 shrinkage 降维之类的时候，那是一定要标准化的，关键是除以标准差而不是去平均。搞计量注重对参数的解释，所以标准化的原因是不一样的。 毛士林：想知道连续变量交乘的经济含义，应该怎么去解释。这种能够解释经济含义吗一般，比如\(x_1\)变动百分之多少，\(x_2\)的调节效应为多少。貌似如果\(x_2\)为 dummy 的话可以，比价容易解释？或者做分组回归，然后对系数差异进行t检验。不知道我之前的做法是否正确，也都是参考已经发表的文章的做法去做的。。 慧航：1) \(x_1\)对\(y\)的影响随着\(x_2\)的大小而不同。2) 系数的经济含义都可以用偏效应这个工具来做，伍德里奇面板数据的前两章一定要好好看。 3) 最好还是放在一个方差里面用 dummy 去检验，不要单独跑回归然后检验。 靠贩剑为生：在还有交叉项的回归中，如果我最关注的是交叉项的系数，例如 \(x_1*x_2\)，那是否在回归的时候一定要加入单独的\(x_1\)项和\(x_2\)项呢？ 慧航：一般来说是一定要的！ 波波儿爸：你做的偏相关模拟可能有问题。\(x_1\)和\(x_2\)本身强相关，\(demeam\_x_1\) 和 \(demeam\_x_2\) 也强相关，所以两个回归 r-square 都高。期待你能试试用弱相关变量模拟一下？最后两个公式 \(r*demean\_x_2\) 的期望怎么等于 0 的？ 慧航：(1) 对，我是为了举反例而已，故意为之；(2) 因为demean, r_hat=r&#43;op(1)。 原文地址：https://www.zhihu.com/question/55089869#answer-52600547。↩ 个人理解下式与上式对比时，只比较了\(x_1\)和\(x_2\)的系数，因此中括号中的部分根本没考虑。↩ 原文地址：https://www.zhihu.com/question/23074134#answer-49194682。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[用 Hexo&#43;Github 搭建博客平台]]></title>
    	<url>/tech/2017/04/17/hexo/</url>
		<content type="text"><![CDATA[[1 为什么选择 Hexo 1.1 优点 1.2 (可能的)缺点 2 安装与配置 2.1 步骤简介 2.2 搭建本地博客 2.2.1 软件安装 2.2.2 博客平台搭建 2.3 安装 NexT 主题 2.4 修改博客配置 site: _config.yml 2.4.1 修改配置文件 2.4.2 添加一级页面 2.4.3 重要功能插件 2.5 修改主题配置 NexT: _config.yml 2.6 修改博客的 CSS 2.6.1 模板调整记录 2.6.2 自定义样式文件调整记录 3 远程部署 4 应用问题 参考内容：http://qjzhixing.com/2015/08/26/ 提醒：出现其他任何的问题，先删除博客目录下的db.json文件，然后清理再部署远程博客，操作时输入以下的命令 hexo clean hexo d -g 1 为什么选择 Hexo 1.1 优点 速度：目前1还不清楚 Hexo 在文章数量较多时的速度如何，但可以肯定 Hexo 的速度虽然比 Jekyll 要快得多（虽然与 Hugo 相比还是有较大差距）； 实时刷新：通过命令hexo -s启动服务，接下来对内容文件进行任何更新都可以直接在浏览器中直接刷新看到更新后的结果； 站内搜索：在 NexT 主题可以轻松实现内建的搜索功能； Mathjax：可以通过hexo-renderer-pandoc实现 Markdown 和 Mathjax 的良好兼容（不清楚 Hugo 中如何使用 Pandoc）； 值得一提的是，Hexo 默认的 Markdown 渲染插件hexo-renderer-marked功能较弱，与 Mathjax 的兼容也不太好2，虽然hexo-renderer-kramed在hexo-renderer-marked的基础上完善了对 Mathjax 的支持，但在功能方面与hexo-renderer-markdown-it相比还是太弱，可惜hexo-renderer-markdown-it与 Mathjax 的兼容性也不太好，并且这个插件非常复杂，通过 Hack 方式改进对 Markdown 的支持也没有好的方案可以借鉴。 总的来讲，Pandoc 的效果是最好的3，唯一的缺点是需要单独安装 Pandoc，但考虑到只是将转换好的静态文件发布到 Github，这一缺点也基本上可以忽视。 1.2 (可能的)缺点 没有直接的方法将_post下的源文件按类别存放到不同的子目录中，但是 Hugo 可以； 内建的内容汇总文件db.json将来可能会非常大，同步到 Github 可能会比较慢，博客内建的搜索功能也可能会变得非常慢； hexo-renderer-pandoc不支持 emoji 表情插件，但hexo-renderer-markdown-it中的子插件markdown-it-emoji支持文本字符类型的表情； 2 安装与配置 2.1 步骤简介 搭建本地博客：在电脑上安装Git、Node.js和Pandoc，利用简单的命令安装好Hexo； 安装并配置NexT主题：演示、使用文档、主题文档； 部署克隆版分站点； 部署远程博客；注册一个Github账号，然后在上面创建一个Repository，简单配置一下，博客的基本框架就建立好了； 2.2 搭建本地博客 2.2.1 软件安装 安装Git：安装过程一路默认即可，安装好后，单击右键就会多出两个Git的按钮； 安装Node.js：可以一路默认，需要的话也可以更改一下安装的路径； 安装 Hexo：中文文档、英文文档；Hexo 的安装需要借助 Node.js 的npm命令，可以理解为 Hexo 是 Node.js 的模块。操作的方式是在任意的位置单击鼠标右键，选择Git bash命令，在里面输入： npm install -g hexo-cli 卸载的话，将上面命令中的install替换成uninstall即可执行卸载。 2.2.2 博客平台搭建 创建 Hexo 文件夹 Hexo文件夹就是博客的文件夹，用于存放全部相关文件（但只有静态生成的文件必需上传到远程 Repository）。 第一步先在某个盘符下新建一个文件夹，重命名（英文字母），假设是在D盘下建立了一个名叫Hexo的文件夹，那么路径就是D:\hexo （后续的操作大多在这个文件夹里进行）； 第二步进入Hexo文件夹单击右键，依旧选择Git bash这一命令，输入以下命令，博客所需要的文件都已经自动建立好了，这比jekyll操作简单多了： hexo init 安装依赖包 npm install 为保证下一环节远程部署成功，还需要执行如下命令： npm install hexo-deployer-git --save 预览本地博客 一系列的安装命令之后，本地博客就算搭建好了，输入如下的命令，然后在浏览器地址栏中输入http://localhost:4000或者http://127.0.0.1:4000就可以查看本地博客。 hexo g hexo s 上面的命令中，hexo g用于生成静态文件，hexo s用于创建 Web 服务器，这两条命令可以合并写为一条： hexo s -g 2.3 安装 NexT 主题 推荐先备份_config.yml文件； 进入到hexo的目录，启动Git bash，输入以下面的命令 Clone 主题： $ cd your-hexo-site $ git clone https://github.com/iissnan/hexo-theme-next themes/next 在_config.yml中修改theme取值为next； 在\hexo\themes\next\_config.yml中修改与主题相关的设置； 2.4 修改博客配置 site: _config.yml 2.4.1 修改配置文件 修改Site段相关内容，包括title, author, language等，其中language取值为zh-Hans； 参考 2.3 修改theme取值为NexT； 修改deploy段内容为： deploy: type: git repository: https://github.com/peng-howard/lives.git branch: master 其中 repository 为lives，本人的主页面https://peng-howard.github.io/是用 Jekyll 生成的（直接在 GitHub 上修改index.html似乎不起作用），因此这里将lives相关内容作为主页面的一个分页面站点存在。如果是部署到 GitHub Pages 的主页面，则使用如下命令： repository: https://github.com/peng-howard/peng-howard.github.com.git 设定本人的社交信息及友情链接等： social: GitHub: https://github.com/your-user-name Twitter: https://twitter.com/your-user-name 微博: http://weibo.com/your-user-name 豆瓣: http://douban.com/people/your-user-name 知乎: http://www.zhihu.com/people/your-user-name social_icons: enable: true # Icon Mappings. # KeyMapsToSocialItemKey: NameOfTheIconFromFontAwesome GitHub: github Twitter: twitter Weibo: weibo 微博: weibo # 友情链接 # Blog rolls links_title: 友情链接 #links_layout: block #links_layout: inline links: MacTalk: http://macshuo.com/ Title: http://example.com/ 设定每页文章显示篇数 第一步：安装相关插件（不必安装：在安装完相关依赖包以后不再安装也是正常的，已经验证） npm install --save hexo-generator-index npm install --save hexo-generator-archive npm install --save hexo-generator-tag 第二步：安装完插件后，在站点配置文件中，添加如下内容 # index 为首页的每页数量 # archive 是存档页每页数量 # tag 不知道在哪里起作用 index_generator: per_page: 10 archive_generator: per_page: 50 yearly: true monthly: true tag_generator: per_page: 15 # 下面的 Pagination 是分类中，每个分类的目录页面中单页的文章数量 # Pagination ## Set per_page to 0 to disable pagination per_page: 50 pagination_dir: page index, archive及tag开头分表代表主页，归档页面和标签页面。其中per_page字段是希望设定的显示篇数。 修改new_post_name取值为:year-:month-:day-:title.md # File name of new posts，这样生成的文件名称不包含年月日部分。 2.4.2 添加一级页面 生成 RSS 文件atom.xml，首先执行如下命令安装插件： npm install hexo-generator-feed --save 然后在主配置文件中添加如下内容： feed: type: atom path: atom.xml limit: 20 hub: content: 添加标签（tags）页面，首先执行如下命令： $ cd your-hexo-site $ hexo new page tags 注意：(1) 其中第一条命令可以不执行，改为先进入 blog 文件所在文件夹，再右键调出Git命令行，然后直接执行第二条命令；(2) hexo new page是建立一个新的文件夹，并在其中加入一个index.md文件，相当于新建一个分站点，而hexo new &amp;quot;...&amp;quot;是建立一篇新的文章。 生成相关文件后，到source/tags目录下修改index.md文件为如下内容： --- title: 标签 date: 2014-12-22 12:39:04 type: &amp;quot;tags&amp;quot; comments: false --- 添加分类页面： $ hexo new page categories 然后修改相关文件为如下内容： --- title: 分类 date: 2014-12-22 12:39:04 type: &amp;quot;categories&amp;quot; comments: false --- 2.4.3 重要功能插件 站内搜索 在主目录中执行如下命令： $ npm install hexo-generator-searchdb --save 在主配置文件中添加如下内容： search: path: search.xml field: post format: html limit: 10000 在主题配置文件修改如下内容： local_search: enable: true 在实际使用中，站内搜索要输入两个以上的字才会有搜索结果，并且可能会搜索不到内容。 为各级标题添加编号 根据http://www.tuicool.com/articles/7BnIVnI，Hexo 的 NexT 主题在右侧的导航栏中可以自动给各级标题添加编号，这利用了 Hexo 3 的一个最新功能，但是正文中却无法使用，因此这里用命令 cnpm install hexo-heading-index --save 安装hexo-heading-index命令，之后在主站点的_config.yml中添加如下内容： heading_index: enable: true index_styles: &amp;quot;{1} {1} {1} {1} {1} {1}&amp;quot; connector: &amp;quot;.&amp;quot; global_prefix: &amp;quot;&amp;quot; global_suffix: &amp;quot; &amp;quot; 之后设定 NexT 主题配置文件_config.yml中的 toc: enable: true # Automatically add list number to toc. number: false number取值为false，这样在启动正文各级标题编号的同时，关闭右侧导航栏中的重复编号。 需要注意的是，Hexo 的正文标题虽然用了h1标记，但来自于头文件中的title项。 正文中建议仍然以#开始第一级标题，否则标题的编号可能会出现意想不到的错误，比如{1}对应的一级标题实际上会是0，如果改成{0}，又会在一级编号前出现一个-。 另外这个包的index_styles中，一级的编号设定是必需的，其它级是可选的，希望将来一级可以选择为空。 hexo-renderer-pandoc的 Markdown 渲染 Hexo 首先用配置文件config.yml和安装的 Markdown 渲染插件对应的 Markdown 解释器对.md文件进行处理，之后 Mathjax 或者其它在线 LaTeX 解释器再根据 Markdown 生成的结果解释其中的公式内容。考虑到 Hexo 自带的hexo-renderer-marked，基于该插件的 LaTeX 修正版hexo-renderer-kramed功能较弱，另一个解释器hexo-renderer-marked-it虽然功能很强，但是和 LaTeX 的兼容不太好，在参考http://shomy.top/2016/10/22/hexo-markdown-mathjax/之后，最终决定使用hexo-renderer-pandoc作为.md文件的解释器。 到 Pandoc 官方站点 下载并安装 Pandoc4； 卸载默认的 Markdown 解释器并安装hexo-renderer-pandoc： cnpm uninstall hexo-renderer-marked --save cnpm install hexo-renderer-pandoc --save 根据https://github.com/wzpan/hexo-renderer-pandoc的提示，在站点的配置文件中，将 Markdown 部分的内容替换为hexo-renderer-pandoc的默认配置： pandoc: filters: extra: meta: mathEngine: 考虑到没有空格的多行在一起组成一个段落时，CJK 类文本处理与英文不一样（后者之间将换行转换成一个空格），需要开启 Pandoc 的east_asian_line_breaks扩展，具体方法是修改hexo-renderer-pandoc的index.js，将其中的 var args = [ &amp;#39;-f&amp;#39;, &amp;#39;markdown&amp;#39;, &amp;#39;-t&amp;#39;, &amp;#39;html&amp;#39;, math, &amp;#39;--smart&amp;#39;] 替换成5 var args = [ &amp;#39;-f&amp;#39;, &amp;#39;markdown&#43;east_asian_line_breaks&amp;#39;, &amp;#39;-t&amp;#39;, &amp;#39;html&amp;#39;, math, &amp;#39;--smart&amp;#39;] 最后，重新启动 Git 命令行，再重新启动 Hexo 服务就可以正常工作。 Markdown 编译器的设定（不再使用） 本部分除fix-cjk外，marked 配置部分不再使用，改用 markdown-it 进行解释，详细参考下面第 5 点。最新进展：markdown-it 与 Mathjax 的兼容性也不太好，改为使用 pandoc 进行 md 的解释，并且 pandoc 中提供了 cjk space 问题的修正，因此 fix-cjk 也不再使用，具体配置见上面第 3 点。 问题6：下面的设置保证 Markdown 中连着的两行不会被解释成为&amp;lt;br/&amp;gt;，但是换行变成了空格？（来自https://wwssllabcd.github.io/blog/2014/12/22/how-to-install-hexo/） 由於新版的Hexo使用hexo-renderer-marked來控制Markdown，而所以还要再调整一下，在站点配置文件中，添加如下内容： # fix_cjk_spacing: false # false 时停用 cjk 换行 - 空格 修正 # https://github.com/hexojs/hexo-renderer-marked # breaks 遇到 CJK 字符时，会将换行符转成一个空格 # https://github.com/lotabout/hexo-filter-fix-cjk-spacing/issues/1 # 提供的方案可以部分修正这个问题，但要更新 npm 包到最新 marked: gfm: true pedantic: false sanitize: false tables: true breaks: false smartLists: true smartypants: false modifyAnchors: &amp;#39;&amp;#39; autolink: false 确保其中breaks是为false，此时fix_cjk_spacing才会处于工作状态；另外还可以设定fix_cjk_spacing: false以禁用这一修正功能（特别是纯英文的 blog）。 但需要注意的是，fix_cjk_spacing可能会出现如下的错误： $ npm install hexo-filter-fix-cjk-spacing --save hexo-site@0.0.0 D:\GitHub\stylus -- hexo-filter-fix-cjk-spacing@0.0.3 npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted {&amp;quot;os&amp;quot;:&amp;quot;darwin&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;any&amp;quot;} (current: {&amp;quot;os&amp;quot;:&amp;quot;win32&amp;quot;,&amp;quot;arch&amp;quot;:&amp;quot;x64&amp;quot;}) 根据 https://github.com/npm/npm/issues/14042 提供的信息，对npm降级或者将npm升级到最新版本可以解决这个问题，这里选择升级npm。由于国内升级npm可能无法成功，因此根据https://npm.taobao.org/的说明，安装命令cnpm： npm install -g cnpm --registry=https://registry.npm.taobao.org 之后用淘宝的镜像更新npm（当然，还可以更新cnpm本身，这个命令与npm实质上是相同的，只是镜像在淘宝）： cnpm install -g npm 安装fix_cjk_spacing也可以： cnpm install hexo-filter-fix-cjk-spacing --save 之后运行hexo相关命令就不会再出错。 Markdonw-it、脚注、表情设置（不再使用） Hexo 的文章内容默认是不支持 emoji 表情的，作为一个有逼格的码农，怎么少得了丰富的表情语言，下面的内容介绍怎么支持 emoji。 Hexo 默认的 markdown 编译插件是hexo-renderer-marked，看了一下相关文档，好像没办法支持 emoji，还好在 Hexo 的plugins页，我们找到了另外一个 markdown 插件hexo-renderer-markdown-it，而且号称速度比默认的还要快，最主要的是，在markdown-it的文档里面，我们发现它可以通过 plugins 的方式支持emoji。 下面我们就来替换 markdown 插件，首先进入博客的主目录（stylus） cnpm un hexo-renderer-marked --save cnpm i hexo-renderer-markdown-it --save 不过此时的 hexo-renderer-markdown-it 还是用不了 emoji 的，我们需要加上 emoji 的 plugin $ cd node_modules/hexo-renderer-markdown-it/ $ cnpm install markdown-it-emoji --save 然后编辑 Hexo 的配置文件_config.yml # fix_cjk_spacing: false # false 时停用 cjk 换行 - 空格 修正 # https://github.com/hexojs/hexo-renderer-marked # breaks 遇到 CJK 字符时，会将换行符转成一个空格 # https://github.com/lotabout/hexo-filter-fix-cjk-spacing/issues/1 提供的方案可以修正这个问题，但要更新 npm 到最新 # MD-it 对 html 中的内容不起作用，但是 cjk-fix 会起作用 markdown: render: html: true xhtmlOut: false breaks: false linkify: true typographer: false quotes: &amp;#39;“”‘’&amp;#39; plugins: - markdown-it-footnote - markdown-it-sup - markdown-it-sub - markdown-it-abbr - markdown-it-emoji anchors: level: 1 collisionSuffix: &amp;#39;v&amp;#39; permalink: false permalinkClass: header-anchor permalinkSymbol: ¶ 与 Hexo 默认的 Markdown 引擎相比，这个引擎要严格一些：默认的引擎可以解释 html 中的 md 部分，但这个引擎在遇到 html 中的 md 时，会跳过去。 注意上面的xhtmlOut用于确保Markdown转换结果符合xhtml的规范；typographer为true用于确保”“以及’‘会自动被纠正为“”和‘’。 MD-it 对 html 中的内容不起作用，但是 cjk-fix 会起作用，为了使用&amp;lt;pre class=&amp;quot;center&amp;quot;&amp;gt;及&amp;lt;pre class=&amp;quot;white&amp;quot;&amp;gt;，在lives中禁用了fix_cjk功能。 关键就是在 plugins 里加上- markdown-it-emoji，其他的配置说明可以参见wiki。 重启Hexo服务，即可生效，这里输入:smile: :smirk: :relieved:就可以显示表情符号，当然直接输入表情也是可以的，具体参考https://github.com/markdown-it/markdown-it-emoji。 表情的安装参考上面的命令，而脚注确定不需要再安装任何包，下面是脚注的例子： Here is a footnote reference,[^1] and another.[^longnote] [^1]: Here is the footnote. [^longnote]: Here&amp;#39;s one with multiple blocks. Subsequent paragraphs are indented to show that they belong to the previous footnote. 这个例子中，“Subsequent…”也是脚注的一部分，因此脚注的结束符号将在“footnote.”之后。 下面是一个 inline 的例子： Here is an inline note.^[Inlines notes are easier to write, since you don&amp;#39;t have to pick an identifier and move down to type the note.] 2.5 修改主题配置 NexT: _config.yml 设定mathjax取值为true，打开 LaTeX 公式支持； 设定use_motion取值为false，关闭页面加载时的动画效果，配置文件中提到5.0.0 及更低版本中，侧栏在 use motion: false 下不会展示，这一说法没有具体印证； 设定scrollpercent取值为true，这样在滚动页面时，右下角会出现当前页面所在位置的百分比； 制作头像图片，保存为source/images/avatar.png，设定主题配置文件中avatar取值为/images/avatar.png； 设定display取值为hide，这样右侧边栏在页面加载时默认关闭，只能通过手工方式加载； 设定since取值为2005； 到https://disqus.com/admin/生成一个 shortname，并加入到主题配置文件中。现有网页上说的在右上角设置中生成的方法打不开； 删除themes\next\layout\_macro\post.swig中的&amp;amp;raquo;，只保留“阅读全文”这四个字； 2.6 修改博客的 CSS 2.6.1 模板调整记录 修改theme/next/source/css/_variables/custom.styl文件，添加如下内容： // 修改成你期望的宽度 $content-desktop = 700px // 当视窗超过 1600px 后的宽度 $content-desktop-large = 960px 实际上本人在theme/next/surce/css中增加了一个haopeng.styl，并将其import到main.styl，用于添加自定义的具体样式内容，工作原理与custom.styl文件非常接近。 在next/source/css/_variables/base.styl中，将$head-bg设定为transparent。 修改了页脚模板文件中的文字内容（\themes\next\layout\_partials\footer.swig）； themes\next\languages中有对应的语言，注意关键词的名称是英文，可以在语言文件中修改或添加关键字，在主题配置文件中修改关键字对应的图标，http://fontawesome.io/icons/列出了全部可用的图标，考虑到最近一次修改的模板中不再使用图标，这一条的作用就暂时不再重要。 修改next/layout/_macro/post-collapse.swig 27 行的 date 的格式，添加年份，这样分类和归档页的标题中就包含年份，同时调整分类和标题页中的 css 的缩进。 在next/layout/_macro/post.swig中，将post-footer中的tag处的#改成图标效果： &amp;lt;i class=&amp;quot;fa fa-fw fa-tag&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; 2.6.2 自定义样式文件调整记录 其它要修改的内容都在haopeng.styl中，需要注意的是：(1) 一些CSS属性可能无法被覆盖，此时在指令后面加!important强制覆盖；(2) 一部分下划线是通过background-color实现的，另一些是通过border-bottom实现的，因此需要根据实际情况做修改；(3) 主页与正文中的文章标题样式名称不相同，要分别修改；(4) 文章标题的文字大小只能通过!important修改；(5) 存档目录页中的文章标题与时间在水平方向没对齐，这与修改了文章标题的文字大小有关，很难调整，最后是通过调整如下内容才做到的： /* 存档页中文章标题与时间对齐 */ .post-title-link span { display: block; } 分类页中文章数量用了浮动体样式； 将背景图片放置到themes/next/source/images中，然后在自定义文件中加入如下内容： body { background: url(/images/bg.png) repeat; } 博客的页脚使用了 Galdeano 字体，原始的方法是通过 NexT 主题配置文件_config.yaml下的 global 字段修改字体，这种方法会在线调用 Google 的远程字体。但这种方法有两个问题，一是放两种字体需要通过语法实现：family: font1|font2，这种语法对应到fonts.googleapis.com的查询字符串时，通过|指定两种字体（如果字体名称有空格，用&#43;），但这种做法会导致正文、标题加载时将这两个字体组成的名称当成一个字体名称，这不是我们正常情形需要的，如果我们只指定一种字体，我们也可能不希望 NexT 自动把相关字体做全局修改。第二个问题是这样需要在线加载字体，即使通过&amp;lt;fonts.useso.com&amp;gt;加载可以提高速度，也未必是我们想要的做法，谁知道将来 360 会不会停止这一服务。 这里推荐的做法是生成本地字体并加载： 从https://gist.github.com/dotJoel/7326331找到Galdeano字体 ttf 文件的下载地址并下载该文件； 到https://www.fontsquirrel.com/tools/webfont-generator，上传下载的字体并生成不同浏览器需要的字体文件； 将字体文件放到themes\next\source\lib\font-Galdeano之中，由于这里将lives。当成站点下的子目录，因此这些文件实际上是放在\lives\themes\next\source\lib\font-Galdeano里面（根据实际情况自己放置）。 需要说明的是，上面的 css 内容没有选择用https://segmentfault.com/q/1010000005679305中介绍的用https://fonts.googleapis.com/css?family=Galdeano里面的内容，这种方法只包含了woff2文件，IE 下无法看到字体效果。而是主要选择用http://smartgrid.ac.cn/63.html提供的方法，根据下面的代码结合https://www.fontsquirrel.com/tools/webfont-generator中提供的 css 文件： @font-face { font-family:&amp;#39;Tangerine-b&amp;#39;; src: url(&amp;#39;./font/Tangerine_Bold.eot&amp;#39;); src: url(&amp;#39;./font/Tangerine_Bold.eot?#iefix&amp;#39;) format(&amp;#39;embedded-opentype&amp;#39;), url(&amp;#39;./font/Tangerine_Bold.woff&amp;#39;) format(&amp;#39;woff&amp;#39;), url(&amp;#39;./font/Tangerine_Bold.ttf&amp;#39;) format(&amp;#39;truetype&amp;#39;), url(&amp;#39;./font/Tangerine_Bold.svg#tangerinebold&amp;#39;) format(&amp;#39;svg&amp;#39;); } 修改后，在haopeng.styl中添加如下代码： /* latin */ @font-face { font-family: &amp;#39;Galdeano&amp;#39;; font-style: normal; font-weight: 400; src: url(&amp;#39;/lives/lib/font-Galdeano/galdeano-webfont.woff2&amp;#39;) format(&amp;#39;woff2&amp;#39;), url(&amp;#39;/lives/lib/font-Galdeano/galdeano-webfont.woff&amp;#39;) format(&amp;#39;woff&amp;#39;); unicode-range: U&#43;0000-00FF, U&#43;0131, U&#43;0152-0153, U&#43;02C6, U&#43;02DA, U&#43;02DC, U&#43;2000-206F, U&#43;2074, U&#43;20AC, U&#43;2212, U&#43;2215; } 原因是生成的文件中没有eot, svg类文件。 在线的思源宋体加载 在 2017.5 时，参考谢益辉的博客，将博客的主字体换成思源宋体，这个过程除了在haopeng.styl中添加样式的支持外，还需要： 在next/layout/_partials/head.swig添加从谢站点上下载的load-typekit.js文件， &amp;lt;script async src=&amp;quot;/stylus/js/load-typekit.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; 说明：不使用注册 Adobe 的 Typekit 服务时提供的代码的原因是上面的脚本不仅能够实时加载思源宋体，还可以在加载之前先判断客户端是否已经安装思源宋体。Adobe 使用的 ID 需要在 VPN 下到 Adobe Typekit 注册，邮箱是haopeng.yn@gmail.com|1A，用 Chrome 提供的翻墙功能注册后无法得到字体服务。 将load-typekit.js复制到合适的位置，将其中的kitid改成这里的oqz0fck，这个来自注册时候提供的信息。 Pandoc 生成的脚注编号中，sup在a之中，虽然修改a的display为inline-flex或inine-grid可以移动a的下划线，但会导致hover背景颜色失效，因此暂时不修改编号的链接效果，这与谢益辉的博客中sup和a的顺序刚好相反。（https://github.com/jgm/pandoc/issues/2583） 其它 CSS 内容的修改非常多，所以这里不再一一解释。 3 远程部署 在 GitHub 上先新建一个 Repository（Public，不需要修改任何设置），然后在站点文件夹中用 hexo d 上传相关文件到远程。上传完成后，再到 GitHub 的 Repository 的设置中修改GitHub Pages中的Source为master branch，这样就可以成功启动博客。在这个过程中可能会调出一个登陆 GitHub 的窗口，输入用户名和密码即可。 由于之前已经配置过 SSH，因此上面不涉及这个过程。 配置 SSH（新机器可能需要） 检查是否已经有 SSH Key，打开 Git Bash，输入 cd ~/.ssh 如果没有这个目录，则生成一个新的 SSH，输入 ssh-keygen -t rsa -C &amp;quot;your e-mail&amp;quot; 其中，your e-mail 是注册 Github 时用到的邮箱。然后接下来几步都直接按回车键，最后生成相关内容文件。 复制公钥内容到 Github 账户信息中 打开~/.ssh/id_rsa.pub文件，复制里面的内容； 打开 Github 官网，登陆后进入到个人设置(点击头像 -&amp;gt; setting)，点击右侧的SSH Keys，点击Add SSH key；填写title之后，将之前复制的内容粘贴到Key框中，最后点击Add key即可。 测试 SSH 是否配置成功，输入 ssh -T git@github.com 如果显示以下，则说明 SSH 配置成功。 Hi username! You&amp;#39;ve successfully authenticated, but GitHub does not provide shell access. 克隆子栏目 直接复制文件夹，新建 Repository 就可以。记得修改 CSS 中背景图片的位置，修改主题配置中站点的目录，修改主配置中的相关信息，修改模板等。 一个特殊的问题是，在node_modules发现了以stylus为名称的子文件夹，这说明最初生成 Hexo 站点子目录stylus时，写入了一些特殊信息，但直接复制这个文件夹似乎没有给另一个子站点lives造成不良影响。 4 应用问题 不要用NexT官方文档中的居中对齐样式，要居中自己直接用 HTML 实际就好； 通过各种方法添加虾米音乐的外链播放都不成功，感觉是被禁止了； 多个标签用[...,...]； Hexo的四个主要命令： hexo g = hexo generate #生成 hexo s = hexo server #启动本地预览 hexo d = hexo deploy #远程部署 hexo n &amp;quot;文章标题&amp;quot; = hexo new &amp;quot;文章标题&amp;quot; #新建一篇博文 其中前三个命令经常组合用： hexo s -g hexo d -g 奇怪的是在部署到远程时，出现过失效的现象，建议直接删除_public文件夹，再重新生成页面，最后再部署，即生成和部署分开进行。更保险的做法是hexo clean之后，再重新生成文件并部署。 将来新安装机器时，怎么做到用现有的文件进行覆盖安装还没有想通（一个可能的问题是基础平台、博客平台、主题都会有更新，当然在安装完 Git 和 Node.js 之后，其它内容可能都在站点子目录中，可以尝试直接覆盖这部分内容），不同机器内容的同步也没有好的方法。 Mathjax 支持带编号的公式 在原有的themes\next\layout\_third-party.mathjax.swig中添加如下内容： &amp;lt;script type=&amp;quot;text/x-mathjax-config&amp;quot;&amp;gt; MathJax.Hub.Config({ TeX: {equationNumbers: {autoNumber: [&amp;quot;AMS&amp;quot;], useLabelIds: true}}, &amp;quot;HTML-CSS&amp;quot;: {linebreaks: {automatic: true}}, SVG: {linebreaks: {automatic: true}} }); &amp;lt;/script&amp;gt; 之后可以像 LaTeX 一样添加\label{}以及\eqref{}, \ref{}。 Mathjax 与 Markdown 的冲突（用 Pandoc 后，不再存在） 原文地址：http://blog.csdn.net/emptyset110/article/details/50123231 Hexo 先用marked.js渲染，然后再交给 MathJax 渲染。在marked.js渲染的时候下划线_是被 escape 掉并且换成了&amp;lt;em&amp;gt;标签，即斜体字，另外 LaTeX 中的\\也会被转义成一个\，这样会导致 MathJax 渲染时不认为它是一个换行符了。 修改marked.js源码的方式来避开这些问题 针对下划线的问题，取消_作为斜体转义，因为marked.js中*也是斜体的意思，所以取消掉_的转义并不影响我们使用markdown，只要我们习惯用*作为斜体字标记就行了。 针对marked.js与Mathjax对于个别字符二次转义的问题，我们只要不让marked.js去转义\\,\{,\}在MathJax中有特殊用途的字符就行了。 具体修改方式，用编辑器打开marked.js（在./node_modules/marked/lib/中） escape: /^\\([\\`*{}\[\]()# &#43;\-.!_&amp;gt;])/,1 em: /^\b_((?:[^_]|__)&#43;?)_\b|^\*((?:\*\*|[\s\S])&#43;?)\*(?!\*)/,1 替换成： escape: /^\\([`*\[\]()# &#43;\-.!_&amp;gt;])/,1 em:/^\*((?:\*\*|[\s\S])&#43;?)\*(?!\*)/, 2017.5.28↩ 可以通过 Hack mark.js的方式做一些简单的优化，参考本文中相关内容或者http://shomy.top/2016/10/22/hexo-markdown-mathjax/。↩ Pandoc 生成的脚注编号中, sup在a之中，虽然修改a的display为inline-flex或inine-grid可以移动a的下划线位置，但会导致hover背景颜色失效，因此暂时不修改编号的链接效果，https://github.com/jgm/pandoc/issues/2583的讨论表明这个问题还没有正式的解决方案。↩ RStudio 的 Pandoc 只在 RStudio 中使用。↩ 这里替换的思路来自https://github.com/wzpan/hexo-renderer-pandoc。↩ 用 Pandoc 进行解释，这个问题已经得到解决。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[ggimage：ggplot2中愉快地使用图片 z]]></title>
    	<url>/tech/2017/03/29/ggimage/</url>
		<content type="text"><![CDATA[[原文地址：https://cos.name/2017/04/ggimage/ 作者简介：余光创，香港大学公共卫生学院，生物信息学博士生。 博客：https://guangchuangyu.github.io， 公众号：biobabble 导言 本文介绍了ggimage包，允许在ggplot2作图时嵌入图片，并支持aes映射，可以把离散型变量映射到不同图片。目前有几个包可以使用图片嵌入做图，但都是针对特定的场景，这里使用ggimage来展示在这些特定领域里的应用，ggimage的设计是通用的，并不被特定场景所限定，文末又介绍了用R图标来画出R、用饼图来画气泡图等实例。 图上嵌图片 R 基础图形库（base graphics）可以在做图的时候嵌入图片，使用的是graphics::rasterImage： imgurl &amp;lt;- &amp;quot;http://phylopic.org/assets/images/submissions/295cd9f7-eef2-441e-ba7e-40c772ca7611.256.png&amp;quot; library(EBImage) x &amp;lt;- readImage(imgurl) plot(1, type = &amp;quot;n&amp;quot;, xlab = &amp;quot;&amp;quot;, ylab = &amp;quot;&amp;quot;, xlim = c(0, 8), ylim = c(0, 8)) rasterImage(x, 2, 2, 6, 4) 如果我们搜索&amp;rdquo;ggplot2 image&amp;rdquo;，会找到类似于下面这样的帖子/博文： r - Inserting an image to ggplot2 - Stack Overflow Add a background png image to ggplot2 | R-bloggers 也就是说通过程序员秘笈，搜索，我们用ggplot2同样也可以做到。 这里我们需要用到annotation_custom(rasterGrob)来把图片加到ggplot2图形中，这和基础图形库是一模一样的。 library(grid) library(ggplot2) p &amp;lt;- ggplot(d = data.frame(x = c(0, 8), y = c(0, 8)), aes(x, y)) &#43; geom_blank() p &#43; annotation_custom(rasterGrob(x), 2, 6, 2, 4) 如果要使用图片来打点画一个散点图，我们就需要for循环，对每一个点进行操作，这显然是底层的操作，而ggplot2是一个高抽象的画图系统，我们希望能够使用ggplot2的语法。 ggimage就是来实现这样一个功能，它只是一个简单的包，允许我们在ggplot2中把离散性变量映射到不同的图片来画图。 实现这个功能的想法已经酝酿很久了，在ggtree的开发中，我实现了phylopic函数来使用Phylopic数据库的图片注释进化树，也实现了subview函数在图上嵌入小图。用图片来注释进化树在进化分析上还是很常见的，特别是在一些分类学的研究中，需要把一些分类学特征在进化树上展示出来，而像我们做病毒，也经常会把一些图片放在进化树上来展示病毒的宿主信息。 ggtree和可视化有关的函数分两类，一类是加注释的图层，另一类是可视化操作树（比如像旋转、合并分支）。操作树的都是普通函数，而加注释的都是geom图层，除了subview和phylopic，这种所谓逼死处女座的存在，我早就想改成了geom_subview和geom_phylopic了（已实现），这也是为什么我要写ggimage的原因了。 安装 ggimage依赖于EBImage来读图片，这是个Bioconductor包，所以我们需要额外的动作来安装它，用setRepositories把Bioconductor软件仓库加进来，这样install.packages也可以搜索到它的包。 setRepositories(ind = 1:2) install.packages(&amp;quot;ggimage&amp;quot;) 实例分析 据我所知目前支持使用图片的R包有CatterPlots, rphylopic, emoGG,ggflags这几个，都是为特定的目的而实现的，都有其特定的应用场景，而ggimage是的geom_image是通用的，通过对它进行简单的包装，同样可以实现这些特殊场景的应用图层。 CatterPlots这个包只可以应用于基础图形库（base graphics）中，通过预设的几个猫图（R对象，随包载入）来画散点图。最近RevolutionAnalytics 有博文介绍。ggplot2没有相应画猫的包。我们可以使用ggimage来画，而且不用限制于CatterPlots预设的几个图形。 library(ggplot2) library(ggimage) mytheme &amp;lt;- theme_minimal() &#43; theme(axis.title = element_blank()) theme_set(mytheme) x &amp;lt;- seq(-2 * pi, 2 * pi, length.out = 30) d &amp;lt;- data.frame(x = x, y = sin(x)) img &amp;lt;- &amp;quot;http://www.belleamibengals.com/bengal_cat_2.png&amp;quot; ggplot(d, aes(x, y)) &#43; geom_image(image = img, size = .1) CatterPlots实现的方式就是上面谈到的rasterImage内部使用了循环。rphylopic同时支持基础图形库（base graphics）和ggplot2，也是一样的实现方式，不过rphylopic内部没有使用循环，一次只能加一个图，它使用的图来自于phylopic数据库。 我们用ggimage同样可以使用phylopic图片： ggplot(d, aes(x, y)) &#43; geom_phylopic(image = &amp;quot;500bd7c6-71c1-4b86-8e54-55f72ad1beca&amp;quot;, size = .1) 图中是翼足目动物。 emoGG是专门来画emoji的，如果要画emoji的话，我推荐我写的emojifont包，在轩哥的showtext基础上，把emoji当做普通字体一样操作，更方便。 emoGG这个包提供了geom_emoji图层，虽然一次可以画出散点，但因为不支持aes映射，而ggimage所提供的geom_emoji则支持映射，下面的例子中我们做了一个简单的回归分析，如果残差&amp;lt;0.5用笑脸表示，&amp;gt;0.5则用哭脸来表示。 set.seed(123) iris2 &amp;lt;- iris[sample(1:nrow(iris), 30), ] model &amp;lt;- lm(Petal.Length ~ Sepal.Length, data = iris2) iris2$fitted &amp;lt;- predict(model) p &amp;lt;- ggplot(iris2, aes(x = Sepal.Length, y = Petal.Length)) &#43; geom_linerange(aes(ymin = fitted, ymax = Petal.Length), colour = &amp;quot;purple&amp;quot;) &#43; geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2]) p &#43; ggimage::geom_emoji(aes(image = ifelse(abs(Petal.Length-fitted) &amp;gt; 0.5, &#39;1f622&#39;, &#39;1f600&#39;))) 如果要用emoGG来做的话，则需要自己切数据分两次来进行： p &#43; emoGG::geom_emoji(data = subset(iris2, (Petal.Length-fitted) &amp;lt; 0.5), emoji = &amp;quot;1f600&amp;quot;) &#43; emoGG::geom_emoji(data = subset(iris2, (Petal.Length-fitted) &amp;gt; 0.5), emoji = &amp;quot;1f622&amp;quot;) 这里我们只分两类(残差是否大于0.5)，所以需要加两次，试想我们的分类变量有多种可能的取值，则我们需要分多次切数据加图层，CatterPlots、rphylopic和emoGG都有这个问题，这也是aes映射之于ggplot2的重要和强大之处，它让我们可以在更高的抽像水平思考， ggflags是支持aes映射的，只不过它只能用来画国旗而已。同样ggimage也提供了相应的geom_flag来使用国旗用于做图。 library(rvest) library(dplyr) url &amp;lt;- &amp;quot;http://www.nbcolympics.com/medals&amp;quot; medals &amp;lt;- read_html(url) %&amp;gt;% html_nodes(&amp;quot;table&amp;quot;) %&amp;gt;% html_table() %&amp;gt;% .[[1]] library(countrycode) library(tidyr) medals &amp;lt;- medals %&amp;gt;% mutate(code = countrycode(Country, &amp;quot;country.name&amp;quot;, &amp;quot;iso2c&amp;quot;)) %&amp;gt;% gather(medal, count, Gold:Bronze) %&amp;gt;% filter(Total &amp;gt;= 10) head(medals) Country Total code medal count Russia 33 RU Gold 13 United States 28 US Gold 9 Norway 26 NO Gold 11 Canada 25 CA Gold 10 Netherlands 24 NL Gold 8 Germany 19 DE Gold 8 首先我们从网站上爬回来2016年各个国家的奥林匹克奖牌数，画出柱状图，并在xlab国家名边上用ggimage画上国旗： p &amp;lt;- ggplot(medals, aes(Country, count)) &#43; geom_col(aes(fill = medal), width = .8) p &#43; geom_flag(y = -2, aes(image = code)) &#43; coord_flip() &#43; expand_limits(y = -2) &#43; scale_fill_manual(values = c(&amp;quot;Gold&amp;quot; = &amp;quot;gold&amp;quot;, &amp;quot;Bronze&amp;quot; = &amp;quot;#cd7f32&amp;quot;, &amp;quot;Silver&amp;quot; = &amp;quot;#C0C0C0&amp;quot;)) ggimage 前面我们介绍了ggimage在一些场景的应用实例，虽然有专门的包针对这些应用场景，但ggimage在这些领域中的表现要比大多数的包要好（支持aes映射）。但ggimage的使用并不限于这些(geom_phylopic，geom_emoji和geom_flag只是通用图层geom_image的简单封装)，这里将展示一些有趣的例子。 用R图标来画R形状 x &amp;lt;- c(2, 2, 2, 2, 2, 3, 3, 3.5, 3.5, 4) y &amp;lt;- c(2, 3, 4, 5, 6, 4, 6, 3, 5, 2) d &amp;lt;- data.frame(x = x, y = y) img &amp;lt;- system.file(&amp;quot;img&amp;quot;, &amp;quot;Rlogo.png&amp;quot;, package = &amp;quot;png&amp;quot;) ggplot(d, aes(x, y)) &#43; geom_image(image = img, size = .1) &#43; xlim(0, 6) &#43; ylim(0, 7) 嵌套式绘图 这里我要展示的是非常有名的气泡图（Bubble Plot），但气泡不是圆圈，而是使用ggplot2画的饼图，我先把饼图保存起来，再用ggimage拿来画，饼图的大小与人口总数正相关。这个例子可以应用到很多场景中去，比如一个时间序列的曲线，你要用统计图在某些时间点上展示相关的信息，比如你要在地图上加某些地方的相关统计信息（如果要在地图上画饼图，可以使用我写的scatterpie包）。 crime &amp;lt;- read.csv(&amp;quot;http://datasets.flowingdata.com/crimeRatesByState2005.tsv&amp;quot;, header = TRUE, sep = &amp;quot;\t&amp;quot;, stringsAsFactors = F) state murder Forcible_rate Robbery aggravated_assult burglary larceny_theft motorvehicletheft population Alabama 8.2 34.3 141.4 247.8 953.8 2650.0 288.3 4627851 Alaska 4.8 81.1 80.9 465.1 622.5 2599.1 391.0 686293 Arizona 7.5 33.8 144.4 327.4 948.4 2965.2 924.4 6500180 Arkansas 6.7 42.9 91.1 386.8 1084.6 2711.2 262.1 2855390 California 6.9 26.0 176.1 317.3 693.3 1916.5 712.8 36756666 Colorado 3.7 43.4 84.6 264.7 744.8 2735.2 559.5 4861515 library(gtable) plot_pie &amp;lt;- function(i) { df &amp;lt;- gather(crime[i, ], type, value, murder:motor_vehicle_theft) ggplot(df, aes(x = 1, value, fill = type)) &#43; geom_col() &#43; coord_polar(theta = &#39;y&#39;) &#43; ggtitle(crime[i, &amp;quot;state&amp;quot;]) &#43; theme_void() &#43; theme_transparent() &#43; theme(legend.position = &amp;quot;none&amp;quot;, plot.title = element_text(size = rel(6), hjust = 0.5)) } pies &amp;lt;- sapply(1:nrow(crime), function(i) { outfile &amp;lt;- paste0(&amp;quot;crime_&amp;quot;, i, &amp;quot;.png&amp;quot;) plot_pie(i) &#43; ggsave(outfile, bg = &amp;quot;transparent&amp;quot;) outfile }) radius &amp;lt;- sqrt(crime$population / pi) crime$radius &amp;lt;- 0.2 * radius/max(radius) crime$pie &amp;lt;- pies leg1 &amp;lt;- gtable_filter( ggplot_gtable( ggplot_build(plot_pie(1) &#43; theme(legend.position = &amp;quot;right&amp;quot;)) ), &amp;quot;guide-box&amp;quot;) ggplot(crime, aes(murder, Robbery)) &#43; geom_image(aes(image = pie, size = I(radius))) &#43; geom_subview(leg1, x = 8.8, y = 50) 我们还可以每次只画一个州的数据，制作成动图。 plot_crime &amp;lt;- function(i) { o &amp;lt;- paste0(i, &amp;quot;.png&amp;quot;) ggplot(crime, aes(murder, Robbery)) &#43; geom_blank() &#43; geom_image(data = crime[i, ], aes(image = pie, size = I(radius))) &#43; geom_subview(p, leg1, x = 8.8, y = 50) &#43; ggsave(o) o } library(magick) library(purrr) order(crime$murder, decreasing = F) %&amp;gt;% map(plot_crime) %&amp;gt;% map(image_read) %&amp;gt;% image_join() %&amp;gt;% image_animate(fps = 2) %&amp;gt;% image_write(&amp;quot;crime.gif&amp;quot;) geom_subview可以图上嵌图，并不需要保存为图片，但对于ggplot2来讲，保存图片也是有好处的，因为ggplot2画图，点线是在数据空间上，随着我们保存图片的大小是按比例缩小或放大的，但文字是在像素空间上，和画图空间并不相关。所以当我们嵌图时缩小了画图窗口之后，字体会显得格外大，微调起来也比较繁琐，这时候保存为合适尺寸的图片，再用geom_image来加上去，显然就轻松得多。 其它来自R社区的例子 SAS博客对M&amp;amp;M巧克力的颜色分布做了分析，通过模拟估计不同颜色的置信区间。这个分析被翻译成R，并产生下图： 其中垂直片段|是真实值，水平片段当然就是置信空间了，而估计值用了ggimage来画不同颜色的巧克力。 另一个例子是迪斯尼电影主人公名字的流行程度: 最近我还添加了geom_pokemon图层，让大家可以用pokemon来画图，比如： ggimage是通用的包，所以可以被应用于不同的领域/场景中，起码可以让我们画出更好玩的图出来，后续我有时间的话，会写一个draw_key_image的函数，实现使用图片来当legend key的功能。 最后祝大家玩得开心！不要把图画得太有魔性哦:) 感谢大为和太云的校稿，特别是大为提出很多修改意见以及给出了用R画R的例子。 参考资料 https://stackoverflow.com/questions/9917049/inserting-an-image-to-ggplot2 https://www.r-bloggers.com/add-a-background-png-image-to-ggplot2/ https://github.com/GuangchuangYu/ggimage https://github.com/Gibbsdavidl/CatterPlots https://github.com/sckott/rphylopic https://github.com/baptiste/ggflags http://blog.revolutionanalytics.com/2017/02/catterplots-plots-with-cats.html http://blogs.sas.com/content/iml/2017/02/20/proportion-of-colors-mandms.html http://rpubs.com/hrbrmstr/mms https://rpubs.com/bhaskarvk/disney https://cran.r-project.org/package=scatterpie]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Hexo 的 NexT 主题中的 pre 环境、Mathjax 公式]]></title>
    	<url>/tech/2017/03/28/mathjax/</url>
		<content type="text"><![CDATA[[Mathjax 指令参考：http://docs.mathjax.org/en/latest/tex.html 提醒：现在通过 Pandoc 解释 Hexo 下的 Markdown， Mathjax 良好兼容，但要注意 Pandoc 对 LaTeX 语法的支持中，行内公式和行间公式分别用 `$...$` 和 `$$ $$` 完成，不支持使用 “\(...\)” 和 “\[...\]” 语法，当然 “\begin{equation}...\end{equation}” 还是可以正常使用的。 公式 支持带编号的公式 在原有的themes\next\layout\_third-party.mathjax.swig中添加如下内容： &amp;lt;script type=&amp;quot;text/x-mathjax-config&amp;quot;&amp;gt; MathJax.Hub.Config({ TeX: {equationNumbers: {autoNumber: [&amp;quot;AMS&amp;quot;], useLabelIds: true}}, &amp;quot;HTML-CSS&amp;quot;: {linebreaks: {automatic: true}}, SVG: {linebreaks: {automatic: true}} }); &amp;lt;/script&amp;gt; 之后可以像 LaTeX 一样添加\label{}以及\eqref{}, \ref{}。 mathop \mathop{\arg\,\min} pre 环境 本模板提供了一个&amp;lt;pre class=&amp;quot;white&amp;quot;&amp;gt;&amp;lt;code&amp;gt;...&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;的环境用于生成背景为白色的代码环境，注意&amp;lt;pre&amp;gt;与&amp;lt;code&amp;gt;之间可以换行，但&amp;lt;code&amp;gt;与文本之间要紧密相联；还提供了&amp;lt;pre class=&amp;quot;center&amp;quot;&amp;gt;&amp;lt;/pre&amp;gt;用于生成文字居中效果，不需要在行与行之间换行！ 由于&amp;gt;和```生成的代码与 html 中的&amp;lt;pre&amp;gt;没有关系，因此无法通过 Pandoc 提供的代码额外属性添加功能添加center或者white，只能用手工方法自己添加 html 来生成上面两种特殊的&amp;lt;pre&amp;gt;环境，普通的&amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;通过缩进 4 个空格的方式完成。 下面是emoji和脚注的示例（注意用```…```生成的代码有行号，而用 4 个空格生成的没有。）： :smile: :smirk: :relieved: Here is an inline note.^[Inlines notes are easier to write, since you don&amp;#39;t have to pick an identifier and move down to type the note.] Here is a footnote reference,[^1] and another.[^longnote] [^1]: Here is the footnote. [^longnote]: Here&amp;#39;s one with multiple blocks. Subsequent paragraphs are indented to show that they belong to the previous footnote. 这个例子中，“Subsequent…”也是脚注的一部分，因此脚注的结束符号将在“footnote.”之后。 提醒：与 Hexo 默认的 Markdown 引擎相比，Markdown-it 引擎要严格一些：默认的引擎可以解释 html 中的 md 部分，但 Markdown-it 在遇到 html 中的 md 时，会跳过去。当然，不选择默认引擎的重要原因是默认引擎没有脚注支持。 根据这篇博客的解释，原生的 Markdown 支持的内容最少，在 Pandoc 的 Markdown 扩展中，除了&amp;lt;script&amp;gt;, &amp;lt;style&amp;gt;之间的 html 内容，其它部分 html 中的 markdown 都可以得到正常的解释。因此上面提醒中的笔记实际上是非常不准确的； emoji只在Markdown-it中可以得到支持，在 Pandoc 中并不支持。 与 Markdown 的冲突解决（使用 Pandoc 后不再有问题） 原文地址：http://blog.csdn.net/emptyset110/article/details/50123231 hexo先用marked.js渲染，然后再交给MathJax渲染。在marked.js渲染的时候下划线_是被escape掉并且换成了&amp;lt;em&amp;gt;标签，即斜体字，另外LaTeX中的\\也会被转义成一个\，这样会导致MathJax渲染时不认为它是一个换行符了。 修改marked.js源码的方式来避开这些问题 - 针对下划线的问题，取消_作为斜体转义，因为marked.js中*也是斜体的意思，所以取消掉_的转义并不影响我们使用markdown，只要我们习惯用*作为斜体字标记就行了。 - 针对marked.js与Mathjax对于个别字符二次转义的问题，我们只要不让marked.js去转义\\,\{,\}在MathJax中有特殊用途的字符就行了。 具体修改方式，用编辑器打开marked.js（在./node_modules/marked/lib/中），将其中的： escape: /^\\([\\`*{}\[\]()# &#43;\-.!_&amp;gt;])/,1 em: /^\b_((?:[^_]|__)&#43;?)_\b|^\*((?:\*\*|[\s\S])&#43;?)\*(?!\*)/,1 替换成： escape: /^\\([`*\[\]()# &#43;\-.!_])/,1 em:/^\*((?:\*\*|[\s\S])&#43;?)\*(?!\*)/,]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[谈谈 Bias-Variance Tradeoff z]]></title>
    	<url>/data/2017/03/25/bias-variance-tradeoff/</url>
		<content type="text"><![CDATA[[原文地址：http://liam0205.me/2017/03/25/bias-variance-tradeoff/ 准确是两个概念。准是 bias 小 variance 小。准确是相对概念，因为 bias-variance tradeoff。 ——Liam Huang 在机器学习领域，人们总是希望使自己的模型尽可能准确地描述数据背后的真是规律。通俗所言的「准确」，其实就是误差小。在领域中，排除人为失误，人们一般会遇到三种误差来源：随机误差、偏差和方差。偏差和方差又与「欠拟合」及「过拟合」紧紧联系在一起。由于随机误差是不可消除的，所以此篇我们讨论在偏差和方差之间的权衡（Bias-Variance Tradeoff）。 定义 数学上定义 首先需要说明的是随机误差。随机误差是数据本身的噪音带来的，这种误差是不可避免的。一般认为随机误差服从高斯分布，记作 \(\epsilon\sim\mathcal N(0, \sigma_\epsilon)\)。因此，若有变量 \(y\) 作为预测值，以及 \(X\) 作为自变量（协变量），那么我们将数据背后的真实规律 \(f\) 记作 \[y = f(X) &#43; \epsilon.\] 偏差和方差则需要在统计上做对应的定义。 偏差（bias）描述的是通过学习拟合出来的结果之期望，与真实规律之间的差距，记作 \(\text{Bias}(X) = E[\hat f(X)] - f(X)\)。 方差（variance）即是统计学中的定义，描述的是通过学习拟合出来的结果自身的不稳定性，记作 \(\text{Var}(X) = E\Bigl[\hat f(X) - E[\hat f(X)]\Bigr]\)。 以均方误差为例，有如下推论 \[\begin{equation} \begin{aligned} \text{Err}(X) &amp;amp;= E\Bigl[\bigl(y - \hat f(X)\bigr)^2\Bigr] \\ &amp;amp;= E\Bigl[\bigl(f(X) &#43; \epsilon - \hat f(X)\bigr)^2\Bigr] \\ &amp;amp;= \left(E[\hat{f}(X)]-f(X)\right)^2 &#43; E\left[\left(\hat{f}(X)-E[\hat{f}(X)]\right)^2\right] &#43;\sigma_\epsilon^2 \\ &amp;amp;= \text{Bias}^2 &#43; \text{Variance} &#43; \text{Random Error}. \end{aligned} \label{eq:err-comp} \end{equation}\] 直观的图示 下图将机器学习任务描述为一个「打靶」的活动：根据相同算法、不同数据集训练出的模型，对同一个样本进行预测；每个模型作出的预测相当于是一次打靶。 http://scott.fortmann-roe.com/docs/BiasVariance.html 左上角的示例是理想状况：偏差和方差都非常小。如果有无穷的训练数据，以及完美的模型算法，我们是有办法达成这样的情况的。然而，现实中的工程问题，通常数据量是有限的，而模型也是不完美的。因此，这只是一个理想状况。 右上角的示例表示偏差小而方差大。靶纸上的落点都集中分布在红心周围，它们的期望落在红心之内，因此偏差较小。另外一方面，落点虽然集中在红心周围，但是比较分散，这是方差大的表现。 左下角的示例表示偏差大二方差小。显而易见，靶纸上的落点非常集中，说明方差小。但是落点集中的位置距离红心很远，这是偏差大的表现。 右下角的示例则是最糟糕的情况，偏差和方差都非常大。这是我们最不希望看到的结果。 举个栗子 现在我们做一个模拟实验，用以说明至此介绍的内容。 首先，我们生成了两组 array，分别作为训练集和验证集。这里，x 与 y 是接近线性相关的，而在 y 上加入了随机噪声，用以模拟真实问题中的情况。 import numpy as np np.random.seed(42) # the answer to life, the universe and everything real = lambda x:x &#43; x ** 0.1 x_train = np.linspace(0, 15, 100) y_train = map(real, x_train) y_noise = 2 * np.random.normal(size = x_train.size) y_train = y_train &#43; y_noise x_valid = np.linspace(0, 15, 50) y_valid = map(real, x_valid) y_noise = 2 * np.random.normal(size = x_valid.size) y_valid = y_valid &#43; y_noise 现在，我们选用最小平方误差作为损失函数，尝试用多项式函数去拟合这些数据。 prop = np.polyfit(x_train, y_train, 1) prop_ = np.poly1d(prop) overf = np.polyfit(x_train, y_train, 15) overf_ = np.poly1d(overf) 这里，对于 prop，我们采用了一阶的多项式函数（线性模型）去拟合数据；对于 overf，我们采用了 15 阶的多项式函数（多项式模型）去拟合数据。如此，我们可以把拟合效果绘制成图。 import matplotlib.pyplot as plt _ = plt.figure(figsize = (14, 6)) plt.subplot(1, 2, 1) prop_e = np.mean((y_train - np.polyval(prop, x_train)) ** 2) overf_e = np.mean((y_train - np.polyval(overf, x_train)) ** 2) xp = np.linspace(-2, 17, 200) plt.plot(x_train, y_train, &amp;#39;.&amp;#39;) plt.plot(xp, prop_(xp), &amp;#39;-&amp;#39;, label = &amp;#39;proper, err: %.3f&amp;#39; % (prop_e)) plt.plot(xp, overf_(xp), &amp;#39;--&amp;#39;, label = &amp;#39;overfit, err: %.3f&amp;#39; % (overf_e)) plt.ylim(-5, 20) plt.legend() plt.title(&amp;#39;train set&amp;#39;) plt.subplot(1, 2, 2) prop_e = np.mean((y_valid - np.polyval(prop, x_valid)) ** 2) overf_e = np.mean((y_valid - np.polyval(overf, x_valid)) ** 2) xp = np.linspace(-2, 17, 200) plt.plot(x_valid, y_valid, &amp;#39;.&amp;#39;) plt.plot(xp, prop_(xp), &amp;#39;-&amp;#39;, label = &amp;#39;proper, err: %.3f&amp;#39; % (prop_e)) plt.plot(xp, overf_(xp), &amp;#39;--&amp;#39;, label = &amp;#39;overfit, err: %.3f&amp;#39; % (overf_e)) plt.ylim(-5, 20) plt.legend() plt.title(&amp;#39;validation set&amp;#39;) 多项式拟合结果 以训练集上的结果来说，线性模型的误差要明显高于多项式模型。站在人类观察者的角度来说，这似乎是显而易见的：数据是围绕一个近似线性的函数附近抖动的，那么用简单的线性模型，自然就无法准确地拟合数据；但是，高阶的多项式函数可以进行各种「扭曲」，以便将训练集的数据拟合得更好。 这种情况，我们说线性模型在训练集上欠拟合（underfitting），并且它的偏差（bias）要高于多项式模型的偏差。 但这并不意味着线性模型在这个问题里，要弱于多项式模型。我们看到，在验证集上，线性模型的误差要小于多项式模型的误差。并且，线性模型在训练集和验证集上的误差相对接近，而多项式模型在两个数据集上的误差，差距就很大了。 这种情况，我们说多项式模型在训练集上过拟合（overfitting），并且它的方差（variance）要高于线性模型的偏差。此外，因为线性模型在两个集合上的误差较为接近，因此我们说线性模型在训练过程中未见的数据上，泛化能力更好。因为，在真实情况下，我们都需要使用有限的训练集去拟合模型，而后工作在无限的真实样本中，而这些真实样本对于模型训练过程都是不可见的。所以，模型的泛化能力，是非常重要的指标。 考虑到两个模型在验证集上的表现，在这个任务上，我们说线性模型表现得较好。 权衡之术 克服 OCD 对于很多人来说，不可避免地会有这样的强迫症：希望训练误差降至 0。 我们说，人想要过得快乐，首先要接纳自己，与自己和解。做机器学习相关的任务也是一样，首先要理解和接受机器学习的基本规律，克服自己的强迫症。 首先，对于误差，在公式 中，我们得知误差中至少有「随机误差」是无论如何不可避免的。因此，哪怕有一个模型在训练集上的表现非常优秀，它的误差是 0，这也不能说明这个模型完美无缺。因为，训练集本身存在的误差，将会被带入到模型之中；也就是说，这个模型天然地就和真实情况存在误差，于是它不是完美的。 其次，由于训练样本无法完美地反应真实情况（样本容量有限、抽样不均匀），以及由于模型本身的学习能力存在上限，也意味着我们的模型不可能是完美的。 因此，我们需要克服强迫症，不去追求训练误差为 0；转而去追求在给定数据集和模型算法的前提下的，逼近最优结果。 最佳平衡点的数学表述 在实际应用中，我们做模型选择的一般方法是： 选定一个算法； 调整算法的超参数； 以某种指标选择最合适的超参数组合。 也就是说，在整个过程中，我们固定训练样本，改变模型的描述能力（模型复杂度）。不难理解，随着模型复杂度的增加，其描述能力也就会增加；此时，模型在验证集上的表现，偏差会倾向于减小而方差会倾向于增大。而在相反方向，随着模型复杂度的降低，其描述能力也就会降低；此时，模型在验证集上的表现，偏差会倾向于增大而方差会倾向于减小。 考虑到，模型误差是偏差与方差的加和，因此我们可以绘制出这样的图像。 偏差与误差的变化趋势 http://scott.fortmann-roe.com/docs/BiasVariance.html 图中的最有位置，实际上是 total error 曲线的拐点。我们知道，连续函数的拐点意味着此处一阶导数的值为 0。考虑到 total error 是偏差与方差的加和，所以我们有，在拐点处： \[\begin{equation} \newcommand{\dif}{\mathop{}\!\mathrm{d}} \frac{\dif\text{Bias}}{\dif\text{Complexity}} = - \frac{\dif\text{Variance}}{\dif\text{Complexity}} \label{eq:sweet} \end{equation}\] 公式 给出了寻找最优平衡点的数学描述。若模型复杂度大于平衡点，则模型的方差会偏高，模型倾向于过拟合；若模型复杂度小于平衡点，则模型的偏差会偏高，模型倾向于过拟合。 过拟合与欠拟合的外在表现 尽管有了上述数学表述，但是在现实环境中，有时候我们很难计算模型的偏差与方差。因此，我们需要通过外在表现，判断模型的拟合状态：是欠拟合还是过拟合。 同样地，在有限的训练数据集中，不断增加模型的复杂度，意味着模型会尽可能多地降低在训练集上的误差。因此，在训练集上，不断增加模型的复杂度，训练集上的误差会一直下降。 因此，我们可以绘制出这样的图像。 训练集和验证集上的误差变化 http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/ 因此， 当模型处于欠拟合状态时，训练集和验证集上的误差都很高； 当模型处于过拟合状态时，训练集上的误差低，而验证集上的误差会非常高。 处理欠拟合与过拟合 有了这些分析，我们就能比较容易地判断模型所处的拟合状态。接下来，我们就可以参考 Andrew Ng 博士提供的处理模型欠拟合/过拟合的一般方法了。 机器学习调试的一般流程 欠拟合 当模型处于欠拟合状态时，根本的办法是增加模型复杂度。我们一般有以下一些办法： 增加模型的迭代次数； 更换描述能力更强的模型； 生成更多特征供训练使用； 降低正则化水平。 过拟合 当模型处于过拟合状态时，根本的办法是降低模型复杂度。我们则有以下一些武器： 扩增训练集； 减少训练使用的特征的数量； 提高正则化水平。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[LaTeX 中的浮动体：处理超宽问题]]></title>
    	<url>/tech/2017/03/22/floats-in-latex-handle-overfull-floats/</url>
		<content type="text"><![CDATA[[原文地址：http://liam0205.me/2017/03/22/floats-in-LaTeX-handle-overfull-flo 前文说了，浮动体主要是处理高度比较大，又不方便分割的内容：比如图片和表格。实际上，此类内容除了在高度上可能很高，它们也可能很宽。LaTeX 在水平方向，会贴着版芯的左边边界，开始排列内容。因此，如果一张图片或者表格的宽度超过了版芯的宽度，那么看起来就像是没有居中，而是偏右。 此篇我们讲一下如何处理此类情况。 缩小 对付内容过大，最直接的办法，就是把它们缩小。对于图片，如果使用了 graphicx 宏包，我们可以使用 width = \linewidth 的参数将图片缩放到正好填满页面宽度的大小，避免「超宽」。对于表格等其他内容，我们可以使用 graphicx 提供的 \resizebox 命令来处理。 \documentclass{article} \usepackage{showframe} \usepackage{graphicx} \begin{document} \begin{table}[!htb] \centering \caption{Oh, this table is overfull!}\label{tab:overfull} \rule{1.1\linewidth}{3cm} \end{table} \begin{table}[!htb] \centering \caption{Imagine that this is a table.}\label{tab:resized} \resizebox{\linewidth}{!}{\rule{1.1\linewidth}{3cm}} \end{table} \begin{figure}[!htb] \centering \includegraphics[width = \linewidth]{example-image} \caption{A fit figure.}\label{fig:example-image} \end{figure} \end{document} 想办法居中 上述通过缩小解决问题，是一种办法。但是，在很多情况下，也会存在问题；比如 表格内容缩小之后，就看不清了； \verb 之类的内容，不能放在大多数 box 之内。 为此，我们需要用别的办法，尝试解决这些问题。 实际上，大多数用户对于这类问题最大的诉求在于：为什么这些超宽的图表不局中了？所以，我们只要解决「居中」的问题，可能就覆盖了绝大部分用户的需求。而这些内容无法居中的原因，我们在介绍部分也说过了：LaTeX 在水平方向，会贴着版芯的左边边界，开始排列内容。因此，如果我们能让 LaTeX 不从版芯的最左边开始排列内容，就有可能解决这个问题。 决定 LaTeX 从何处开始排列内容的，是 \leftskip 这个宏。在 LaTeX2e 中，它被默认定义为 \z@。也就是说，从版芯的左边边界处开始排列内容。我们可以修改这个宏，比如改为 \setlength{\leftskip}{-20pt}，那么 LaTeX 将从版芯左边边界左边的 20pt 的位置开始排列内容。 \documentclass{article} \usepackage{showframe} \usepackage{graphicx} \begin{document} \begin{table}[!htb] \centering \caption{Oh, this table is overfull!}\label{tab:overfull} \setlength{\leftskip}{-20pt} \rule{1.1\linewidth}{3cm} \end{table} \end{document} 同理，我们有 \rightskip，用于确定水平方向排版的终止位置与版芯右边界之间的距离。 我们知道，TeX 的 skip 是所谓的「弹簧」，允许在一定程度上进行缩放；而所谓的居中，实际上就是在版芯两侧，有两个力量相等的无限弹簧，同时向中间挤压内容。因此，我们不难得到对 \leftskip 和 \rightskip 的几个要求： 默认情况，应该贴着两侧边界； 最差的情况，应该允许内容向左右两侧延伸，超过版芯但不超过纸张宽度； 同时具有让内容居中的能力。 因此，我们可以将它们设置为（粗略地）： \setlength{\leftskip}{0pt plus 1fil minus \marginparwidth} \setlength{\rightskip}{\leftskip} 为了让它更好用，我们可以把他们收纳在一个新的命令当中（包含了一些额外的工作）： \documentclass{article} \usepackage{showframe} \usepackage{graphicx} \makeatletter \newcommand*{\centerfloat}{% \parindent \z@ \leftskip \z@ \@plus 1fil \@minus \marginparwidth \rightskip \leftskip \parfillskip \z@skip} \makeatother \begin{document} \begin{table}[!htb] \centerfloat \caption{Oh, this table is adjusted!}\label{tab:adjusted} \rule{1.1\linewidth}{3cm} \end{table} \end{document} 当然，你也可以通过 \makebox 命令来达成目标，不过这样依然无法容纳含有类似 \verb 的内容： \documentclass{article} \usepackage{showframe} \usepackage{graphicx} \begin{document} \begin{table}[!htb] \centering \caption{Oh, this table is adjusted!}\label{tab:adjusted} \makebox[0pt][c]{\rule{1.1\linewidth}{3cm}} \end{table} \end{document} 使用 adjustbox 宏包 Martin Scharrer 有发布名为 adjustbox 的宏包，提供了类似 graphicx 宏包中 \includegraphics 命令的 key-value 参数，用以实现各类 box 效果。值得一提的是，宏包提供的几个宏、环境，都可以容纳 \verb 之类的内容。很是好用。 \documentclass{article} \usepackage{showframe} \usepackage{adjustbox} \begin{document} \begin{table}[!htb] \centering \caption{Oh, this table is adjusted by the package adjustbox!}\label{tab:adjusted} \adjustbox{center}{\rule{1.1\linewidth}{3cm}} \end{table} \end{document} 效果和使用我们定义的 \centerfloat 命令类似，这里就不重复贴图了。 倘若把它倒过来…… 上面的介绍，基本都仅限于处理 overfull 程度不大、超出版芯程度不多的情形。如果你有一个大胖娃娃，他/她使得版芯宽度严重超载，那么你可能要考虑把它旋转九十度了。 rotating 宏包提供了 sidewaystable 和 sidewaysfigure 环境（以及带 * 的版本，用于在双栏模式下通栏排版），分别作为对应 table 和 figure 的工具。使用这些环境，能使图表旋转 90° 摆放。 \documentclass{article} \usepackage{showframe} \usepackage{rotating} \begin{document} \begin{sidewaystable}[!htb] \centering \caption{Let&#39;s rock!}\label{tab:rotated} \rule{0.8\linewidth}{3cm} \end{sidewaystable} \end{document} 需要注意的是，当旋转过来之后，「长宽」就交换了。因此，我们这里使用 0.8\linewidth 实际上是相对版芯的高度的 0.8 倍。此外，rotating 宏包默认将内容逆时针旋转了 90°，你也可以在调用宏包时传入 clockwise 参数，得到顺时针旋转的版本。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[商业承兑汇票和银行承兑汇票的区别 z]]></title>
    	<url>/prof/2017/03/22/acceptance-bill/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/53595037 银行承兑汇票 简称银票。是由采购方出票，向采购方开户银 它的票面长成这个样子。这个是新版的票据，从网上搜到很多银票的票样有的是旧版的，已经作废不用了。 它是怎么用的呢，流程大致是这样的。 举个例子比较清楚：A 公司向 B 公司买一批货物或者支付劳务什么的，总价款 1000 万，需要向 B 公司付款。但是 A 公司不想付现金给 B 公司。可能 A 公司没钱，或者暂时不想付给 B 公司现金，所以 A 公司就和 B 公司谈判，本公司要钱没有，要银行承兑汇票倒是有一张，B 公司为了实现销售，就同意了 A 公司的条件。A 公司就到了自己的开户银行，申请开立一张银行承兑汇票，这张银行承兑汇票的意思就是，我要给 B 公司付款，但是我现在没钱，直接打个白条 B 公司不认，现在我要开出一张汇票，请银行帮我做个付款承诺：这张汇票到期后，银行会到期无条件承诺兑付这张票据。银行同意后，A 公司开出一张银行承兑汇票，银行在承兑行处盖章，A 公司将这张票据交给 B 公司。B 公司拿到银行承兑汇票后，到期日时，B 公司将银行承兑汇票交回银行，银行负责将票据上的款项划给B公司。完毕。 流程图 商业承兑汇票 简称商票，是由企业直接签发的，用于买方远期支付给卖方的资金的信用凭证。 票面长成这个样子，从票面上看，没有了银行承兑的相关内容。 它的使用方法是这样的：首先企业从银行买回来商业承兑汇票。买卖双方企业在交易过程中，约定使用商业承兑汇票支付。买方开出商业承兑汇票，盖上自己的财务章，然后交给卖方，买方拿到商业承兑汇票后，在票据到期日的前三天将票据交回给买方的银行，请求买方的银行从买方的账户上划款给卖方。完毕。 区别 总体来讲，银行承兑汇票和商业承兑汇票有以下的区别： 承兑人不同：银行承兑汇票是由银行承兑的，商业承兑汇票是企业承兑的。但是有些企业为了让交易对手接受自己的商业承兑汇票，会请银行帮忙，做保证兑付，就是在商业承兑汇票上盖上银行的承兑章，达到和银行承兑汇票一样的效果。 信用不同：一般来讲，银行承兑汇票的信用和支付效率要优于商业承兑汇票。因为大家都认为银行比较靠谱。但并不完全一样，有时候一张农商行开具的银行承兑汇票，就没有一张由大型上市或者国企公司开出的商业承兑汇票的信用高。 最后，说一下电子票据。 以前票据都是纸质的，传递起来不方便，有丢失的风险，还有假冒、破损、背书不清楚等一系列问题。为了解决上述问题，有些银行开始做电子票据了，就是把开票的要素直接放在网上银行中完成。但是这些票据只能在一家银行内流转，流转起来不方便。人民银行顺应民意，搞了一个全国一体化的票据开票流转的平台，就是人行电票系统。他和纸质票据在功能上没有太大区别，但是电子票据的期限可以是一年的，而纸质票据最多是半年。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[LaTeX 中的浮动体：基础篇 z]]></title>
    	<url>/tech/2017/03/11/floats-in-latex-basic/</url>
		<content type="text"><![CDATA[[原文地址：http://liam0205.me/2017/03/11/floats-in-LaTeX-basic/ 此篇介绍一下 LaTeX 中的浮动体基 浮动体是什么 在实际撰写文稿的过程中，我们可能会碰到一些占据篇幅较大，但同时又不方便分页的内容。（比如图片和表格，通常属于这样的类型）此时，我们通常会希望将它们放在别的地方，避免页面空间不够而强行置入这些内容导致 overfull vbox 或者大片的空白。此外，因为被放在别的地方，所以，我们通常需要对这些内容做一个简单的描述，确保读者在看到这些大块的内容时，不至于无从下手去理解。同时，因为此类内容被放在别的地方，所以在文中引述它们时，我们无法用「下图」、「上表」之类的相对位置来引述他们。于是，我们需要对它们进行编号，方便在文中引用。 注意到，使用浮动体的根本目的是避免不合理的分页或者大块的空白，为此，我们需要将大块的内容移至别的地方。与之相辅相成的是浮动体的一些特性： 是一个容器，包含某些不可分页的大块内容； 有一个简短的描述，比如图题或者表题； 有一个编号，用于引述。 在 LaTeX 中，默认有 figure 和 table 两种浮动体。（当然，你还可以自定义其他类型的浮动体）在这些环境中，可以用 \caption{} 命令生成上述简短的描述。至于编号，也是用 \caption{} 生成的。这类编号遵循了 TeX 对于编号处理的传统：它们会自动编号，不需要用户操心具体的编号数值。 至于「别的地方」是哪里，LaTeX 为浮动体启用了所谓「位置描述符」的标记。基本来说，包含以下几种 h - 表示 here。此类浮动体称为文中的浮动体（in-text floats）。 t - 表示 top。此类浮动体会尝试放在一页的顶部。 b - 表示 bottom。此类浮动体会尝试放在一页的底部。 p - 表示 float page，浮动页。此类浮动体会尝试单独成页。 LaTeX 会将浮动体与文本流分离，而后按照位置描述符，根据相应的算法插入 LaTeX 认为合适的位置。 \documentclass{article} \begin{document} Figure \ref{fig:dummy} is a dummy figure to show the use of basic floats in \LaTeX{}. \begin{figure}[htb] \rule{4cm}{3cm} % a black box, treat it as a dummy figure \caption{Dummy figure}\label{fig:dummy} \end{figure} \end{document} 限制浮动效果 有些强（chu）迫（nv）症（zuo）宝宝希望保留浮动体的标题以及编号的功能，但是希望浮动体「乖乖待在插入的位置」。 对于这些小朋友，老夫必须说：「这是病，得治」。 说它是「病」，是因为浮动效果本身是好的；相反，禁止浮动效果，可能导致页面出现大片的空白。另一方面，这些小朋友希望浮动体待在原地，很可能是习惯了「下图」、「上表」这样的引述方式；而没有使用科技论文标准的「图 1」、「表 2」的因数方式。 因此，老夫墙裂建议各位小朋友，不要管它，随它浮动去吧。 当然，在一些极端的情况，也会出现 LaTeX 无法很好地处理浮动体放置位置的情况。这时候需要我们做一些辅助工作，帮助和限制 LaTeX 的浮动算法。 如果希望避免浮动体跨过 \section 等章节标题，可以使用 placeins 宏包。它能在章节标题前，强制输出上一章节中尚未输出的浮动体。 \usepackage[section]{placeins} 如果希望彻底禁止某个浮动体的浮动效果，可以使用 float 宏包提供的 H 位置选项。 \usepackage{float} % ... \begin{figure}[H] % ... \begin{table}[H] % ... 浮动体过多报错 LaTeX 是有底线的上限的。LaTeX 会把所有尚未确定位置的浮动体，放入 \@freelist 中暂存。而 \@freelist 默认情况下，最多能处理 18 个浮动体。因此，在某些极端情况下，如果 LaTeX 暂时无法处理的浮动体数目超过 18 个时，就会报错。 ! LaTeX Error: Too many unprocessed floats. 此时有两种解决问题的思路： 强制输出所有尚未确定位置的浮动体，清空 \@freelist； 增强 LaTeX 的处理能力。 对于第一种思路，我们可以用 \clearpage，或者 placeins 宏包提供的 \FloatBarrier 命令。两个命令都会输出所有尚未输出的浮动体。不同的是，\clearpage 会做一些额外的工作，比如另起一页，继续排版。个人建议使用 \FloatBarrier 命令，遵循「一个命令只做好一件事」的原则。 如果使用了 \FloatBarrier 命令，还是经常会报错提示未处理的浮动体过多，那么就要考虑第二种思路了。对于第二种思路，我们可以使用 morefloats 宏包。\usepackage[morefloats = 18]{morefloats}，来增加 18 个槽位，以便能够向 \@freelist 放入更多的浮动体。 在 2015 年，David Carlisle 在新版的 LaTeX2e (2015) 中实现了 \extrafloats 命令，可以方便地新增更多的槽位。具体用法只需在导言区执行该命令即可：\extrafloats{500}。 浮动体上下的垂直距离 最近总有人不爽 LaTeX 浮动体与周围文本的默认间距。LaTeX 浮动体相关的定义都可以在 source2e 当中找到，这里罗列重要的间距如下。 \floatsep - 相邻两个浮动体之间的垂直距离。 \textfloatsep - 页面中最后一个 t 模式的浮动体与文本的间距；页面中第一个 b 模式的浮动体与文本的间距。 \intextsep - 页面中共 h 模式的浮动体上下与文本的间距。 因此，你可以通过 \setlength 命令修改上述三个垂直距离，以便调整浮动体与前后文本的距离了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[语法高亮：在 lstinline 中给行内代码添加背景颜色 z]]></title>
    	<url>/tech/2017/03/07/lstlistings-bgcolor/</url>
		<content type="text"><![CDATA[[原文地址： https://tex.stackexchange.com/questions/357227/adding-background-color-to-verb-or-lstinline-command-without-colorbox 比较推荐的方法是用xparse宏包提供的DeclareDocumentCommand，使用这种方法的好处是用这种方法定义的命令\ \documentclass{article} \usepackage{xcolor,xparse} \usepackage{listings} \definecolor{cmdbg}{rgb}{0.8,0.8,0.8} \lstset{% basicstyle=\ttfamily, breaklines = true, backgroundcolor=\color{cmdbg}, } \usepackage{realboxes} % 提供 \Colorbox \DeclareDocumentCommand{\ccmd}{v}{% 参数 v 表示工作方法类似于 \verb \Colorbox{cmdbg}{\csname lstinline\endcsname!#1!}% } \begin{document} % demo using \lstinline only This is \lstinline|my code| % demo using \lstinline and \Colorbox This is \Colorbox{cmdbg}{\lstinline|my code|} \clist{my code} \end{document} 问题是： 将其中的article改成ctexbook或者ctexart之后，由\ccmd{\my code}指令生成的效果中\my code中的\会消失掉，只显示my code，而直接用\Colorbox{cmdbg}{\lstinline|\my code|}生成的效果中\不会消失，仍然正确显示为\my code。这说明 ctex 与xparse之间存在冲突。 也试过更新 ctex 包和 l3 相关的包后再试，但是更新总遇到问题，所以暂时放弃，不清楚是否有直接的方法可以 hack。 用xpatch方法在\lstinline上挂一个\Colorbox也可以，但是这种做法将导致\lstinline内的内容无法断行，不清楚前面使用xparse的方法是否也存在这一问题。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[欧洲卫生辛酸史 z]]></title>
    	<url>/arts/2017/02/26/health-history-in-europe/</url>
		<content type="text"><![CDATA[[罗马人将尿罐中的尿倒出窗外的做法持续了好几百年。很多人深受蓄意或无心的“天降暴雨”之害。受害者可向法院提出诉讼，要求赔偿。收取的损害赔偿金包括：医疗费用，以及当前和今后因缺工而失掉的薪水。由于被告并不总是能够被明确指认，罚款通常在所有住在倾倒污物区域的居民中收取。 同时代的作品也描写了从寓所窗户扔出的夜壶所导致的混乱状态。尤维纳利斯（Juvenal）所作的第三首讽刺诗将这种屡发的事故描述为“从不知名的高处猛然落下的暴雨”。约翰·德莱顿（John Dryden，公元 1671—1700 年）将其诠释为如下诗篇： 除非你已预先找好自己的位置，否则再想寻欢作乐就为时已晚。命运很多时候都是凑巧，因为街上有醒着的窗户：祈求万能的主、并料想不太可能摊上便壶的份儿。 1666 年的瘟疫和 1667 年的火情是 17 世纪英国的标志性事件。塞缪尔· 佩皮斯（Samuel Pepys）在其日志中描述了那些可怕的年代，其中谈到伦敦城内从上方窗户落下的飘忽不定的粪便。于乱世中出生的乔纳森·斯威夫特（Jonathan Swift），用如下语句来描绘伦敦的街道： 肉摊、粪堆、内脏和血液中产生的废弃物，溺死的幼犬、腥臭的西鲱，都浸湿在泥淖中，死猫混杂着芜菁的嫩叶随着洪流翻滚而下。 欧洲城市的居民们保留了罗马时代的习俗，将夜壶倒往窗外以处理其“内物”。幸而人们还礼貌地告知路人，要其留神即将降临的厄运。法国妇女在“内物”落下之前高呼“小心水！”英国人则将这种叫法改为“留心便座”，兴许是因为厕所的前身被称为“便座”吧。英国人往往还在其后加上一句“上帝保佑您”。在意大利语中，这句话变成“拿走您的提灯”。谦和有礼的男士们都走在女性的左侧，以保护后者，使其免受窗内所发的诡异莫测的攻击。这一习俗传承至今。 有人认为夜壶可任意处置。他们把整个壶扔到大马路上，而不是壶内物。这种野蛮的垃圾处理方式所导致的混乱状态致使巴黎官方于1395年颁布了一项法令，严禁将夜壶扔出窗外。然而，法国公民在17世纪时仍未改变这一陋习。凡尔赛市市长发表了一项声明，禁止“所有人将人体排泄物及其他垃圾扔出窗外”。 1666年英格兰流行瘟疫时，国王查理二世从伦敦移驾到牛津，以免疾病秧及自己。牛津市民安东尼·伍德（Anthony Wood）在日记中描述了国王殿下及其随从们骇人听闻的言行举止： 他们貌似整洁大方、华丽鲜艳，实则肮脏龌龊、兽性十足，所到之处皆留下满地粪便，烟囱，书房，油菜棚，地下室，无一幸免。他们粗鲁无礼、蓬头垢面、寻花问柳；目空一切、了无生趣且粗枝大叶。 奥地利的安妮（Anne），“太阳王”的母后，在法兰西宫殿背后的花毯上小便时被人撞见。 公爵夫人夏洛特·伊丽莎白（the Duchess Charlotte Elizabeth）是路易十四的随行人员之一。她一直不离其左右，从凡尔赛宫一路陪伴到枫丹白露宫，发觉自己只能在众目睽睽之下进行排便，夏洛特·伊丽莎白于是对女修道院院长颇为赏识，后者在每次有“需要”的时候都能够瞅准时机。只有在夜晚人们才能找回一点隐私。对被派去守护国王的瑞士禁卫队留在街上的“纪念品”，公爵夫人也只能摇头兴叹。 “大上榻”和“小上榻”典礼标志着白昼的结束。“大上榻”，或称“大就寝”，乃路易宽衣上榻的一种仪式。好几位贵族候于一旁，翘首企盼自己能有幸在国王更衣时为其秉持烛台。安寝准备就绪后，仅有屈指可数、精挑细选出的贵族能留在皇帝身边。这些享受非常待遇的人，花了高达 1.5 万个金路易1，才获得此项殊荣：顶礼膜拜皇帝着手另一个仪式，也就是小上榻。国王殿下亮出他尊贵的屁股，端坐在“便桶椅”上，即封闭式马桶上，举行这天最后的皇室典礼。 即便在现代的早期，那些恶臭满天、污物横流且不断受到病疫侵袭的欧洲城市，某些不讲卫生的做法也为大众所认可。马车夫被容许在车轮上小便。如前所述，坐在马桶上接宾待友也毫不失礼。不过，到了17世纪末年，上述两种做法都被视为有失体统。 约翰四世（John IV）和阿方佐六世（Alfonzo VI）统治年间，葡萄牙海岸线上的马德拉群岛（Madeira）对卫生事务严苛无比。聚会者如被发现在户外大小便，即遭逮捕拘留，或被认为是粗鄙猥琐之人。因此，他们都在门廊或门口撒尿。 17 世纪德国爱侣们对于性顾虑重重，他们当中很多人带着成堆的问题去求助村里的接生婆。对惧怕婚礼当晚圆房不顺者，安抚的处方通常是，穿过结婚戒指撒尿。显然，倘若男士能够瞄准结婚戒指中间那一小孔，他一定是个中好手。如果女性想草草完事，又不愿与他直接对质，她就得暗地里在这个不中意的爱人鞋子里抹一点儿自己的粪便。如此一来，在他的潜意识里，该女子的气味与臭味有了某种抹不掉的关系，这种气味会使他无心逗留。 坐浴盆是路易十四统治时代为了使宫延里的人不用宽衣就可以清洗隐私部位而专门发明的，1751年，人们用“篷巴度夫人的迷恋物”来暗指这一洗涤生殖器的装置。而自打 1763 年托马斯·史莫莱特（Thomas Smollet）发表其旅途信函以来，坐浴盆变得臭名昭著起来。史莫莱特写道：“法兰西女性的非凡的粗鄙岂为常理可容！她们当着男宾的面儿褪去脏乱不堪的衬衣，与之高谈阔论自己沐浴之事、内服药、还有坐浴盆！”在英国人及其他西方人眼里，法国坐浴盆是其国民道德败坏的结果，用来在纵欲后清洗生殖器。 走出房屋使用后院中分离的厕所极为不便，因此很多人仍依赖夜壶。乔纳森·斯威夫特在其 1745 年出版的著作《仆佣指南》（Direction to Servants）一书中，对自己深恶痛绝的那一做法颇有微辞： 我对那些自以为是而又懒散怠惰的女士深恶痛绝，她们从不费力踱入花园采撷一支半朵玫瑰，而总是据守一个臭烘烘的器皿，有时就在卧房里，或不由分说地在某个昏暗的隔间里，解决自己最差强人意的需求。而你们，则往往要把满满一容器东西带走。这些容器不仅熏臭了整个房间，也令身边所有人对她们的衣物掩鼻回避。那么，要根除她们这一令人作呕的恶习，我建议你们，作为其后果的承担者，在公开的场合将其器皿搬走，当着男仆们的面儿把它抬到主梯下。而且，倘若有人敲门，把容器端在手里去打开临街大门。这一切，不出意外的话，会使你的女主人宁肯大费周折到适宜场所去排便，也不愿将自己那些脏物暴露在家中所有男丁的眼皮底下。 18 世纪不少居民对室内抽水马桶的出现将信将疑。作家霍勒斯·沃波尔（Horace Walpole）坚信，只有腐化堕落的人才使用抽水马桶。沃波尔描述了自己 1760 年拜访依丽亚·雷丽亚·查得利（Aelia Laelia Chudley）一家时的情形，他说：“不过，最令人惊奇的莫过于每间卧房里的厕所了：由一大块红木设计而成……带着坑洞，青铜制的把手，以及冲水开关，等等。我不禁说道，这是我所见过的最为荒淫无度的家庭！” 每段历史都为学者提供了异彩纷呈的关于卫生设备及卫生习俗的趣闻轶事。18 世纪也毫不例外。 莫扎特（Mozart）给他的表妹写了些奇怪的求爱信。其中一封的结束语为：“好了，祝你晚安，不过先得在床上拉屎，让它无处不在。” 第一个靠马桶过活的家伙约翰尼（Johnny）是个流动小贩。这家伙游走于苏格兰爱丁堡城（Edinburgh）的街头巷尾，使劲儿叫卖。他在客人大小便时为其提供“隐私”。他身披宽大的黑色斗篷，手握便壶，高声叫喊“如欲方便，付半便士！”顾客来到他跟前，递给他半便士。顾客蹲坐在尿壶上之时，他便用大斗篷将其遮盖。 1796 年出版的一幅题为《国家厕所》（National Conveniences）的漫画展现了英格兰人对抽水马桶的无比自豪，及对欧洲其他国家的鄙夷之情。漫画中，英格兰人坐在自己的抽水马桶上，苏格兰人伏于水桶上，法兰西人蹲的是茅坑，荷兰人则在池塘里排便。 英法百年战争于 1453 年宣告结束，然而接下来的唇枪舌战又持续了好几个世纪。在维多利亚时代，英国人多番指责法国人放荡不羁和粗暴无礼。1810 年游历英国的法国人路易·西蒙回敬了英国人的鄙夷之辞。在其游记中，西蒙尽数自己目睹被访的众多英国家庭将便壶或封闭马桶置于餐厅一角时的惊诧。他写道，英格兰人使用这一器具时毫无愧色，甚至时常与宾客谈笑风生。显然，此时法国人已然将自己的“法国礼仪”输出他国了。 以前的欧洲女人是没有内裤的，裙下是真空。裙下的束裤是现代史中最早的内裤雏形，而这要拜一个舞女所赐。1727 年，一位女芭蕾舞演员的裙子被舞台布景上的一个东西挂住了。她的屁股让台下的男人几近疯狂。从此，巴黎通过了一项治安规定：女演员或者舞女不着束裤不得登台表演。但登台表演外的欧洲女人裙下仍然不穿束裤。 18 世纪欧洲人治疗疾病的方法并不比前几个世纪先进多少。中风患者被告知要喝下一杯来自体魄健康之人的尿液。尿与盐的混合物可令“性格”乖张之人变得温和。还有人认为，将人的粪便晒干、磨成粉状，吹入病人眼中，白内障便会消失。 在 17 世纪的欧洲，从王后到妓女都喜欢在公众场合把她们的一只或两只乳房裸露出来，那时乳房裸露比今天更容易被接受，而且这种着装方式还被看作是贞洁和荣耀的象征。英国沃里克大学讲师安吉拉·M·琼斯研究了剑桥大学收藏的近 2000 幅 17 世纪木版画，这些木版画上的女人大多数都把她们的乳房裸露在外。琼斯说，那时的歌单和低俗小说的描述也印证了这一点——女人们经常穿着低开领衣，露出她们的胸脯和乳房。 在17世纪，乳房裸露不仅不会看作是俗丽，而且这种行为被认为是女性贞洁的象征。 “裸露乳房是女子向人展现她的正派和青春的表示，”琼斯说，“她是在向人们展示她那苹果般的从未用过的处女乳房，炫耀她们的贞洁、美丽和青春年少。贵族阶层的女性为了保持自己乳房的美，从来不让孩子吃自己的奶，而是找奶妈代替。” 琼斯说，尽管上层社会以外的女子也可能接受这种时尚，但它开始时只是作为非常流行的上层时尚，表现了她们的高贵、美丽和正派。丈夫们也对妻子的穿着感到自豪，因为这是她们在展示贞洁的荣耀。 琼斯认为这一流行时尚可能始自艾格尼丝-苏拉，她是 15 世纪法王 Charles VII 心爱的情妇，后来才渐渐流传开来。英国皇室成员们也崇尚露乳的风气。英国查理一世皇后海丽塔。玛丽亚就请知名的时装设计师 Inigo Jones 为自己定制了一件露出乳头的礼服，而在光荣革命之后与荷兰丈夫威廉结婚的玛丽二世也经常当众露乳。 沃里克大学历史学教授伯纳德·坎普同意英国历史上乳房裸露非常流行的观点，但这不是什么丑闻。坎普说那时，只要在正确的场合穿这样的服装，是和她们的贞洁和荣誉完全协调的。他说，也有很多保守派和清教徒，如威廉·普林，反对这种流行的服饰。 坎普说，1633 年，威廉·普林曾经在海丽塔。玛丽亚王后参加了一次宫廷假面舞会的表演后，批评她“看起来就像个妓女”，他得到的惩罚是被政府割去了耳朵。裸露乳房的时尚在 18 世纪和 19 世纪曾多次出现，包括维多利亚时代，琼斯说，在这些裸胸被视为正常的时代，女性暴露她们的肩膀和大腿则被认为是惊世骇俗的，那会被认为是在勾引男人。 从《香水》看欧洲19世纪前的卫生状况 很多人都知道巴黎香水业的发展与繁荣很大程度上是和 19 世纪以前整个巴黎城的肮脏不堪，处处散发着恶臭有关，其实不光是巴黎，欧洲的各个城市都是同样肮脏无比。电影《香水》开始时候的描写可谓触目惊心，不过还只是片面的展示，在那个时代，浪漫的文艺的欧洲城市卫生状况到底如何呢？如果说罗密欧长了一嘴黄牙，靴子上尽是大便痕迹，朱丽叶一辈子没洗过几次澡，满头油腻，惊掉下巴的各位是否还心驰神往？ 其实在古罗马时就已经有了比较有效率的城市排水系统，这在一定程度上使城市总体卫生状况得以良好地控制。然而这些古代杰出的城市建设经验却丝毫未能影响中部欧洲的文明。随著罗马文明的衰落、古代都市的毁灭，中世纪欧洲的排水工程又回到无节制的原始状态，居民家中一般都没有下水管道和厕所。人有三急时怎么解决？一般情况下，人们会悄悄地找一处角落解决。歌德曾讲述过他的经历，有一次歌德在意大利加尔达的一家旅馆住宿，他询问去哪里“方便”，旅馆里的人平静地告诉他就在院子里。当时的人们将自家后面的小巷和附近的沟渠当作倾倒污物的地方。为数不多的茅厕和粪坑通常离饮用水源不远，城市雨、污水和排泄物都在路边简单的露天的雨水道中流过，饮水极易受到污染。欧洲有下水道系统已经是19世纪中期以后的事了。在此之前，城市里街道和广场成了真正的污物倾倒场，市民们将粪水和垃圾从窗户倒向街上，街上经常是粪水横流、臭气熏天。1270 年的巴黎的一项法律中规定：“任何人均不得自楼台窗倾倒粪便，白天黑夜均不可，否则将处以罚金”。但巴黎的市民显然不喜欢遵守这一法令。因此在一个世纪后又有一项新的法令：“如果愿意大叫三声‘注意尿’,则可以倾倒。”大量的羊、猪等家畜以及拉车的牛、马肆意排泄，使城市环境脏上加脏。此外，肉贩子和屠户还当街进行牲畜的宰杀和开膛。 在德国的纽伦堡城里，敞开的下水道穿越各家各户，汇入河流。当时人们处理粪便的方法是把它倒入河中，埋进坑里，或用船运出城外。那个时代人们更看重便利，而不是健康。当河流不再能够容纳如此多垃圾的时候，人们就用推车把废弃物运到城外。低潮时的景象更是惨不忍睹，因为水的短缺使污物无法漂走。巴黎城中堆积起来的粪便被倾倒到城墙外侧，减少了城内的些许污秽。不幸的是，随着巴黎的繁荣昌盛，其粪堆也日渐庞大。最后，粪堆的规模扩大到了如此地步，以至于人们出于安全的考虑而不得不将围墙筑高，以防敌军可能从粪堆顶部攻击巴黎城。英格兰的河流用来输送粪便，等到粪便堆积了几个河流那么深时，河道就停滞了。伦敦的弗利特河收集了一座桥上 11 个公厕和 3 个下水道的残留物。不足为奇，该河停止流动，弗利特河也变成了弗利特街。而伦敦桥上的厕所每年向泰晤士河倾倒 2000 吨粪便。流行病的复发促使很多欧洲官员责令人们使用粪坑，而不是河流来处理粪便。令人感到悲哀的是，人们往往对这类忠告不以为然。巴黎警局于 1522 年、1525 年和 1539 年间发布命令，要求市民安置和使用排水沟及公厕。由于没有急切的需要，巴黎市民仍旧将垃圾倒在城里各条街道，使巴黎成为“臭味之城”。 16 世纪和 17 世纪的文学作品中之所以总是不厌其烦地提到粪便问题，正是以揶揄的方式说明当时的朝臣连个方便的地方都没有的尴尬局面。既然如此，于是大家便在壁炉、门后、墙上和阳台上随地大小便。宫中甬道的每块石头上、宏伟的迎宾台阶上到处是大小便。1578 年，亨利三世实在受不了，便下令起床之前把宫殿刷洗干净。1606 年，亨利四世在圣日耳曼离宫居住时曾下令禁止一切不文明的行为，但就在颁布禁令的当天，小王储就因为冲着自己房间的墙壁撒尿被抓了个正着。路易十四(1638 年 9 月 5 日—1715 年 9 月 1 日)为了解决凡尔赛宫、卢浮宫和枫丹白露宫到处是大小便的问题，只有采用一个办法，那就是轮流搬家。每月搬一次家，人们糟蹋这一处时，清扫另一处。19 世纪的反日耳曼风潮中人们总是谴责日耳曼人不开化，实际上根本不是那么回事，帕拉蒂娜王妃发现法国宫廷如此肮脏时表现出的惊恐就在情理之中了。她在给汉诺威的女选帝侯的信中写道：枫丹白露的人“随地屙屎，街上粪便随处可见”。 城市污秽不堪，人们的个人卫生状况也从文艺复兴时期开始倒退。人体禁忌观念的产生和梅毒、鼠疫等疾病的出现是导致这种状况的原因。16 世纪的医生们认为，水会削弱器官的功能，并使人体暴露在有害空气中，如果水渗入毛孔中会传染各种疾病。当时甚至还流传着一层污垢能抵抗疾病侵袭的说法。因此，个人卫生只能采取“干洗”的方式，即用一块干毛巾擦身。 17 世纪的文章建议儿童用白布清洁脸和眼睛。因为用水清洗有损视力，会引起牙病和感冒，使脸色苍白，而且对天气的冷热更加敏感，根据法国人乔治?维加雷洛的调查，欧洲的上流社会在个人卫生方面也拒绝用水。在路易十四(1638 年 9 月 5 日—1715 年 9 月 1 日)统治时期，最爱干净的贵妇人每年也仅洗两次澡。而路易十四本人也要在医生的指导下谨慎地沐浴。 从古罗马留传下来的公共浴室时代全民洗澡的辉煌不复存在，而肮脏的躯体被看作更能接近上帝。圣亚伯拉罕 50 年不洗脸，不洗脚。圣西蒙任蠕虫在他溃烂的伤口上拱动而从不清洗，圣尤拂拉西亚进了一座女修道院，里面有 130 多个修女，她们从不洗澡，在中世纪修道院只准许修士一年洗2次澡，在著名的克兰尼修道院中一共只有3条毛巾。圣伯努瓦修道院的院规规定，即使病人的病情需要入浴，审查手续也非常严格，尤其是对身体强壮和“年轻”修士更为严格。不洗澡成了圣洁的象征。人们会毫不犹豫地把那些有足够勇气不洗澡的人册封为圣人，亨利四世(1553 年 12 月 13 日—1610 年 5 月 14 日，法国国王 1589—1610 年)的母亲一辈子不洗澡，被册封为圣女阿涅丝。 另外也别以为罗密欧朱丽叶就餐时就和电影里一样彬彬有礼地拿着刀叉吃饭，事实上，用叉作餐具是 18 世纪中期以后的事，之前都是用刀将食物割开用手爪着吃，单独的餐具、盘子和杯子的使用也是 18 世纪中后期才开始的，人们在喝汤时用同一只器皿，而且大家用一个酒杯喝酒。在 13 世纪以前，欧洲人在吃东西时还都全用手指头。在使用手指头进食时，还有一定的规矩：罗马人以用手指头的多寡来区分身份，平民是五指齐下，有教养的贵族只用三个手指，无名指和小指是不能沾到食物的。这一进餐规则一直延续到 16 世纪，仍为欧洲人所奉行。 刻有路易十三等人头像的法国金币，第一次世界大战以前在法国使用，相当于 20 法郎金币——译者注。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[SAS 使用 proc report 实现同比、环比、占比、sql 的窗口函数 z]]></title>
    	<url>/tech/2017/02/09/proc-report/</url>
		<content type="text"><![CDATA[[原文地址：http://www.cnblogs.com/SSSR/p/6904636.html 使用 SAS 实现同比、环比、占比，其中环比和占比是使 report实现的，环比使用data步实现，但是其中每年的总计是使用proc report来实现的。 proc report可以实现proc print, proc tabluate, proc sort, proc means以及data步的一些功能，所以有中想法，把proc report当做是进行复杂统计的实现方法之一，比如 sql 中的开窗函数就可以用proc report实现。 以下是具体的代码和数据。代码参考自：Using PROC REPORT To Produce Tables With Cumulative Totals and Row Differences 总结：report的compute步中是先一列一列的计算，新增列的时候可以用前面列的数据，跟在data步中新建列感觉区别不大，可以使用data步中的函数。 最后的BREAK AFTER year / SUMMARIZE SKIP OL UL ;这里只能是summarize求和，不能是其他的。 想着测试一下data步中的lag函数在report的compute中是否可以使用，没想到呀！居然可以直接求出来同比，大赞，记录下。 DATA quarter; DO year=97 TO 99; DO j=1 TO 12; IF j=1 THEN xx=&#39;1dec1997&#39;d; QUARTER=QTR( intnx(&#39;month&#39;,xx,J) ); DO n=1 to 100; sales=int(normal(123)*(20)&#43;quarter*7); IF QUARTER=3 THEN SALES=SALES-15; OUTPUT; END; END; END; RUN; PROC FORMAT ; VALUE PCTA .=&#39;(na)&#39; OTHER=[PERCENT8.0]; VALUE DOLLARA .=&#39;(na)&#39; OTHER=[DOLLAR8.0]; RUN; ods html file=&#39;c:/myhtml.htm&#39;; PROC REPORT DATA=QUARTER NOWD OUT=Six SPLIT=&amp;quot;*&amp;quot; CENTER HEADSKIP HEADLINE; COLUMN ( year quarter ) ( sales=salessum pct) (diff diff_pct pct_tongbi) ; DEFINE year / GROUP; DEFINE quarter / GROUP FORMAT=8. CENTER; DEFINE salessum / ANALYSIS sum FORMAT=DOLLAR8. SUM ; DEFINE pct / computed FORMAT=PERCENT8.0 ; DEFINE diff / COMPUTED FORMAT= DOLLAR8.0 ; DEFINE diff_pct / COMPUTED FORMAT=percent9.0; define pct_tongbi/computed format=percent9.0; COMPUTE BEFORE year ;*modify; r=0; last=0; total=salessum; ENDCOMP; COMPUTE pct;/*实现了sql的窗口函数*/ pct=salessum/total; ENDCOMP; COMPUTE diff ; r&#43;1; IF r=1 THEN diff=. ; else DO; if _BREAK_ EQ &amp;quot; &amp;quot; THEN diff=salessum-last ; else diff = . ; end; last = salessum; ENDCOMP; COMPUTE diff_pct ; diff_pct= (diff/(last-diff) ); ENDCOMP; COMPUTE pct_tongbi;/*计算同比，可以直接使用lag函数，so data步中的很多函数估计就都可以在report中使用了！*/ pct_tongbi=salessum/lag6(salessum)-1; ENDCOMP; BREAK AFTER year / SUMMARIZE SKIP OL UL ; RUN; ods html close; 以下代码比较复杂，计算同比使用了data步。 DATA quarter; DO year=97 TO 99; DO j=1 TO 12; IF j=1 THEN xx=&#39;1dec1997&#39;d; QUARTER=QTR( intnx(&#39;month&#39;,xx,J) ); DO n=1 to 100; sales=int(normal(123)*(20)&#43;quarter*7); IF QUARTER=3 THEN SALES=SALES-15; OUTPUT; END; END; END; RUN; PROC FORMAT ; VALUE PCTA .=&#39;(na)&#39; OTHER=[PERCENT8.0]; VALUE DOLLARA .=&#39;(na)&#39; OTHER=[DOLLAR8.0]; RUN; /*这个是实现占比和环比的，生成了一个数据集，all也在这里生成了*/ ods html file=&#39;c:/myhtml.htm&#39;; PROC REPORT DATA=QUARTER NOWD OUT=Six SPLIT=&amp;quot;*&amp;quot; CENTER HEADSKIP HEADLINE; COLUMN ( year quarter ) ( sales=salessum pct) (diff diff_pct) ; DEFINE year / GROUP; DEFINE quarter / GROUP FORMAT=8. CENTER; DEFINE salessum / ANALYSIS sum FORMAT=DOLLAR8. SUM ; DEFINE pct / computed FORMAT=PERCENT8.0 ; DEFINE diff / COMPUTED FORMAT= DOLLAR8.0 ; DEFINE diff_pct / COMPUTED FORMAT=percent9.0; COMPUTE BEFORE year ;*modify; r=0; last=0; total=salessum; ENDCOMP; COMPUTE pct; pct=salessum/total; ENDCOMP; COMPUTE diff ; r&#43;1; IF r=1 THEN diff=. ; else DO; if _BREAK_ EQ &amp;quot; &amp;quot; THEN diff=salessum-last ; else diff = . ; end; last = salessum; ENDCOMP; COMPUTE diff_pct ; diff_pct= (diff/(last-diff) ); ENDCOMP; *BREAK AFTER year / SUMMARIZE SKIP OL UL ; RUN; ods html close; /*对report中生成的数据集进行进一步的加工*/ DATA sixout(keep=year quarterx salessum pct diff_pct); retain year quarterx salessum pct diff_pct ; set six; if quarter=. then quarterx=&#39;ALL&#39;; else quarterx=quarter; if not missing(_break_) then pct=1; RUN; /*排序，为下一步求同比做准备*/ proc sort data=sixout out=sixout; by year quarterx; run; /*求同比直接用lag5函数即可，这个大家都知道，*/ /* 但是有时候我们会遇到今年和去年的分类数据不同，必去去年一季度有数据，但是今年为0，就不现实了， 所以这个时候我们还需要先将所有的分类（季度）数据和年份进行全匹配，缺失的填充为0，然后再进行处理 */ data sixx_result; set sixout; *lag5=lag5(salessum); tongbi=salessum/lag5(salessum)-1; format tongbi PERCENT8.2 ; format pct PERCENT8.2 ; format diff_pct PERCENT8.2 ; run;]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[SAS 关于宏、宏函数、宏变量、data 步、proc 步和 call execute 的理解 z]]></title>
    	<url>/tech/2017/02/09/sas-macro/</url>
		<content type="text"><![CDATA[[原文地址：http://www.cnblogs.com/SSSR/p/6380957.html SAS 宏和宏函数的问题困扰了我三年之久，终于在昨日 SAS 宏和宏函数是两个东西，自定的宏并不是宏函数（在其他编程语言中自定义函数和语言本身函数是一样的，受此影响！） 结论：SAS 宏并不具备 SAS 宏函数的功能，它仅仅只是一段文本，这段文本中如果有参数和宏函数，我们只是把参数替换掉和宏函数执行了，然后生成一个正常的文本（包含data步和proc步）提交给 SAS 运行。 遇到宏函数时会直接执行，遇到宏时会直接进行文本替换（宏中的宏函数也会直接执行），宏函数返回的文本会和其他的data步和proc步组合然后一起提交给 SAS 步运行。 宏函数直接执行，SAS 函数需要在data步中执行（%put和put的区别） 下面来看两个简单的例子： options symbolgen mprint;*将宏编译的过程也打印出来，以便进行测试; %macro s(i); %put &amp;amp;i.; %mend; data _null_; set sashelp.class; put name; %s(&#39;program&#39;); run; /*以下是结果：汉字为注释 105 %macro s(i); 106 107 %put &amp;amp;i.; 108 %mend; 109 110 data _null_; 111 set sashelp.class; 112 put name; 113 %s(&#39;program&#39;); //下面是这句宏调用的解析 SYMBOLGEN: Macro variable I resolves to &#39;program&#39; // I是%s的宏参数，被替换成了program。 // 下面是直接打印了program，这个是为啥呢？ // 就是因为 %put 是宏函数，会立即执行， // 从而先打印了program， // 然后才会将正常的data步提交给sas执行， // run;后的部分是宏编译完以后执行的结果。 &#39;program&#39; 114 run; Alfred Alice Barbara Carol Henry James Jane Janet Jeffrey John Joyce Judy Louise Mary Philip Robert Ronald Thomas William NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: DATA statement used (Total process time): real time 0.01 seconds */ %macro t(i); put &amp;amp;i.; %mend; data _null_; set sashelp.class; %t(name); %s(&#39;programe&#39;) run; /*以下为结果：中文为注释 135 %macro t(i); 136 137 put &amp;amp;i.; 138 %mend; 139 140 data _null_; 141 set sashelp.class; 142 %t(name); SYMBOLGEN: Macro variable I resolves to name //宏变量I被解析成name，注意这个name不带双引号， MPRINT(T): put name; //这里put name和data set等一起被提交给sas执行 143 %s(&#39;programe&#39;) SYMBOLGEN: Macro variable I resolves to &#39;programe&#39; //这个因为是%put所以被直接执行了。 &#39;programe&#39; 144 run; Alfred Alice Barbara Carol Henry James Jane Janet Jeffrey John Joyce Judy Louise Mary Philip Robert Ronald Thomas William NOTE: There were 19 observations read from the data set SASHELP.CLASS. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds */ proc print data=sashelp.class; run; 再来看一个复杂的例子： options symbolgen mprint; data b; input a1 a2 a3; cards; 1 2 3 ; run; %macro varx; %do i=0 %to 2; proc sql ; select a%eval(&amp;amp;i.&#43;1) into:a%eval(&amp;amp;i.&#43;1) from b; quit; %let j=%eval(&amp;amp;i.&#43;1); data _null_; a= &amp;amp;&amp;amp;a&amp;amp;j.; put a; run; %end ; %mend varx; %varx; /*以下为结果：中文为注释，整个程序循环了三次i=0 1 2, 所以会生成三次proc sql和三次data步，然后提交给sas运行。 从这个就可以看出sas宏的一个作用，减少代码量，将重复的代码进行封装。 145 data b; 146 input a1 a2 a3; 147 cards; NOTE: The data set WORK.B has 1 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 149 ; 150 run; //以上为数据集生成 151 152 %macro varx; 153 %do i=0 %to 2; 154 proc sql ; 155 select a%eval(&amp;amp;i.&#43;1) into:a%eval(&amp;amp;i.&#43;1) 156 from b; 157 quit; 158 %let j=%eval(&amp;amp;i.&#43;1); 159 data _null_; 160 a= &amp;amp;&amp;amp;a&amp;amp;j.; 161 put a; 162 run; 163 %end ; 164 %mend varx; 165 %varx; MPRINT(VARX): proc sql ; SYMBOLGEN: Macro variable I resolves to 0 SYMBOLGEN: Macro variable I resolves to 0 MPRINT(VARX): select a1 into:a1 from b; MPRINT(VARX): quit; // 从proc sql到这里就是宏varx编译 // 生成的一段proc sql代码， // 这部分提交给sas运行。 NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds SYMBOLGEN: Macro variable I resolves to 0 MPRINT(VARX): data _null_; SYMBOLGEN: &amp;amp;&amp;amp; resolves to &amp;amp;. SYMBOLGEN: Macro variable J resolves to 1 SYMBOLGEN: Macro variable A1 resolves to 1 MPRINT(VARX): a= 1; MPRINT(VARX): put a; MPRINT(VARX): run; // 从data步到这一行是宏varx编译生成的第二段代码data步， // 这段代码中宏变量I被编译成了0，j直接被计算成了1. // 这段代码也同样展示了select into生成的宏变量 // 在其他程序中调用的执行顺序。 1 NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds MPRINT(VARX): proc sql ; SYMBOLGEN: Macro variable I resolves to 1 SYMBOLGEN: Macro variable I resolves to 1 MPRINT(VARX): select a2 into:a2 from b; MPRINT(VARX): quit; //重复proc sql NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds SYMBOLGEN: Macro variable I resolves to 1 MPRINT(VARX): data _null_; SYMBOLGEN: &amp;amp;&amp;amp; resolves to &amp;amp;. SYMBOLGEN: Macro variable J resolves to 2 SYMBOLGEN: Macro variable A2 resolves to 2 MPRINT(VARX): a= 2; MPRINT(VARX): put a; MPRINT(VARX): run; //重复data 2 NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds MPRINT(VARX): proc sql ; SYMBOLGEN: Macro variable I resolves to 2 SYMBOLGEN: Macro variable I resolves to 2 MPRINT(VARX): select a3 into:a3 from b; MPRINT(VARX): quit; //重复proc sql NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds SYMBOLGEN: Macro variable I resolves to 2 MPRINT(VARX): data _null_; SYMBOLGEN: &amp;amp;&amp;amp; resolves to &amp;amp;. SYMBOLGEN: Macro variable J resolves to 3 SYMBOLGEN: Macro variable A3 resolves to 3 MPRINT(VARX): a= 3; MPRINT(VARX): put a; MPRINT(VARX): run; //重复data 3 NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds */ 未完，后续预告call execute（http://bbs.pinggu.org/thread-2377205-1-1.html 此链接的讲解非常透彻，同时call excute可以调用宏，而且讲解了在使用宏和宏变量时单引号和双引号的处理）。 在 Python 和 R 语言中，有一个for循环的功能非常好用，但是在 SAS 中没有类似的功能。鉴于SAS data步中的 PDV 是一行一行的读取执行的，so 可以结合call execute来实现for循环功能： 以下代码实现的功能是将三行数据生成三个数据集，数据集的名称是 dataset 列的值。 data a; input A B dataset $; datalines; 1 2 ds_1 6 7 ds_2a 2 3 ds_100c ; run; data _null_; set a; call execute(&#39;data &#39; ||dataset||&#39;;&#39;||&#39;set a (firstobs=&#39; ||_n_|| &#39; obs=&#39; || _n_ || &#39;);run;&#39; ); run; /* or */ data _null_; set a; call execute(&#39;data &#39; || dataset || &#39;;&#39; || &#39;set a ; if dataset= &amp;quot;&#39; || dataset || &#39;&amp;quot;; run ;&#39; ); run;]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[A Plain Markdown Post]]></title>
    	<url>/tech/2016/12/30/a-plain-markdown-post/</url>
		<content type="text"><![CDATA[[This is a post written in plain Markdown (*.md) instead of R Markdown (*.Rmd). The major differences are: You cannot run any R code in a plain Markdown document, whereas in an R Markdown document, you can embed R code chunks (```{r}); A plain Markdown post is rendered through Blackfriday, and an R Markdown document is compiled by rmarkdown and Pandoc. There are many differences in syntax between Blackfriday&amp;rsquo;s Markdown and Pandoc&amp;rsquo;s Markdown. For example, you can write a task list with Blackfriday but not with Pandoc: Write an R package. Write a book. &amp;hellip; Profit! Similarly, Blackfriday does not support LaTeX math and Pandoc does. I have added the MathJax support to this theme (hugo-lithium-theme) but there is a caveat for plain Markdown posts: you have to include math expressions in a pair of backticks (inline: `$ $`; display style: `$$ $$`), e.g., $S_n = \sum_{i=1}^n X_i$.1 For R Markdown posts, you do not need the backticks, because Pandoc can identify and process math expressions. When creating a new post, you have to decide whether the post format is Markdown or R Markdown, and this can be done via the rmd argument of the function blogdown::new_post(), e.g. blogdown::new_post(&amp;quot;Post Title&amp;quot;, rmd = FALSE) This is because we have to protect the math expressions from being interpreted as Markdown. You may not need the backticks if your math expression does not contain any special Markdown syntax such as underscores or asterisks, but it is always a safer choice to use backticks. When you happen to have a pair of literal dollar signs inside the same element, you can escape one dollar sign, e.g., \$50 and $100 renders &amp;ldquo;\$50 and $100&amp;rdquo;. ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python 类中 super() 和 __init__() 的区别 z]]></title>
    	<url>/tech/2016/12/22/super-init/</url>
		<content type="text"><![CDATA[[原文地址：https://my.oschina.net/jhao104/blog/682322 单继承时 super() 和 __init__() 实现的功能是类似的 class Base(object): def __init__(self): print &#39;Base create&#39; class childA(Base): def __init__(self): print &#39;creat A &#39;, Base.__init__(self) class childB(Base): def __init__(self): print &#39;creat B &#39;, super(childB, self).__init__() base = Base() a = childA() b = childB() 输出结果： Base create creat A Base create creat B Base create 使用super()继承时不用显式引用基类。 super() 只能用于新式类中 把基类改为旧式类，即不继承任何基类 class Base(): def __init__(self): print &#39;Base create&#39; 执行时，在初始化b时就会报错： super(childB, self).__init__() TypeError: must be type, not classobj super 不是父类，而是继承顺序的下一个类 在多重继承时会涉及继承顺序，super()相当于返回继承顺序的下一个类，而不是父类，类似于这样的功能： def super(class_name, self): mro = self.__class__.mro() return mro[mro.index(class_name) &#43; 1] mro()用来获得类的继承顺序。例如： class Base(object): def __init__(self): print &#39;Base create&#39; class childA(Base): def __init__(self): print &#39;enter A &#39; # Base.__init__(self) super(childA, self).__init__() print &#39;leave A&#39; class childB(Base): def __init__(self): print &#39;enter B &#39; # Base.__init__(self) super(childB, self).__init__() print &#39;leave B&#39; class childC(childA, childB): pass c = childC() print c.__class__.__mro__ 输出结果如下： enter A enter B Base create leave B leave A (&amp;lt;class &#39;__main__.childC&#39;&amp;gt;, &amp;lt;class &#39;__main__.childA&#39;&amp;gt;, &amp;lt;class &#39;__main__.childB&#39;&amp;gt;, &amp;lt;class &#39;__main__.Base&#39;&amp;gt;, &amp;lt;type &#39;object&#39;&amp;gt;) super和父类没有关联，因此执行顺序是A —&amp;gt; B—&amp;gt;—&amp;gt;Base 执行过程相当于：初始化childC()时，先会去调用childA的构造方法中的super(childA, self).__init__()，super(childA, self)返回当前类的继承顺序中childA后的一个类childB；然后再执行childB().__init()__，这样顺序执行下去。 在多重继承里，如果把childA()中的super(childA, self).__init__()换成Base.__init__(self)，在执行时，继承childA后就会直接跳到Base类里，而略过了childB： enter A Base create leave A (&amp;lt;class &#39;__main__.childC&#39;&amp;gt;, &amp;lt;class &#39;__main__.childA&#39;&amp;gt;, &amp;lt;class &#39;__main__.childB&#39;&amp;gt;, &amp;lt;class &#39;__main__.Base&#39;&amp;gt;, &amp;lt;type &#39;object&#39;&amp;gt;) 从super()方法可以看出，super()的第一个参数可以是继承链中任意一个类的名字， 如果是本身就会依次继承下一个类；如果是继承链里之前的类便会无限递归下去1； 如果是继承链里之后的类便会忽略继承链汇总本身和传入类之间的类； 比如将childA()中的super改为：super(childC, self).__init__()，程序就会无限递归下去。如： File &amp;quot;C:/Users/Administrator/Desktop/crawler/learn.py&amp;quot;, line 10, in __init__ super(childC, self).__init__() File &amp;quot;C:/Users/Administrator/Desktop/crawler/learn.py&amp;quot;, line 10, in __init__ super(childC, self).__init__() File &amp;quot;C:/Users/Administrator/Desktop/crawler/learn.py&amp;quot;, line 10, in __init__ super(childC, self).__init__() File &amp;quot;C:/Users/Administrator/Desktop/crawler/learn.py&amp;quot;, line 10, in __init__ super(childC, self).__init__() File &amp;quot;C:/Users/Administrator/Desktop/crawler/learn.py&amp;quot;, line 10, in __init__ super(childC, self).__init__() super()可以避免重复调用 如果childA继承Base，childB继承childA和Base，如果childB需要调用Base的__init__()方法时，就会导致__init__()被执行两次： enter A Base create leave A Base create 使用super()时可避免重复调用 class Base(object): def __init__(self): print &#39;Base create&#39; class childA(Base): def __init__(self): print &#39;enter A &#39; super(childA, self).__init__() print &#39;leave A&#39; class childB(childA, Base): def __init__(self): super(childB, self).__init__() b = childB() print b.__class__.mro() enter A Base create leave A [&amp;lt;class &#39;__main__.childB&#39;&amp;gt;, &amp;lt;class &#39;__main__.childA&#39;&amp;gt;, &amp;lt;class &#39;__main__.Base&#39;&amp;gt;, &amp;lt;type &#39;object&#39;&amp;gt;] 不理解什么是继承链里之前的类。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[如何理解 Python 装饰器？z]]></title>
    	<url>/tech/2016/12/13/decorator/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/26930016 李冬：原理 Attention：为了解释得通俗易懂，所 学习 Python 有一段时间了，对于装饰器的理解又多了一些，现在我重新再写一次对于装饰器的理解。在讲之前我需要先铺垫一下基础知识，如果你已经掌握了就请跳过。 基础知识 万物皆对象 在 Python 中，不管什么东西都是对象。对象是什么东西呢？对象就是你可以用来随意使用的模型。当你需要的时候就拿一个，不需要就让它放在那，垃圾回收机制会自动将你抛弃掉的对象回收。可能对这个理解有一点云里雾里的感觉，甚至还觉得对这个概念很陌生。其实如果你都学到装饰器这里了，你已经使用过不少对象啦。比如，我写了一个函数： def cal(x, y): result = x &#43; y return result 这时，你可以说，你创造了一个叫做cal()的函数对象。然后，你这样使用了它： cal(1,2) 或者，你这样使用了它： calculate = cal calculate(1，2) 在第一种方式下，你直接使用了cal这个函数对象； 在第二种方式下，你把一个名为calculate的变量指向了cal这个函数对象。如果各位对类的使用很熟悉的话，可以把这个过程看作实例化。 也就是说，对象，就像是一个模子，当你需要的时候，就用它倒一个模型出来，每一个模型可以有自己不同的名字。在上面的例子中，calculate是一个模型，而你写的cal()函数就是一个模子。 理解函数带括号和不带括号时分别代表什么意思 在上一个例子中，如果你只是写一个cal（也就是没有括号），那么此时的cal仅仅是代表一个函数对象；当你这样写cal(1, 2)时，就是在告诉编译器执行 cal 这个函数。 理解带星号的参数是什么意思 这个属于函数基础，要是你还没有听说过，那么就该回去好好复习一下了。具体讲解我就略过了。 装饰器 装饰器是什么 装饰器，顾名思义，就是用来“装饰”的。它长这个样： @xxx 其中xxx是你的装饰器的名字。它能装饰的东西有：函数、类。 为什么需要装饰器 有一句名言说的好（其实是我自己说的）：“每一个轮子都有自己的用处”，所以，每一个装饰器也有自己的用处。装饰器主要用来“偷懒”（轮子亦是如此）。比如：你写了很多个简单的函数，你想知道在运行的时候是哪些函数在执行，并且你又觉得这个没有必要写测试，只是想要很简单的在执行完毕之前给它打印上一句&#39;Start&#39;，那该怎么办呢？你可以这样： def func_name(arg): print &#39;Start func_name&#39; sentences 这样做没有错，but， 你想过没有，难道你真的就想给每一个函数后面都加上那么一句吗？等你都运行一遍确定没有问题了，再回来一个一个的删掉print不觉得麻烦吗？什么？你觉得写一个还是不麻烦的，那你有十个需要添加的函数呢？二十个？三十个？（请自行将次数加到超过你的忍耐阈值）……如果你知道了装饰器，情况就开始渐渐变得好一些了，你知道可以这样写了： def log(func): def wrapper(*arg, **kw): print &#39;Start %s&#39; % func return func(*arg, **kw) return wrapper @log def func_a(arg): pass @log def func_b(arg): pass @log def func_c(arg): pass 其中，log()函数是装饰器。 把装饰器写好了之后，只需要把需要装饰的函数前面都加上@log就可以了。在这个例子中，我们一次性就给三个函数加上了print语句。可以看出，装饰器在这里为我们节省了代码量，并且在你的函数不需要装饰的时候直接把@log去掉就可以了，只需要用编辑器全局查找然后删除即可，快捷又方便，不需要自己手工的去寻找和删除print的语句在哪一行。 装饰器原理 在上一段中，或许你已经注意到了log 函数是装饰器这句话。没错，装饰器是函数。 接下来，我将带大家探索一下，装饰器是怎么被造出来的，来直观的感受一下装饰器的原理。先回到刚才的那个添加&#39;Start&#39;问题。假设你此时还不知道装饰器。将会以Solution的方式呈现。 S1：我有比在函数中直接添加print语句更好的解决方案！ def a(): pass def b(): pass def c(): pass def main(): print &#39;Start a&#39; a() print &#39;Start b&#39; b() print &#39;Start c&#39; c() 感觉这样做好像没什么错，并且还避免了修改原来的函数，如果要手工删改print语句的话也更方便了。嗯，有点进步了，很不错。 S2：我觉得刚刚那个代码太丑了，还可以再优化一下！于是你这样写了： def a(): pass def b(): pass def c(): pass def decorator(func): print &#39;Start %s&#39;% func func() def main(): decorator(a) decorator(b) decorator(c) 你现在写了一个函数来代替你为每一个函数写上print语句，好像又节省了不少时间。你欣喜的喝了一口 coffee，对自己又一次做出了进步感到很满意。嗯，确实是这样。于是你选择出去上了个厕所，把刚刚憋的尿全部都排空（或许还有你敲代码时喝的 coffee）。回来之后，顿时感觉神清气爽！你定了定神，看了看自己刚才的“成果”，似乎又感到有一些不满意了。因为你想到了会出现这样的情况： def main(): decorator(a) m = decorator(b) n = decorator(c) &#43; m for i in decorator(d): i = i &#43; n ...... 来，就说你看到满篇的 decorator 你晕不晕！大声说出来！ S3：你又想了一个更好的办法。于是你这样写了： def a(): pass def b(): pass def c(): pass def decorator(func): print &#39;Start %s&#39; % func return func a = decorator(a) b = decorator(b) c = decorator(c) def main(): a() b() c() 这下总算是把名字给弄回来了，这样就不会晕了。你的嘴角又一次露出了欣慰的笑容（内心 OS：哈哈哈，爷果然很 6！）。于是你的手习惯性的端起在桌上的 coffee，满意的抿了一口。coffee 的香味萦绕在唇齿之间，你满意的看着屏幕上的代码，突然！脑中仿佛划过一道闪电！要是a、b、c三个函数带参数我该怎么办？！你放下 coffee，手托着下巴开始思考了起来，眉头紧锁。像这样写肯定不行： a = decorator(a(arg)) 此时的本应该在 decorator 中做为一个参数对象的a加上了括号，也就是说，a在括号中被执行了！你只是想要a以函数对象的形式存在，乖乖的跑到 decorator 中当参数就好了。执行它并不是你的本意。那该怎么办呢？你扶了扶眼镜，嘴里开始念念有词“万物皆对象，万物皆对象……”你的额头上开始渐渐的渗出汗珠。突然，你的身后的背景暗了下来，一道光反射在眼镜上！不自觉的说了句“真相はひとつだけ”！ S4（终极）：你飞速的写下如下代码1。 def a(arg): pass def b(arg): pass def c(arg): pass def decorator(func): def wrapper(*arg, **kw) print &#39;Start %s&#39; % func return func(*arg, **kw) return wrapper a = decorator(a) b = decorator(b) c = decorator(c) def main(): a(arg) b(arg) c(arg) decorator() 函数返回的是wrapper，wrapper是一个函数对象。而a = decorator(a)就相当于是把a指向了wrapper，由于wrapper可以有参数，于是变量a也可以有参数了！ 终于！你从焦灼中解脱了出来！不过， 有了前几次的经验，你这一次没有笑。你又仔细想了想，能不能将a = decorator(a)这个过程给自动化呢？于是你的手又开始在键盘上飞快的敲打，一会儿过后，你终于完成了你的“作品”。你在 Python 中添加了一个语法规则，取名为@，曰之装饰器。你此时感觉有些累了， 起身打开门， 慢步走出去，深吸一口气，感觉阳光格外新鲜。你的脸上终于露出了一个大大的笑容。 讲到这里，我想大家应该差不多都明白了装饰器的原理。在评论中有知友问到，要是我的装饰器中也有参数该怎么办呢？要是看懂了刚才添加参数的解决方案，也就不觉得难了。再加一层就解决了。 def decorator(arg_of_decorator): def log(func): def wrapper(*arg, **kw): print &#39;Start %s&#39; % func #TODO Add here sentences which use arg_of_decorator return func(*arg, **kw) return wrapper return log xlzd：同时支持带参数与不带参数情形 既支持不带参数(如log()), 又支持带参数(如log(&#39;text&#39;))的 decorator： import functools def log(argument): if not callable(argument): def decorator(function): @functools.wraps(function) def wrapper(*args, **kwargs): print &#39;before function [%s()] run, text: [%s].&#39; % (function.__name__, text) rst = function(*args, **kwargs) print &#39;after function [%s()] run, text: [%s].&#39; % (function.__name__, text) return rst return wrapper return decorator def wrapper(*args, **kwargs): print &#39;before function [%s()] run.&#39; % function.__name__ rst = argument(*args, **kwargs) print &#39;after function [%s()] run.&#39; % function.__name__ return rst return wrapper zhijun liu：拓展阅读 装饰器本质上是一个 Python 函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 先来看一个简单例子： def foo(): print(&#39;i am foo&#39;) 现在有一个新的需求，希望可以记录下函数的执行日志，于是在代码中添加日志代码： def foo(): print(&#39;i am foo&#39;) logging.info(&amp;quot;foo is running&amp;quot;) bar()、bar2()也有类似的需求，怎么做？再写一个logging在bar函数里？这样就造成大量雷同的代码，为了减少重复写代码，我们可以这样做，重新定义一个函数：专门处理日志，日志处理完之后再执行真正的业务代码 def use_logging(func): logging.warn(&amp;quot;%s is running&amp;quot; % func.__name__) func() def bar(): print(&#39;i am bar&#39;) use_logging(bar) 提示：逻辑上不难理解，但是这样的话，我们每次都要将一个函数作为参数传递给use_logging()函数。而且这种方式已经破坏了原有的代码逻辑结构，之前执行业务逻辑时，执行运行bar()，但是现在不得不改成use_logging(bar)。那么有没有更好的方式的呢？当然有，答案就是装饰器。 简单装饰器 def use_logging(func): def wrapper(*args, **kwargs): logging.warn(&amp;quot;%s is running&amp;quot; % func.__name__) return func(*args, **kwargs) return wrapper def bar(): print(&#39;i am bar&#39;) bar = use_logging(bar) bar() 函数use_logging()就是装饰器，它把执行真正业务方法的func包裹在函数里面，看起来像bar被use_logging装饰了。在这个例子中，函数进入和退出时 ，被称为一个横切面(Aspect)，这种编程方式被称为面向切面的编程(Aspect-Oriented Programming)。 @符号是装饰器的语法糖，在定义函数的时候使用，避免再一次赋值操作 def use_logging(func): def wrapper(*args, **kwargs): logging.warn(&amp;quot;%s is running&amp;quot; % func.__name__) return func(*args) return wrapper @use_logging def foo(): print(&amp;quot;i am foo&amp;quot;) @use_logging def bar(): print(&amp;quot;i am bar&amp;quot;) bar() 如上所示，这样我们就可以省去bar = use_logging(bar)这一句了，直接调用bar()即可得到想要的结果。如果我们有其他的类似函数，我们可以继续调用装饰器来修饰函数，而不用重复修改函数或者增加新的封装。这样，我们就提高了程序的可重复利用性，并增加了程序的可读性。 装饰器在 Python 使用如此方便都要归因于 Python 的函数能像普通的对象一样能作为参数传递给其他函数，可以被赋值给其他变量，可以作为返回值，可以被定义在另外一个函数内。 带参数的装饰器 装饰器还有更大的灵活性，例如带参数的装饰器：在上面的装饰器调用中，比如@use_logging，该装饰器唯一的参数就是执行业务的函数。装饰器的语法允许我们在调用时，提供其它参数，比如@decorator(a)。这样，就为装饰器的编写和使用提供了更大的灵活性。 def use_logging(level): def decorator(func): def wrapper(*args, **kwargs): if level == &amp;quot;warn&amp;quot;: logging.warn(&amp;quot;%s is running&amp;quot; % func.__name__) return func(*args) return wrapper return decorator @use_logging(level=&amp;quot;warn&amp;quot;) def foo(name=&#39;foo&#39;): print(&amp;quot;i am %s&amp;quot; % name) foo() 上面的use_logging()是允许带参数的装饰器。它实际上是对原有装饰器的一个函数封装，并返回一个装饰器。我们可以将它理解为一个含有参数的闭包。当我们使用@use_logging(level=&amp;quot;warn&amp;quot;)调用的时候，Python 能够发现这一层的封装，并把参数传递到装饰器的环境中。 类装饰器 再来看看类装饰器，相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点。使用类装饰器还可以依靠类内部的__call__方法，当使用@形式将装饰器附加到函数上时，就会调用此方法。 class Foo(object): def __init__(self, func): self._func = func def __call__(self): print (&#39;class decorator runing&#39;) self._func() print (&#39;class decorator ending&#39;) @Foo def bar(): print (&#39;bar&#39;) bar() functools.wraps 使用装饰器极大地复用了代码，但是他有一个缺点就是原函数的元信息不见了，比如函数的docstring、__name__、参数列表，先看例子： # 装饰器 def logged(func): def with_logging(*args, **kwargs): print func.__name__ &#43; &amp;quot; was called&amp;quot; return func(*args, **kwargs) return with_logging # 函数 @logged def f(x): &amp;quot;&amp;quot;&amp;quot;does some math&amp;quot;&amp;quot;&amp;quot; return x &#43; x * x 该函数完成等价于： def f(x): &amp;quot;&amp;quot;&amp;quot;does some math&amp;quot;&amp;quot;&amp;quot; return x &#43; x * x f = logged(f) 不难发现，函数f()被with_logging取代了，当然它的docstring，__name__就是变成了with_logging函数的信息了。 print f.__name__ # prints &#39;with_logging&#39; print f.__doc__ # prints None 这个问题就比较严重的，好在我们有functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器函数中，这使得装饰器函数也有和原函数一样的元信息了。 from functools import wraps def logged(func): @wraps(func) def with_logging(*args, **kwargs): print func.__name__ &#43; &amp;quot; was called&amp;quot; return func(*args, **kwargs) return with_logging @logged def f(x): &amp;quot;&amp;quot;&amp;quot;does some math&amp;quot;&amp;quot;&amp;quot; return x &#43; x * x print f.__name__ # prints &#39;f&#39; print f.__doc__ # prints &#39;does some math&#39; 内置装饰器 @staticmathod、@classmethod、@property 装饰器的顺序 @a @b @c def f (): 等效于 f = a(b(c(f))) 复制原始函数的属性参考《廖雪峰 Python 教程》笔记 2中的@functools.wraps。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[为什么经济管理类论文建模型前都要做一个描述性统计？ - 慧航 - 问答]]></title>
    	<url>/prof/2016/12/03/descriptive-statistics/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/23074134#answer-49194682 描述性统计是非常重要 实际上，描述性统计不仅仅应该是我们论文的第一张表，在我们实际做实证的过程中，在实际做出分析之前，做变量的描述性统计也是必须要做的。 在做实证的过程中，做描述性统计包括且不仅限于以下方面的作用： 发现数据中的异常（比如本该为正的出现负数，本该为比例的超过了 1 等等）； 通过分布图发现离群值点； 检查数据满足分析所需要的假设（比如是否有 censored 情况，以及模型本该需要的分布、support、对称等各种假设）； 检查数据缺失情况； 检查数据是否符合直觉； 在某些情况下，检查数据是否符合分析的要求（比如做 Logit、Probit 回归的时候，=1的样本是不是太少） 以上的工作在实际做分析之前都要做，而对于读者和审稿人来说，可能也是他们感兴趣的点，所以也必须报告出来让大家大体知道你手上的数据是怎样的。特别是有些数据是作者自己从网上抓取的、调查的等等，可能这份数据是独一无二的，更需要给读者和审稿人一个明确的交待。 另外，在做实际分析的时候都需要清洗数据，报告描述性统计也能让读者和审稿人大体了解你都做了什么样的清洗工作，或者说被你清洗过后的数据是怎样的，往往这是非常重要的。 描述性统计不仅仅是简单的描述，也可以帮助讲故事，甚至很多的 idea 就是从看描述性统计中得到的。通过分类做描述性统计等手段，我们可以从中得到一些符合直觉的、反直觉的观察，进而提出故事、思考故事。比如下图是一个女性劳动参与的描述性统计： 那么具有什么特征的女性更倾向于工作呢？下图做了一定的分解： 可以看到，不参加工作的女性更有可能有小孩抚养，教育程度更低，更有可能是西德地区等等。观察这些描述性统计都可以带来一些启发，或者提出问题。 除以上原因外，还有个非常重要的原因，就是帮助读者和审稿人阅读回归表格。很多人做回归的时候，出于某些目的会对一些变量 scaling 等等，但是读者和审稿人往往希望知道这些变量的「经济显著性」究竟有多强。仅仅得到一个显著的结果往往是不够的，关心的变量x变动之后对结果y究竟有多大影响？因为单位的问题，有些时候往往难以比较。所以经常我们可能关心「当x变动一个标准差之后，y有多大的影响」，这个时候就需要使用描述性统计和回归表格结合起来一起看了。还有比如上面的age同时有age^2，那么当年龄增加1，平均而言会对y有多大影响呢？这个时候可能会需要age的均值，同样需要诉诸于描述性统计。类似此类的问题，没有描述性统计的情况下，读者是没办法计算的。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[共轭梯度法计算回归 z]]></title>
    	<url>/data/2016/11/23/conjugate-gradient-for-regression/</url>
		<content type="text"><![CDATA[[原文地址：https://cos.name/2016/11/conjugate-gradient-for-regression/ 共轭梯度示意 轮回眼 共轭梯度示意图（图片来源：维基百科） 引子 之所以写这篇文章，是因为前几天统计之都的微信群里有同学提了一个问题，想要对一个很大的数据集做回归。然后大家纷纷给出了自己的建议，而我觉得共轭梯度算回归的方法跟这个背景比较契合，所以就正好写成一篇小文，与大家分享一下。 说到算回归，或许大家都会觉得这个问题太过简单了，如果用 \(X\) 表示自变量矩阵，\(y\) 表示因变量向量，那么回归系数的最小二乘解就是 \(\hat{\beta}=(X&amp;#39;X)^{-1}X&amp;#39;y\)。（本文完） 哎等等，别真走啊，我们的主角共轭梯度还没出场呢。前面的这个算系数的公式确实非常简洁、优雅、纯天然、不做作，但要往里面深究的话，还是有很多问题值得挖掘的。 最简单暴力的方法，就是从左向右，依次计算矩阵乘法，矩阵求逆，又一个矩阵乘法，最后是矩阵和向量的乘法。如果你就是这么算的，那么可以先默默地去面壁两分钟了。 更合理的方法，要么是对 \(X&amp;#39;X\) 进行 Cholesky 分解，要么是对 \(X\) 进行 QR 分解，它们基本上是现在算回归的软件中最常见的方法。关于暴力方法和矩阵分解方法的介绍和对比，可以参见这个B站上的视频。（什么？你问我这么严肃的话题为什么要放B站上？因为大部分时间都是在吐槽啊） 好，刚才去面壁的同学现在应该已经回来了，我们继续。前面这些通过矩阵运算求回归系数的方法，我们可以统称为直接法。叫这个名字，是因为它们都可以在确定数目的步骤内得到最终的结果。而与之相对的，则叫做迭代法，意思是通过不断更新已经得到的结果，来逐渐逼近真实的取值。打个比方，你想要知道一瓶82年的拉菲值多少钱，直接法就是去做调研，原料值多少，品牌值多少，加工费多少，运输费多少……然后加总起来得到最终的定价；而迭代法就是去问酒庄老板，你先随便蒙一个数，然后老板告诉你高了还是低了，反复循环，总能猜个八九不离十。 说到这里，你自然要问了，既然算回归的软件大都是用直接法，为什么还要考虑迭代法？莫非直接法有什么不好的地方？这就说到问题的点子上了。 首先，如果我们假设数据有 \(n\) 行 \(p\) 列，那么我们会发现，\(X&amp;#39;X\) 的维度就是 \(p\times p\)，而如果变量数特别多，那么这个矩阵就会以平方的速度增大，那时候不要说算矩阵分解，即使是要存储这个大矩阵，可能都会遇到很多麻烦。 第二点，往好的方面讲，直接法给出的结果精度一般非常高，但在许多实际问题中，可能小数点后面三位保证正确就足够了，而直接法可能会为了保证十三位的精度而多出非常多的计算量。用直接法得到高精度的结果，再舍入成低精度的实际需求，总有一种买椟还珠的感觉。相反，迭代法是一个向真相逐渐靠近的过程，如果在中途已经可以保证需要的精度，那么就随时可以停止，节省计算时间。 第三点就更偏技术层面一点。通常而言，如果数据很大，那么很有可能矩阵 \(X\) 会带有某种稀疏特性，也就是说其中会有非常多的零元素。稀疏矩阵具有一些高效的存储方法和矩阵运算算法，但用直接法得到 \(X&amp;#39;X\) 之后，它就往往不再是稀疏矩阵了，于是存储量和计算量都会陡增。换言之，本来具有计算优势的稀疏矩阵，在直接法中却并不能发挥出它的优势来。 那么是不是有某种迭代法可以克服这些缺点呢？巧的是，本文要介绍的共轭梯度法就是其中之一。（哎说实话写这句的时候我自己都不信有多巧，前面铺垫这么多&#43;设问句&#43;巧合明显是作者刻意安排的啊，太明显了……） 什么是共轭梯度法？ 共轭，其实是线性代数里面的一个概念。给定一个正定矩阵 \(A\)，如果两个向量 \(u\) 和 \(v\) 满足 \(u&amp;#39;Av=0\)，就说 \(u\) 和 \(v\) 是关于 \(A\) 共轭的。一个 \(m\times m\) 的正定矩阵，最多可以有 \(m\) 组相互共轭的向量，而它们就组成了 \(m\) 维向量空间里的一组基 \(\{p_1,p_2,\ldots,p_m\}\)。通过线性代数的知识我们知道，给定了一组基后，向量空间里的任何一个元素就可以写成这组基的线性组合，比如 \[x=\sum_{i=1}^m \alpha_i p_i.\] 在回归模型中，回归系数 \(\hat{\beta}\) 正是线性方程组 \(Ax=b\) 的解，其中 \(A=X&amp;#39;X\)，\(b=X&amp;#39;y\)。而共轭梯度法（Conjugate gradient, CG），就是想像上面这个式子一样，把解 \(x\) 表达成共轭向量基的线性组合：只要依次算出所有的共轭向量 \(p_i\) 和对应的系数 \(\alpha_i\)，就可以得出 \(x\)。而在实际情况中，有可能用更少数目的 \(p_i\) 就能得到对 \(x\) 的良好近似，于是在这个意义上 CG 就是一种迭代法了。 那么为什么要叫共轭“梯度”呢？这是因为前面的这个公式还有另一种理解。考虑一个函数 \(f(x)=\frac{1}{2}x&amp;#39;Ax-b&amp;#39;x\)，我们很容易发现它取最小值的点正是方程 \(Ax=b\) 的解。如果我们用最优化的思路去解 \(Ax=b\)，就是要找到一个 \(x\) 使得 \(f(x)\) 达到最小。一般情况下，我们会采用“最速下降”的算法，即给定一个初始值 \(x_0\)，计算当前的梯度，然后沿着该梯度方向移动到下一个更新值 \(x_1\)，再计算梯度，如此反复循环。而共轭梯度法，则是说我们并不是沿着梯度走，而是沿着所谓的“共轭梯度”，即 \(p_i\)，进行移动。 至于为什么应该用共轭梯度而不是梯度，我建议感兴趣的读者看一看文章最后的那篇参考文献，其中对共轭梯度的优势进行了非常详细的阐述。一个直观的理解就是，普通的梯度法往往会有重复移动的方向（如文首图片中的绿线），而共轭梯度保证了每次移动的方向是共轭的（即关于 \(A\) 是正交的，如文首图片中的红线），因此不会有重复的劳动。关于 CG 的理论说来那个话就长了，因此本文不在这方面做过多的论述（其实是因为作者太懒），我在这里更想强调的其实是它的计算过程，参见图 1。 图1：共轭梯度法算法流程 神奇在哪里？ 图 1 所示的算法基本上可以展现出 CG 最重要的几个特性。 首先第一点，从图 1 可以看出，与 \(A\) 有关的运算只是一个矩阵乘法 \(Ap_k\)，剩下的部分都是向量之间的运算，没有任何其他更复杂的操作。而我们知道，矩阵与向量的乘法是很容易编程实现的，而且即使当矩阵很大的时候，它的内存占用量也非常小。纵观整个算法，基本上只需要存储若干个向量，所以在这个层面上，共轭梯度法非常适合内存受限的情形。 然后第二点，就如之前所说，共轭梯度法是一种迭代法，但它最奇特的一点在于，它同时又能保证在 \(m\) 步内完成计算。所以从某种层面上说，它兼具了直接法和迭代法的优点，好的情形下可以提前终止，最差的情况也能在 \(m\) 步内完成。 第三点，由于共轭梯度法中的大矩阵只参与乘法运算，所以稀疏矩阵的高效算法就可以派上用场了。可能你会说，\(A=X&amp;#39;X\) 不是已经破坏了稀疏性了吗？但实际上，在计算 \(Ap_k\) 的时候，可以先计算 \(v=Xp_k\)，再计算 \(Ap_k=X&amp;#39;v\)，这样两步分开来都是稀疏矩阵的运算。 代码实现 如前所说，CG 的一大优势在于编程实现非常简单。不依赖于任何附加包，我们就可以用几十行 R 代码搞定其核心算法。 ## Target: solve linear equation Ax = b. A is positive definite ## Ax -- A function to calculate the matrix-vector product ## `A * x` given a vector `x` as the first argument ## b -- Vector of the right hand side of the equation ## x0 -- Initial guess of the solution ## eps -- Precision parameter ## verbose -- Whether to print out iteration information cg = function(Ax, b, x0 = rep(0, length(b)), eps = 1e-6, verbose = TRUE, ...) { m = length(b) x = x0 r = b - Ax(x0, ...) p = r r2 = sum(r^2) for(i in 1:m) { Ap = Ax(p, ...) alpha = r2 / sum(p * Ap) x = x &#43; alpha * p r = r - alpha * Ap r2_new = sum(r^2) err = sqrt(r2_new) if(verbose) cat(sprintf(&amp;quot;Iteration %d, err = %.8f\n&amp;quot;, i, err)) if(err &amp;lt; eps) break beta = r2_new / r2 p = r &#43; beta * p r2 = r2_new } x } 或许会有读者疑问，为什么我要把矩阵乘法定义成一个函数参数 Ax，而不是直接在算法过程中写矩阵乘法。这是因为，某些情况下矩阵乘法可能有特殊的实现，用户只需要定义好相应的函数，就可以直接调用上面的这段程序，而不需要去修改算法的细节。使用上面的程序，一个简单的模拟例子如下： ## Simulation example set.seed(123) n = 10000 p = 1000 x = matrix(rnorm(n * p), n) b = rnorm(p) y = x %*% b beta_direct = solve(crossprod(x), crossprod(x, y)) mat_vec_mult = function(x, mat) { as.numeric(crossprod(mat, mat %*% x)) } xy = as.numeric(crossprod(x, y)) beta_cg = cg(mat_vec_mult, xy, mat = x) max(abs(beta_direct - beta_cg)) ## [1] 7.422063e-12 其中 CG 程序打印出了如下的信息： Iteration 1, err = 80261.88521243 Iteration 2, err = 24276.83688338 ... Iteration 21, err = 0.00000622 Iteration 22, err = 0.00000197 Iteration 23, err = 0.00000062 可以看出，CG 在第 23 步迭代后就收敛了，在我的机器上耗时约 0.82 秒，而直接法总共耗时约 5.3 秒，是 CG 的将近 6.5 倍。 真有这么神奇？ 看到这个结果，我估计小伙伴们都惊呆了。如果效果真这么好，那赶紧拿它去跑跑回归试试啊。于是我到 UCI 机器学习数据库上找了一个中等大小的数据集，包含 53500 个观测和 384 个自变量，然后兴冲冲地跑了个 CG（这里完全只是为了演示算法，实际处理数据时，请千万千万先对数据的背景有所了解，然后再考虑建模，切记切记）： dat = read.csv(&amp;quot;slice_localization_data.csv&amp;quot;) n = nrow(dat) y = dat$reference x = as.matrix(dat[, -c(1, ncol(dat))]) / sqrt(n) xy = as.numeric(crossprod(x, y)) coeffs = cg(mat_vec_mult, xy, mat = x) ## Iteration 1, err = 11262.97730747 ## Iteration 2, err = 4471.54099614 ## Iteration 3, err = 1783.28640925 ## ... ## Iteration 100, err = 2.94723420 ## Iteration 101, err = 4.60232106 ## Iteration 102, err = 4.02014578 ## ... ## Iteration 200, err = 0.63018214 ## Iteration 201, err = 1.67568741 ## Iteration 202, err = 0.49243538 ## ... ## Iteration 382, err = 0.16954617 ## Iteration 383, err = 1.05050962 ## Iteration 384, err = 0.11322079 纳尼？？怎么跟剧本写的不一样啊？说好的提前收敛呢？就算不提前不是说最多 \(m\) 步就收敛吗？我文章都写到这里了突然被打脸还怎么圆场啊？ （此处过去了半个小时……） 当崩塌的三观逐渐恢复的时候，就开始回过头来反思哪儿出了问题。其实，本文在之前有个非常重要的细节非常容易被忽视掉，大家把文章翻回第二节的第一句话，那里对矩阵 \(A\) 加了一个定语：正定。正定的代数意义表现在矩阵所有的特征值都大于 0，而在回归中，它等价于数据矩阵 \(X\) 是满秩的，换言之，没有多重共线性的存在。而如果我们检查一下这个数据中 \(X&amp;#39;X\) 的行列式，就会发现它等于 0，也就是说有多重共线性的存在——原来我们之前兴冲冲地犯了一个美丽的错误。 知道哪儿出错了就好办了，对于多重共线性，其中的一种应对办法就是给 \(X&amp;#39;X\) 的对角线上加上一个很小的常数 \(\lambda\)，这也就是我们常说的岭回归。我们重新修改一个岭回归版的矩阵运算函数，设定好 \(\lambda\) 参数和精度，再放到 CG 中去运行： ridge = function(x, mat, lambda = 0.01) { as.numeric(crossprod(mat, mat %*% x)) &#43; lambda * x } coeffs_ridge = cg(ridge, xy, eps = 1e-3, mat = x, lambda = 0.01) ## Iteration 1, err = 11256.55983300 ## Iteration 2, err = 4455.13459864 ## Iteration 3, err = 1767.78523995 ## ... ## Iteration 61, err = 0.00164239 ## Iteration 62, err = 0.00127173 ## Iteration 63, err = 0.00092021 这一回迭代 63 次就以 0.001 的精度收敛了，耗时约 4.2 秒。而更进一步，如果查看原始数据就会发现，这个数据的稀疏比例非常大，所以我们可以把矩阵转换成稀疏格式，再来尝试运行 CG： library(Matrix) xsp = as(x, &amp;quot;sparseMatrix&amp;quot;) coeffs_sparse_ridge = cg(ridge, xy, eps = 1e-3, mat = xsp, lambda = 0.01) 最后耗时约 2.6 秒。 总结 前面那个错误使用 CG 的例子并不是我杜撰的，而是我在准备这篇文章的时候真实发生的事情。对于我自己而言也是一个教训：跑算法跑模型的时候，一定要仔细检查假定条件，然后对数据要有充分的了解，否则前方的终点就会跟非正定的 CG 一样，不收敛啊。 相信通过模拟和实际数据的例子，读者可以更直观地感受到 CG 的如下一些优点： 实现简单，会矩阵乘法就行，不会的话请会的人吃顿饭就够了； 内存占用小，妈妈再也不用担心花钱给我加内存了； 可以控制收敛精度，想到哪儿停就到哪儿停； 可以充分利用稀疏矩阵或者其他特殊的矩阵构造加快运算，激发小宇宙潜能。 本文的代码可以在 Github 上查看和下载。 参考文献：An Introduction to the Conjugate Gradient Method without the Agonizing Plain HarryZhu：对于非正定的矩阵，添加一个 lambda 参数 相当于增加了松弛变量，利用约束条件来填满多重共线性，不知道这样理解有没有问题1。 liyun： 基于各种下降迭代的解析解不仅仅适用于普通最小二乘，各种广义线性模型都可用，比如logit； 共轭梯度法还可以并行计算，这对本身就分布存储的非常大的数据（比如上亿行）就非常关键了。如果一定要用 QR分解，分布式的情况下就只能做 bootstrap 类了…。 个人觉得是加了以后对角线上的数就可以保证不再是 0，这样就可以避免多重共线性。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Pandoc 中的 Markdown 语法 z]]></title>
    	<url>/tech/2016/10/26/markdown-in-pandoc/</url>
		<content type="text"><![CDATA[[1 小技巧汇总 2 概述 3 段落 3.1 段落和换行符 3.2 Extension: escaped_line_breaks 4 标题 4.1 Setext 风格语法 4.2 ATX 风格语法 4.2.1 Extension: blank_before_header 4.2.2 标题标识符 Extension: header_attributes 4.2.3 Extension: auto_identifiers 4.2.4 Extension: implicit_header_references 5 块引用 5.1 Extension: blank_before_blockquote 6 代码块{code 6.1 缩进式代码块{tab-codeChunck} 6.2 围栏式代码块 6.2.1 Extension: fenced_code_blocks 6.2.2 Extension: backtick_code_blocks 6.2.3 Extension: fenced_code_attributes 7 行文本块 7.1 Extension: line_blocks 8 列表 8.1 无序列表 8.1.1 四空格原则{4-spaces-rule} 8.2 有序列表{ordered-lists} 8.2.1 Extension: fancy_lists 8.2.2 Extension: startnum 8.3 定义列表（dl, dt, dd 等） 8.3.1 Extension: definition_lists 8.4 编号列表{numbered-lists} 8.4.1 Extension: example_lists 8.5 紧凑和宽松列表 8.6 截断列表 9 水平线 10 表格 10.1 Extension: table_captions 10.2 Extension: simple_tables（不推荐） 10.3 Extension: multiline_tables（支持对齐、原始内容换行） 10.4 Extension: grid_tables（支持块元素） 10.5 Extension: pipe_tables（支持对齐） 11 反斜线转义符 11.1 转义符 11.1.1 Extension: all_symbols_escapable 12 行内格式 12.1 强调 12.1.1 Extension: intraword_underscores 12.2 删除线 12.2.1 Extension: strikeout 12.3 上标和下标 12.3.1 Extension: superscript, subscript 12.4 行内代码块 12.4.1 Extension: inline_code_attributes 13 HTML 代码 13.1 Extension: raw_html 13.2 Extension: markdown_in_html_blocks 14 链接 14.1 行内链接 14.2 引用链接 14.2.1 Extension: shortcut_reference_links 14.3 内部链接 14.4 图片 14.4.1 Extension: implicit_figures 14.4.2 Extension: link_attributes 15 脚注 15.1 Extension: footnotes 16 元数据块 16.1 标题块 Extension: pandoc_title_block 16.2 YAML 元数据 Extension: yaml_metadata_block 16.3 引用 Extansion: citation 17 YAML 语法简单介绍 原文地址： http://www.cnblogs.com/baiyangcao/p/pandoc_markdown.html http://www.bagualu.net/wordpress/archives/5284 1 小技巧汇总 博客中提供的 &amp;lt;pre class=&amp;quot;white&amp;quot;&amp;gt;...&amp;lt;/pre&amp;gt; 和 &amp;lt;pre class=&amp;quot;center&amp;quot;&amp;gt;...&amp;lt;/pre&amp;gt; 只能通过 html 实现； 普通的 &amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;...&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt; 通过缩进 4 个空格实现； 块级的代码高亮通过 ```…``` 方法实现，其中的 ` 可以被替换成~，并且可以添加 class, id, 代码的语言名称 等内容； 行内代码通过 ` 来包括，也可以通过 &amp;lt;code&amp;gt;...&amp;lt;/code&amp;gt; 来实现，生成标题 class, id, 代码的语言名称 的方法与块级代码类似； &amp;lt;hn&amp;gt; 也可以添加 class, id 等； 在行内代码块中显示 ` 需要用 ` ` `，在行间高亮代码中显示 ```，只需要将 ` 替换成 ~ 就可以，不需要用 RMarkdown 中的做法； 现在通过 Pandoc 解释 Hexo 下的 Markdown，基本上可以保证解释的结果能够与 Mathjax 良好兼容，但要注意 Pandoc 对 LaTeX 语法的支持中，行内公式和行间公式分别用 `$...$` 和 `$$ $$` 完成，不支持使用 “\(...\)” 和 “\[...\]” 语法，当然 “\begin{equation}...\end{equation}” 还是可以正常使用的； 脚注的语法是 ^[]，另外一种脚注内容单独放置的语法暂时不使用； 删除线用一对 ~~ 实现，上标用一对 ^ 实现，下标用一对 ~ 实现； 使用 &amp;gt; 生成块级内容时，空行最好也添加上 &amp;gt;，这样在原始的 Markdown 规范和 Pandoc 的扩展规范中都能正常工作； 2 概述 Pandoc 中支持扩展修订版本的 Markdown 语法 使用 Pandoc 中支持的 Markdown 语法用 -f markdown； 使用标准 Markdown 语法用 -f markdown_strict； Pandoc 所支持的语法各种对标准 Markdown 语法的扩展可以通过在格式后以 &#43;EXTENSION 添加或 -EXTENSION 去除，如： -f markdown-footnotes 表示识别除了 footnotes 扩展之外的所有 Pandoc Markdown 语法； -f markdown_strict&#43;footnotes&#43;pipe_tables 表示识别标准 Markdown 语法加上 footnotes 和 pipe_tables 扩展语法。 3 段落 段落是指一个或多个空行之后的多行文本，文本中的换行都被视作空格，如若要输出换行，则应在行末添加两个或多个空格 注： 段落之后也应加一个空行，以区分段落和其他部分，如：列表。 如下Markdown语法 这是一个段落 - 列表项1 - 列表项2 翻译成HTML如下： 这是一个段落 - 列表项1 - 列表项2 若要正确的显示列表应在段落后添加一个空行，如下： 这是一个段落 - 列表项1 - 列表项2 3.1 段落和换行符 段落之间有一个空行。如果没有空行，那么它们会被认为是一个段落。同一个段落中不同的行之间的换行符会被一个空格代替。这个对于中文而言，造成的后果是，段落中会莫名其妙的多出几个空格，看着奇怪。Pandoc 中有一个扩展能自动处理这种换行符，这个扩展叫east_asian_line_breaks，这个扩展在 Pandoc 中没有被缺省的打开，所以需要手动打开，即使用-from markdown&#43;east_asian_line_breaks，这个扩展是在 Pandoc 的 1.16 版本中实现，此前的版本中是没有这个扩展的。因此，如果你使用这个扩展是发现这个扩展不认识，那你需要升级你的 Pandoc 的版本。 一行结束时，如果要强制换行，可以在行尾输入两个及以上空格。不然，Pandoc 会以上面的方式进行段内换行的处理。一种可能的例外是写多行诗时，一个小块可能以&amp;lt;br /&amp;gt;的形式放在一起，之后又用一个&amp;lt;p&amp;gt;开始一个新的小块，也许后面的行文本块方法是一种可行的做法，或者在一行结尾处用\来达到目的。 | 咏梅 | 啊！梅花 | 红 | | 霜 一行一行一行\ 二行二行二行 三行三行三行 3.2 Extension: escaped_line_breaks 也可以通过在行末添加一个反斜线\来换行1，如： 这是第一行\ 这是第二行 4 标题 Pandoc中支持两种标题语法：Setext 和 ATX。 4.1 Setext 风格语法 setext 风格标题是一行文本下跟一行=符号（表示一级标题）和-符号（表示二级标题），文本中可以包含如斜体、加粗等行内格式2 一级标题 ======= 二级标题 ------- 4.2 ATX 风格语法 ATX 风格标题就是我们通常所用的 Markdown 标题语法，在行首添加一到六个#符号表示不同级别的标题，编译成对应的 html 标签&amp;lt;hn&amp;gt;，如一个#表示一级标题，会编译成 HTML 标签&amp;lt;h1&amp;gt;，与 setext 风格相同，文本中可以包含如斜体、加粗等行内格式。 4.2.1 Extension: blank_before_header 标准 Markdown 语法并不要求在标题前添加一个空行，但是 Pandoc 语法却要求标题前添加一个空行（除了文档开头） 4.2.2 标题标识符 Extension: header_attributes 通常在 html 和 LaTeX 中使用，可以通过在标题行末添加如下形式的标识符来为标题添加属性： {#identifier .class .class key=value key=value} 其中的id是最有用的，通常用于文内的交叉引用。使用id做文内引用的方法是[标题名](#id)，这样在生成的 html 和 pdf 文档中就会有到这个标题的超链了。其中的标题名不必要和该标题的名字一样，只要保证#id一样就可以了。 identifier会被编译成 html 文档中的id属性，class会被合并成 html 文档中的class属性 4.2.3 Extension: auto_identifiers 没有显示指定identifier的标题会根据标题内容自动分配一个唯一标识，标题文本生成identifier的顺序如下： 移除格式、连接等； 移除脚注(footnotes)； 移除除了下划线_和连接符-之外的标点符号； 用连接符-替换所有空格和换行符； 将所有字母转换成小写； identifier不能以数字和标点符号开头； 如果文本此时为空，则取section做标识符； 如果自动生成的标识符相同则根据顺序在标识符后添加-1、-2等。 4.2.4 Extension: implicit_header_references Pandoc 默认每个标题都定义了引用链接，故对于标题# 标题1，可以使用[标题1]或者[标题1][]引用，注意，引用链接是区分大小写的。 5 块引用 块引用是指一个或多个段落或其他块元素（如列表或标题），每一行以一个&amp;gt;符号和一个可选的空格开头（注意：&amp;gt;符号并不需要在行首，但是不可缩进超过三个空格） &amp;gt; 块引用 &amp;gt; 段落 &amp;gt; &amp;gt; 1. 列表1 &amp;gt; 2. 列表2 块引用并不是每一行都需要以&amp;gt;符号开头，只需在每一个区域的首行添加&amp;gt;即可，如下文本和上述的文本有相同的效果。 警告：实际上在本博客的设置下后一种情形出现了两个 block，即两种情形下的显示结果并不相同，这显然与 Pandoc 的blank_before_blockquote扩展有关。因此保险起见，还是推荐使用第一种方式的块引用。另一种可以考虑的解决方法是使用标准的 Markdown，但是根据需要加上少量 Pandoc 的特有扩展，这样可以保证两种情形的效果相同。 &amp;gt; 块引用 段落 &amp;gt; 1. 列表1 2. 列表2 注： 1. 块引用可以嵌套使用； 2. &amp;gt;后的空格作为块引用标识的一部分，若是在块引用中添加代码， 则需在&amp;gt;后添加五个空格。 5.1 Extension: blank_before_blockquote 标准 Markdown 语法并不要求在块引用前添加一个空行，但是 Pandoc 语法却要求在块引用前添加一个空行（除了文档开头外）。 6 代码块{codeChunck} 6.1 缩进式代码块{tab-codeChunck} 由四个空格或一个tab缩进的文本取做代码块，区块中的特殊字符、空格和换行都会被保留，而缩进的空格和tab会在输出中移除，但在代码块中的空行不必缩进 using System; public class Program { public static void Main() { Console.Write(&amp;#39;Hello World!&amp;#39;); } } 6.2 围栏式代码块 6.2.1 Extension: fenced_code_blocks 除了标准的缩进式代码块之外，Pandoc 还支持围栏式代码块， 代码块以三个或三个以上的~符号行开始，以等于或多于开始行~个数符号行结束， 若是代码块中含有~，只需使开始行和结束行中的~符号个数多于代码块中的即可 ~~~~~ ~~~~ code here ~~~~ ~~~~~~ 6.2.2 Extension: backtick_code_blocks 与fenced_code_blocks相同，只不过使用反引号`替换波浪线~而已 6.2.3 Extension: fenced_code_attributes 与标题标识符相同，在波浪线或反引号代码块的首行添加属性即可，如下： ~~~ { #id .cs .numberLines } using System; public class Program { public static void Main() { Console.Write(&amp;#39;Hello World!&amp;#39;); } } ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 如上代码中添加的 cs 类可以用于代码 HTML 和 LaTex 输出的代码高亮，Pandoc 所支持高亮的语言可以通过在命令行中输入pandoc --version查看， 除了上述方式设置代码块的高亮语言，也可通过如下方式设置 ```cs using System; public class Program { public static void Main() { Console.Write(&amp;#39;Hello World!&amp;#39;); } } ``` 7 行文本块 7.1 Extension: line_blocks 行文本块是指一系列由|和一个空格开头的行，在输出中可以保留空格和换行， 不会像段落那样将换行符转换成空格，可用于诗文和地址的排版 | 咏梅 | 啊！梅花 | 红 | | 霜 8 列表 8.1 无序列表 列表项以星号*、加号&#43;或减号-开头，如下： * 列表项1 * 列表项2 * 列表项3 这样输出的列表是紧凑型的列表，若是要输出宽松型列表， 可在列表项之间添加空行即可 * 列表项1 * 列表项2 * 列表项3 8.1.1 四空格原则{4-spaces-rule} 一个列表项里可以包含多个段落或其他块级别内容， 但是其次的段落都应该以一个空行和四个空格缩进开始 * 列表项1 第一段落 列表项1 第二段落 * 列表项2 第一段落 { code } 列表中可以嵌套列表，每一层嵌套列表都需要添加四个空格或一个tab缩进， 并且每一层应该使用不同的起始符。 * 1.1 &#43; 1.1.1 - 1.1.1.1 - 1.1.1.2 &#43; 1.1.2 * 1.2 8.2 有序列表{ordered-lists} 有序列表项以数字、一个点.和一个空格开头， 并且取第一个列表项数字为基准，依次向下排，故下面两个列表是一样的 **列表1** 1. 一 2. 二 3. 三 --- **列表2** 1. 一 5. 二 9. 三 8.2.1 Extension: fancy_lists 不像标准 Markdown 语法只能使用阿拉伯数字作为有序列表标识， Pandoc中还支持大小写字母、罗马数字，或用括号、右括号标识列表项， 但其后的文本需与列表标识隔开至少一个空格， 若是一个大写字母和一个点做标识，则需在其后跟两个空格。 fancy_lists扩展还支持使用#来代替数字 #. 列表项1 #. 列表项2 8.2.2 Extension: startnum Pandoc 支持自定义的列表起始数字，而且会在每次使用不同的列表标识便重新开始一个新列表， 如下会创建三个列表 (3) 列表1项1 (7) 列表1项2 1. 列表2项1 * 列表2项1 8.3 定义列表（dl, dt, dd 等） 8.3.1 Extension: definition_lists 词语1 : 词语1的定义 词语1的第二段定义 第二段定义是多行的 词语2 : 词语2的定义 定义列表形式如上，术语独占一行，其后可以跟一个空行，然后是一个或多个定义，每一个定义以:和~开头，可以缩进一到两个空格。 一个术语可以包含多个定义，一个定义可以包含多个区块（段落、代码块、列表等），而每一个区块都应以四个空格或一个tab缩进。 8.4 编号列表{numbered-lists} 8.4.1 Extension: example_lists 特殊的列表标识符@用于连续编号列表，整个文档中的@符号从1开始编号， 依次类推，如下的前三个@会分别替换为1, 2, 3 (@) My first example will be numbered (1). (@) My second example will be numbered (2). Explanation of examples. (@) My third example will be numbered (3). (@good) This is a good example. As (@good) illustrates, ... @ 后可以加上一个字符串来表示一个标签，用于在其他地方引用这个序号，如上例中的 @good 会被 4 来替换。 8.5 紧凑和宽松列表 若列表项前插入一个空行，则会将当前列表项作为段落处理（用&amp;lt;p&amp;gt;标签包裹），从而输出“宽松”的列表，反之则会输出“紧凑”的列表。 8.6 截断列表 1. 列表1项1 2. 列表1项2 1. 列表2项1 2. 列表2项2 如上想要输出两个列表，却会输出一个有四项的列表，要想“截断”列表1，则可在两个列表之间插入一行没有缩进的行，如 HTML 注释 1. 列表1项1 2. 列表1项2 &amp;lt;!-- --&amp;gt; 1. 列表2项1 2. 列表2项2 9 水平线 一行由三个或三个以上*、-或_组成的会输出一个水平线。 10 表格 Pandoc 中支持 simple_tables, multiline_tables, grid_tables 和 pipe_tables 四种表格。要注意单元格中内容的换行与块元素是不同的概念，原始内容可以换行与显示结果中的换行是不同的概念。 10.1 Extension: table_captions 四种表格都可以通过在表格前或后添加一个以Table:（或:）开头的段落表示表格的表头。 10.2 Extension: simple_tables（不推荐） 简单的表格形如下 Right Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: 简单表格实例 列首行和表格中的每一行都应独占一行，列对齐方式由列头和其下虚线行的相对位置决定： 右对齐： 虚线行与列头右对齐，而左端超过列头 左对齐： 虚线行与列头左对齐，而右端超过列头 居中： 虚线行超出列头两端 默认： 虚线行与列头两端对齐（一般情况下默认是左对齐） 表格必须以一个空行或一行虚线行加一个空行结束，而且有时可以忽略列头行。 注：中文环境不推荐使用这种方式选择对齐方式，反正小生是玩不好。 10.3 Extension: multiline_tables（支持对齐、原始内容换行） 跨行表格允许列首行和表格中的行可以分多行撰写，但是不支持单元格的跨行和跨列，跨行表格必须以一行虚线行开始，以一行虚线行和一个空行结束，行与行之间应有一个空行 ------------------------------------------------------------- Centered Default Right Left Header Aligned Aligned Aligned ----------- ------- --------------- ------------------------- First row 12.0 一个行跨 多行的例子 Second row 5.0 这是另一行 注意表格行与行 之间的空行哦~~ ------------------------------------------------------------- Table: 这个是标题 也能跨行的啦~~ 跨行表格的列首行也可以被忽略，也可以只包含一行，但是这一行后必须跟着一个空行 10.4 Extension: grid_tables（支持块元素） 网格表格中列首行与其他行需要使用一行=隔开，但是在没有列首行的表格中可以忽略，网格表格中的单元格可以包含任意区块（段落、代码块、列表等），但对齐方式和单元格的跨行跨列都是不支持滴~ : 网格表格样例 &#43;---------------&#43;---------------&#43;--------------------&#43; | Fruit | Price | Advantages | &#43;===============&#43;===============&#43;====================&#43; | Bananas | $1.34 | - built-in wrapper | | | | - bright color | &#43;---------------&#43;---------------&#43;--------------------&#43; | Oranges | $2.10 | - cures scurvy | | | | - tasty | &#43;---------------&#43;---------------&#43;--------------------&#43; 10.5 Extension: pipe_tables（支持对齐） | Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | : `pipe_tables`表格样例 pipe_tables 每一列之间用竖线|隔开，列首行和其余行之间用虚线行隔开，虚线行中用冒号:来决定列的对其方式，表格中两端的|竖线列可以忽略，而且|只是用来隔开列，而不必对齐，但是 pipe_tables 的列首行不能忽略，要是想要生成没有列首的表格，只要在列首行的单元格置空便可 | -----|-----: 苹果 | 2.05 梨 | 1.37 橘子 | 3.09 注： pipe_tables 中的单元格不可包含如段落、列表等区块元素 11 反斜线转义符 11.1 转义符 11.1.1 Extension: all_symbols_escapable 除了在代码块和行内代码中，反斜线后的任何字符和空格都会按照字面输出，Markdown 语法中能被转义的字符如下 \`*_{}[]()&amp;gt;#&#43;-.! 12 行内格式 12.1 强调 用一个 * 或 _ 包裹起来的文本会被输出为斜体，而用一对 ** 或 __ 包裹起来的文本会被输出为 加粗。 强调文本：这里是*斜体*，这里是**加粗** 若是 * 或 _ 符号前后有 空格，或用 \ 转义，则不会输出为斜体或加粗。 这里 * 不会被翻译成斜体 *, 而且 \*这里也不会\*。 12.1.1 Extension: intraword_underscores 文本中的成对_不会被翻译成斜体， 这个扩展的意思是 Pandoc 不会把文字中间的_解释为加粗的语法，因为 Pandoc 认为在文字中使用_是一个普遍的现象。如果解释为加粗反而会带来不便，因此如果要在文字中间使用强调（加粗）语法，请使用*。 12.2 删除线 12.2.1 Extension: strikeout 一对~~所包裹的文本会添加一条删除线 ~~这个文本被删除了~~ 12.3 上标和下标 12.3.1 Extension: superscript, subscript 上标可以通过一对^标识，而下标可以通过一对~标识， 若上下标中包含空格，可以通过\转义空格 H~2~O 2^10^ = 1024 P~a\ cat~ 12.4 行内代码块 小段的行内代码块可以使用一对`包裹， 而行内代码块中含有反引号`，可以用双反引号包裹代码块， 但是行内代码块中的转义符\没有转义的作用。 这是一个`行内代码块` 这是一个行内代码块的反引号`` ` `` 这是一个行内代码块的反引号` ` `，效果与上一行在本博客中类似 这是一个行内代码块的反斜线`\` 12.4.1 Extension: inline_code_attributes 与代码块一样，行内代码块后写可以添加属性，形式如下： `行内代码块`{ #identifier .class key=value } 13 HTML 代码 13.1 Extension: raw_html 可以直接在文档中插入 HTML 代码（除了代码块等&amp;lt;, &amp;gt;和&amp;amp;符不会被翻译的地方之外） 13.2 Extension: markdown_in_html_blocks 使用 markdown_strict 格式的时候，HTML 代码中的 Markdown 语法不会被翻译， 但是使用 Pandoc 的 Markdown 格式时，HTML 代码中的 Markdown 语法也会被翻译， 但是有一个例外，HTML 代码&amp;lt;script&amp;gt;和&amp;lt;style&amp;gt;标签中的 Markdown 语法也不会被翻译。 14 链接 如果用尖括号包裹一个 URL 或 email 地址，就会输出一个链接： &amp;lt;http://google.com&amp;gt; &amp;lt;sam@green.eggs.ham&amp;gt; 14.1 行内链接 行内链接文本由方括号[]包裹，其后跟 URL 链接用圆括号()包裹， 圆括号内的 URL 后可以用双引号&amp;quot;包裹一串字符串作为链接标题， 这是 [行内链接](链接地址), 并且这是 [一个 带有标题的行内链接](http://fsf.org &amp;quot;链接标题，鼠标悬停时显示&amp;quot;) Email 的链接地址应该跟在mailto后面 [给我邮件哦~~](mailto:sam@green.eggs.ham) 14.2 引用链接 显示引用链接包含链接和链接定义两部分， 链接定义可以出现在文档其他部分，链接之前或之后皆可；链接由方括号[]包裹的链接文本和由方括号[]包裹的链接标签组成 [链接文本1][Label1] [链接文本2][Label2] 链接定义由方括号[]包裹的链接标签、冒号、空格和 链接地址（可以用尖括号&amp;lt;&amp;gt;包裹）组成， 其后还可以跟空格&#43;链接标题，由单引号&#39;, &amp;quot;或圆括号()包裹 [Label1]: http://www.baidu.com &amp;quot;百度一下&amp;quot; [Label2]: http://www.google.com &amp;quot;谷歌一下&amp;quot; 注： 链接标签不区分大小写。 隐式引用链接中的链接标签部分为空， 而链接定义中的链接标签由链接文本替换。 详情见[官方网站][] [官方网站]: http://guanfangwangzhan.com 14.2.1 Extension: shortcut_reference_links 隐式引用链接中的空方括号[]可以忽略。 14.3 内部链接 可以使用identifier来链接到文档中的其他章节， 链接地址形如#identifier 见[标题](#标题) 见[标题] [标题]: #标题 14.4 图片 如果链接前添加!，链接则会被作为图片处理， 而链接文本则会被作为图片的alt属性处理。 ![月亮](月亮.jpg &amp;quot;十五的月亮&amp;quot;) 14.4.1 Extension: implicit_figures 若是图片作为一个独自的段落存在，则图片的连接文本会被当做标题处理 ![这是标题](图片地址.png) 但若是想要将图片作为一般的行内图片处理，只需确保图片不是当前行的唯一内容即可， 比如在行末添加一个反斜线： ![这个就不是标题了~~](图片地址.png)\ 14.4.2 Extension: link_attributes 链接或图片后可以像其他元素一样添加属性 { #identifier .class key=value } 行内图片 ![图片](地址.png){ #id .class width=20 } 和一个带属性的[引用链接] [引用链接]: http://www.baidu.com &amp;quot;百度一下&amp;quot; { #id2 .class key=value } 15 脚注 15.1 Extension: footnotes Here is a footnote reference,[^1] and another.[^longnote] [^1]: Here is the footnote. [^longnote]: Here&amp;#39;s one with multiple blocks. Subsequent paragraphs are indented to show that they belong to the previous footnote. { some.code } The whole paragraph can be indented, or just the first line. In this way, multi-paragraph footnotes work like multi-paragraph list items. This paragraph won&amp;#39;t be part of the note, because it isn&amp;#39;t indented. 脚注的identifier不可包含空格，tab或换行， 在输出中脚注会按照顺序编号，如上例中的[^longnote]会被编号2， 脚注虽不必一定放在文档的末尾，但也不可以出现在其他的区块中（如列表、块引用、表格等）3。 16 元数据块 这一小部分内容可以在后面学习过 Pandoc 模板以后在来看。 在 Markdown 中可以嵌入元数据块，这些块用来给定文档的一些属性，这些块分为两种，一种是使用在文章最前面的标题块，还有一种是使用在文中任意地方的，叫YAML 块，分别介绍如下： 16.1 标题块 Extension: pandoc_title_block 这个块放在文件的最前面，格式如下 % 标题 % 作者，多个作者使用分号分开 % 日期 这样的内容将会被解析，解析的结果可能会出现在最后的文档中（比如在 pdf 或者 html 的标题以及作者部分，之所以说“可能”，是因为自定义模板中可能没有这些输出）。这三个元素也可以只有其中一个或者两个，在这种情况下，在相应的地方要留出空行，比如 % 我的标题 % % 2016年4月6日 标题以及作者可以是多行的，第二行以后的文字需要缩进。多个作者可使用分号分开，也可使用多行格式，例如： % 我的标题 多行的标题 % 作者1 作者2 % 2016年4月6日 日期必须在一行，不能分行。 关于这些元素在最后输出文件中的位置，可以参考后面的 Pandoc 模板部分。 16.2 YAML 元数据 Extension: yaml_metadata_block yaml 块是用---开头，以---或者...结束的一个块，这中块可以出现在文中的任何地方，每个块开始的地方，必须在前面留出一个空行。yaml 块也可以放在一个独立的文件中，在使用 Pandoc 的时候，把这个 yaml 文件作为输入就可以了。Pandoc 在处理多个输入文件的时候，总是把这些文件合成一个文件，然后再进行处理，因此一个单独的 yaml 文件是不会有问题的。但是请注意一定要以---把 yaml 块包起来。 下面是一个处理独立 yaml 文件的命令行 pandoc aa.md metadata.yaml -s -o aa.html yaml 块中定义的变量会加入到文件的 metadata 中。如果一个变量被多个块定义，那么它的值为第一个被定义的地方，后面的定义无效。另外如果一个变量以下划线结束，它将不会被 pandoc 处理。 下面是一个元数据块的例子，第一行中包含:，因此需要一个引号包起来，在 abstract 定义中，使用|来开始一个缩进的块。 --- title: &amp;#39;This is the title: it contains a colon&amp;#39; author: - name: Author One affiliation: University of Somewhere - name: Author Two affiliation: University of Nowhere tags: [nothing, nothingness] abstract: | This is the abstract. It consists of two paragraphs. ... 在上面的这个块中，作者部分有自定义的域，因此需要一个自定义的模板来显示它们，下面是一个模板的例子： $for(author)$ $if(author.name)$ $author.name$$if(author.affiliation)$ ($author.affiliation$)$endif$ $else$ $author$ $endif$ $endfor$ 16.3 引用 Extansion: citation 这是关于如何添加参考文献的内容。 17 YAML 语法简单介绍 前面提到了 Markdown 中的元数据块可以通过 YAML 语法来指定。 YAML 全称为（Yet Another Markup Language），也是一种标签语言。是以数据为表达目标的语言。通过空格和分行来分隔数据单元。下面是一个实例： house: family: name: Doe parents: - John - Jane children: - Paul - Mark - Simone address: number: 34 street: Main Street city: Nowheretown zipcode: 12345 其中连续的项目通过-来表示，map 结构的key/value结构使用:来分隔。要求同一级别的数据前面缩进的空格数要相同。在上面的例子中，Simone可使用hourse.family.children[3]来引用。 其他的相关内容： 要添加注释，使用#符号。 布尔值使用true和false 空值使用null或者~ 数值支持 整数，如12 八进制数，如012 16进制数，如0xC 浮点数，如1.24 指数，如1.2e3 无穷: .inf 结束前，我们看这样一个例子： title: - type: main text: My Book - type: subtitle text: An investigation of metadata 在这个例子中，要访问My Book,应该使用title[1].text。这里的两个-表示title下是一个数组结构。 这是在表格单元格中添加换行的唯一形式。↩ 在本博客中，斜体的display被设置成了block并且居中对齐，主要用于图、表的标题。↩ 未理解。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[泡沫的本质：明知倾家荡产，你还一往无前 z]]></title>
    	<url>/prof/2016/10/23/nature-of-bubble/</url>
		<content type="text"><![CDATA[[原文地址：http://www.sohu.com/a/116942136_465184 作者：朱宁1 来源：转载自扑克投资家（ID：puoket 导读：许多人将今年楼市的疯狂和去年股市的泡沫做类比，认为这两者有着很大的相同之处，但是同样是调控，同样也是限购，为何股市泡沫瞬间演变成股灾，而楼市依然坚挺。那到底是什么主推了泡沫，又是什么刺破了泡沫？今天观察君向你们推荐朱宁老师的演讲，一篇让你对市场理解上升 N 个档次的行为金融学笔记。认真读后，也许你会对金融市场上的泡沫和虚假繁荣有更深层次的理解！ 首先给大家介绍一下我的导师，席勒教授，席勒教授因为自己对于行为金融的研究获得了 2013 年的诺贝尔经济学奖，这里我想强调一点的是，席勒教授是和另外两位——芝加哥大学的金融学教授，尤金法玛教授和拉尔斯汉森教授三个人一起分享，这很可能是诺贝尔经济学奖颁发四十年以来第一次两位持有完全相反的学术态度的学者分享同一年的诺贝尔经济学奖。 因为可能学过金融的同学都学过有效市场假说，法玛教授在六十年代就提出了有效市场假说。而席勒教授直到八十年代才提出了作为整个行为金融奠基的研究，也就是下面要为大家介绍的一个研究。 我觉得我们中国学界对诺贝尔奖都有非常强的情节，我和我的同学沟通的时候就说，其实的诺贝尔奖不是那么复杂，席勒教授就是因为画了一幅图得了诺奖。那么这幅图讲了什么呢？ 他基本上讲了过去一百多年美国资本市场真实的市场波动，也就是我们看到的这条实线。以及市场的基本面的价值，就是用市场的盈利水平乘以市场平均市盈率就是他所谓市场的基本面价值。 所得到的一个结论奠定了整个行为金融学的一个基础，也就是最近一年大家在整个上证 A 股所亲身经历感受到的，就是我们的基本面出现了波动，我们的经济出现了波动但是远远不足以大到解释我们过去六个月在上证 A 股市场所产生的波动。 所以席勒教授想在三十年前想要了解，为什么资本市场价格的变化要远远超出资本市场基本面变化的趋势。那么从各种传统的估值理论也好，有效市场假说也好，公司金融理论也好，都很难解释这个现象。那么为什么可以用行为金融来解释呢？因为我们从以前传统的新古典的完全理性的假说里面无法解释这个现象。 行为金融学的力量 从我九十年代在耶鲁大学学习以来，我觉得过去短短 20 年里，行为金融在整个金融领域产生的影响力，在整个实践者心中的地位都获得了非常高速的发展。 我可以毫不遮掩地和大家讲，就在十五年前，2000 年的时候我们在美国开行为金融和行为经济研讨会得时候，每次开会的人都是固定的三十个人，这三十个人都认为自己肩负着一个新的研究领域的使命，所以是在这种使命的驱使下，我们推动了行为金融非常大的发展。 对行为金融来说，它研究的一个很重要的领域就是泡沫，泡沫是一个非常有趣非常重要也是非常难以理解的一个领域。按照哈佛大学经济史学家金德尔伯格教授的说法，泡沫就是从基本面上来看不健全的商业事业，同时往往伴随着高度的投机性，价格上涨是因为投机者不断买入相信价格不断上涨。 每次在高金学院讲授行为金融的时候是四天的课程，我想要把四天的课程压缩到一个小时里面，我的挑战还是蛮大的。我会给大家介绍人类金融史上第一次金融泡沫。就是在荷兰这个第一个全球金融中心所爆发的郁金香的狂热。 在郁金香泡沫的顶端大家可以看到一颗最受大家追捧的种子可以卖到六千荷兰盾，而当时在阿姆斯特丹市中心的小型联排别墅的平均价格是三百荷兰盾，也就是说一颗郁金香的种子可以买二十个 CBD 的联排别墅。如果我们以上海 CBD 或者北京 CBD 的联排别墅的价格乘以二十的话我们就可以估计一下那个郁金香的种子在今天可以卖到一个什么样的价格。 那么怎么可能一个郁金香的种子可以涨到这么一个程度？我看很多嘉宾都很年轻，我在我小的时候我听说在吉林搞过一次君子兰的泡沫，一株君子兰当时可以卖到 100 万，当时一百万现在轻轻松松就相当于一个亿。 那么一株花怎么可能获得这么高的市场追捧？或者说我们的股票怎么可能受到这么多的市场追捧？我们某些地区的房地产怎么可能会有这么高的估值水平？这是我们在泡沫或者说是行为金融里面非常感兴趣的一个研究领域。 在 1987 年出版的一本历史书里面，这本书叫做《富人的尴尬》，说在郁金香泡沫最顶端的时候，一颗郁金香可以给卖者换回来两马车的小麦，四马车的黑麦，四条肥牛，八头猪，十二头羊，两牛头的葡萄酒，四吨黄油，一千磅的奶酪，一张床一件衣服和一个大酒杯。 这些都反应出了资产或者商品的价格可以如此大地背离它的基本面。不出大家的意料，这些郁金香在一年以后，很多的价格下跌 95%，99%，只剩到原来顶峰价格的 5% 甚至 1%。 为什么会出现这种泡沫，为什么这种泡沫在不断发生，这是我们行为金融关注的一个领域，这是很难通过传统的新古典的经济理论和金融理论来解释的。为什么在整个时期，所有的市场参与者，所有的政策制订者都像被催眠或者被洗脑了一样？ 如果你对任何一个身处泡沫的人说，你觉得我们是不是身处泡沫，你得到的回答一定是否定。我在很多专栏中写道，泡沫有趣的地方正是在于所有身处泡沫的人都认为这不是一个泡沫。如果他认为是一个泡沫，那么他就会卖出，他卖出，价格就不会继续上涨，不会继续上涨，泡沫就会破灭。 我们看见荷兰出现过郁金香的泡沫，在英国取代荷兰成为全球经济和金融的霸主的时候曾经出现过南海泡沫。南海泡沫就不给大家介绍过多的信息，只提一点，人类历史上最伟大的物理学家之一，英国的艾萨克牛顿爵士，在南海泡沫初期赚了大钱，之后又在整个泡沫破灭的时候，把全部的身价全部都赔了进去。他在赔钱之后说了一句话，“我能够预测天体之间的距离，但我无法预测人心的狂热。” 在此之后英国在十九世纪，1845 到 1846 年曾经出现过一次非常严重的英国铁路泡沫，这个铁路泡沫的一个结果就是英国仍然是这个世界上铁路密度最大的国家。在我们修建高铁之前，英国全国的铁路长度相当于我们国家全国的铁路长度。 那么最近的有日本的房地产泡沫，大家可以猜猜在日本房地产泡沫顶端的时候，东京最繁华的银座地区最优质的写字楼一平米卖到什么价钱？10 万，20 万，50 万，100 万？100 万是正确答案，单位是美元。所以看起来我们中国地产商还有很大希望对不对。 那么当年卖一百万一平米的房地产现在卖多少钱？去掉两个零，一万，这就是泡沫。但是身处泡沫之中，所有日本居民，所有日本投资者，所有基金经理，没有人会认为这是泡沫。不但没有人认为这是泡沫，而且大家会认为日本会统治全世界，日本会买下全世界最主要的资产，最抢手的房产。 我们中国也有一个很有趣的现象，就是 05 年到 07 年中国的权证市场，比较有经验的投资者可能会记得当时的权证市场，出现过一只权证下午就要到期了，上午还能炒到两三块钱的价位，而且一天的换手率可以达到十倍——在同一天里面持有这个权证的投资者换了十遍，虽然很多人都知道这个权证在今天下午收盘的时候一分钱都不值。所以大家根本无视资产的基本面，而只看一个击鼓传花的投资心理——只要价格上涨，价格就会继续上涨。 国内上一次泡沫在 08 年，全球领域上一次泡沫是互联网泡沫，再往前走八年美国曾经出现过存款贷款银行的泡沫。我们发现在人类过去两三百年的金融历史的发展过程中，泡沫发生的频率不是降低了而是升高了。在这里作一个宣传，在今年十月份会出一本我的新书叫做《刚性泡沫》，我会在这本书里面解释为什么在全球领域泡沫发生的频率越来越高了而不是越来越低了。 那么什么导致泡沫？我在《财经》文章里讲了： 第一，对于任何一个泡沫能够形成泡沫必须要有一个新事物，要有一个新寄托，要有一个新经济。郁金香是一个整个欧洲都没有见过的花，蒸汽机是一个改变人类经济社会进程的一个重大发明，互联网带领了人类整个生产方式生活方式的改变。只有新概念新理念新产品，才有不确定性，才有炒作的空间。 第二，任何一次泡沫都离不开天量的流动性。这点我不用给大家解释，我们的货币供应量是美国的 130%，是全国货币供应量最大的国家。 第三，政府的支持，这点我不解释。 第四，缺乏经验的投资者，我们看到无论是荷兰，无论是英国，无论是美国，无论是日本，无论是迪拜，任何一个新兴市场，在取代旧经济成为世界金融霸主的时候往往会经历一次或者几次金融或者经济泡沫，很大程度上是因为在这个经济体上存在很多年轻有财富又有很强的创富心理的投资者。 所以在这里我想给大家解释一下，我们研究行为金融很大程度上是因为我们希望金融学的理论和金融学的实际更紧密的结合。我记得席勒教授每年在美国经济研究局开会的时候都会说，“我认为所有好的金融学都是行为金融学。”那么他的一个非常好的伙伴，我觉得很可能在今后也会获得诺贝尔经济学奖的芝加哥大学的教授理查德斐乐，前两年在中国出了一本非常好的新书，我也会给大家作一个推荐。斐乐教授说，“我认为行为金融的成功，在于现在已经没有行为金融了，所有的金融研究者和实践者都在采用行为金融的思路进行思考和实践。” 那么为什么经济会出现难以预测和难以解释的非常大的波动，为什么资本市场会出现非常大的，难以解释的波动泡沫和崩盘。金融学在过去五十年的发展很大程度上希望借助于科学，希望借助于数学和物理学，所以有很多很成功的金融学研究者都受过物理学的教育。 前一段时间我翻了美国和奥地利很有名的经济学家哈耶克在当年获得诺贝尔经济学奖后发言，曾经讲到了经济学和自然科学最大的区别在于，在自然科学中我们不需要考虑人的存在，而在社会科学一个很重要的领域恰恰就是人。 你真的认识自己吗？ 所以行为金融学很重要的贡献就是它把心理学、社会科学和传统的金融学的研究领域和思路连接在一起。现在很多心理学家还说我们人类对自己大脑的了解，还不如我们对于外太空和海洋深处的了解。 我希望大家看上面这幅图来判断上面两条黄线那条长一些，正确答案显然是一样长，但很多的观众会认为上面的长一些，有的时候大家认为上面的长一些，但你的大脑马上会告诉你这是因为参照系。 这幅图是荷兰十七世纪非常著名的版画家埃舍尔，大家仔细看一下这幅图大家就会发现有趣的地方在于，在每个阶段你都会发现这幅图非常非常合理，你把所有的信息放在一起你就会发现这幅图是如此荒谬。 这幅图让我想起了前段时间非常非常流行的电影叫做盗梦空间。所以在过去二十年心理学家希望通过大量的实验让大家了解到，人类的认知过程实际上比人类本身了解事物要复杂和难以预测得多。 那这幅图时间关系我不给大家做过多的解释，但是这幅图是二十多年前才被心理学家设计出来为了说明一个现象。大家看这里一个 A 的方块和 B 的方块，我想问的不是哪个更长一些，而是哪个方块的颜色更深一些？正确答案是一样的。大家都认为不可能的对不对，那么我帮助大家增加一点参照系让大家感觉一下这两个方块是一模一样的。 还有这幅图，我让大家看三秒钟，再闭上眼睛想一想脑海里还有什么东西，大家脑海里往往会留下的是一个大的白色的向上的三角形和一个大的黑色的倒三角形，有点像以色列的国旗。有趣的是大的白色的三角形是在画面里面没有出现过的，完全是我们脑海里才出现的这么一个现象。 我们上市公司公报里的很多信息并没有让投资者去买这只股票，很多投资者自己会想出很多让自己觉得这是一个好公司这是一个好股票的信息。 这是一个什么动物？鸭子？兔子？一会是鸭子一会是兔子？那么无论是基金经理也好，还是个人投资者也好，都会有这种经历。早上起床你就想到某只股票很有吸引力，你现在就想买，开盘后跌下去你心里就会想今天就算了，对于完全一样的一个筹码我们的认知可以在一秒钟之后发生一百八十度的改变，这只是我们所说的视觉幻觉里面一点点小小的皮毛。 我用了这么多的案例，就是希望大家认识到，我们人的认知过程是如此的复杂。 快思考、慢思考 之所以这么复杂，是因为在人的整个进化过程中，按照《思考，快与慢》这本书上所说，人类是用两种不同的思维方式来思考的。一种是慢思考，比如一乘二一直乘到五十结果是多少，就要拿出纸来算一算；如果我现在说，屋子后面有一只老虎，大家的反应就是跑，大家不会说去想一想这是一直东北虎还是华南虎，不会去想一想这是一只公老虎还是母老虎，不会去想这只老虎早上吃了饭还是没吃饭。我们有一种经过进化形成的本能，可以很快的，不需要通过你的大脑中枢就可以做出的决定。 大家觉得投资是适用于快思考还是慢思考？有多少次我们买股票是因为我们吃饭的时候有朋友告诉我们这是一只好股票，有多少次买股票是因为某次节目上有一个著名的分析员推荐这只股票，有多少次买股票是因为我有一个朋友在这个公司工作，他说这个公司是一个好公司。这些是快思考还是慢思考？ 我在课上往往会给大家再问一些问题，这些问题会越来越难。什么样的老鼠两只脚？谢谢高总，米老鼠，下一个问题有点难，什么样的鸭子有两只脚？我听到所有的鸭子，但还是有很多人说唐老鸭，说唐老鸭的就是快思考，因为你的思路已经在和米老鼠有关的迪斯尼的奇妙世界里，我问你的任何和鸭子有关的问题你都会说唐老鸭。 第二个问题，会难一些，请大家环顾一下周围的朋友，估计一下自己的驾驶水平在这个会场中是处于最好的 20%，较好的 20%，中等的 20%，较差的 20% 还是最差的 20%？ 我们没办法在现场做这个测试，我们在美国和中国很多场合进行了类似的测试，几乎是屡试不爽，几乎没有人会认为自己的水平达不到在座的所有人的平均水平。在我在加州大学任教五年期间只有一个同学坦诚自己的水平达不到班上的平均水平。 我问他为什么这么谦虚，他说这是因为在过去的半年里面出过两次事故开过三次罚单。这个现象在整个心理学、行为经济学就叫做过度自信。这种涉及到我们主观判断的领域里面，无论是涉及到我们的智商，我们的情商，我们的人缘，考试成绩，我们都会有一种过度自信，我们会认为我们的能力会比我们真实的能力要高一些。 在你在进行投资的时候，和其他千万个投资者在进行竞争进行博弈的时候，你在把握自己信息的同时，还必须想一想你的对手是谁。如果我们去澳门赌博的话，我们玩德州扑克，一桌是德州扑克的全球冠军，一桌是从来没有玩过德州扑克的菜鸟，你会选择哪一桌？ 我们的 A 股市场之所以特别，很大程度上，就是因为我们存在着大量的没有投资能力的投资者，而这些没有投资能力的投资者恰恰认为自己和巴菲特的投资水平不相上下。正是有这么多的散户投资者所以我们的机构投资者才会不得不屈从于，或者说不得不利用散户投资者的行为偏差，所以这就是行为金融学对资产定价来说很重要的而贡献。 我个人认为，金融市场特殊的地方在于所有的信息都是有用的信息，看多的信息我们可以买入，看空的信息我们可以卖出。 行为偏差 很多人认为为什么市场上形成泡沫，很多人认为这是因为投资者的行为偏差。那么有什么样子的行为偏差呢，我们给大家介绍了过度自信，但是还有很多其他的行为偏差，时间关系就给大家过一下。 一个非常重要的行为偏差叫做代表性偏差，什么意思呢，如果有一枚硬币让大家猜是正面还是反面，大家说一半一半。那么要是还是丢硬币丢了十次都是正面，然后让大家猜是正面还是反面，大家说很可能是正面，有人说一半一半。实际上还是一半一半，但是当出现反复的事实之后很多人都会受这个事实的影响，这就是为什么泡沫会变成泡沫。 咱们买股票就是赚钱效应，有人赚了钱了，越有人赚钱了越有人觉得这只股票应该赚钱，所以这就是为什么股票市场给投资者带来的收益其实远远低于股票指数上涨的幅度。比如市场上涨了 100%，而这个股票给社会带来的财富是远远低于 100%的。 在牛市最开始上涨的时候是没有多少投资者介入的，也没有多少资金介入，当牛市涨到顶端的时候，也就是我们看见的 A 股开户数最高涨的时候，往往是市场见顶的时候。很多投资者完全无视市场基本面，完全无视市场上累积的风险，认为凡是股票上涨了今后还会继续上涨。 邱国鹭先生最近写了一本书叫做《投资最简单的事》，很多人跟他说，投资者已经觉得投资够简单的了，你再写这本书会有更多不明就里的投资者涌入到市场中去。 第三点叫做情绪化交易，什么叫做情绪化交易给大家举一个很经典的例子全球顶级的金融学杂志 Journal of Finance 金融学期刊所揭露的。有研究表明，在欧洲市场和美国市场，当一个国家的足球队打入到世界杯决赛，但是没有能够获得世界杯冠军的话，那么第二天这个国家所在的市场会比历史平均水平下跌百分之零点五。我读完这个的第一反应就是为中国的股民长舒一口气，很显然想打入世界杯的决赛这个事和我们中国没啥关系。 那么为什么类似足球，篮球，英式橄榄球这些体育比赛的结果会和股票市场有关系？无独有偶，我曾经做过研究，在纽约上空云量比较少，太阳比较晴朗的时候，纽交所在那天的股票表现会好一些。 类似的，上交所的数据也有类似的结果。天气和股票市场有什么关系？通过这些例子我想让大家认识到投资在很多时候不是一个很客观的过程，在座的很多是量化投资者，我觉得量化投资很大的价值在于可以克服我们一些主观的甚至我们所不知道的一些行为偏差和行为趋势。 再有一点是什么呢，对损失、对模糊和对悔恨的规避。我先讲对损失的规避，这点非常非常普遍。我想在座的一定有人手里还持有中石油的股票对不对，非常多中石油的高管和大家处在类似的境地。那么为什么大家不愿意把手上的股票卖了呢？因为那种做法叫做割肉啊对不对。 炒股票本来是一个民事行为，到了割肉就成了刑事风险了对不对。“割肉”两个字非常形象地说明了心理学里面人类对于损失的强烈规避心理。股市波动对心理有影响，股市上涨 50% 再下跌 30%，虽然股市指数本身没有什么变化，但是投资者的心理，是更高兴了，没有变化还是更不高兴了？从心理学的角度来说是更不高兴了，因为人们对损失的痛恶远远大于对从收益里取得的快乐。 2015 年是大牛市很难预料，但在任何一个投资时点上，一个投资者的投资组合里，1/3 到 1&amp;frasl;4 的股票是浮盈的，2/3 到 3&amp;frasl;4 的股票是浮亏的，大家可以回家看看自己的投资账户，我相信和这个规律不会差的太多。 那么什么叫做对于模糊的规避呢？ 我曾经在台湾有一个研究，台湾投资者如果在上市公司工作的话，他的投资组合里面有超过 49% 的投资在同一个股票里面，这只股票就是自己本公司上市公司的股票。这是最不模糊的公司，最熟悉的公司。 但是这样的反例在于，2000 年的时候美国有一个公司叫做安然公司，这个公司因为大量的财务造假导致公司破产，在它破产的时候，这个公司 65% 的退休金投资在一个公司的股票上，就是这家公司自己的股票上。 所以这种投资者对于损失的规避和对于模糊的规避直接导致了投资者不能止损，直接导致了投资者的投资组合是一个非常非常不分散的投资组合。所以这种行为偏差导致了投资者做出了各种从事后看很难理解的投资决定。 大家就会问说如果散户投资者是这么不理性，做出如此多的错误的决定，那么为什么我们广大在座的机构投资者会犯类似的错误，为什么不做反向的操作，在这个过程中获利呢？这就涉及到行为金融里面一个非常重要的概念叫做有限套利。有限套利这个概念和卖空有非常重要的关系，我以前写过一篇文章叫做《谁是善意的做空者》。 那么究竟什么是善意的做空什么是恶意的做空，从金融学角度看，我个人觉得没有任何的区别，做空就是认为资产价格太高了，之后会下跌，所以我放空单，希望在下跌过程中获利，为什么做多就一定是善意，做空就一定是恶意的，为什么股市上涨就是好的，股市下跌就是坏的，从金融学上讲没有任何道理。 但是因为监管者、投资者，因为在座的各位觉得放空单的风险太大了，不愿意进行这种套利行为，所以才导致了泡沫的一次又一次的诞生和一次又一次的吹大，这就是为什么我要写这本书叫做《刚性泡沫》。你越是不希望泡沫诞生，越是给投资者提供担保，越是和投资者放心说 4500 点之上，越是产生意想不到的泡沫。 我给大家介绍了很多投资者的行为偏差，那么我们怎么利用投资者的错误来为自己所用呢。这是当年我在离开加州大学的时候进业界工作的，包括高盛，包括雷曼，包括 BGI，包括巴克莱，包括黑石都请我去，很大程度上是因为我当年发表过一篇论文，很惭愧这篇论文发表了整整快十年了。 我当时研究的思路是这样的，按照我们行为金融学理论，它的创建者之一萨默斯先生，曾经的美国财务部部长，哈佛大学的校长，他的一系列理论认为我们在市场上存在两类投资者：一种是散户，还有一种是在座的各位专业投资者。 散户投资者往往是受到自己的情绪和自己的心里偏差的影响，而且我们说的散户投资者他们的情绪他们的，行为偏差是高度相关的。 今年年初的时候我写过一篇文章叫做《牛市中的羊群》，也就是散户在牛市中绝对是抱团取暖，每个人都不知道股市是涨还是跌，相互之间打电话说你看怎么样，我看能涨，我看也是，买了。 投资者的行为很大程度上是高度相关的，而这种高度相关性会推动资产价格远离资产价格基本面应该有的水平，直到这个远离基本面的价格太多了，无以为继了，才会出现重大的调整。这就是以萨默斯为首的，叫做有限套利情况下市场上的资产价格可能出现的一个现象。 那么我们做实证分析，利用了全美的市场上每一个交易的基础，我们可以估算出小规模的交易和大规模的交易他们整个交易的资金流向，就是小投资者在买什么样的股票，大投资者在买什么样的股票。 一个有趣的现象，我们发现确实和萨默斯他们的研究所说，我们发现散户投资者的资金流向是高度接近的。我们看看这一部分的投资者在买什么样的股票，我可以非常非常放心地再说另一部分的投资者在买什么样的股票。 因为我们的相关系数在短期内可以达到 60%，这是一个非常高的相关关系。由于广大的散户投资者都在按一个类似的行为偏差或者类似的情绪在进行交易，那么所有的散户在一起就形成了市场上一个巨大的机构，虽然这个机构的行为和我们真正的机构的行为是非常不一样的。 那么散户的投资者的行为究竟会给你带来正向的投资推动还是负向的投资的推动，我们发现在一周到三周的区间里面，散户如果集中买什么公司的股票，什么公司的股票就会上涨，但是这些短期上涨的公司，在今后的一个月或者一年的时间里，股票会明显跑输大盘。原因就在于这些散户短期投资的股票已经被炒得过高了，所以在中长期必须出现整个真实价格的回调。 所以我给大家介绍一下，我们在做这个研究之后我们做了许多类似的拓展的行为金融的研究，我们在很多市场里面进行了实验。因为美国在 07 年做了买卖价差的改革，原来的策略不太适用，我们就开发了新的策略。在国内我们就觉得这个现象其实更明显，很多时候很多基金、投资者并不一定会采用量化策略，像我们这样作一个学术研究去发现一个趋势，但是很多时候投资者的交易心理和我们所说的做法差不多。 我举一个很极端的例子，就是涨停敢死队，涨停敢死队就是利用的就是之前讲到的代表性的偏差：认为昨天涨停的明天涨停的概率还会很大。所以有的时候并不需要好的公司，而只是涨停这个事件本身，就可以吸引散户足够多的关注，散户的关注就可以进一步把公司的股价推高，可能造成第二次涨停。 这种做法虽然是基于中国有涨跌停板的实际情况，非常有创意，但是这种策略背后的原因，其实是散户这种跟风和利用短期的趋势去预测股市今后的走势的这种行为偏差所造成的。 行为金融学在中国 所以我觉得中国的 A 股市场有趣的地方在于散户的比例很高，而且散户投资者的金融素养比较差，同时在一个大家非常浮躁，都希望在短期创造大量财富的过程中，大家都不愿意对很多具体的信息和具体的内容进行研究。 但是国内A股市场在量化研究领域的挑战在于宏观系统性的风险很大，很难预测央行哪一次又降息，哪一次又会禁止卖空交易，同时市场由于波动很大之后，又会很难说过去历史的经验会不会在今后会重复出现。 我觉得行为金融理论在整个中国的资本市场有很多的不同的运用的方式，最重要的，中国仍然是一个宏观面的市场，再怎么讲什么择时和选股，很大程度上你还要看大盘什么时候趋于稳定。 所以怎么利用行为金融来预测泡沫的顶部，我们获得了一些成果，我们很多模型确实在六月底的时候显示出过市场确实出现过很多不稳定的状态，但是从学术角度我不能说我会百分之百的相信这些模型。 同时我们国内有很多投资者的情绪指标，散户的开户数，基金的申购和赎回的比率，IPO 是一个在国外被广泛用来衡量市场情绪的指标，这在国内不是很适用，但你可以采用很多策略，如果是券商的话，你可以通过客户的交易行为来判断市场的情绪达到一个前所未有的或者说极端危险的高度。 这对散户来讲很可怕的一点在于，我们知道，巴菲特说过一句话叫做在被人贪婪的时候我恐惧，在别人恐惧的时候我贪婪，这叫做价值投资。但是我们散户的投资理念是什么？ 别人贪婪的时候我更贪婪，别人恐惧的时候我更恐惧。所以这种逆向思维的做法，我觉得很大程度上，是行为金融学传递的一个很重要的信息。在中国的A股市场上，有很多完全不知道自己在往哪个地方走的羊群，如果你是一头大灰狼的话，你只要知道羊群在往哪走，就在那里等着，张开嘴，等着吃“羊肉火锅”就行了。 那么行为金融学在中国的实践，我个人回国快五年里面，我觉得我做了两件事： 第一我希望从社会层面，帮助我们广大的散户投资者，不要成为整天被大灰狼吃的小肥羊，希望能提升投资者的素养，加强他们的风险意思，让他们明确自己的投资目标，这点我觉得大家都是在金融行业的从业人员，所以通过今天自己对行为金融的分享，我也希望大家能用这些知识帮助自己的客户，提升他们的投资水平。 第二点，我们曾经在美国资本市场说，如果投资者不愿意改变他们原来错误的做法，我们就要通过自己的交易，让他们不得不承受损失，从而意识到自己并不是一个很成功的投资者。所以基于行为金融的投资哲学，基于行为金融的风险管控模型，和基于行为金融的策略和因子分析，我觉得这里面有很多可以进一步发掘和推动这个领域。 如果你认真读完了全文，我相信你应该对行为金融学有了一个初步的了解。其实，任何泡沫都是人造成了，在金融市场当所有人的思维达成共振后，其行为就会同步！而你想要摆脱思维的定式，这是很困难的一件事情。今天观察君给你们推荐一本，美国作家丹尼尔·卡尼曼 《思维，快与慢》，在书中，卡尼曼会带领我们体验一次思维的终极之旅。同时观察君，也欢迎各位给我们留言，交流，对于泡沫你社怎么理解的？ 朱宁，师从诺贝尔经济学奖得主罗伯特·席勒，上海交通大学上海高级金融学院副院长，耶鲁大学国际金融中心研究员。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[热门数据挖掘模型应用入门（一）：LASSO 回归 z]]></title>
    	<url>/data/2016/10/10/data-mining-1-lasso/</url>
		<content type="text"><![CDATA[[原文地址：https://cosx.org/2016/10/data-mining-1-lasso/ 作者简介：侯澄钧，俄亥俄州立大学运筹学博士，目前在美国从事财产事故险(Property &amp;amp; Casualty)领域的保险产品开发，涉及数据分析，统计建模，产品算法优化等方面的工作。 模型简介 Kaggle网站（https://www.kaggle.com/）成立于2010年，是当下最流行的进行数据发掘和预测模型竞赛的在线平台。与Kaggle合作的公司可以在网站上提出一个问题或者目标，同时提供相关数据，来自世界各地的计算机科学家、统计学家和建模爱好者，将受领任务，通过比较模型的某些性能参数，角逐出优胜者。通过大量的比赛，一系列优秀的数据挖掘模型脱颖而出，受到广大建模者的认同，被普遍应用在各个领域。在保险行业中用于拟合广义线性模型的LASSO回归就是其中之一。 LASSO回归的特点是在拟合广义线性模型的同时进行变量筛选（variable selection）和复杂度调整（regularization）。因此，不论目标因变量（dependent/response varaible）是连续的（continuous），还是二元或者多元离散的（discrete），都可以用LASSO回归建模然后预测。这里的变量筛选是指不把所有的变量都放入模型中进行拟合，而是有选择的把变量放入模型从而得到更好的性能参数。复杂度调整是指通过一系列参数控制模型的复杂度，从而避免过度拟合（overfitting）。对于线性模型来说，复杂度与模型的变量数有直接关系，变量数越多，模型复杂度就越高。更多的变量在拟合时往往可以给出一个看似更好的模型，但是同时也面临过度拟合的危险。此时如果用全新的数据去验证模型（validation），通常效果很差。一般来说，变量数大于数据点数量很多，或者某一个离散变量有太多独特值时，都有可能过度拟合。 LASSO回归复杂度调整的程度由参数 λ 来控制，λ 越大对变量较多的线性模型的惩罚力度就越大，从而最终获得一个变量较少的模型。LASSO回归与Ridge回归同属于一个被称为Elastic Net的广义线性模型家族。这一家族的模型除了相同作用的参数 λ 之外，还有另一个参数 α 来控制应对高相关性（highly correlated）数据时模型的性状。LASSO回归 α=1，Ridge回归 α=0，一般Elastic Net模型 0&amp;lt;α&amp;lt;1。这篇文章主要介绍LASSO回归，所以我们集中关注 α=1 的情况，对于另外两种模型的特点和如何选取最优 α 值，我会在章节“Elstic Net模型家族简介”做一些简单阐述。 目前最好用的拟合广义线性模型的R package是 glmnet，由LASSO回归的发明人，斯坦福统计学家Trevor Hastie领衔开发。它的特点是对一系列不同 λ 值进行拟合，每次拟合都用到上一个 λ 值拟合的结果，从而大大提高了运算效率。此外它还包括了并行计算的功能，这样就能调动一台计算机的多个核或者多个计算机的运算网络，进一步缩短运算时间。 下面我们就通过一个线性回归和一个Logistic回归的例子，了解如何使用 glmnet 拟合LASSO回归。另外，之后的系列文章我打算重点介绍非参数模型（nonparametric model）中的一种，Gradient Boosting Machine。然后通过一个保险行业的实例，分享一些实际建模过程中的经验，包括如何选取和预处理数据，如何直观得分析自变量与因变量之间的关系，如何避免过度拟合，如何衡量和选取最终模型。 线性回归 我们从最简单的线性回归（Linear Regression）开始了解如何使用 glmnet 拟合LASSO回归模型，所以此时的连接函数（link function）就是恒等，或者说没有连接函数，而误差的函数分布是正态分布。 首先我们装载 glmnet package，然后读入试验用数据“LinearExample.RData”，下载链接： library(glmnet) load(&amp;quot;LinearExample.RData&amp;quot;) 之后在workspace里我们会得到一个100×20的矩阵 x 作为输入自变量，100×1的矩阵 y 作为目标因变量。矩阵 x 代表了我们有100个数据点，每个数据点有20个统计量（feature）。现在我们就可以用函数 glmnet() 建模了： fit = glmnet(x, y, family=&amp;quot;gaussian&amp;quot;, nlambda=50, alpha=1) 好，建模完毕，至此结束本教程 🙂 觉得意犹未尽的朋友可以接着看下面的内容。 参数 family 规定了回归模型的类型： family=&amp;quot;gaussian&amp;quot; 适用于一维连续因变量（univariate） family=&amp;quot;mgaussian&amp;quot; 适用于多维连续因变量（multivariate） family=&amp;quot;poisson&amp;quot; 适用于非负次数因变量（count） family=&amp;quot;binomial&amp;quot; 适用于二元离散因变量（binary） family=&amp;quot;multinomial&amp;quot; 适用于多元离散因变量（category） 参数 nlambda=50 让算法自动挑选50个不同的 λ 值，拟合出50个系数不同的模型。alpha=1输入 α 值，1是它的默认值。值得注意的是，glmnet 只能接受数值矩阵作为模型输入，如果自变量中有离散变量的话，需要把这一列离散变量转化为几列只含有0和1的向量，这个过程叫做One Hot Encoding。通过下面这个小例子，我们可以了解One Hot Encoding的原理以及方法： df=data.frame(Factor=factor(1:5), Character=c(&amp;quot;a&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;c&amp;quot;), Logical=c(T,F,T,T,T), Numeric=c(2.1,2.3,2.5,4.1,1.1)) model.matrix(~., df) ## (Intercept) Factor2 Factor3 Factor4 Factor5 Characterb Characterc ## 1 1 0 0 0 0 0 0 ## 2 1 1 0 0 0 0 0 ## 3 1 0 1 0 0 1 0 ## 4 1 0 0 1 0 1 0 ## 5 1 0 0 0 1 0 1 ## LogicalTRUE Numeric ## 1 1 2.1 ## 2 0 2.3 ## 3 1 2.5 ## 4 1 4.1 ## 5 1 1.1 ## attr(,&amp;quot;assign&amp;quot;) ## [1] 0 1 1 1 1 2 2 3 4 ## attr(,&amp;quot;contrasts&amp;quot;) ## attr(,&amp;quot;contrasts&amp;quot;)$Factor ## [1] &amp;quot;contr.treatment&amp;quot; ## ## attr(,&amp;quot;contrasts&amp;quot;)$Character ## [1] &amp;quot;contr.treatment&amp;quot; ## ## attr(,&amp;quot;contrasts&amp;quot;)$Logical ## [1] &amp;quot;contr.treatment&amp;quot; 除此之外，如果我们想让模型的变量系数都在同一个数量级上，就需要在拟合前对数据的每一列进行标准化（standardize），即对每个列元素减去这一列的均值然后除以这一列的标准差。这一过程可以通过在 glmnet() 函数中添加参数 standardize=TRUE 来实现。 回到我们的拟合结果 fit。作为一个R对象，我们可以把它当作很多函数的输入。比如说，我们可以查看详细的拟合结果： print(fit) ## Call: glmnet(x = x, y = y, family = &amp;quot;gaussian&amp;quot;, alpha = 1, nlambda = 50) ## ## Df %Dev Lambda ## [1,] 0 0.0000 1.631000 ## [2,] 2 0.1476 1.351000 ## [3,] 2 0.2859 1.120000 ## [4,] 4 0.3946 0.927900 ## [5,] 5 0.5198 0.768900 ## [6,] 6 0.6303 0.637100 ## [7,] 6 0.7085 0.528000 ## [8,] 7 0.7657 0.437500 ## [9,] 7 0.8081 0.362500 ## [10,] 7 0.8373 0.300400 ## [11,] 7 0.8572 0.248900 ## [12,] 8 0.8721 0.206300 ## [13,] 8 0.8833 0.170900 ## [14,] 8 0.8909 0.141600 ## [15,] 8 0.8962 0.117400 ## [16,] 9 0.8999 0.097250 ## [17,] 9 0.9027 0.080590 ## [18,] 10 0.9046 0.066780 ## [19,] 11 0.9065 0.055340 ## [20,] 15 0.9081 0.045850 ## [21,] 16 0.9095 0.038000 ## [22,] 17 0.9105 0.031490 ## [23,] 18 0.9113 0.026090 ## [24,] 19 0.9119 0.021620 ## [25,] 19 0.9123 0.017910 ## [26,] 19 0.9126 0.014840 ## [27,] 19 0.9128 0.012300 ## [28,] 19 0.9129 0.010190 ## [29,] 19 0.9130 0.008446 ## [30,] 19 0.9131 0.006999 ## [31,] 20 0.9131 0.005800 ## [32,] 20 0.9131 0.004806 ## [33,] 20 0.9132 0.003982 ## [34,] 20 0.9132 0.003300 ## [35,] 20 0.9132 0.002735 ## [36,] 20 0.9132 0.002266 每一行代表了一个模型。列 Df 是自由度，代表了非零的线性模型拟合系数的个数。列 %Dev 代表了由模型解释的残差的比例，对于线性模型来说就是模型拟合的R^2（R-squred）。它在0和1之间，越接近1说明模型的表现越好，如果是0，说明模型的预测结果还不如直接把因变量的均值作为预测值来的有效。列 Lambda 当然就是每个模型对应的 λ 值。我们可以看到，随着 λ 的变小，越来越多的自变量被模型接纳进来，%Dev 也越来越大。第31行时，模型包含了所有20个自变量，%Dev 也在0.91以上。其实我们本应该得到50个不同的模型，但是连续几个 %Dev 变化很小时 glmnet() 会自动停止。分析模型输出我们可以看到当 Df 大于9的时候，%Dev 就达到了0.9，而且继续缩小 λ，即增加更多的自变量到模型中，也不能显著提高 %Dev。所以我们可以认为当 λ 接近0.1时，得到的包含9个自变量的模型，可以相当不错的描述这组数据。 我们也可以通过指定 λ 值，抓取出某一个模型的系数： coef(fit, s=c(fit$lambda[16],0.1)) ## 21 x 2 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot; ## 1 2 ## (Intercept) 0.150672014 0.150910983 ## V1 1.322088892 1.320532088 ## V2 . . ## V3 0.677692624 0.674955779 ## V4 . . ## V5 -0.819674385 -0.817314761 ## V6 0.523912698 0.521565712 ## V7 0.007293509 0.006297101 ## V8 0.321450451 0.319344250 ## V9 . . ## V10 . . ## V11 0.145727982 0.142574921 ## V12 . . ## V13 . . ## V14 -1.061733786 -1.060031309 ## V15 . . ## V16 . . ## V17 . . ## V18 . . ## V19 . . ## V20 -1.025371209 -1.021771038 需要注意的是，我们把指定的 λ 值放在 s= 里，因为在后面Logistic回归的部分我们还用到了 s=&amp;quot;lambda.min&amp;quot; 的方法指定 λ 的值。当指定的 λ 值不在 fit$lambda 中时，对应的模型系数由linear interpolation近似得到。我们还可以做图观察这50个模型的系数是如何变化的： plot(fit, xvar=&amp;quot;lambda&amp;quot;, label=TRUE) 图中的每一条曲线代表了每一个自变量系数的变化轨迹，纵坐标是系数的值，下横坐标是 log⁡(λ)，上横坐标是此时模型中非零系数的个数。我们可以看到，黑线代表的自变量1在 λ 值很大时就有非零的系数，然后随着 λ 值变小不断变大。我们还可以尝试用 xvar=&amp;quot;norm&amp;quot; 和 xvar=&amp;quot;dev&amp;quot; 切换下横坐标。 接下来当然就是指定 λ 值，然后对新数据进行预测了： set.seed(91) nx = matrix(rnorm(5*20),5,20) predict(fit, newx=nx, s=c(fit$lambda[16],0.1)) ## 1 2 ## [1,] 2.0309573 2.0273151 ## [2,] -1.9362780 -1.9328610 ## [3,] 1.1048789 1.1047725 ## [4,] 0.5156294 0.5154747 ## [5,] 1.4621024 1.4618535 下面我们再来看几个 glmnet() 函数的其他功能。使用 upper.limits 和 lower.limits，我们可以指定模型系数的上限与下限： lfit=glmnet(x, y, lower=-.7, upper=.5) plot(lfit, xvar=&amp;quot;lambda&amp;quot;, label=TRUE) 上限与下限可以是一个值，也可以是一个向量，向量的每一个值作为对应自变量的参数上下限。有时，在建模之前我们就想凸显某几个自变量的作用，此时我们可以调整惩罚参数。每个自变量的默认惩罚参数是1，把其中的某几个量设为0将使得相应的自变量不遭受任何惩罚： p.fac = rep(1, 20) p.fac[c(5, 10, 15)] = 0 pfit = glmnet(x, y, penalty.factor=p.fac) plot(pfit, xvar=&amp;quot;lambda&amp;quot;, label = TRUE) 我们可以看到，自变量5/10/15的系数一直不为0，而其他的参数系数绝对值随着 λ 值变小而变大。 Logistic回归 当面对离散因变量时，特别是面对二元因变量（Yes/No）这样的问题时，Logistic回归被广泛使用。此时我们用 family=&amp;quot;binomial&amp;quot; 来应对这种目标因变量是二项分布（binomial）的情况。 试验用数据“LogisticExample.RData”里储存了100×30的矩阵 x，和元素是0/1长度是100的向量 y，下载链接： load(&amp;quot;LogisticExample.RData&amp;quot;) 我们可以用上一节介绍的 glmnet() 函数来拟合模型，然后选取最优的 λ 值。但是在这种方法下，所有数据都被用来做了一次拟合，这很有可能会造成过拟合的。此时，当我们把得到的模型用来预测全新收集到的数据时，结果很可能会不尽如人意。所以只要条件允许，我们都会用交叉验证（cross validation）拟合进而选取模型，同时对模型的性能有一个更准确的估计。 set.seed(91) cvfit = cv.glmnet(x, y, family = &amp;quot;binomial&amp;quot;, type.measure = &amp;quot;class&amp;quot;) 这里的 type.measure 是用来指定交叉验证选取模型时希望最小化的目标参量，对于Logistic回归有以下几种选择： type.measure=&amp;quot;deviance&amp;quot; 使用deviance，即-2倍的log-likelihood type.measure=&amp;quot;mse&amp;quot; 使用拟合因变量与实际应变量的mean squred error type.measure=&amp;quot;mae&amp;quot; 使用mean absolute error type.measure=&amp;quot;class&amp;quot; 使用模型分类的错误率（missclassification error） type.measure=&amp;quot;auc&amp;quot; 使用area under ROC curve，是现在最流行的综合考量模型性能的一种参数 除此之外，在 cv.glmnet() 里我们还可以用 nfolds 指定fold数，或者用 foldid 指定每个fold的内容。因为每个fold间的计算是独立的，我们还可以考虑运用并行计算来提高运算效率，使用 parallel=TRUE 可以开启这个功能。但是我们需要先装载package doParallel。下面我们给出在Windows操作系统和Linux操作系统下开启并行 library(doParallel) # Windows System cl &amp;amp;lt;- makeCluster(6) registerDoParallel(cl) cvfit = cv.glmnet(x, y, family = &amp;quot;binomial&amp;quot;, type.measure = &amp;quot;class&amp;quot;, parallel=TRUE) stopCluster(cl) # Linux System registerDoParallel(cores=8) cvfit = cv.glmnet(x, y, family = &amp;quot;binomial&amp;quot;, type.measure = &amp;quot;class&amp;quot;, parallel=TRUE) stopImplicitCluster() 同样的，我们可以绘制 cvfit 对象： plot(cvfit) 因为交叉验证，对于每一个 λ 值，在红点所示目标参量的均值左右，我们可以得到一个目标参量的置信区间。两条虚线分别指示了两个特殊的 λ 值： c(cvfit$lambda.min, cvfit$lambda.1se) ## [1] 0.03741031 0.05956780 lambda.min 是指在所有的 λ 值中，得到最小目标参量均值的那一个。而 lambda.1se 是指在 lambda.min 一个方差范围内得到最简单模型的那一个 λ 值。因为 λ 值到达一定大小之后，继续增加模型自变量个数即缩小 λ 值，并不能很显著的提高模型性能，lambda.1se 给出的就是一个具备优良性能但是自变量个数最少的模型。同样的，我们可以指定 λ 值然后进行预测： predict(cvfit, newx=x[1:5,], type=&amp;quot;response&amp;quot;, s=&amp;quot;lambda.1se&amp;quot;) ## 1 ## [1,] 0.2992175 ## [2,] 0.8319748 ## [3,] 0.6160852 ## [4,] 0.2180918 ## [5,] 0.6416046 这里的 type 有以下几种选择： type=&amp;quot;link&amp;quot; 给出线性预测值，即进行Logit变换之前的值 type=&amp;quot;response&amp;quot; 给出概率预测值，即进行Logit变换之后的值 type=&amp;quot;class&amp;quot; 给出0/1预测值 type=&amp;quot;coefficients&amp;quot; 罗列出给定 λ 值时的模型系数 type=&amp;quot;nonzero&amp;quot; 罗列出给定 λ 值时，不为零模型系数的下标 另外，当已有了一个模型之后，我们又得到了几个新的自变量，如果想知道这些新变量能否在第一个模型的基础上提高模型性能，可以把第一个模型的预测因变量作为一个向量放到函数选项 offset 中，再用 glmnet 或者 cv.glmnet 进行拟合。 Elstic Net模型家族理论简介 在这一节我们会了解一些关于Elastic Net模型家族的理论。首先我们先来看看一般线性Elastic Net模型的目标函数： 目标函数的第一行与传统线性回归模型完全相同，即我们希望得到相应的自变量系数 \(\beta\)，以此最小化实际因变量y与预测应变量 \(\beta x\) 之间的误差平方和。而线性Elastic Net与线性回归的不同之处就在于有无第二行的这个约束，线性Elastic Net希望得到的自变量系数是在由 \(t\) 控制的一个范围内。这一约束也是Elastic Net模型能进行复杂度调整，LASSO回归能进行变量筛选和复杂度调整的原因。我们可以通过下面的这张图来解释这个道理： 先看左图，假设一个二维模型对应的系数是 \(\beta_1\) 和 \(\beta_2\)，然后 \(\hat{\beta}\) 是最小化误差平方和的点，即用传统线性回归得到的自变量系数。但我们想让这个系数点必须落在蓝色的正方形内，所以就有了一系列围绕 \(\hat{\beta}\) 的同心椭圆，其中最先与蓝色正方形接触的点，就是符合约束同时最小化误差平方和的点。这个点就是同一个问题LASSO回归得到的自变量系数。因为约束是一个正方形，所以除非相切，正方形与同心椭圆的接触点往往在正方形顶点上。而顶点又落在坐标轴上，这就意味着符合约束的自变量系数有一个值是0。所以这里传统线性回归得到的是 \(\beta_1\) 和 \(\beta_2\) 都起作用的模型，而LASSO回归得到的是只有 \(\beta_2\) 有作用的模型，这就是LASSO回归能筛选变量的原因。 而正方形的大小就决定了复杂度调整的程度。假设这个正方形极小，近似于一个点，那么LASSO回归得到的就是一个只有常量（intercept）而其他自变量系数都为0的模型，这是模型简化的极端情况。由此我们可以明白，控制复杂度调整程度的 λ 值与约束大小 \(t\) 是呈反比的，即 λ 值越大对参数较多的线性模型的惩罚力度就越大，越容易得到一个简单的模型。 另外，我们之前提到的参数 α 就决定了这个约束的形状。刚才提到LASSO回归（α=1）的约束是一个正方形，所以更容易让约束后的系数点落在顶点上，从而起到变量筛选或者说降维的目的。而Ridge回归（α=0）的约束是一个圆，与同心椭圆的相切点会在圆上的任何位置，所以Ridge回归并没有变量筛选的功能。相应的，当几个自变量高度相关时，LASSO回归会倾向于选出其中的任意一个加入到筛选后的模型中，而Ridge回归则会把这一组自变量都挑选出来。至于一般的Elastic Net模型（0&amp;lt;α&amp;lt;1），其约束的形状介于正方形与圆形之间，所以其特点就是在任意选出一个自变量或者一组自变量之间权衡。 下面我们就通过Logistic回归一节的例子，来看看这几种模型会得到怎样不同的结果： # CV for 11 alpha value for (i in 0:10) { assign(paste(&amp;quot;cvfit&amp;quot;, i, sep=&amp;quot;&amp;quot;), cv.glmnet(x, y, family=&amp;quot;binomial&amp;quot;, type.measure=&amp;quot;class&amp;quot;, alpha=i/10)) } # Plot Solution Paths par(mfrow=c(3,1)) plot(cvfit10, main=&amp;quot;LASSO&amp;quot;) plot(cvfit0, main=&amp;quot;Ridge&amp;quot;) plot(cvfit5, main=&amp;quot;Elastic Net&amp;quot;) 通过比较可以看出，Ridge回归得到的模型一直都有30个自变量，而 α=0.5 时的Elastic Net与LASSO回归有相似的性能。 学习资料 本文的图片来自Trevor Hastie教授的著作“The Elements of Statistical Learning”，我觉得这本书在parametric model这一方向的阐述尤其精彩，对于其他数据挖掘方向也有十分全面的介绍。 更全面关于glmnet的应用，可以参考 https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html，本文的两个例子也出自这篇vignette。 关于Elastic Net模型家族的特点和优劣，可以参考这里。 最后，感谢COS编辑部的指正，也感谢大家的阅读。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[根据浏览器类型、屏幕分辨率调用不同的样式文件 z]]></title>
    	<url>/tech/2016/09/23/js-css-webagent/</url>
		<content type="text"><![CDATA[[原文地址：http://blog.csdn.net/duchao123duchao/article/details/52638506 根据分辨 CSS 样式 @media screen and (min-width: 320px) and (max-width: 480px){ 在这里写小屏幕设备的样式 } @media only screen and (min-width: 321px) and (max-width: 1024px){ 这里写宽度大于321px小于1024px的样式(一般是平板电脑) } @media only screen and (min-width: 1029px){ 这里写pc客户端的样式 } 用 js 根据客户端输出对应样式 /* 事实上用 asp、php 后台判断更保险，js 在前端，有可能被用户禁止 */ function loadCSS() { if((navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|iOS|iPad|Android|wOSBrowser|BrowserNG|WebOS)/i))) { document.write(&#39;&amp;lt;link href=&amp;quot;css/pad-phone.css&amp;quot; rel=&amp;quot;stylesheet&amp;quot; type=&amp;quot;text/css&amp;quot; media=&amp;quot;screen&amp;quot; /&amp;gt;&#39;); } else { document.write(&#39;&amp;lt;link href=&amp;quot;css/pc.css&amp;quot; rel=&amp;quot;stylesheet&amp;quot; type=&amp;quot;text/css&amp;quot; media=&amp;quot;screen&amp;quot; /&amp;gt;&#39;); } } loadCSS(); 既判断分辨率，也判断浏览器 应 E.Qiang 提议，重新完善代码，使之成为判断浏览器类型屏幕分辨率自动调用不同 CSS 的代码，代码如下： &amp;lt;SCRIPT LANGUAGE=&amp;quot;JavaScript&amp;quot;&amp;gt; &amp;lt;!-- if (window.navigator.userAgent.indexOf(&amp;quot;MSIE&amp;quot;)&amp;gt;=1) { var IE1024=&amp;quot;&amp;quot;; var IE800=&amp;quot;&amp;quot;; var IE1152=&amp;quot;&amp;quot;; var IEother=&amp;quot;&amp;quot;; ScreenWidth(IE1024,IE800,IE1152,IEother) } else { if (window.navigator.userAgent.indexOf(&amp;quot;Firefox&amp;quot;)&amp;gt;=1) { // 如果浏览器为Firefox var Firefox1024=&amp;quot;&amp;quot;; var Firefox800=&amp;quot;&amp;quot;; var Firefox1152=&amp;quot;&amp;quot;; var Firefoxother=&amp;quot;&amp;quot;; ScreenWidth(Firefox1024,Firefox800,Firefox1152,Firefoxother) } else { // 如果浏览器为其他 var Other1024=&amp;quot;&amp;quot;; var Other800=&amp;quot;&amp;quot;; var Other1152=&amp;quot;&amp;quot;; var Otherother=&amp;quot;&amp;quot;; ScreenWidth(Other1024,Other800,Other1152,Otherother) } } function ScreenWidth(CSS1,CSS2,CSS3,CSS4) { if ((screen.width == 1024) &amp;amp;&amp;amp; (screen.height == 768)) { setActiveStyleSheet(CSS1); } else { if ((screen.width == 800) &amp;amp;&amp;amp; (screen.height == 600)) { setActiveStyleSheet(CSS2); } else { if ((screen.width == 1152) &amp;amp;&amp;amp; (screen.height == 864)) { setActiveStyleSheet(CSS3); } else { setActiveStyleSheet(CSS4); } } } } function setActiveStyleSheet(title){ document.getElementsByTagName(&amp;quot;link&amp;quot;)[0].href=&amp;quot;style/&amp;quot;&#43;title; } //--&amp;gt; &amp;lt;/SCRIPT&amp;gt; 解释 var IE1024=&amp;quot;&amp;quot;; var IE800=&amp;quot;&amp;quot;; var IE1152=&amp;quot;&amp;quot;; var IEother=&amp;quot;&amp;quot;; 引号里面分别填写用户使用 IE 并且分辨率为1024*768, 800*600, 1152*864的时候要使用的 css 文件名。 var Firefox1024=&amp;quot;&amp;quot;; var Firefox800=&amp;quot;&amp;quot;; var Firefox1152=&amp;quot;&amp;quot;; var Firefoxother=&amp;quot;&amp;quot;; 引号里面分别填写用户使用 FF 并且分辨率为1024*768, 800*600, 1152*864的时候要使用的 css 文件名。 var Other1024=&amp;quot;&amp;quot;; var Other800=&amp;quot;&amp;quot;; var Other1152=&amp;quot;&amp;quot;; var Otherother=&amp;quot;&amp;quot;; 引号里面分别填写用户使用其他浏览器并且分辨率为1024*768, 800*600, 1152*864的时候要使用的 css 文件名。 例子 不判断分辨率，只判断浏览器，实现根据浏览器类型自动调用不同CSS。 &amp;lt;SCRIPT LANGUAGE=&amp;quot;JavaScript&amp;quot;&amp;gt; &amp;lt;!-- if (window.navigator.userAgent.indexOf(&amp;quot;MSIE&amp;quot;)&amp;gt;=1) { // 如果浏览器为IE setActiveStyleSheet(&amp;quot;default.css&amp;quot;); } else { if (window.navigator.userAgent.indexOf(&amp;quot;Firefox&amp;quot;)&amp;gt;=1) { // 如果浏览器为Firefox setActiveStyleSheet(&amp;quot;default2.css&amp;quot;); } else { // 如果浏览器为其他 setActiveStyleSheet(&amp;quot;newsky.css&amp;quot;); } } function setActiveStyleSheet(title){ document.getElementsByTagName(&amp;quot;link&amp;quot;)[0].href=&amp;quot;style/&amp;quot;&#43;title; } //--&amp;gt; &amp;lt;/SCRIPT&amp;gt; 解释： 如果浏览器为 IE，则调用 default.css 如果浏览器为 Firefox，则调用 default2.css 如果浏览器为其他，则调用 newsky.css 用法：放在&amp;lt;/head&amp;gt;前面即可。 只要求判断根据屏幕宽度选择不同的 CSS 样式表。 &amp;lt;script language=javascript&amp;gt; &amp;lt;!-- if (screen.width == 800) { document.write(&#39;&amp;lt;link rel=stylesheet type=&amp;quot;text/css&amp;quot; href=&amp;quot;css800.css&amp;quot;&amp;gt;&#39;) } else { document.write(&#39;&amp;lt;link rel=stylesheet type=&amp;quot;text/css&amp;quot; href=&amp;quot;css1024.css&amp;quot;&amp;gt;&#39;) } //--&amp;gt; &amp;lt;/script&amp;gt;]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[可能是目前最好的词云解决方案wordcloud2 z]]></title>
    	<url>/tech/2016/08/10/wordcloud2/</url>
		<content type="text"><![CDATA[[原文地址：https://cosx.org/2016/08/wordcloud2/ 注：广大的段子手朋友们，下次再用R做词云的时候，记得在wo 最近写了一个wordcloud2的R包。调用一个JS的库(wordcloud2.js)实现wordcloud。 与旧的wordcloud相比，新的wordcloud2 能更有效的利用词与词的间隔来插入数据，更可以根据图片或者文字来绘制定制化的词云。 install.packages(&amp;quot;wordcloud2&amp;quot;) library(wordcloud) library(wordcloud2) wordcloud(demoFreq$word, demoFreq$freq) wordcloud2(demoFreq) 由于使用了Rstudio出品的htmlwidgets框架，knitr和shiny也都支持。以下是wordcloud2包的一个基本介绍。 基本的函数有两个: wordcloud2: 提供基本的词云功能 letterCloud: 使用选定的词绘制词云(就像刚才那个wordcloud2的图片) 为了文章的流畅性，详细的使用参数我就不细说了，可以去看下我留在CRAN的文档。下面会给大家介绍三种模式，按需选择。 懒人模式：默认参数，顺便自己可以调一调颜色角度什么的； 自定义模式：根据字符来生成词云； 极客模式：根据输入的图片来生成同样形状的词云。 1. 懒人模式 开启懒人模式之后，玩家可以把准备好的数据框(第一列词名，第二列词频)扔给wordcloud2函数，wordcloud2会自动处理，生成一个动态的页面。就像上面那幅和wordcloud对比的图片一样。 统计之都(COS)的词云 wordcloud2的包里面存了一个词与词频的数据，是从统计之都的订阅抓出来的信息，并做了分词和词频表。数据的名字是demoFreqC。 data(&amp;quot;demoFreqC&amp;quot;) head(demoFreqC) V2 V1 1 数据 2304 3 统计 1413 4 用户 855 5 模型 846 7 分析 773 8 数据分析 750 使用wordcloud对这个数据进行可视化，颜色使用的是亮色随机，背景是灰色。 # Windows # Sys.setlocale(&amp;quot;LC_CTYPE&amp;quot;,&amp;quot;eng&amp;quot;) wordcloud2(demoFreqC, color = &amp;quot;random-light&amp;quot;, backgroundColor = &amp;quot;grey&amp;quot;) 2.自定义模式 自定义模式允许用户去设定一个字符，然后根据这个字符，生成一个形状相同的词云，比如下面的两个例子。 wordcloud2做的头像 htmlwidgets gallery是一个展示各种引用js做可视化R包的平台。几周前我把wordcloud2传了上去，为了让大家更好的认识我,我把我微博的头像作为展示图传了上去，就像这样： 这个头像也是用本包实现的，使用lettercloud完成，letter=“R”选择了字符R的形状，颜色随机亮色，背景黑色，大小选的是0.3。数据是自己编的一个文件，可以在这里下载。 dat = read.csv(&amp;quot;chiffon.csv&amp;quot;,header = T) dat = dat[,2:3] letterCloud(dat,letter = &amp;quot;R&amp;quot;, color = &amp;quot;random-light&amp;quot;, backgroundColor = &amp;quot;black&amp;quot;,size = 0.3) 淘宝卖什么 亚磊 顺手爬了淘宝搜索“男”或者“女”的结果， 用letterCloud生成的词云，你以为这是用图片做的形状么？实际上是用了一个showtext包来实现的特殊字符。有代码有真相，大家慢慢读。 3. 极客模式 说实话，很多时候字母什么的并不能满足段子手的需求，有时候需要根据自己的图片来生成一个定义好形状的词云，wordcloud2里面也是支持的，只需要给wordcloud2传一个图片路径就好了，是不是很容易？ 一只小蓝鸟 wordcloud2函数支持输入自定义图片来作为词云的背景形状，需要注意的是自己定义图片的时候需要 “黑白” 的来作为词云的形状输入。本文使用了该包自带的一个 小鸟 图片。 figPath = system.file(&amp;quot;examples/t.png&amp;quot;,package = &amp;quot;wordcloud2&amp;quot;) wordcloud2(demoFreq, figPath = figPath, size = 1.5,color = &amp;quot;skyblue&amp;quot;) 蜘蛛侠 浩彬老撕绘制的蜘蛛侠，是不是很帅气？ A&amp;amp;Q 没错，可以用CRAN进行安装了，猛击此处。 绘制出来的结果有交互效果，鼠标移到一个词上可以看到词频。 刚开始的时候有一个shape变量，可以画五角星，心形等形状的词云，后来开发出了基于图片的词云，已经开始嫌弃这个功能了：） 中文乱码的问题 如果是windows需要Sys.setlocale(&amp;quot;LCCTYPE&amp;quot;,&amp;quot;eng&amp;quot;) 输入的文字需要是中文UTF-8 Studio抽风的问题 有时候第一次使用带图片或者文字的词云,Rstudio可能不显示 需要手动刷新或者在浏览器中打开 大小不满意 调Size 浏览器打开,调成你喜欢的大小,刷新! 对了,点赞是在这里:https://github.com/Lchiffon/wordcloud2]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[R 语言做符号计算 z]]></title>
    	<url>/tech/2016/07/08/r-symbol-calculate/</url>
		<content type="text"><![CDATA[[原文地址：https://cosx.org/2016/07/r-symbol-calculate 本文作者：黄湘云，2011-2015年在中国矿业大学（北京）的数学与应用数学专业获得学士学位，并从2015年至今在中国矿业大学（北京）统计学专业硕士在读，主要研究方向为复杂数据分析。 引言 谈起符号计算，大家首先想到的可能就是大名鼎鼎的Maple，其次是Mathematica，但是他们都是商业软件，除了其自身昂贵的价格外，对于想知道底层，并做一些修改的极客而言，这些操作也很不可能实现。自从遇到R以后，还是果断脱离商业软件的苦海，R做符号计算固然比不上Maple，但是你真的需要Maple这样的软件去做符号计算吗？我们看看R语言的符号计算能做到什么程度。 符号计算 1.符号微分 在R中能够直接用来符号计算的是表达式，下面以Tetrachoric函数为例， $$\tau(x)=\frac{(-1)^{j-1}}{\sqrt{j !}}\phi^{(j)}(x)$$ 其中 $$\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$$ 在R里，声明表达式对象使用expression函数 NormDensity &amp;lt;- expression(1 / sqrt(2 * pi) * exp(-x^2 / 2)) class(NormDensity) ## [1] &amp;quot;expression&amp;quot; 计算一阶导数 D(NormDensity, &amp;quot;x&amp;quot;) ## -(1/sqrt(2 * pi) * (exp(-x^2/2) * (2 * x/2))) deriv(NormDensity, &amp;quot;x&amp;quot;) ## expression({ ## .expr3 &amp;lt;- 1/sqrt(2 * pi) ## .expr7 &amp;lt;- exp(-x^2/2) ## .value &amp;lt;- .expr3 * .expr7 ## .grad &amp;lt;- array(0, c(length(.value), 1L), list(NULL,c(&amp;quot;x&amp;quot;))) ## .grad[, &amp;quot;x&amp;quot;] &amp;lt;- -(.expr3 * (.expr7 * (2 * x/2))) ## attr(.value, &amp;quot;gradient&amp;quot;) &amp;lt;- .grad ## .value ## }) deriv3(NormDensity, &amp;quot;x&amp;quot;) ## expression({ ## .expr3 &amp;lt;- 1/sqrt(2 * pi) ## .expr7 &amp;lt;- exp(-x^2/2) ## .expr10 &amp;lt;- 2 * x/2 ## .expr11 &amp;lt;- .expr7 * .expr10 ## .value &amp;lt;- .expr3 * .expr7 ## .grad &amp;lt;- array(0, c(length(.value), 1L), list(NULL, c(&amp;quot;x&amp;quot;))) ## .hessian &amp;lt;- array(0, c(length(.value), 1L, 1L), list(NULL, ## c(&amp;quot;x&amp;quot;), c(&amp;quot;x&amp;quot;))) ## .grad[, &amp;quot;x&amp;quot;] &amp;lt;- -(.expr3 * .expr11) ## .hessian[, &amp;quot;x&amp;quot;, &amp;quot;x&amp;quot;] &amp;lt;- -(.expr3 * (.expr7 * (2/2) - .expr11 ## ＊.expr10)) ## attr(.value, &amp;quot;gradient&amp;quot;) &amp;lt;- .grad ## attr(.value, &amp;quot;hessian&amp;quot;) &amp;lt;- .hessian ## .value ## }) 计算 n 阶导数 DD &amp;lt;- function(expr, name, order = 1) { if (order &amp;lt; 1) stop(&amp;quot;&#39;order&#39; must be &amp;gt;= 1&amp;quot;) if (order == 1) D(expr, name) else DD(D(expr, name), name, order - 1) } DD(NormDensity, &amp;quot;x&amp;quot;, 3) ## 1/sqrt(2 * pi) * (exp(-x^2/2) * (2 * x/2) * (2/2) &#43; ((exp(-x^2/2) * ## (2/2)-exp(-x^2/2)*(2*x/2)*(2*x/2))*(2*x/2)&#43; ## exp(-x^2/2) * (2 * x/2) * (2/2))) 2. 表达式转函数 很多时候我们使用R目的是计算，符号计算后希望可以直接代入计算，那么只需要在deriv中指定 function.arg 参数为 TRUE。 DFun &amp;lt;- deriv(NormDensity, &amp;quot;x&amp;quot;, function.arg = TRUE) DFun(1) ## [1] 0.2419707 ## attr(,&amp;quot;gradient&amp;quot;) ## x ## [1,] -0.2419707 DFun(0) ## [1] 0.3989423 ## attr(,&amp;quot;gradient&amp;quot;) ## x ## [1,] 0 从计算结果可以看出，deriv不仅计算了导数值还计算了原函数在该处的函数值。我们可以作如下简单验证： Normfun &amp;lt;- function(x) 1/sqrt(2 * pi) * exp(-x^2/2) Normfun(1) ## [1] 0.2419707 Normfun(0) ## [1] 0.3989423 在讲另外一个将表达式转化为函数的方法之前，先来一个小插曲,有没有觉得之前计算 3 阶导数的结果太复杂了，说不定看到这的人，早就要吐槽了！其实这个问题已经有高人写了 Deriv 包 [1] 来解决，请看： DD(NormDensity, &amp;quot;x&amp;quot;, 3) ## 1/sqrt(2 * pi) * (exp(-x^2/2) * (2 * x/2) * (2/2) &#43; ((exp(-x^2/2) * ## (2/2)-exp(-x^2/2)*(2*x/2)*(2*x/2))*(2*x/2)&#43; ## exp(-x^2/2) * (2 * x/2) * (2/2))) library(Deriv) Simplify(DD(NormDensity, &amp;quot;x&amp;quot;, 3)) ## x * (3 - x^2) * exp(-(x^2/2))/sqrt(2 * pi) 三阶导数根本不在话下，如果想体验更高阶导数，不妨请读者动动手！表达式转函数的关键是理解函数其实是由参数列表 (args) 和函数体 (body) 两部分构成，以前面自编的 Normfun 函数为例： body(Normfun) ## 1/sqrt(2 * pi) * exp(-x^2/2) args(Normfun) ## function (x) ## NULL 而函数体被一对花括号括住的就是表达式，查看 eval 函数帮助，我们可以知道 eval 计算的对象就是表达式。下面来个小示例以说明此问题： eval({x&amp;lt;-2;x^2}) eval(body(Normfun)) Normfun(2) ## [1] 4 eval(body(Normfun)) ## [1] 0.05399097 Normfun(2) ## [1] 0.05399097 至此我们可以将表达式转化为函数，也许又有读者耐不住了，既然可以用 eval 函数直接计算，干嘛还要转化为函数？这个主要是写成函数比较方便，你可能需要重复计算不同的函数值，甚至放在你的算法的中间过程中……(此处省略500字，请读者自己理解)。终于又回到开篇处 Tetrachoric 函数，里面要计算任意阶导数，反正现在是没问题了，管他几阶，算完后化简转函数，请看： Tetrachoric &amp;lt;- function(x, j) { (-1)^(j-1)/sqrt(factorial(j))*eval(Simplify(DD(NormDensity,&amp;quot;x&amp;quot;,j))) } Tetrachoric(2, 3) ## [1] -0.04408344 有时候我们有的就是函数，这怎么计算导数呢？按道理，看完上面的过程，这已经不是什么问题啦！ Simplify(D(body(Normfun), &amp;quot;x&amp;quot;)) ## -(x * exp(-(x^2/2))/sqrt(2 * pi)) 作为本节的最后，献上函数图像，这个函数的作用主要是计算多元正态分布的概率，详细内容参看 [2]。 3.符号计算扩展包 Ryacas 想要做更多的符号计算内容，如解方程，泰勒展开等，可以借助第三方R扩展包 Ryacas [3]。 suppressPackageStartupMessages(library(Ryacas)) yacas(&amp;quot;Solve(x/(1&#43;x) == a, x)&amp;quot;) ## [1] &amp;quot;Starting Yacas!&amp;quot; ## expression(list(x == a/(1 - a))) yacas(expression(Expand((1 &#43; x)^3))) ## expression(x^3 &#43; 3 * x^2 &#43; 3 * x &#43; 1) yacas(&amp;quot;OdeSolve(y&#39;&#39;== 4 * y)&amp;quot;) ## expression(C95 * exp(2 * x) &#43; C99 * exp(-2 * x)) yacas(&amp;quot;Taylor(x, a, 3) Exp(x)&amp;quot;) ## expression(exp(a) &#43; exp(a) * (x - a) &#43; (x - a)^2 * exp(a)/2 &#43; ## (x - a)^3 * exp(a)/6) 4.符号计算在优化算法中的应用 学过运筹学或者数值分析课程的可能知道，有不少优化算法是要求导或者求梯度的，如拟牛顿算法，最速下降法和共轭梯度法，还有求解非线性方程组的拟牛顿算法及其修正算法。下面以求 Rosenbrock 函数的极小值为例： 符号微分 fun &amp;lt;- expression(100 * (x2 - x1^2)^2 &#43; (1 - x1)^2) D(fun, &amp;quot;x1&amp;quot;) ## -(2 * (1 - x1) &#43; 100 * (2 * (2 * x1 * (x2 - x1^2)))) D(fun, &amp;quot;x2&amp;quot;) ## 100 * (2 * (x2 - x1^2)) 调用拟牛顿法求极值 fr &amp;lt;- function(x) { x1 &amp;lt;- x[1] x2 &amp;lt;- x[2] 100 * (x2 - x1 * x1)^2 &#43; (1 - x1)^2 } grr1 &amp;lt;- function(x) { x1 &amp;lt;- x[1] x2 &amp;lt;- x[2] c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1), 200 * (x2 - x1 * x1)) } optim(c(-1.2,1), fr, grr1, method = &amp;quot;BFGS&amp;quot;) ## $par ## [1] 1 1 ## ## $value ## [1] 9.594956e-18 ## ## $counts ## function gradient ## 110 43 ## ## $convergence ## [1] 0 ## ## $message ## NULL 仿照 Tetrachoric 函数的写法，可以简写 grr 函数（这个写法可以稍微避免一点复制粘贴）： grr2&amp;lt;-function(x){ x1 &amp;lt;- x[1] x2 &amp;lt;- x[2] c(eval(D(fun, &amp;quot;x1&amp;quot;)), eval(D(fun, &amp;quot;x2&amp;quot;))) # 表达式微分 } optim(c(-1.2,1), fr, grr2, method = &amp;quot;BFGS&amp;quot;) ## $par ## [1] 1 1 ## ## $value ## [1] 9.594956e-18 ## ## $counts ## function gradient ## 110 43 ## ## $convergence ## [1] 0 ## ## $message ## NULL 如果调用 numDeriv 包 [4]，可以再少写点代码： library(numDeriv) grr3 &amp;lt;- function(x) { grad(fr, c(x[1], x[2])) # 函数微分 } optim(c(-1.2, 1), fr, grr3, method = &amp;quot;BFGS&amp;quot;) ## $par ## [1] 1 1 ## ## $value ## [1] 9.595012e-18 ## ## $counts ## function gradient ## 110 43 ## ## $convergence ## [1] 0 ## ## $message ## NULL 如果一定要体现符号微分的过程，就调用 Deriv 包： library(Deriv) fr1 &amp;lt;- function(x1, x2) { # 函数形式与上面不同 100 * (x2 - x1 * x1)^2 &#43; (1 - x1)^2 } grr2 &amp;lt;- function(x) { Deriv(fr1, cache.exp = FALSE)(x[1], x[2]) # 符号微分 } optim(c(-1.2, 1), fr, grr2, method = &amp;quot;BFGS&amp;quot;) ## $par ## [1] 1 1 ## ## $value ## [1] 9.594956e-18 ## ## $counts ## function gradient ## 110 43 ## ## $convergence ## [1] 0 ## ## $message ## NULL 从上面可以看出函数（Deriv与optim）之间不兼容：Deriv与optim接受的函数形式不同，导致两个函数（fr 与 fr1）的参数列表的形式不一样，应能看出 fr 这种写法更好些。 注： 求极值和求解方程（组）往往有联系的，如统计中求参数的最大似然估计，有不少可以转化为求方程（组），如 stat4 包 [5] 的 mle 函数。 目标函数可以求导，使用拟牛顿算法效果比较好，如上例中 methods 参数设置成 CG，结果就会不一样。 nlm、optim 和 nlminb 等函数都实现了带梯度的优化算法。 不过话又说回来，真实的场景大多是目标函数不能求导，一阶导数都不能求,更多细节请读者参见 optim 函数帮助。 还有一些做数值优化的 R 包，如 BB 包 [6] 求解大规模非线性系统，numDeriv 包是数值微分的通用求解器，更多的内容可参见 https://cran.rstudio.com/web/views/Optimization.html。 除了数值优化还有做概率优化的 R 包，如仅遗传算法就有 GA [7]，gafit [8]，galts [9]，mcga [10]，rgenoud [11]，gaoptim [12]，genalg [13] 等 R 包，这方面的最新成果参考文献 [14]。 R软件信息 sessionInfo() ## R version 3.1.3 (2015-03-09) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 8 x64 (build 9200) ## ## locale: ## [1] LC_COLLATE=Chinese (Simplified)_China.936 ## [2] LC_CTYPE=Chinese (Simplified)_China.936 ## [3] LC_MONETARY=Chinese (Simplified)_China.936 ## [4] LC_NUMERIC=C ## [5] LC_TIME=Chinese (Simplified)_China.936 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] numDeriv_2014.2-1 Ryacas_0.2-12.1 Deriv_3.7.0 knitr_1.13 ## ## loaded via a namespace (and not attached): ## [1] evaluate_0.9 formatR_1.3 highr_0.5.1 magrittr_1.5 ## [5] RevoUtils_7.4.0 stringi_1.0-1 stringr_1.0.0 tools_3.1.3 参考文献 [1] Andrew Clausen and Serguei Sokol. Deriv: Symbolic Differentiation, 2016. R package version 3.7.0. [2] Bernard Harris and Andrew P. Soms. The use of the tetrachoric series for evaluating multivariate normal probabilities. Journal of Multivariate Analysis, 10(2):252–267, 1980. [3] Ryacas: R interface to the yacas computer algebra system, 2014. R package version 0.2-12.1. [4] Paul Gilbert and Ravi Varadhan. numDeriv: Accurate Numerical Derivatives, 2015. R package version 2014.2-1. [5] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2015. [6] Ravi Varadhan and Paul Gilbert. BB: An R package for solving a large system of nonlinear equations and for optimizing a high-dimensional nonlinear objective function. Journal of Statistical Software, 32(4):1–26, 2009. [7] Luca Scrucca. GA: Genetic Algorithms, 2016. R package version 3.0.1. [8] Telford Tendys. gafit: Genetic Algorithm for Curve Fitting, 2012. R package version 0.4.1. [9] Mehmet Hakan Satman. galts: Genetic algorithms and C-steps based LTS (Least Trimmed Squares) estimation, 2013. R package version 1.3. [10] Mehmet Hakan Satman. Machine coded genetic algorithms for real parameter optimization problems. Gazi University Journal of Science, 26(1):85–95, 2013. [11] Walter R. Mebane, Jr. and Jasjeet S. Sekhon. Genetic optimization using derivatives: The rgenoud package for R. Journal of Statistical Software, 42(11):1–26, 2011. [12] Fernando Tenorio. gaoptim: Genetic Algorithm optimization for real-based and permutation-based problems, 2013. R package version 1.1. [13] Egon Willighagen and Michel Ballings. genalg: R Based Genetic Algorithm, 2015. R package version 0.2.0. [14] L. Scrucca. On some extensions to GA package: hybrid optimisation, parallelisation and islands evolution. ArXiv e-prints, May 2016. [15] Yihui Xie. knitr: A General-Purpose Package for Dynamic Report Generation in R, 2016. R package version 1.13.]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[REmap入门示例 z]]></title>
    	<url>/tech/2016/06/29/introduction-to-remap/</url>
		<content type="text"><![CDATA[[REmap是一个基于Echarts2.0 http://echarts.baidu.com/echarts2/的一个R包。主要的目的是为广大数据玩家提供一个简便的，可交互的地图数据可视化工具。目前托管在github，https://github.com/lchiffon/REmap。 使用如下步骤安装： library(devtools) install_github(&#39;lchiffon/REmap&#39;) REmap目前更新到V0.3，提供百度迁徙，分级统计，百度地图，热力图等功能的实现。 提示:请使用Chrome或者Firefox来作为默认浏览器 最后要声明的一点：这个包的目的是简化使用和学习的流程，如果你是一个好学的geek，请深入的学习Echarts！ 特性 使用Echarts2.0封包，地图绘制使用的是SVG图形 采用百度API来自动获取城市的经纬度数据 支持Windows！ 使用向导 获取经纬度 获取经纬度的函数是基于BaiduAPI的一个获取地理位置的功能。这个函数不仅是REmap下的一个功能，实际上，你也可以用它来抓取城市经纬度数据： 基本函数: get_city_coord 获取一个城市的经纬度 get_geo_position 获取一个城市向量的经纬度 library(REmap) city_vec = c(&amp;quot;北京&amp;quot;,&amp;quot;Shanghai&amp;quot;,&amp;quot;广州&amp;quot;) get_city_coord(&amp;quot;Shanghai&amp;quot;) ## [1] 121.47865 31.21562 get_geo_position (city_vec) ## lon lat city ## 1 116.6212 40.06107 北京 ## 2 121.4786 31.21562 Shanghai ## 3 113.3094 23.39237 广州 注：windows用户会看到city一列为utf-8编码，可以使用get_geo_position (city_vec2)$city查看列向量的信息。(我能说我最好的建议是换Mac么？) 绘制迁徙地图 绘制地图使用的是主函数remap remap(mapdata, title = &amp;quot;&amp;quot;, subtitle = &amp;quot;&amp;quot;, theme =get_theme(&amp;quot;Dark&amp;quot;)) mapdata 一个数据框对象，第一列为出发地点，第二列为到达地点 title 标题 subtitle 副标题 theme 控制生成地图的颜色，具体将会在get_theme部分说明 set.seed(125) origin = rep(&amp;quot;北京&amp;quot;,10) destination = c(&#39;上海&#39;,&#39;广州&#39;,&#39;大连&#39;,&#39;南宁&#39;,&#39;南昌&#39;, &#39;拉萨&#39;,&#39;长春&#39;,&#39;包头&#39;,&#39;重庆&#39;,&#39;常州&#39;) dat = data.frame(origin,destination) out = remap(dat,title = &amp;quot;REmap示例数据&amp;quot;,subtitle = &amp;quot;theme:Dark&amp;quot;) plot(out) 该地图会写成一个html文件，保存在电脑里面，并通过浏览器打开该文件。以下的方式都可以看到这个地图： ## Method 1 remap(dat,title = &amp;quot;REmap实例数据&amp;quot;,subtitle = &amp;quot;theme:Dark&amp;quot;) ## Method 2 out = remap(dat,title = &amp;quot;REmap实例数据&amp;quot;,subtitle = &amp;quot;theme:Dark&amp;quot;) out ## Method 3 plot(out) 个性化地图 正如之前所说的，为了简化学习和使用的流程，REmap并没有封装太多的参数。（真的不是我懒）如果想更个性化地调整Echarts的参数，请移步Echarts的官方文档http://echarts.baidu.com/doc/doc.html REmap中get_theme提供了迁徙地图中常用颜色的调整： get_theme(theme = &amp;quot;Dark&amp;quot;, lineColor = &amp;quot;Random&amp;quot;, backgroundColor = &amp;quot;#1b1b1b&amp;quot;, titleColor = &amp;quot;#fff&amp;quot;, borderColor = &amp;quot;rgba(100,149,237,1)&amp;quot;, regionColor = &amp;quot;#1b1b1b&amp;quot;) theme 默认主题，除了三个内置主题，可以使用“none”来自定义颜色 a character object in (&amp;ldquo;Dark&amp;rdquo;, &amp;ldquo;Bright&amp;rdquo;, &amp;ldquo;Sky&amp;rdquo;, &amp;ldquo;none&amp;rdquo;) lineColor 线条颜色，默认随机，也可以使用固定颜色 Control the color of the line, “Random” for random color backgroundColor 背景颜色 Control the color of the background titleColor 标题颜色 Control the color of the title borderColor 边界颜色（省与省之间的信息） Control the color of the border regionColor 区域颜色 Control the color of the region 颜色可以使用颜色名（比如&amp;rsquo;red&amp;rsquo;, &amp;lsquo;skyblue&amp;rsquo;等），RGB（&amp;rdquo;#1b1b1b&amp;rdquo;, &amp;ldquo;#fff&amp;rdquo;）或者一个rgba的形式（&amp;rdquo;rgba(100,100,100,1)&amp;ldquo;），可以在这里找到颜色对照表。 默认模板：Bright ## default theme:&amp;quot;Bright&amp;quot; set.seed(125) out = remap(dat,title = &amp;quot;REmap实例数据&amp;quot;,subtitle = &amp;quot;theme:Bright&amp;quot;, theme = get_theme(&amp;quot;Bright&amp;quot;)) plot(out) 更改线条颜色 ## set Line color as &#39;orange&#39; set.seed(125) out = remap(dat,title = &amp;quot;REmap实例数据&amp;quot;,subtitle = &amp;quot;theme:Bright&amp;quot;, theme = get_theme(&amp;quot;None&amp;quot;, lineColor = &amp;quot;orange&amp;quot;)) plot(out) 更改其他颜色 ## Set Region Color out = remap(dat,title = &amp;quot;REmap实例数据&amp;quot;,subtitle = &amp;quot;theme:Bright&amp;quot;, theme = get_theme(&amp;quot;None&amp;quot;, lineColor = &amp;quot;orange&amp;quot;, backgroundColor = &amp;quot;#FFC1C1&amp;quot;, titleColor = &amp;quot;#1b1b1b&amp;quot;, regionColor = &#39;#ADD8E6&#39;)) plot(out) 参考资料 Github链接 我的博客：七风阁 REmap，重新定义你的地图slides]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[张志华教授：机器学习——统计与计算之恋 z]]></title>
    	<url>/data/2016/06/23/machine-learning-statistics-and-computation/</url>
		<content type="text"><![CDATA[[原文地址：https://cosx.org/2016/06/machine-learning-statistics-and-computation/ 编辑部按：本文是从张志华老师在第九届中国R语言会议和上海交通大学的两次讲座中整理出来的，点击此处观看幻灯片。张志华老师是上海交通大学计算机科学与工程系教授，上海交通大学数据科学研究中心兼职教授，计算机科学与技术和统计学双学科的博士生指导导师。在加入上海交通大学之前，是浙江大学计算机学院教授和浙江大学统计科学中心兼职教授。张老师主要从事人工智能、机器学习与应用统计学领域的教学与研究，迄今在国际重要学术期刊和重要的计算机学科会议上发表70余篇论文，是美国“数学评论”的特邀评论员，国际机器学习旗舰刊物Journal of Machine Learning Research 的执行编委，其公开课《机器学习导论》和《统计机器学习》受到广泛关注。 大家好，今天我演讲的主题是 “机器学习：统计与计算之恋”。我用了一个很浪漫的名字，但是我的心情是诚惶诚恐的。一则我担心自己没有能力驾驭这么大的主题，二则我其实是一个不解风情之人，我的观点有些可能不符合国内学术界的主流声音。 最近人工智能或者机器学习的强势崛起，特别是刚刚过去的AlphaGo和韩国棋手李世石九段的人机大战，再次让我们领略到了人工智能或机器学习技术的巨大潜力，同时也深深地触动了我。面对这一前所未有的技术大变革，作为10多年以来一直从事统计机器学习一线教学与研究的学者，希望借此机会和大家分享我个人的一些思考和反思。 在这场人工智能发展的盛事里，我突然发现，对我们中国的学者来说，好像是一群看热闹的旁观者。不管你承认还是不承认，事实就是和我一代的或者更早的学者也只能作为旁观者了。我们能做的事情是帮助你们—中国年轻的一代，让你们在人工智能发展的大潮中有竞争力，做出标杆性的成就，创造人类文明价值，也让我有个加油欢呼的主队。 我的演讲主要包含两部分，在第一部分，首先对机器学习发展做一个简要的回顾，由此探讨机器学习现象所蕴含的内在本质，特别是讨论它和统计学、计算机科学、运筹优化等学科的联系，以及它和工业界、创业界相辅相成的关系。在第二部分，试图用“多级”、“自适应”以及 “平均”等概念来简约纷繁多彩的机器学习模型和计算方法背后的一些研究思路或思想。 第一部分：回顾和反思 一、 什么是机器学习 毋庸置疑，大数据和人工智能是当今是最为时髦的名词，它们将为我们未来生活带来深刻的变革。数据是燃料，智能是目标，而机器学习是火箭，即通往智能的技术途径。机器学习大师Mike Jordan和Tom Mitchell 认为机器学习是计算机科学和统计学的交叉，同时是人工智能和数据科学的核心。 It is one of today’s rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. &amp;mdash; M. I. Jordan 通俗地说，机器学习就是从数据里面挖掘出有用的价值。数据本身是死的，它不能自动呈现出有用的信息。怎么样才能找出有价值的东西呢？第一步要给数据一个抽象的表示，接着基于表示进行建模，然后估计模型的参数，也就是计算，为了应对大规模的数据所带来的问题，我们还需要设计一些高效的实现手段。 我把这个过程解释为机器学习等于矩阵&#43;统计&#43;优化&#43;算法。首先，当数据被定义为一个抽象的表示时，往往形成一个矩阵或者一个图，而图其实也是可以理解为矩阵。统计是建模的主要工具和途径，而模型求解大多被定义为一个优化问题，特别是，频率统计方法其实就是一个优化问题。当然，贝叶斯模型的计算牵涉随机抽样方法。而之前说到面对大数据问题的具体实现时，需要一些高效的方法，计算机科学中的算法和数据结构里有不少好的技巧可以帮助我们解决这个问题。 借鉴Marr的关于计算机视觉的三级论定义，我把机器学习也分为三个层次：初级、中级和高级。初级阶段是数据获取以及特征的提取。中级阶段是数据处理与分析，它又包含三个方面，首先是应用问题导向，简单地说，它主要应用已有的模型和方法解决一些实际问题，我们可以理解为数据挖掘；第二，根据应用问题的需要，提出和发展模型、方法和算法以及研究支撑它们的数学原理或理论基础等，我理解这是机器学习学科的核心内容。第三，通过推理达到某种智能。最后，高级阶段是智能与认知，即实现智能的目标。从这里，我们看到，数据挖掘和机器学习本质上是一样的，其区别是数据挖掘更接地于数据库端，而机器学习则更接近于智能端。 二、 机器学习的发展历程 我们来梳理一下机器学习的发展历程。上个世纪90年代以前，我对此认识不够，了解不深，但我觉得当时机器学习处于发展的平淡期。而1996-2006年是其黄金时期，主要标志是学术界涌现出一批重要成果，比如，基于统计学习理论的SVM和boosting等分类方法，基于再生核理论的非线性数据分析与处理方法，以lasso为代表的稀疏学习模型及应用等等。这些成果应该是统计界和计算机科学界共同努力成就的。 然而，机器学习也经历了一个短暂的徘徊期。这个我感同身受，因为那时我在伯克利的博士后工作结束，正面临找工作，因此当时我导师Mike Jordan教授和我进行了多次交流，他一方面认为机器学习正处于困难期，工作职位已趋于饱满，另一方面他向我一再强调，把统计学引入到机器学习的思路是对的，因为以统计学为基础的机器学习作为一个学科其地位已经被奠定。主要问题是机器学习是一门应用学科，它需要在工业界发挥出作用，能为他们解决实际问题。幸运地是，这个时期很快就过去了。可能在座大多数人对这个时期没有印象，因为中国学术发展往往要慢半拍。 现在我们可以理直气壮地说机器学习已经成为计算机科学和人工智能的主流学科。主要体现在下面三个标志性的事件。 首先，2010年2月，伯克利的Mike Jordan教授和CMU的Tom Mitchell教授同时被选为美国工程院院士，同年5月份，Mike Jordan和斯坦福的统计学家Jerome Friedman又被选为美国科学院院士。我们知道许多著名机器学习算法比如CART、MARS 和GBM等是 Friedman教授等提出。 随后几年一批在机器学习做出重要贡献的学者先后被选为美国科学院或工程院院士。比如，人工智能专家的Daphne Koller，Boosting的主要建立者Robert Schapire，Lasso的提出者Robert Tibshirani， 华裔著名统计学习专家郁彬老师，统计机器机器学习专家的Larry Wasserman，著名的优化算法专家 Stephen Boyd等。同时，机器学习专家、深度学习的领袖Toronto大学Geoffrey Hinton 以及该校统计学习专家Nancy Reid 今年分别被选为美国工程院和科学院的外籍院士。 这是当时Mike给我祝贺他当选为院士时的回信： Thanks for your congratulations on my election to the National Academy. It’s nice to have machine learning recognized in this way. 因此，我理解在美国一个学科能否被接纳为主流学科的一个重要标志是其代表科学家能否被选为院士。我们知道Tom Mitchell 是机器学习早期建立者和守护者，而Mike Jordan是统计机器学习的奠基者和推动者。 这个遴选机制无疑是先进的，它可以促使学科良性发展，适应社会动态发展和需求。相反，如果某某通过某种方式被评选为本国院士，然后他们就掌握了该国学术话语权和资源分配权。这种机制可能会造成一些问题，比如一些过剩学科或者夕阳学科会得到过多的发展资源。而主流学科则被边缘化。 其次，2011年的图灵奖授予了UCLA的Judea Pearl教授，他主要的研究领域是概率图模型和因果推理，这是机器学习的基础问题。我们知道，图灵奖通常颁给做纯理论计算机科学的学者，或者早期建立计算机架构的学者。而把图灵奖授予Judea Pearl教授具有方向标的意义。 第三，是当下的热点，比如说深度学习、AlphaGo、无人驾驶汽车、人工智能助理等等对工业界的巨大影响。机器学习切实能被用来帮助工业界解决问题。工业界对机器学习领域的才人有大量的需求。不仅仅需要代码能力强的工程师，也需要有数学建模和解决问题的科学家。 让我们具体地看看工业界和机器学习的之间关系。我之前在谷歌研究院做过一年的访问科学家，我有不少同事和以前学生在IT界工作，平时实验室也经常接待一些公司的来访和交流，因此了解一些IT界情况。 我理解当今IT的发展已从传统的微软模式转变到谷歌模式。传统的微软模式可以理解为制造业，而谷歌模式则是服务业。谷歌搜索完全是免费的，服务社会，他们的搜索做得越来越极致，同时创造的财富也越来越丰厚。 财富蕴藏在数据中，而挖掘财富的核心技术则是机器学习。深度学习作为当今最有活力一个机器学习方向，在计算机视觉、自然语言理解、语音识别、智力游戏等领域的颠覆性成就。它造就了一批新兴的创业公司。 三、 统计与计算 我的重点还是要回到学术界。我们来重点讨论统计学和计算机科学的关系。CMU 统计系教授Larry Wasserman最近刚被选为美国科学院院士。他写了一本名字非常霸道的书，《All of Statistics》。在这本书引言部分关于统计学与机器学习有个非常有趣的描述。他认为原来统计是在统计系，计算机是在计算机系，这两个是不相来往的，而且互相都不认同对方的价值。计算机学家认为那些统计理论没有用，不解决问题，而统计学家则认为计算机学家只是在重新建造轮子，没有新意。然而，他认为这个情况现在改变了，统计学家认识到计算机学家正在做出的贡献，而计算机学家也认识到统计的理论和方法论的普遍性意义。所以，Larry写了这本书，可以说这是一本为统计学者写的计算机领域的书，为计算机学者写的统计领域的书。 现在大家达成了一个共识: 如果你在用一个机器学习方法，而不懂其基础原理，这是一件非常可怕的事情。也是由于这个原因，目前学术界对深度学习还是心存疑虑的。深度学习已经展示其强大的实际应用的效果，但其中的原理目前大家还不是太清楚。 让我们进一步地来分析统计与计算机的关系。计算机学家通常具有强的计算能力和解决问题的直觉，而统计学家长于理论分析，具有强的建模能力，因此，两者有很好的互补性。 Boosting，SVM和稀疏学习是机器学习界也是统计界，在近十年或者是近二十年来，最活跃的方向，现在很难说谁比谁在其中做的贡献大。比如，SVM的理论其实很早被Vapnik等提出来了，但计算机界发明了一个有效的求解算法，而且后来又有非常好的实现代码被陆续开源给大家使用，于是SVM就变成分类算法的一个基准模型。再比如，KPCA是由计算机学家提出的一个非线性降维方法，其实它等价于经典MDS。而后者在统计界是很早就存在的，但如果没有计算机界从新发现，有些好的东西可能就被埋没了。 机器学习现在已成为统计学的一个主流方向，许多著名统计系纷纷招聘机器学习领域的博士为教员。计算在统计已经变得越来越重要，传统多元统计分析是以矩阵为计算工具，现代高维统计则是以优化为计算工具。另一方面，计算机学科开设高级统计学课程，比如统计学中的核心课程“经验过程”。 我们来看机器学习在计算机科学占什么样的地位。最近有一本还没有出版的书 “Foundation of Data Science, by Avrim Blum, John Hopcroft, and Ravindran Kannan,”作者之一John Hopcroft是图灵奖得主。在这本书前沿部分，提到了计算机科学的发展可以分为三个阶段：早期、中期和当今。早期就是让计算机可以运行起来，其重点在于开发程序语言、编译原理、操作系统，以及研究支撑它们的数学理论。中期是让计算机变得有用，变得高效。重点在于研究算法和数据结构。第三个阶段是让计算机具有更广泛的应用，发展重点从离散类数学转到概率和统计。那我们看到，第三阶段实际上就是机器学习所关心的。 现在计算机界戏称机器学习“全能学科”，它无所不在。一方面，机器学习有其自身的学科体系；另一方面它还有两个重要的辐射功能。一是为应用学科提供解决问题的方法与途径。说的通俗一点，对于一个应用学科来说，机器学习的目的就是把一些难懂的数学翻译成让工程师能够写出程序的伪代码。二是为一些传统学科，比如统计、理论计算机科学、运筹优化等找到新的研究问题。 四、 机器学习发展的启示 机器学习的发展历程告诉我们：发展一个学科需要一个务实的态度。时髦的概念和名字无疑对学科的普及有一定的推动作用，但学科的根本还是所研究的问题、方法、技术和支撑的基础等，以及为社会产生的价值。 机器学习是个很酷的名字，简单地按照字面理解，它的目的是让机器能象人一样具有学习能力。但在前面我们所看到的，在其10年的黄金发展期，机器学习界并没有过多地炒作“智能”，而是更多地关注于引入统计学等来建立学科的理论基础，面向数据分析与处理，以无监督学习和有监督学习为两大主要的研究问题，提出和开发了一系列模型、方法和计算算法等，切实地解决工业界所面临的一些实际问题。近几年，因应大数据的驱动和计算能力的极大提升，一批面向机器学习的底层架构又先后被开发出来，深度神经网络的强势崛起给工业界带来了深刻的变革和机遇。 机器学习的发展同样诠释了多学科交叉的重要性和必要性。然而这种交叉不是简单地彼此知道几个名词或概念就可以的，是需要真正的融化贯通。Mike Jordan教授既是一流的计算机学家，又是一流的统计学家，所以他能够承担起建立统计机器学习的重任。而且他非常务实，从不提那些空洞无物的概念和框架。他遵循自下而上的方式，即先从具体问题、模型、方法、算法等着手，然后一步一步系统化。Geoffrey Hinton教授是世界最著名的认知心理学家和计算机科学学家。虽然他很早就成就斐然，在学术界名声卓越，但他一直活跃在一线，自己写代码。他提出的许多想法简单、可行又非常有效，因此被称为伟大的思想家。正是由于他的睿智和力行，深度学习技术迎来了革命性的突破。 机器学习这个学科同时是兼容并收。我们可以说机器学习是由学术界、工业界、创业界(或竞赛界)等合力而造就的。学术界是引擎，工业界是驱动，创业界是活力和未来。学术界和工业界应该有各自的职责和分工。学术界职责在于建立和发展机器学习学科，培养机器学习领域的专门人才；而大项目、大工程更应该由市场来驱动，由工业界来实施和完成。 五、 国内外发展现状 我们来看看机器学习在国际的发展现状。我主要看几所著名大学的情况。在伯克利，一个值得深思的举措是机器学习的教授同时在计算机系和统计学都有正式职位，而且据我所知，他们不是兼职，在两个系都有教授课程和研究的任务的。伯克利是美国统计学的发源地，可以说是当今统计学的圣地，然而她兼容并蓄、不固步自封。Mike Jordan教授是统计机器学习的主要建立者和推动者，他为机器学习领域培养了一大批优秀的学生。统计系的主任现在是Mike，然而他早年的教育并没有统计或数学背景。可以说，Berkeley的统计系成就了Mike，反过来他也为Berkeley的统计学发展创造了新的活力，建立了无可代替的功勋。 斯坦福和伯克利的统计是公认世界最好的两个。我们看到，斯坦福统计系的主流方向就是统计学习，比如我们熟知的《Elements of statistical learning》一书就是统计系几位著名教授撰写的。Stanford计算机科学的人工智能方向一直在世界占主导地位，特别在不确定推理、概率图模型、概率机器人等领域成就斐然，他们的网络公开课 《机器学习》、《概率图模型》以及《人工智能》等让世界受益。 CMU是一个非常独特的学校，她并不是美国传统的常春藤大学。可以说，它是以计算机科学为立校之本，它是世界第一个建立机器学习系的学校。Tom Mitchell 教授是机器学习的早期建立者之一和守护者，他一直为该校本科生教《机器学习》课程。然而，这个学校统计学同样强，尤其，她是贝叶斯统计学的世界研究中心。 在机器学习领域，多伦多大学有着举足轻重的地位，她们机器学习研究组云集了一批世界级的学者，在“Science” 和“Nature”发表多篇论文，实属罕见。Geoffrey Hinton 教授是伟大的思想家，但更是践行者。他是神经网络的建立者之一，是BP算法和深度学习的主要贡献者。正是由于他的不懈努力，神经网络迎来了大爆发。Radford Neal 教授是Hinton学生，他在贝叶斯统计领域，特别是关于MCMC做出了一系列的重要工作。 国际发展现状 那么我们来看看国内的现状。总的来说，统计和计算机科学这两个学科处于Larry所说的初期各自为战的阶段。面向大数据的统计学与计算机科学的交叉研究是机遇也是挑战。 我之前在浙江大学曾经参与其统计交叉学科中心的组建，由此对统计界有所了解。统计学在中国应该还是一个弱势学科，最近才被国家定为一级学科。我国统计学处于两个极端，一是它被当作数学的一个分支，主要研究概率论、随机过程以及数理统计理论等。二是它被划为经济学的分支，主要研究经济分析中的应用。而机器学习在统计学界还没有被深度地关注。因此，面向于数据处理、分析的IT和统计学的深度融合有巨大的潜力。 虽然，我并没有跟国内机器学习或者人工智能学术界有深入的接触，但我在国内计算机系工作近8年时间，一直在一线从事机器学习相关的教学与研究，应该对机器学习的现状有一定的发言权。机器学习的确在中国得到了广泛的关注，也取得了一定的成绩，但我觉得高品质的研究成果稀缺。热衷于对机器学习的高级阶段进行一些概念炒作，它们通常没有多大的可执行性；偏爱大项目、大集成，这些本更应该由工业界来实施；而理论、方法等基础性的研究不被重视，认为理论没有用处的观点还大有市场。 计算机学科的培养体系还基本停留在它的早期发展阶段。大多数学校都开设了人工智能与机器学习的课程，但无论是深度还是前沿性都落后于学科的发展，不能适应时代的需要。人才的培养无论质量和数量都无法满足工业界的需求。这也是国内IT公司与国际同类公司技术上有较大差距的关键原因。 第二部分：几个简单的研究思路 在这部分，我的关注则回到机器学习的研究本身上来。机器学习内容博大精深，而且新方法、新技术正源源不断地被提出、被发现。这里，我试图用“多级”、“自适应”以及 “平均”等概念来简约纷繁多彩的机器学习模型和计算方法背后的一些研究思路和思想。希望这些对大家理解机器学习已有的一些模型、方法以及未来的研究有所启发。 1. 多级 (Hierarchical) 首先，让我们来关注“多级 这个技术思想。我们具体看三个例子。 第一个例子是隐含数据模型，它就是一种多级模型。作为概率图模型的一种延伸，隐含数据模型是一类重要的多元数据分析方法。隐含变量有三个重要的性质。第一，可以用比较弱的条件独立相关性代替较强的边界独立相关性。著名的de Finetti 表示定理支持这点。这个定理说，一组可以交换的随机变量当且仅当在某个参数给定条件下，它们可以表示成一组条件随机变量的混合体。这给出了一组可以交换的随机变量的一个多级表示。即先从某个分布抽一个参数，然后基于这个参数，独立地从某个分布抽出这组随机变量。第二，可以通过引入隐含变量的技术来方便计算，比如期望最大算法以及更广义的数据扩充技术就是基于这一思想。具体地，一些复杂分布，比如t-distribution, Laplace distribution 则可以通过表示成高斯尺度混合体来进行简化计算。第三，隐含变量本身可能具有某种有可解释的物理意思，这刚好符合应用的场景。比如，在隐含狄利克雷分配(LDA)模型，其中隐含变量具有某种主题的意思。 Latent Dirichlet Allocation 第二例子，我们来看多级贝叶斯模型。在进行MCMC抽样后验估计时，最上层的超参数总是需要先人为给定的，自然地，MCMC算法收敛性能是依赖这些给定的超参数的，如果我们对这些参数的选取没有好的经验，那么一个可能做法我们再加一层，层数越多对超参数选取的依赖性会减弱。 Hierarchical Bayesian Model 第三例子，深度学习蕴含的也是多级的思想。如果把所有的节点全部的放平，然后全连接，就是一个全连接图。而CNN深度网络则可以看成对全连接图的一个结构正则化。正则化理论是统计学习的一个非常核心的思想。CNN和RNN是两大深度神经网络模型，分别主要用于图像处理和自然语言处理中。研究表明多级结构具有更强的学习能力。 Deep Learning 2. 自适应 (Adaptive) 我们来看自适应这个技术思路，我们通过几个例子来看这个思路的作用。 第一个例子是自适应重要采样技术。重要采样方法通常可以提高均匀采样的性能，而自适应则进一步改善重要采样的性能。 第二个例子，自适应列选择问题。给定一个矩阵A，我们希望从中选取部分列构成一个矩阵C，然后用$CC^&#43;A$去近似原矩阵A，而且希望近似误差尽可能小。这是一个NP难问题。在实际上，可以通过一个自适应的方式，先采出非常小一部分$C_1$，由此构造一个残差，通过这个定义一个概率，然后用概率再去采一部分$C_2$，把$C_1$ 和 $C_2$ 合在一起组成$C$。 第三个例子，是自适应随机迭代算法。考虑一个带正则化的经验风险最小问题，当训练数据非常多时，批处理的计算方式非常耗时，所以通常采用一个随机方式。存在的随机梯度或者随机对偶梯度算法可以得到参数的一个无偏估计。而通过引入自适应的技术，可以减少估计的方差。 第四个例子，是Boosting分类方法。它自适应调整每个样本的权重，具体地，提高分错样本的权重，而降低分对样本的权重。 3. 平均 (Averaging) 其实，boosting 蕴含着平均思想，即我最后要谈的技术思路。简单地说，boosting是把一组弱分类器集成在一起，形成一个强的分类器。第一好处是可以降低拟合的风险。第二，可以降低陷入局部的风险。第三，可以扩展假设空间。Bagging同样是经典的集成学习算法，它把训练数据分成几组，然后分别在小数据集上训练模型，通过这些模型来组合强分类器。另外这是一个两层的集成学习方式。 经典的Anderson 加速技术则是通过平均的思想来达到加速收敛过程。具体地，它是一个叠加的过程，这个叠加的过程通过求解一个残差最小得到一个加权组合。这个技术的好处，是没有增加太多的计算，往往还可以使数值迭代变得较为稳定。 另外一个使用平均的例子是分布式计算中。很多情况下分布式计算不是同步的，是异步的，如果异步的时候怎么办？最简单的是各自独立做，到某个时候把所有结果平均，分发给各个worker, 然后又各自独立运行，如此下去。这就好像一个热启动的过程。 正如我们已经看到，这些思想通常是组合在一起使用的，比如boosting模型。我们多级、自适应和平均的思想很直接，但的确也很有用。 在AlphaGo和李世石九段对弈中，一个值得关注的细节是，代表Alpha Go方悬挂的是英国国旗。我们知道AlphaGo是由deep mind团队研发的，deep mind是一家英国公司，但后来被google公司收购了。科学成果是世界人民共同拥有和分享的财富，但科学家则是有其国家情怀和归属感。 位低不敢忘春秋大义，我认为我国人工智能发展的根本出路在于教育。先哲说：“磨刀不误砍柴夫”。只有培养出一批又一批的数理基础深厚、计算机动手执行力极强，有真正融合交叉能力和国际视野的人才时，我们才会有大作为。 致谢 上述内容是根据我最近在第九届中国R语言会议和上海交通大学的两次讲座而整理出来的，特别是R会主办方统计之都的同学们帮我做了该次演讲的记录。感谢统计之都的太云、凌秉和象宇的邀请，他们和统计之都的伙伴们正在做一件意义影响深远的学术公益，你们的情怀和奉献给了我信心来公开宣讲自己多年来的真实认识和思考。感谢我的学生们帮助我准备这个讲演报告，从主题的选定，内容的选取，材料的收集以及ppt的制作他们都给了我极大的支持，更重要的是，他们让我在机器学习领域的求索一直不孤独。谢谢大家！ 附：演讲幻灯片下载]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python 爬虫：一些常用的爬虫技巧总结 z]]></title>
    	<url>/tech/2016/06/19/spider/</url>
		<content type="text"><![CDATA[[原文地址：https://my.oschina.net/jhao104/blog/647308 用 Python 也差不多一年多了，Python 应用最多的场 Web 快速开发、爬虫、自动化运维：写过简单网站、写过自动发帖脚本、写过收发邮件脚本、写过简单验证码识别脚本。爬虫在开发过程中也有很多复用的过程，这里总结一下，以后也能省些事情。 基本抓取网页 get方法 import urllib2 url = &amp;quot;http://www.baidu.com&amp;quot; response = urllib2.urlopen(url) print response.read() post方法 import urllib import urllib2 url = &amp;quot;http://abcde.com&amp;quot; form = {&#39;name&#39;:&#39;abc&#39;,&#39;password&#39;:&#39;1234&#39;} form_data = urllib.urlencode(form) request = urllib2.Request(url,form_data) response = urllib2.urlopen(request) print response.read() 使用代理 IP 在开发爬虫过程中经常会遇到 IP 被封掉的情况，这时就需要用到代理 IP；在urllib2包中有ProxyHandler类，通过此类可以设置代理访问网页，如下代码片段： import urllib2 proxy = urllib2.ProxyHandler({&#39;http&#39;: &#39;127.0.0.1:8087&#39;}) opener = urllib2.build_opener(proxy) urllib2.install_opener(opener) response = urllib2.urlopen(&#39;http://www.baidu.com&#39;) print response.read() Cookies处理 Cookies 是某些网站为了辨别用户身份、进行 Session 跟踪而储存在用户本地终端上的数据(通常经过加密)，Python 提供了cookielib模块用于处理cookies，cookielib模块的主要作用是提供可存储 cookie 的对象，以便于与urllib2模块配合使用来访问 Internet 资源。 代码片段： import urllib2, cookielib cookie_support= urllib2.HTTPCookieProcessor(cookielib.CookieJar()) opener = urllib2.build_opener(cookie_support) urllib2.install_opener(opener) content = urllib2.urlopen(&#39;http://XXXX&#39;).read() 关键在于CookieJar()，它用于管理 HTTP cookie 值、存储 HTTP 请求生成的 cookie、向传出的 HTTP 请求添加 cookie 的对象。整个 cookie 都存储在内存中，对CookieJar实例进行垃圾回收后 cookie 也将丢失，所有过程都不需要单独去操作。 手动添加 cookie cookie = &amp;quot;PHPSESSID=91rurfqm2329bopnosfu4fvmu7; kmsign=55d2c12c9b1e3; KMUID=b6Ejc1XSwPq9o756AxnBAg=&amp;quot; request.add_header(&amp;quot;Cookie&amp;quot;, cookie) 伪装成浏览器 某些网站反感爬虫的到访，于是对爬虫一律拒绝请求。所以用urllib2直接访问网站经常会出现HTTP Error 403: Forbidden的情况，对有些 header 要特别留意，Server 端会针对这些 header 做检查 User-Agent：有些 Server 或 Proxy 会检查该值，用来判断是否是浏览器发起的 Request Content-Type：在使用 REST 接口时，Server 会检查该值，用来确定 HTTP Body 中的内容该怎样解析。 这时可以通过修改http包中的header来实现，代码片段如下： import urllib2 headers = { &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6&#39; } request = urllib2.Request( url = &#39;http://my.oschina.net/jhao104/blog?catalog=3463517&#39;, headers = headers ) print urllib2.urlopen(request).read() 页面解析 对于页面解析最强大的当然是正则表达式，这个对于不同网站不同的使用者都不一样，就不用过多的说明，附两个比较好的网址： 正则表达式入门：http://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html 正则表达式在线测试：http://tool.oschina.net/regex/ 其次就是解析库了，常用的有两个lxml和BeautifulSoup，对于这两个的使用介绍两个比较好的网站： lxml：http://my.oschina.net/jhao104/blog/639448 BeautifulSoup：http://cuiqingcai.com/1319.html 对于这两个库，我的评价是，都是 HTML/XML 的处理库，Beautifulsoup纯 Python 实现，效率低，但是功能实用，比如能用通过结果搜索获得某个 HTML 节点的源码；lxmlC语言编码，高效，支持Xpath。 验证码的处理 对于一些简单的验证码，可以进行简单的识别。本人也只进行过一些简单的验证码识别。但是有些反人类的验证码，比如 12306，可以通过打码平台进行人工打码，当然这是要付费的。 gzip压缩 有没有遇到过某些网页，不论怎么转码都是一团乱码。哈哈，那说明你还不知道许多 Web 服务具有发送压缩数据的能力，这可以将网络线路上传输的大量数据消减 60% 以上。这尤其适用于 XML web 服务，因为 XML 数据的压缩率可以很高。 但是一般服务器不会为你发送压缩数据，除非你告诉服务器你可以处理压缩数据。于是需要这样修改代码： import urllib2, httplib request = urllib2.Request(&#39;http://xxxx.com&#39;) request.add_header(&#39;Accept-encoding&#39;, &#39;gzip&#39;) 1 opener = urllib2.build_opener() f = opener.open(request) 这是关键：创建Request对象，添加一个Accept-encoding头信息告诉服务器你能接受 gzip 压缩数据，然后就是解压缩数据： import StringIO import gzip compresseddata = f.read() compressedstream = StringIO.StringIO(compresseddata) gzipper = gzip.GzipFile(fileobj=compressedstream) print gzipper.read() 多线程并发抓取 单线程太慢的话，就需要多线程了，这里给个简单的线程池模板，这个程序只是简单地打印了1-10，但是可以看出是并发的。虽然说 Python 的多线程很鸡肋，但是对于爬虫这种网络频繁型，还是能一定程度提高效率的。 from threading import Thread from Queue import Queue from time import sleep # q 是任务队列 # NUM 是并发线程总数 # JOBS 是有多少任务 q = Queue() NUM = 2 JOBS = 10 # 具体的处理函数，负责处理单个任务 def do_somthing_using(arguments): print arguments # 这个是工作进程，负责不断从队列取数据并处理 def working(): while True: arguments = q.get() do_somthing_using(arguments) sleep(1) q.task_done() # fork NUM 个线程等待队列 for i in range(NUM): t = Thread(target=working) t.setDaemon(True) t.start() # 把 JOBS 排入队列 for i in range(JOBS): q.put(i) # 等待所有 JOBS 完成 q.join()]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[数据江湖，回归5式 z]]></title>
    	<url>/data/2016/06/04/five-useful-regression-models/</url>
		<content type="text"><![CDATA[[今天要跟大家分享的主题叫做：数据江湖，回归5式！ 如今啊，大数据时代，群雄割据，天下大乱。各位童鞋，闯荡江湖，凶险难测。没一些必备的看家的本领 为此呢，熊大教大家几招防身绝技，叫做：回归5式！简单的说，就是5种最常见的回归模型。这5个招式，看似简单，却是熊大行走江湖的看家本领。回归5式，就如同少林长拳，看似平淡无奇，但是如果辅以深厚的内力，就能威力无比。 所以呀，今天除了要教给大家这回归5式以外，熊大还要跟大家说道说道这内力的修为。没有深厚的内力修为，任何绝妙的功夫，都是花拳绣腿。 好了，闲话少说，先从回归5式开始。 回归分析第1式：线性回归，或者更严格地说，是普通线性回归。 前面我们说了，什么是回归分析？回归分析就是关于XY相关性的分析。那么具体到线性回归，它的主要特征是什么呢？ 普通线性回归的主要特征就是：它的因变量必须是连续型数据。什么是连续型数据呀？简单通俗的讲，就是得是连续的。例如：身高、体重、价格、温度都是典型的连续型数据。但是，在实际工作中，由于所有的计算机，实际上都只能存储有限位有效数字，因此，在真实的数据江湖里，不存在严格的连续数据，只有近似的。接下来，我们讨论一下，普通线性回归在数据江湖中，有哪些重要应用？太多了！随便说几个。 股票 先说一个简单刺激的：股票投资。 这里的因变量Y是某只股票或者资产组合的未来收益率。这是一个连续型的因变量。如果我们能够建立Y和一系列X的相关关系，例如X可以是该股票背后企业的财务特征，我们就可以通过X去预测Y，然后通过合理的交易策略，实现超额收益率，俗称：发大财！ 客户价值 其次，再考虑一个关于消费者的案例：客户终身价值。 这里的因变量Y是一个目标客户，从现在开始，到未来无限远时间，所能够给企业创造的收入，经过一定的利率折现到现在的价值。如果我们能够建立Y和一系列X的相关关系，例如X可以是这些消费者的人口统计特征以及过去的消费记录，我就可以通过X去预测Y。这样可以帮助我们识别潜在的高价值客户。 医疗健康 最后，再说一个关乎医疗健康的案例 大家都知道，高血压是一个非常普遍的慢性疾病，是个人或者社保医疗支出中的一大块。而血压这个Y也是一个连续型数据。深刻理解一个人的血压Y，同各种相关因素X（例如：饮食习惯、服药习惯）之间的关系，对于改进健康、降低医疗开支，有重要的意义。 总结一下，在我们通往价值的坎坷道路上，一定会遇到各种各样的数据挑战 。他们中绝大多数，都可以被规范成为回归分析问题。而只要这个问题的Y是连续型数据，那么回归分析第1式“线性回归”，基本能搞定！ 回归分析第2式：0-1回归。 如果我的因变量不是连续的怎么办？例如：是0-1型数据。什么是“0-1型数据”？0-1型数据就是说呀，这个数据只可能有两个取值。 例如：性别，只有“男”、“女”两个取值；消费者的购买决策，只有“买”或者“不买”两个取值；病人的癌症诊断，只有“得癌症”或者“不得癌症”两个取值。类似地，大家可以给出很多0-1型数据的例子来。 碰到这种数据挑战的时候，线性回归就不好使了。你需要的是回归分析第2式：0-1回归。0-1回归主要砍的就是0-1型因变量的问题。0-1型的因变量又包含了很多很多的招数，我个人认为，其实大同小异，最常见的有两招就可以了。一招是：逻辑回归，也叫做Logistic Regression；另外一招是：Probit Regression。 具体想学的同学，大家可以去查“广义线性模型”相关的武林秘籍，我就不再这里赘述了。我主要想跟大家分享的是：“0-1回归”是一个非常重要的回归模型，你要不会这招，休想行走数据江湖，永远不可能到达价值的彼岸。 为什么这么说？因为相关的重要应用太多了，咱们说几个时髦有趣的。 互联网征信 第一个例子。现在征信特别火，尤其是互联网征信。 征信是啥？征信就是对某个体的信誉做评估。啥是信誉？就是如果我借钱给你，你按时还钱的概率有多大？所以，对于这个业务而言，因变量就是一个借款人是否会还钱。而0-1回归的主要使命，就是评价该借款人未来还钱的可能性。是一个介于0和1之间的概率。如果产品经理愿意，就可以把这个概率经过一定的单调变换，变成一个具体的征信得分。你看，0-1回归重要不？ 网上购物 再跟大家看一个例子。大家都喜欢网上购物，什么淘宝、京东、天猫啥的。每一次登陆进自己的账户，我们看到了什么？是不是总能看到一些被推荐的商品，“猜你喜欢”，对不？这些商品是怎么被推荐出来的？这个背后啊，也是一个0-1回归的问题。 举个例子，咱先找一堆$X_1$描述消费者的特征（什么性别啦、年龄啦、购物习惯啦等等），然后咱们再找一堆的$X_2$描述商品特征（什么品类呀、价格呀、品牌呀、型号呀等等）。咱把这两堆X放在一起，问一个问题：说您会买吗？$Y=0$表示不会，$Y=1$表示会。这就是一个标准的0-1回归问题了。 有了这个模型，我们可以知道：对于什么样的消费者，推荐什么样的商品，会产生什么样的购买概率。然后在所有的待选商品中，挑选概率最大的（例如5个），呈现在您的眼前。这就成就了个性化推荐。 有人说了：“王老师，您说的不对，我们用的模型可不是逻辑回归那么简单，老复杂了。”这个木有问题，真正的工程实践，所用的模型，有可能更简单，也有可能更复杂。但是，都逃离不了0-1回归的本质所在。 社交网络 最后，再分享一个有趣的例子。现在啊，这社交网络特别火，什么Facebook、Twitter、QQ、微信、微博、陌陌等等。 对于社交平台而言，帮助用户发现好友、建立丰富紧密的好友关系，非常重要。为此啊，几乎所有的社交平台都有“推荐好友”这个功能。这个功能从本质上讲，跟个性化推荐商品一样，也是一个0-1回归的问题。稍微有点区别，可能是，在社交平台上做推荐，我们多了一大类全新的X，就是社交关系。 例如，在我们考虑是否要给张三推荐李四的时候，一个重要的X变量就是“他们之间有多少共同好友”，或者“在张三的关注中，有多少人关注了李四”，这些重要的X变量是根据社交网络的结构推算出来的，对于预测“张三是否会真的关注李四”，帮助巨大。 总结一下，在数据的江湖里，你一定会碰到“0-1数据”的挑战。如果没有0-1回归分析护身，通往价值的道路一定是坎坷无比。 回归分析第3式：定序回归。 什么是定序回归？就是因变量是定序数据的回归分析。那么，什么又是定序数据呢？定序数据就是关乎顺序的数据，但是又没有具体的数值意义。 定序回归 考虑一个特别常见的例子。例如，咱公司出一款新的矿泉水，叫做“狗熊山泉，有点不甜”。我想知道消费者对它的喜好程度。因此啊，我决定请人来品尝一下，然后呢，根据他的喜好程度，给出一个打分。1表示非常不喜欢，2表示有点不喜欢，3表示一般般，4表示有点喜欢，5表示非常喜欢。这就是我关心的因变量。 这种数据常见吗？非常常见！有什么特点？ 它没有数值意义，不能做任何代数运算。例如，您不能做加法。我不能说：1（很不喜欢）加上一个2（有点不喜欢）居然等于了3（表示一般般）。这显然不对。这就是该数据的第一个特点，没有具体的数值意义。 这个数据的第二个特点是它的顺序很重要。例如：1（很不喜欢）就一定要排在2（有点不喜欢的前面），而2（有点不喜欢）就必须要排在3（一般般的前面）。这个顺序呀，很重要！这就是为什么人们管它叫做“定序数据”。 我们说了，定序数据没有具体的数值意义。因此，我们不能确信：2（有点不喜欢）和1（很不喜欢）的差距，是否正好等于5（超级喜欢）和4（有点喜欢）之间的差距。事实上，基本上不可能相等，因为没那么巧！ 既然这些取值之间的间距到底是多少，谁也说不清楚。那么，把很不喜欢定义为\(Y=1\)，还是\(Y=1.5\)，还是说\(Y=-3\)，都无所谓。同样的，如何定义有点不喜欢，也随意。但是只要这个定义，不破坏顺序就可以了。这就是定序数据的核心要义。 定序回归应用的常见的战场有哪些？前面说了，消费者调查，请大家表达自己的偏好。在线下，这就是最普通的市场调研；在线上，就可能是豆瓣上人们对一个电影的打分评级；在医学应用中，有些重要的心理相关的疾病（例如：抑郁症）也会涉及到定序数据。这就是回归分析第三式：定序回归。 回归分析第4式：计数回归。 什么是计数回归？就是因变量是计数数据的回归分析。那么，什么又是计数数据呢？就是数数的数据。例如，谁家有几个孩子，养了几条狗。 计数回归 有什么特点？既然是数数，它就必须是非负的整数。不能是负数，说谁家有负3个孩子，没这事。不能是小数，例如说谁家养了1.25只狗，也没这说法。 计数数据在哪些地方常见？例如：客户关系管理中，有一个经典的RFM模型，其中这个F，就是frequency，说的是一定时间内，客户到访的次数。可以是0次，也可以是1次、2次、很多次。但是，不能是-2次，更不能是2.3次。这样清楚吗？ 计数数据还出现在医学研究中。一个癌症病人体内肿瘤的个数：0是没有，也可以是1个、2个、或者很多个。 计数数据还出现在社会研究中。例如，二胎政策放开，一对夫妻最后到底如何选择要生育多少个孩子呢？可以是0个、1个，也可以是2个。但是，不能是-2个，也不能是0.7个。 要应对计数型因变量，咱就需要回归分析第4式：计数回归。计数回归也有很多招数。最常见的是泊松回归、负二项回归、零膨胀泊松回归等方法。欲知详情，请参见各路统计学秘籍。 回归分析第5式：生存回归。 生存回归是生存数据回归的简称。而生存数据回归就是因变量是生存数据的回归分析。什么是生存数据？生存数据就刻画的一个现象或个体，存续生存了多久，也就是我们常说的生存时间。 生存回归 因此，我们要清晰定义两个东西。一个是出生，一个是死亡。以人的自然出生为出生，以人的自然死亡为死亡，就定义了一个人的寿命，这就是一个典型的生存数据。该数据，对（例如）寿险精算非常重要。 如果以一个电子产品（例如：灯泡）第一次使用为出生，到最后报废为死亡，就决定了产品的使用寿命。 如果以一个消费者的注册成为我家的网站为出生，到某天离我而去，再也不登陆为死亡，这定义了一个消费者的生命周期。 如果，以一个企业的工商注册为出生，到破产注销为死亡，这刻画的是企业的生命周期。 如果，以一个创业团队获得A轮融资为出生，到创业板上市为死亡（请注意呀，这是一个开心的死亡），这刻画的是风险投资回报的周期。 由此可见，生存数据无处不在啊。要分析这种数据，您就需要回归分析第5式：生存回归。这样清楚吗？ 诶，且慢。细心的同学一定会问：诶，这听起来生存数据不就是一个连续型的数据吗？为什么不用线性回归呢？咱把数据做一个对数变换，线性回归它就搞不定了吗？啊哈，您可真是问到点子上了。您说的一点都没错，本来啊，线性回归是可以搞定生存数据的，如果生存数据是被精确观察到的。 什么？有可能生存数据没有被精确观测到吗？是的呀，考虑一个具体的例子。 咱以人的寿命为例，我们关心一个问题：一个人是否学习统计学（这个X），是否会影响得到他的寿命（Y）？看，这是一个典型的回归分析问题吧。为此，我们调查了很多数据，隔壁老王不幸被抽中，为此我们想知道老王到底能活多久。 老王 老王今年60，身体倍儿棒，吃饭倍儿香，核心问题是他还没挂呢，我们怎么知道他要活多久呢？咋办呢？要不再等个几十年，等老王挂了，知道他的精确寿命了，咱们再做分析？这怎么能行呢，万一，我先挂了怎么办！谁来做分析？所以，老王的寿命，这个Y到底应该怎么办呢？我们对它不是一无所知，因为他已经虚度春秋60载了，所以，我们知道老王的Y一定比60大。这是一个宝贵的信息。但是，大多少，我不知道。所以，在数据上我们是怎么记录这个事情呢？我们把Y记作60&#43;，看到这个神奇的“&#43;”没。只要一个数据后面跟着一个“&#43;”，这表明真实的数据比这个大，但是，大多少不知道。 这种数据叫什么？这种数据叫作Censored Data，中文被称作截断的数据。 如果，咱们的生存数据，没有任何数据被截断，那么回归分析第1式“线性回归”，基本搞定。但是，真实的生存数据，常常存在大量的被截断现象。在这种情况下，普通线性回归是束手无策的。因此，我们必须技出回归分析第5式：生存回归。 生存回归也有非常多的招式，这是一个很大的学科。其中有两招最常见，一招叫Cox等比例风险模型，第二招叫做AFT加速失效模型。有兴趣的童鞋，请参考相关的武林秘籍。 到此为止呀，我就把熊大看家的本领“回归5式”给大家介绍完了。时间有限，只能跟大家分享基本思想，还有重要的应用场景。具体怎么学，大家需要去翻看相关的统计学秘籍，市场上无穷多。 最后，想跟大家强调一点。仅仅会这回归5式，是不够的。我们前面说如果没有深厚的内力修为，这就是花拳秀腿而已。只有在深厚内力的支撑下，这5式才能助你在数据的江湖里，降妖伏魔。 那么，这神奇的内力是什么呢？听好了啊，这是熊大的绝密，我不告诉别人的。深厚的内力就是对业务或者科学问题的深刻的理解。 所以请记住熊大的名言：要想闯荡数据江湖，您需要：（1）回归5式；（2）深厚的内力。衷心祝愿每一个童鞋，在数据的江湖里，一帆风顺，马到成功。谢谢大家。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[互联网征信中的信用评分模型 z]]></title>
    	<url>/data/2016/05/24/credit-scoring-model-in-internet-credit-reporting/</url>
		<content type="text"><![CDATA[[原文地址：https://cosx.org/2016/05/credit-scoring-model-in-internet-credit-reporting/ 摘要：面向小微商户以及个人消费的小微信贷是当前互联网金融的重要发展方向，并且正在经历爆发式增长。在这个增长过程中，如何在没有实物抵押的情况下，通过互联网大数据分析实现快速准确征信是一个非常重要的问题。为此，不同的数据来源将各显神通地为信用评估提供依据。本文将通过一个真实的案例出发，进行分析和探讨，针对用户历史行为数据建立信用评分模型，并通过该模型改进信用评估的预测效果。 关键词: 小微信贷；互联网征信；信用评分；Logistic回归模型 一、业务介绍 1. 行业介绍 小微信贷，我们定义为金额较小，并且没有抵押担保，完全靠信用的借贷行为。小微信贷可以面向个人（2C），也可以面向小微企业（2B）。对于2C类业务而言，常常是小额短期信用贷款，这是贷款是为解决借款人临时性的消费需要而发放的期限在1年以内、金额在20万元及以下的、毋需提供担保的人民币信用贷款。对于2B类业务而言，由于小微企业的信贷需求特点是 “短、小、频、急”，这种小额、短期、分散的特征更类似于零售贷款，对资金流动性的要求更高。 小微信贷的产生具有其深刻的市场原因。在2B层面，小微企业受到传统金融机构的歧视和排挤，资金供给严重不足最终导致了小微企业的营养不良。就其原因，主要是小微企业没有足够的抵押物。而传统银行贷款，往往需要企业提供足够的抵押品。小微企业处于发展初级阶段，有限的资金都必须用在刀口上，所以只有少量资金可以用来购买土地房产等固定资产。于是造成传统银行在小微业务方面的产品较少。这造成了小微企业的融资需求与银行产品不匹配。另一方面，很多小微企业的财务报表不完全，从而造成信息不对称，银行对它们没有足够了解的基础上，不敢轻易放贷。而事实上，对银行来说，相对于大企业业务，小微信贷业务成本更高，这也导致银行放贷动力不足。而恰恰相反，在互联网平台上，信息不对称在一定程度上得到降低，而资金的供需双方在降低了交易成本的情况下直接对接，有些电商平台也开始为自己平台上的中小企业提供融资信贷服务。 图1 银行的大企业贷款与小微企业贷款 在2C方面，个人信用贷款主要为了购车、房屋装修、旅游等消费使用，但一般不能用于支付购房款。一般来说信用良好、有固定居所以及稳定收入、还贷能力充足的人能够申请信用贷款。传统的信贷审核仍然存在以下缺点：效率低、速度慢、准确性差。由于这些问题存在，使其很难适应互联网信贷的审核模式。另一方面，信用卡对普罗大众来说能够支配的信用额度有限。因此，仅依靠金融领域已有产品并不能满足用户对贷款金额以及时效性的细化需求。网络信贷的出现将更快地撮合这一过程，从而大量地减少了时间成本。 自余额宝推出以来，互联网金融的产品如雨后春笋般出现。移动互联网正在逐渐渗透到传统金融的各个环节，目标是使金融能够以更低的成本、更多样化的方式、更优质的体验覆盖更多的人群。网络信贷平台将小额信用贷款用互联网的方式进行了改造，从而更多的小微企业和个人有了融资渠道，并且这一过程变得更加便捷。 图2 个人信用贷款的用途 2.业务背景介绍 由于小微借贷中的借贷并没有抵押和担保，那么唯一依靠的就是个人或者企业的信用，因此征信过程及其重要。而我们关注的要点就在于如何利用互联网平台的海量信息来帮助我们快速准确的完成这一过程。马云在给阿里巴巴员工的一封内部邮件中说，以控制为出发点的IT时代正在走向激活生产力为目的的DT(data technology)数据时代。随着计算机技术的发展，大数据运算变得越来越现实，基于大数据应用服务的公司不断崭露头角。大数据在营销领域的应用将广告变成了“窄告”精准营销，而在互联网金融大热的背景下，大数据在金融业征信方面的应用也在逐渐兴起。 2015年1月5日，中国人民银行印发《关于做好个人征信业务准备工作的通知》，公布了首批获得个人征信牌照的8家机构名单，这8家机构分别为：腾讯征信有限公司、芝麻信用管理有限公司、深圳前海征信中心股份有限公司、鹏元征信有限公司、中诚信征信有限公司、中智诚征信有限公司、拉卡拉信用管理有限公司、北京华道征信有限公司。而央行的征信中心是一个基础数据库，已获批的个人征信机构将提供一些增值和创新服务。所以在征信的平台上，各路英雄将利用起自家数据各显神通。 如何能够在B2C或者P2P平台上也能够使用海量的互联网数据帮助完成征信的过程，是我们希望能够在互联网不断颠覆传统的今天看到的事。本项研究的目标就是通过收集到的用户历史行为数据建立预测用户信用评分的模型。本文的数据收集来自一家小贷公司，为了数据隐私考虑，本文将隐去一部分数据信息以及分析结果，但是不影响分析结果的客观性。 具体而言，我们称提供数据的公司为熊小贷（为公司匿名），熊小贷公司的业务包括2B业务和2C业务。公司已经通过自己的综合数据分析得到了对于2B和2C业务的统一用户信用得分，称为熊得分。但是熊得分广泛针对于公司所有的产品以及所有用户，使得对于特定产品的信用评分对于用户的违约情况预测精度并不高。熊小贷公司希望通过某款产品用户特有信息的数据支持，建立针对该特定产品的用户信用评分模型，预期目标是，通过数据支持建立起的该产品用户信用评分模型对于用户是否违约的预测精确度要高于通用的熊得分。直观了解公司的2B和2C业务可以从图3得出，公司包括针对不同用户的不同产品。例如熊分期是针对熊得分400以上用户的定制产品。在手机客户端下载APP“熊小贷”点击进入即可开始信用生活。之后再点击“熊分期”即可进入产品页面，后续会要求用户进行注册以及填写个人信息等内容。 图3 熊小贷公司产品概况 本案例中将针对已收集的用户信息，匹配公司数据库中的用户交易行为信息，以及已采用“熊分期”产品贷款的用户的违约情况，建立信用评分模型。这一信用模型将是从业务目标出发，结合业务背景而建立的。模型将预测用户的信用评分作为参考，并直接指导业务层面的是否可以对用户发放贷款。具体的数据采集以及变量清理过程将在第二章着重介绍，模型建立将在第三章进行介绍。 二、数据描述 1. 数据采集 熊小贷公司的某股东具有第三方支付渠道，故可以收集到独特的用户通过银行卡的交易信息，注册用户只要通过第三方支付渠道进行交易，就能够被记录在案，而通过用户的手机号，就可以搜索到用户的历史交易行为数据。这一数据源具有其独特的优势，直接避免了传统征信行业中用户自填数据存在的重大问题：a. 对个人或者企业进行评级刻画时维度较为单一，不能得到综合全面的立体式评级；b. 财务数据造假的可能性较大，例如工资自填，如果用户为了得到贷款审批而夸大了公司水平，小贷公司很难进行确认； c. 缺乏其他数据进行交叉验证，使得评级的可靠性降低。但是熊小贷公司采集到的数据则比传统数据丰富得多。熊小贷公司共采集了前文提及的产品“熊分期”对应近3万个注册用户信息，自注册APP起的总交易笔数近450万行，以及相对应的其他信息，这些交易信息所处的时间区间跨度超过一年。其中违约用户近1万个，非违约用户近2万个。值得注意的是，此样本为为模型建立抽取的抽样数据并不代表真实注册用户中的违约用户与非违约用户比例。 具体而言，熊小贷公司的“熊分期”产品所对应的注册用户数据结构主要包括以下四张表格，分别为交易事实表，用户信息表，商户分类信息表，以及银行卡信息表，它们之间的关系如图4所示。这四张表格分别通过不同的关键字连接，具体如下：a. 用户信息表可以通过用户手机号码和交易事实表对应，一个用户手机号码对应多个交易事实记录；b. 商户分类信息表可以通过商户编号和交易事实表对应，多个交易事实可能产生于一个商户编号；c. 银行卡信息表可以通过银行卡的前几位数字（称为卡标首）和交易事实表中的卡号对应。 四张表格中包含的具体变量信息分别如下： 用户信息表：手机号，注册渠道（手机APP注册，或者网站注册），身份证（提供数据已加密），提取数据前最近一次的登录时间，注册时间等。 商户分类信息表：创建时间，商户号，商户名称，商户1级分类代码，商户1级分类名称，商户2级分类代码，商户2级分类名称，商户3级分类代码，商户3级分类名称，商户4级分类代码，商户4级分类名称等。 银行卡信息表（银行卡卡片类型信息）：卡标首（银行卡的前几位数据，决定了属于哪一家银行的哪一种类型的卡片），首长度（例如前6位数字决定了卡片类型，则首长度为6），账户类型，账户类型名称（例如储蓄卡或者信用卡），卡代码，银行代码，银行名称等。 交易事实表：流水号（每一笔交易会被制定一个流水号码），交易手机号，交易时间，商户号，账单号，支付金额（支付金额等于账单金额加手续费的总和），账单金额，手续费等。 综上所述，上述四张表格可以通过不同的关键字分别进行匹配连接，从而能够对应到一个用户的多行交易信息在下一小节中，我们将详细阐述数据的变量提取过程。 图4 熊分期产品用户数据结构 2.数据清理变量采集 首先，按照关键字将所有四张表格合并。每一条观测以一个用户的一次交易行为为单位，包含用户个人信息，银行卡信息，商户分类信息等，总观测行数近450万条。直接处理并不是一个好的解决方法，这样产生的数据量太大，且不直观利于分析。我们的目标是将所有的信息汇总到每一个用户，定义关于用户的衍生变量。从而，我们分成两个方向的变量来刻画用户特征，分别是：用户基础信息变量以及用户分类信息变量。接下来，我们将进行详细的定义。 用户基础信息变量 此处生成的用户基础信息变量包括直接可以从数据中收集的变量以及衍生变量，如下所示： （1） 用户的熊得分：由于目标要对于原始的综合熊得分进行预测精度的改进，我们将“熊得分”作为解释变量纳入模型，“熊得分”是熊小贷公司的业务人员根据实际业务定义的指标体系从而生成的得分，我们认为其具备一定的业务经验以及背景知识，从而也具备较强的预测能力。熊得分越低，越有可能违约。 （2，3）用户性别，用户年龄：通过用户身份证号可以提取一个用户的性别，年龄信息，这些数据我们通过与熊小贷公司沟通完成，熊小贷公司并不直接提供用户的身份证号。获取的身份证信息均为加密过后的信息。 （4）用户的注册年龄：即用户从第一次有交易记录开始距离现在的时间长度，以天为单位。 （5）交易笔数：用户自从第一笔交易记录开始，被记录的交易总笔数。 （6）所有行为均值：用户被记录的所有交易行为的平均金额。 （7）所有行为最大值：用户自从第一笔交易开始，所有交易行为的金额的最大值。这个指标可以度量用户的极端行为情况。直观来看，极端行为越大的用户越有可能违约。 （8）借贷比率：用户所有行为中，采用贷记卡（或称为信用卡）交易的次数占所有采用贷记卡或者借记卡（或称为储蓄卡）交易次数的比率。这个指标衡量的是用户的交易习惯，例如，有些人习惯直接采用储蓄卡进行消费，而也有些人习惯每个月采用信用卡先进行消费，到了还款日再按时还款，这样既可以消费，又不影响每个月的理财计划。这两种消费习惯的用户群体不同。 （9）银行卡数：我们每个人需要的银行卡数并不多，银行卡太多的用户与正常用户群体不同。例如，如果需要多张信用卡消费，但是却还不上信用卡的人可能更容易违约。 通过以上信息，我们定义了以上这9个用户基础信息变量。 用户分类信息变量与RFM模型 在征信问题中，我们通常会面临一个典型的问题，在本案例中也是如此，当一个用户对应多条行为关系，如何按照一套既定的指标体系汇总到个人呢？在本案例中，每一个用户对应的交易记录小至几十条，大至一千条，通过交易事实对应到商户分类信息表格，我们可以将其归之为不同类型的交易，例如在超市购物，购买游戏点卡，交水电煤气费用等。对应每一种类别的行为，都需要通过一种标准化的方式计算对应每一个人的变量，来衡量对应一系列行为的人的特征。为此，我们引入营销学中经常采用的RFM模型（参见美国数据库营销研究所Arthur Hughes的研究）。 图5 RFM模型图示 在营销领域，RFM模型是用来衡量客户的价值和客户的创利能力的重要工具和手段。这个模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱三项指标来描述该客户的综合价值，具体如下： R（Recency），最近一次消费，指上一次购买的时间到现在的距离。理论上，上一次消费时间越近的用户应该是相对而言活跃的用户。因此这些用户对于提供即时的商品或是服务也最有可能产生反应。而我们通常也会发现，对于0到6个月用户收到营销人员的沟通信息会多于31至36个月的用户。 F（Frequency），消费频率，即用户在限定的期间内产生购买的总次数。最常产生购买的用户，忠诚度最高的用户。 M（Monetary），某个用户所有消费金额的平均值。通过这一指标可以验证“帕雷托法则”(Pareto’s Law)，也就是说公司80%的收入来自20%的顾客。 在此案例中，我们将重新定义这三个指标，并借助这三个指标来概括用户所产生一类行为的特征，如表格1所示。而由于这三个指标并不能够衡量用户产生行为的波动性，所以我们增加一个指标S（Standard Deviation）来衡量用户行为的波动性。例如，对于购买游戏点卡类行为，我们可以定义R为用户最近一次购买游戏点卡距离数据提取时间的时间间隔，F定义为一年内用户购买游戏点卡的次数（考虑到用户注册时间不一样，此处采用的频数需要采用用户年龄进行标准化，即总次数除以用户年龄。用户年龄的定义将在下文中详述），M定义为一年内用户每次购买游戏点卡的平均金额，S定义为用户每次购买游戏点卡金额的标准差。我们将所有变量记作类别名称加指标简称的形式，例如游戏R。 表1 RFM指标定义 指标简称 指标定义 R 一年内用户最后一次产生某类行为距离提取数据的时间 F 用户在一年内产生某次行为的频数 M 用户在一年内产生某类行为的平均金额 S 一年内该类行为产生金额的标准差 用户行为的分类通过银行卡信息表，以及商户分类信息表，根据业务场景，我们提取了以下类别，每个类别都对应着以上我们已经定义好的RFMS四个指标。类别包括： （1）借记类：刻画用户使用储蓄卡的交易行为。不同的用户习惯不同，采用储蓄卡和信用卡的倾向也可能不同。 （2）消费类：刻画用户的日常消费行为。日常消费行为的金额以及频次不同，用户的还款能力可能不同。 （3）信贷类：刻画用户之前的小额贷款类行为。用户之前如果有其他消费贷款类行为可能已经习惯进行消费贷款，从而可能具备更良好的信用状况。 （4）转账类：刻画用户的转账行为。经常通过熊小贷APP转账的用户可能与不转账的用户行为不同。 （5）话费类：刻画用户的话费充值交易行为。话费充值是否规律与充值金额多少都可能意味着用户群体不同。 （6）公缴类：刻画用户交水，电，煤气费等交易行为。公缴费用的多少与是否规律也可能说明用户群体的不同。 （7）游戏类：刻画用户购买游戏点卡的行为。经常玩游戏的用户群体可能与不玩游戏的人不同。 （8，9）四大行卡类以及中型银行卡类：四大行包括中国银行，中国农业银行，中国工商银行，中国建设银行，中型银行包括招商银行，浦发银行，兴业银行，平安银行等。这个指标的设定有以下两方面原因：a. 不同公司的工资卡不同，小型创业公司一般采用中型银行的银行卡；b. 四大行的信用卡发放较为保守，所以能够申请到四大行信用卡的人可能和采用其他银行信用卡的用户群体不同。 （10）白金及金卡类：通过卡标首可以对应到银行卡是属于哪家银行的哪种类型的卡，例如招商银行的金葵花卡。我们搜索整理了相应银行的金卡和白金卡卡种名称，并对应到每一个用户。我们初步认为，拥有白金卡和金卡的用户具备更高的还款能力，所以用户群体不同。 综上所述，我们一共提取了10个类别，每个类别4个指标，共计40个分类信息变量来全方位立体的刻画一个用户的全部交易行为。 数据预处理与数据汇总 为了数据质量考虑，我们去掉“用户注册年龄”小于10天的用户，原因在于，这一部分用户的观察行为较少，并不足以代表稳定的用户自身特征。另外，在对数据进行描述分析后，模型建立之前，我们将所有连续数据做对数处理，并进行标准化。由于所有样本均来自于交易行为的归纳汇总，故只要用户有交易行为，分类信息变量就不存在缺失，故此处不需要缺失数据填补。 最终，我们的数据共包括28816个用户，其中违约用户为9115个，非违约用户19701个；结合9个用户基础信息变量，我们的数据共包括49个解释性变量，这49个变量也包含着对于业务的理解和思考。这对于刻画用户的所有行为而言，只是初步的探索和尝试，但是相较于只采用用户自填信息进行建模而言，已经更为综合和全面。 三、数据建模 在建模部分，我们将先通过几个变量的基础描述分析，以说明变量的特征，继而通过建立逻辑回归模型对于预测效果进行阐述。 1. 描述性分析 出于数据隐私考虑，我们只针对于其中6个变量作箱线图分析。箱线图能提供有关数据位置和分散情况的关键信息，尤其在比较不同的总体数据时更可表现其差异。此处我们通过对比箱线图对数据进行分析。我们用 0表示违约用户， 1表示非违约用户。 （1）熊得分与是否违约。从箱线图可以看出，非违约用户与违约用户的箱线图有明显的差别，这表明熊得分对于违约与非违约用户具有较好的区分度。 （2）交易笔数与是否违约。从箱线图可以看出，非违约用户较违约用户而言，交易笔数更高。 （3）用户所有行为均值与是否违约。从箱线图可以看出，非违约用户与违约用户相比较，所有行为金额的均值较高。 （4）借记卡F与是否违约：通过箱线图可以得出，非违约用户的借记卡F平均高于违约用户。表明非违约用户借记卡的使用频数更高。 （5）四大行M与是否违约：通过箱线图得出，非违约用户的四大行卡的行为平均值较高，这说明非违约用户更多使用四大行的银行卡。 （6）信贷R与是否违约：从图中能够得出结论，非违约用户通过APP产生借贷行为距离现在的日期，相对于违约用户而言较近。 通过以上描述分析，我们已经能够观察到在所提取特征中违约用户与非违约用户的不同，通过回归分析，我们将进一步说明通过变量特征的设定带来的预测效果的提升。 图6 典型变量箱线图 2.模型设定及估计结果 本案例中采用逻辑回归进行建模，原因在于，根据逻辑回归结果，我们能够直观看到每一个变量对于因变量是否违约的作用大小，有利于系数解释。但由于变量过多，我们很难对于所有变量进行符合预期的系数解释，进一步地，我们采用BIC的方法选择模型。相比较于AIC而言，BIC模型选择的方法选择的变量个数较少，更有利于模型的估计系数解释。 BIC选模型的估计系数结果如表2所示，由于系数较多，我们省去估计量的估计误差和P值的具体数值，只用“*”标注P值的大小。但注意，当回归系数过多时我们难以直观展示系数结果，为此，我们根据估计系数的正负将变量分类，再按照绝对值的大小排序整理得到图7和图8。由于模型主要分析“熊得分”之外其他变量带来的预测效果的提升，我们只绘制除“熊得分”之外其他变量的系数。 我们并不逐一解释每一个系数的大小及其含义。值得注意的是，从图7中我们可以总结归纳得出非违约用户的特征，从图8中可以总结归纳出违约用户的特征，通过这两幅图已经可以总结出较为直观的对于违约与非违约用户的理解。我们通过进一步观察可以直观得出的结论包括但不限于： 在其他变量控制不变的情况下，借贷比例越高的用户违约可能性越低，这与银行的信用卡额度提升相似，对于经常使用信用卡并按时还款的客户更有可能被银行认为信用良好，从而希望改用户提高信用额度，而如果不经常使用信用卡则无法判断。从而借贷比例越高，表明用户越习惯使用信用卡，进一步越有可能是信用良好的用户。另一方面，用户申请银行的信用卡需要通过银行的信用评估，故有信用卡的用户较没有信用卡的用户会有所不同。 表2 BIC选模型回归系数结果表 变量名 估计值 p值 变量名 估计值 p值 熊得分 26.301 *** 转账R -0.336 *** 借贷比率 1.752 *** 年龄 -0.254 *** 借记卡F 0.438 *** 公缴R -0.230 用户所有行为均值 0.412 *** 四大F -0.175 *** 交易笔数 0.136 *** 四大R -0.168 *** 借记卡M 0.083 *** 中型F -0.122 *** 中型M 0.016 *** 中型R -0.119 ** 四大M 0.014 * 消费F -0.096 ** 信贷R -0.922 *** 金卡F -0.083 *** 银行卡数 -0.674 *** 转账M -0.082 *** 信贷F -0.633 *** 公缴M -0.062 ** 用户所有行为最大值 -0.474 *** 游戏M -0.059 * 转账F -0.386 *** 信贷S -0.035 *** 公缴F -0.370 * 注：***表示P&amp;lt;0.01，**表示0.01≤P&amp;lt;0.05，*表示0.05≤P&amp;lt;0.1。 借记卡F越大用户违约可能性越低。这表明其他变量不变，用户使用储蓄卡频数越高，越可能是信用良好的用户。 在其他变量保持不变的情况下，用户每一次交易行为的平均值越大越可能是非违约用户。 控制其他变量不变，信贷R越小越可能是非违约用户。这一点与之前观察的箱线图吻合，越近产生消费借贷行为的人，越有可能是非违约用户。注意这里产生的借贷行为与Y对应的是否违约并非同一次借贷行为。 其他变量水平不变，银行卡数越多，越有可能是违约用户。 保持其他变量不变，用户行为最大值越大，越可能是违约用户。这也验证了我们之前的结论，用户的极端行为越极端，越有可能是违约用户。 图7 BIC选模型结果正系数 图8 BIC选模型结果负系数 3. 模型预测结果 我们将对比以下三种模型的预测效果，因变量Y相同，为用户是否违约（ 1，用户非违约， 0，用户违约）：a. 只根据“熊得分”建立的逻辑回归模型；b. 用所有49个变量，包含用户基础信息变量与用户分类信息变量建立的逻辑回归模型；c. 在b模型的基础上通过BIC模型选择方法建立的模型。 衡量模型的预测效果可以采用指标ROC（Receiver Operating Characteristic）曲线或者AUC（Area Under Curve）值。其中ROC曲线的横坐标为false positive rate（FPR），也称为Specificity，刻画的是模型预测错了，认为为1但真实为0的观测占所有真实为0的观测的比例。纵坐标为true positive rate（TPR），也称为Sensitivity，刻画的是模型预测所识别出的为1且真实为1的观测占所有真实为1的观测的比例。ROC曲线越贴近左上角，表明模型的预测效果越好。AUC是ROC曲线下的面积，这一指标取值越大表明模型预测的效果越好。通常在计算中也可以考虑如下公式： 其中 表示 的真实取值为0的集合， 表示 的真实取值为1的集合， 表示 真实取值为0的观测个数， 表示 真实取值为1的观测个数。这一指标可以理解为，真实为1的 的预测非违约可能性不小于真实为0的 的预测非违约可能性的比例。 为了模型对比，我们随机将所有数据划分为训练数据集（80%）和测试数据集（20%）， 在训练集上估计模型的回归系数，将所有系数带入测试数据中进行计算，预测非违约可能性。随机拆分被重复了100次。我们随机抽取其中一次绘制ROC曲线如图9所示。其中，Score表示模型a的预测结果，模型中只包含“熊得分”，Full model表示模型b对应的预测结果，BIC表示模型c对应的预测结果。从图中可以得出结论，在b模型与c模型的预测效果接近，二者都要远远好于a模型的预测效果。进一步地，我们将100次随机拆分计算得到的AUC值取平均。出于行业数据机密，我们此处不汇报AUC的绝对提升结果。但是相对于 a模型而言，b模型和c模型的预测效果将相对提升13.6%。这将直接使得我们能够在实际业务中更精准的判断出用户的信用状况。 图9 回归模型对比ROC曲线 四、业务实施 对于小贷公司而言，收益是靠收益率体现，而成本则体现在坏账率。也就是说对于现在的市场存在收益和成本的不对等。所以才会有P2P公司如雨后春笋般出现，为了在这个利润丰腴的市场分一杯羹。现在的小贷公司可以通过较为严苛的指标筛选用户，从而保证违约率一直维持在较低的水平，也就是说，现在的网络借贷公司也许并不需要借助纷繁复杂的技术手段就可以获得较高的收益。但是随着市场的饱和，越来越多的公司进入竞争的行列，在可预见的将来，我们可以看到违约率的提高，收益率的下降。到了那时候，拼的不再是跨入市场门槛的勇气，而是谁能够通过技术实力真正的选准用户，稳定住坏账率这一指标。 在业务层面，首先，我们需要通过更多数据来验证以上模型结果真实可靠。通过熊小贷公司提供的不同数据，我们已经验证了这一结果。而通过以上数据建立模型的预测结果可以直接进行以下两方面的工作： 利用预测非违约率来辅助判断是否应该批准用户贷款，即进行用户选择。例如，同样的两个用户申请贷款，A用户的预测非违约率为0.85，B用户的预测非违约率为0.27。那么在资金预算有限的条件下，如果只能批准一个用户进行贷款发放，通过模型就会选择发放给非违约可能性更高的用户A用户。也就是说，通过模型的预测能够帮助我们筛选发放贷款的目标用户。而精准的预测将能够降低我们的贷款成本。 利用预测概率改进APP中的熊得分。例如，通过线性变换可以将预测概率P转化为400至800的用户得分Q，Q=400&#43;400×P。从而能够更新平台上的熊得分，使得熊得分更加准确。 另外，本案例中的模型证实了用户的交易记录数据对于征信模型的重要作用，对于业务上拓展数据源也有重要的指导意义。 五、总结讨论 本文针对互联网征信背景下的信用评估模型进行了探讨，通过具体的案例证明了用户历史行为数据对于用户的信用评估具有重要作用。这其中最值得注意的是，业务背景对于指导建立模型具有不可替代的作用，所有的变量产生都应该建立在对于业务背景知识的透彻了解之上。另外，关于本案例的研究有以下可能的改进方向： 在数据中我们只考虑违约情况为01变量，即，违约与非违约。其中，如果用户违约天数大于等于7天被定义为违约，否则为非违约。如果能够记录用户真实的违约天数作为连续变量，将可能对于模型预测有进一步的帮助。 在本案例中收集的数据只是通过某特定第三方支付平台的所有交易数据，并不能代表用户的所有银行卡交易状况，如果能够收集到用户的所有数据，则将对于用户的交易行为做出更全面客观的刻画。 从本案例的研究中可知，不同平台的数据对于信用评估可能有不同的优势。从央行下发牌照的8家机构看来，不同机构的数据源不同，各具特色，于是如何能够将不同数据源的数据进行统一后，建立综合的信用评估模型，是值得深入探讨的问题。例如，一个用户可能在京东平台和淘宝平台上的信用得分不一样，这与用户的平台偏好有关，但是如果能够根据不同平台的结果综合对用户进行信用评估，则将得到更准确的结果。 参考文献 [1] 陈文. 网络借贷与中小企业融资[M]. 经济管理出版社, 2014. [2] 胡援成, 卢华征. 中小企业融资的调查与思考[J]. 管理世界, 2003(10):11-13. [3] 李焰, 高弋君, 李珍妮,等. 借款人描述性信息对投资人决策的影响——基于P2P网络借贷平台的分析[J]. 经济研究, 2014(S1):143-155.]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[你真的理解 Python 中的 MRO 算法吗？z]]></title>
    	<url>/tech/2016/05/22/mro/</url>
		<content type="text"><![CDATA[[原文地址：http://xymlife.com/2016/05/22/python_mro/ Python: 多继承模式下 MRO(Method Resolution Order) 的计算方式 感觉表达式有问题， C3，所以不再看“计算方式”一文。 前言 MRO（Method Resolution Order）：方法解析顺序。 Python 语言包含了很多优秀的特性，其中多重继承就是其中之一，但是多重继承会引发很多问题，比如二义性，Python 中一切皆引用，这使得他不会像 C&#43;&#43; 一样使用虚基类处理基类对象重复的问题，但是如果父类存在同名函数的时候还是会产生二义性，Python 中处理这种问题的方法就是 MRO。 历史中的 MRO 如果不想了解历史，只想知道现在的 MRO 可以直接看最后的 C3 算法，不过 C3 所解决的问题都是历史遗留问题，了解问题，才能解决问题，建议先看历史中 MRO 的演化。 Python2.2以前的版本：经典类（classic class）时代 经典类是一种没有继承的类，实例类型都是type类型，如果经典类被作为父类，子类调用父类的构造函数时会出错。这时 MRO 的方法为DFS（深度优先搜索（子节点顺序：从左到右））。 Class A: # 是没有继承任何父类的 def __init__(self): print &amp;quot;这是经典类&amp;quot; inspect.getmro(A)可以查看经典类的 MRO 顺序 import inspect class D: pass class C(D): pass class B(D): pass class A(B, C): pass if __name__ == &#39;__main__&#39;: print inspect.getmro(A) &amp;gt;&amp;gt; (&amp;lt;class __main__.A at 0x10e0e5530&amp;gt;, &amp;lt;class __main__.B at 0x10e0e54c8&amp;gt;, &amp;lt;class __main__.D at 0x10e0e53f8&amp;gt;, &amp;lt;class __main__.C at 0x10e0e5460&amp;gt;) MRO 的 DFS 顺序如下图： DFS 两种继承模式在DFS下的优缺点 正常继承模式，两个互不相关的类的多继承，这种情况 DFS 顺序正常，不会引起任何问题； 棱形继承模式，存在公共父类（D）的多继承（有种D字一族的感觉），这种情况下 DFS 必定经过公共父类（D），这时候想想，如果这个公共父类（D）有一些初始化属性或者方法，但是子类（C）又重写了这些属性或者方法，那么按照 DFS 顺序必定是会先找到D的属性或方法，那么C的属性或者方法将永远访问不到，导致C只能继承无法重写（override）。这也就是为什么新式类不使用 DFS 的原因，因为他们都有一个公共的祖先object。 Python2.2版本：新式类（new-style class）诞生 为了使类和内置类型更加统一，引入了新式类。新式类的每个类都继承于一个基类，可以是自定义类或者其它类，默认承于object。子类可以调用父类的构造函数。 这时有两种 MRO 的方法1. 如果是经典类 MRO 为 DFS（深度优先搜索（子节点顺序：从左到右））。2. 如果是新式类 MRO 为 BFS（广度优先搜索（子节点顺序：从左到右））。 Class A(object): # 继承于object def __init__(self): print &amp;quot;这是新式类&amp;quot; A.__mro__ # 可以查看新式类的顺序 MRO 的 BFS 顺序如下图： BFS 两种继承模式在 BFS 下的优缺点 正常继承模式，看起来正常，不过实际上感觉很别扭，比如B明明继承了D的某个属性（假设为foo），C中也实现了这个属性foo，那么 BFS 明明先访问B然后再去访问C，但是为什么foo这个属性会是C？这种应该先从B和B的父类开始找的顺序，我们称之为单调性。 棱形继承模式，这种模式下面，BFS 的查找顺序虽然解了 DFS 顺序下面的棱形问题，但是它也是违背了查找的单调性。因为违背了单调性，所以 BFS 方法只在Python2.2中出现了，在其后版本中用C3算法取代了 BFS。 Python2.3 到 Python2.7：经典类、新式类和平发展 因为之前的 BFS 存在较大的问题，所以从 Python2.3 开始新式类的 MRO 取而代之的是 C3 算法，我们可以知道 C3 算法肯定解决了单调性问题，和只能继承无法重写的问题。C3 算法具体实现稍后讲解。 MRO 的 C3 算法顺序如下图：看起简直是 DFS 和 BFS 的合体有木有。但是仅仅是看起来像而已。 左边类 DFS，右边类 BFS Python3 到至今：新式类一统江湖 Python3 开始就只存在新式类了，采用的 MRO 也依旧是 C3 算法。 C3 算法解决了单调性问题和只能继承无法重写问题，在很多技术文章包括官网中的 C3 算法，都只有那个 merge list 的公式法，想看的话网上很多，自己可以查。但是从公式很难理解到解决这个问题的本质。我经过一番思考后，我讲讲我所理解的 C3 算法的本质。如果错了，希望有人指出来。假设继承关系如下(官网的例子)： class D(object): pass class E(object): pass class F(object): pass class C(D, F): pass class B(E, D): pass class A(B, C): pass if __name__ == &#39;__main__&#39;: print A.__mro__ 首先假设继承关系是一张图（事实上也是），我们按类继承是的顺序（class A(B, C)括号里面的顺序B, C），子类指向父类，构一张图。 我们要解决两个问题：单调性问题和不能重写的问题。 很容易发现要解决单调性，只要保证从根(A)到叶(object)，从左到右的访问顺序即可。 那么对于只能继承，不能重写的问题呢？先分析这个问题的本质原因，主要是因为先访问了子类的父类导致的。那么怎么解决只能先访问子类再访问父类的问题呢？如果熟悉图论的人应该能马上想到拓扑排序，这里引用一下百科的的定义: 对一个有向无环图(Directed Acyclic Graph, 简称 DAG)G进行拓扑排序，是将G中所有顶点排成一个线性序列，使得图中任意一对顶点u和v，若边(u,v)∈E(G)，则u在线性序列中出现在v之前。通常，这样的线性序列称为满足拓扑次序(Topological Order)的序列，简称拓扑序列。简单的说，由某个集合上的一个偏序得到该集合上的一个全序，这个操作称之为拓扑排序。 因为拓扑排序肯定是根到叶（也不能说是叶了，因为已经不是树了），所以只要满足从左到右，得到的拓扑排序就是结果，关于拓扑排序算法，大学的数据结构有教，这里不做讲解，不懂的可以自行谷歌或者翻一下书，建议了解完算法再往下看。 那么模拟一下例子的拓扑排序：首先找入度为 0 的点，只有一个A，把A拿出来，把A相关的边剪掉，再找下一个入度为0的点，有两个点(B,C)，取最左原则，拿B，这是排序是AB，然后剪B相关的边，这时候入度为0的点有E和C，取最左。这时候排序为ABE，接着剪E相关的边，这时只有一个点入度为0，那就是C，取C，顺序为ABEC。剪C的边得到两个入度为0的点（DF），取最左D，顺序为ABECD，然后剪D相关的边，那么下一个入度为0的就是F，然后是object。那么最后的排序就为ABECDFobject。 不理解：这个过程的算法由什么语言完成呢？ 对比一下 A.__mro__的结果 (&amp;lt;class &#39;__main__.A&#39;&amp;gt;, &amp;lt;class &#39;__main__.B&#39;&amp;gt;, &amp;lt;class &#39;__main__.E&#39;&amp;gt;, &amp;lt;class &#39;__main__.C&#39;&amp;gt;, &amp;lt;class &#39;__main__.D&#39;&amp;gt;, &amp;lt;class &#39;__main__.F&#39;&amp;gt;, &amp;lt;type &#39;object&#39;&amp;gt;) 完全正确！本应该就这里完了，但是后期一些细心的读者还是发现了问题。以上算法并不完全正确。感谢@Tiger要好好写论文指出。 下面我们来看看这个问题：Tiger 指出了两点，一点是图中左右顺序比较难区分，还有一点是某种不可序列化的情况下，我的算法会有一些问题，针对这两点我做了改进。 先来看看出错的情况： class A(object): pass class B(object): pass class C(A, B): pass class D(B, A): pass class E(C, D): pass 构成对应的图，如下其中橙色的线是改进的地方。 如果使用原来的算法，我们搞不清楚A和B谁在左边谁在右边，所以会选择其中之一，继续拓扑下去，其实这里已经是有歧义了不能够解析出正确的顺序，应该报错，这使我重新思考了左右的问题。 我们可以发现其中左右问题无非出现在两种情况，第一种情况是：图中E先继承C，再继承D；第二种情况是：先继承C的基类，再去继承D。针对这两种情况给出的方案就是图中添加的橙色的边，表示的是第一种情况的顺序问题，比如C-&amp;gt;D，就是表示E(C,D&amp;gt;中的继承顺序。 那么第二种情况怎么保证先C的基类，然后再考虑D呢。我们可以这么做，如果出现多个入度为 0 的点，我们先找是刚刚剪出来的点的基类的点。这里可以看之前官网的那个例子，在E点和C点选择的时候，因为E是B的基类点，所以先选它，其实这也很容易实现，只需要记录下每个节点的子类点（可能有多个）。 那么左右的问题也就解决了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[使用 Hexo 与 NexT 搭建博客的避坑总结 z]]></title>
    	<url>/tech/2016/05/08/hexo-experiences/</url>
		<content type="text"><![CDATA[[1 Hexo 2 NexT 3 主题编写 原文地址：http://yangfch3.com/2016/05/08/hexo-experiences/ 这个博客是使用 Hexo 与 NexT 搭建起来的，在搭建的过程中遇到了一些莫名其妙的问题，记录在此。对于官方文档已经有介绍的，在此不表。 1 Hexo Hexo 是有中文文档的，当然更推荐看英文文档 关于几个文件夹功能的释义 scaffolds为模板文件夹，模板用于预先定义 new 出来的文章的 layout； source内_drafts文件夹用于放置草稿，使用hexo --draft可查看所有草稿文章；草稿完稿后可以使用publish命令发布到_post文件夹内，等待generate。 自定义的其他模板和 post 模板相同，new 出来的文章都将储存到source/_posts文件夹 当你不想你的文章被发布，同时又不想删除文章，将文章的Front-Matter中的layout:设为false 文章的Front-matter采用 YAML 语法格式，多个标签与分类不是用逗号隔开，而是采用 YAML 中的列表/并列写法 Hexo 支持更强大的quote块：可以添加引用作者、来源、链接等；在有这些需求的时候可以引用 {% blockquote [author[, source]] [link] [source_link_title] %} content {% endblockquote %} Hexo 支持更强大的 code 块：支持为 code 块添加标题和链接；当我们需要引用某个链接内的代码时可以使用，一般情况下使用 md 的代码块语法即可 {% codeblock [title] [lang:language] [url] [link text] %} code snippet {% endcodeblock %} 引入 gist 时，插入fileName似乎会失败，所以，引入 gist 时只需要使用gist hash-id即可 {% gist 5b3ee7efd535ab63cd56 %} Hexo 支持更使用特定的语法，插入指定大小的图片，如下： // 语法 {% img [class names] /path/to/image [width] [height] [title text [alt text]] %} // 实例 {% img full-image /hexo-experiences/PL01.jpg 180 180 hello %} // 生成的代码 &amp;lt;img src=&amp;quot;/blog/hexo-experiences/PL01.jpg&amp;quot; class=&amp;quot;full-image&amp;quot; width=&amp;quot;180&amp;quot; height=&amp;quot;180&amp;quot; title=&amp;quot;hello&amp;quot;&amp;gt; 值的注意的几点： - 路径名必须以`/`开始，否则会解析出错 - 路径是相对于`conifg`内的`root`的，这一点挺坑，可以在`source/`下新建一个`uploads`文件夹用于专门放置这些图片资源 {\% img hi /uploads/images/test.jpg 100 100 hello hello %} - 图片宽高只能使用数值，不能包含字符串，也不能是百分数 - 最后一个字段可以为图片添加标题 引入某个文件中的代码，使用include_code // 语法 {% include_code [title] [lang:language] path/to/file %} // 实例 {% include_code DOMUtil lang:javascript demo.js %} 值得注意的是：code代码所在的文件必须在downloads/code/目录下，否则无法获取 引用其他文章的路径，基本功能不大 // 语法 {% post_path slug %} // 实例 {% post_path OS-Brief-Intro %} // &amp;gt;&amp;gt; /blog/2016/05/03/OS-Brief-Intro/ 引用其他文章的链接，用处很大，但是有一个小坑 // 语法 {% post_link slug [title] %} // 实例 {% post_link OS-Brief-Intro 操作系统 %} 小坑：不能放在一段的段首，否则 md 文档或解析错误，出现莫名奇妙的 bug。 引用文章的资源：获取到的是文章对应asset目录下的资源 // 语法 {% asset_path slug %} {% asset_img slug [title] %} {% asset_link slug [title] %} // 实例 {% asset_path 01.png %} {% asset_img 01.png 图片 %} {% asset_link 01.png 图片 %} 将_config.yml文件中的post_asset_folder选项设为true，便可在new一篇新文章的同时创建对应的资源文件夹。引用资源文件夹内的文件请使用13中使用的方法，可以防止首页展示时链接错误的问题。 链接：数据文件 hexo generate --watch可以监听文件变动，自动generate hexo g -f可以强制重新生成，防止一些更改后无法generate的清理 自动提交脚本：deploy.sh hexo generate -f echo &amp;quot;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;What is your commit message to blog-material repo?&amp;quot; read COMMIT1 git add --all git commit -m &amp;quot;$COMMIT1&amp;quot; echo &amp;quot;commited&amp;quot; git push echo &amp;quot;pushed all to blog-material repo&amp;quot; cd public echo &amp;quot;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;What is your commit message to blog repo?&amp;quot; read COMMIT2 git add --all git commit -m &amp;quot;$COMMIT2&amp;quot; echo &amp;quot;commited&amp;quot; git push echo &amp;quot;pushed all to blog repo&amp;quot; cd ../ 实现 RSS 订阅：hexojs/hexo-generator-feed Hexo 的markdown解析引擎不支持脚注，可以使用插件实现。 但是笔者在使用了LouisBarranqueiro/hexo-footnotes之后发现hexo server命令无法使用了。（PH：本人是正常的） 更换默认的 md 渲染引擎（hexo-renderer-marked），改为 hexo-renderer-markdown-it，见配置hexo-renderer-marked-it。几大优点： 支持脚注解析 支持上下标 支持emoji – 需要额外配置 2 NexT NexT 的一些菜单页（如：标签页、分类页、归档页）需要自己添加，方法见链接 NexT 的i18n可以在theme/next/language下的.yml文件下自己定制 NexT 支持文本居中的引用 {% centerquote %}blah blah blah{% endcenterquote %} {% cq %} blah blah blah {% endcq %} NexT 中的图片可以自由地突破容器宽度的限制（扩大26%） {% fullimage /image-url, alt, title %} {% fi /image-url, alt, title %} 在为文章创建 Tags 的时候，避免标签内出现&amp;amp;，否则生成的.xml文件在浏览器端会解析错误，并且订阅功能也会出现故障。 3 主题编写 Hexo 提供的辅助函数中截取一段长文字的前n个字符 // swig 的写法 {% truncate(&amp;#39;long string&amp;#39;, {length: n}) %} // 实例 {% truncate(post.description, {length:n}) %} 在 swig 中，胡子或胡子百分号内不能再使用胡子或胡子百分号 DIY 主题，写法参考链接]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[矩阵求导 z]]></title>
    	<url>/prof/2016/04/21/matrix-diff/</url>
		<content type="text"><![CDATA[[矩阵求导好像读书的时候都没学过，因为讲矩阵的课程上不讲求导，讲求导的课又不提矩阵。如果从事机器学习方面的工作，那就一定会遇到矩阵求导的东西。 根据\(y,\pmb{y},Y\)与\(x,\pmb{x},X\)的不同类型（实值，向量，矩阵），给出了具体的求导公式，以及一堆相关的公式，查起来都费劲。 矩阵求导 类型 单值 向量 矩阵 单值 \(\partial y/\partial x\) \(\partial y/\partial x\) \(\partial \pmb{y}/\partial Y\) 向量 \(\partial y/\partial \pmb{x}\) \(\partial \pmb{y}/\partial \pmb{x}\) 矩阵 \(\partial y/\partial X\) 其实在实际的机器学习工作中，最常用到的就是实值函数\(y\)对向量\(\pmb{x}\)的求导，定义如下（其实就是\(y\)对向量\(\pmb{x}\)的每一个元素求导）： \[ \frac{\partial y}{\partial \pmb{x}}= \begin{bmatrix} \dfrac{\partial y}{\partial x_1}\\ \dfrac{\partial y}{\partial x_2}\\ \vdots\\ \dfrac{\partial y}{\partial x_n} \end{bmatrix} \] 实值函数\(y\)对矩阵\(X\)求导也类似： \[ \frac{\partial y}{\partial X}= \begin{bmatrix} \dfrac{\partial y}{\partial x_{11}} &amp;amp; \dfrac{\partial y}{\partial x_{12}}&amp;amp; \cdots &amp;amp;\dfrac{\partial y}{\partial x_{1n}}\\ \dfrac{\partial y}{\partial x_{21}} &amp;amp; \dfrac{\partial y}{\partial x_{22}}&amp;amp;\cdots &amp;amp;\dfrac{\partial y}{\partial x_{2n}}\\ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\ \dfrac{\partial y}{\partial x_{n1}} &amp;amp; \dfrac{\partial y}{\partial x_{n2}}&amp;amp;\cdots &amp;amp;\dfrac{\partial y}{\partial x_{nn}}\\ \end{bmatrix} \] 因为有监督的机器学习的一般套路是给定输入\(\pmb{x}\)，选择一个模型\(f\)作为决策函数，由\(f(\pmb{x})\)预测出\(\hat{y}\)。而要得到\(f\)的参数\(\theta\)，需要定义一个 loss 函数来定义当前的预测值\(\hat{y}\)和实际值\(y\)之间的接近程度，模型学习的过程就是求使得 loss 函数\(L(f(\pmb{x}),y)\)最小的参数\(\theta\)。这是一个最优化的问题，实际应用中都是用和梯度相关的最优化方法，如梯度下降，共轭梯度，拟牛顿法等等。为方便推倒有以下公式： \[ \frac{\partial \beta^T\pmb{x}}{\partial \pmb{x}} =\beta, \quad\frac{\partial \pmb{x}^T\pmb{x}}{\partial \pmb{x}} =2\pmb{x}, \quad\frac{\partial \pmb{x}^T A\pmb{x}}{\partial \pmb{x}} =(\mathbf{A}&#43;\mathbf{A}^T)\pmb{x} \] 其实只要掌握上面的公式，就能搞定很多问题了。 为了方便推导，下面列出一些机器学习中常用的求导公式，其中 andrew ng 那一套用矩阵迹的方法还是挺不错的，矩阵的迹也是实值的，而一个实数的迹等于其本身，实际工作中可以将 loss 函数转化成迹，然后再求导，可能会简化推导的步骤。 \[\text{tr}(a)=a\] \[\text{tr}(AB)=\text{tr}(BA)\] \[\text{tr}(ABC)=\text{tr}(CAB)=\text{tr}(BCA)\] \[\frac{\partial{\text{tr}(AB)}}{A}=B^T\] \[\text{tr}(A)=\text{tr}(A^T)\] \[\frac{\partial{\text{tr}(ABA^TC)}}{A}=CAB&#43;C^TAB^T\] 以上只是一些最基本的公式，能够解决一些问题，主要是减少大家对矩阵求导的恐惧感。关于矩阵方面的更多信息可以参考上面的 wiki 链接以及《Matrix cookbook》。 参考资料 未完成工作：1, 3, 4 中的内容还有待进一步整理，事实上整理完成 1 中的内容就完全足够，但第 4 项中关于 Notation 的总结也不错。 https://en.wikipedia.org/wiki/Matrix_calculus http://blog.sina.com.cn/s/blog_8eac0b290101fsqb.html http://www.voidcn.com/blog/liuuze5/article/p-5037874.html http://blog.csdn.net/u012045426/article/details/52343676]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[mxnet：结合R与GPU加速深度学习 z]]></title>
    	<url>/data/2016/04/07/mxnet-r/</url>
		<content type="text"><![CDATA[[近年来，深度学习可谓是机器学习方向的明星概念，不同的模型分别在图像处理与自然语言处理等任务中取得了前所未有的好成绩。在实际的应用中，大家除了关心模型的准确度，还常常希望能比较快速地完成模型的训练。一个常用的加速手段便是将模型放在GPU上进行训练。然而由于种种原因，R语言似乎缺少一个能够在GPU上训练深度学习模型的程序包。 DMLC（Distributed (Deep) Machine Learning Community）是由一群极客发起的组织，主要目标是提供快速高质量的开源机器学习工具。近来流行的boosting模型xgboost便是出自这个组织。最近DMLC开源了一个深度学习工具mxnet，这个工具含有R，python，julia等语言的接口。本文以R接口为主，向大家介绍这个工具的性能与使用方法。 一、五分钟入门指南 在这一节里，我们在一个样例数据上介绍mxnet的基本使用方法。目前mxnet还没有登录CRAN的计划，所以安装方法要稍微复杂一些。 如果你是Windows/Mac用户，那么可以通过下面的代码安装预编译的版本。这个版本会每周进行预编译，不过为了保证兼容性，只能使用CPU训练模型。 install.packages(&amp;quot;drat&amp;quot;, repos=&amp;quot;https://cran.rstudio.com&amp;quot;) drat:::addRepo(&amp;quot;dmlc&amp;quot;) install.packages(&amp;quot;mxnet&amp;quot;) 如果你是Linux用户或者想尝试GPU版本，请参考这个链接里的详细编译教程在本地进行编译。 安装完毕之后，我们就可以开始训练模型了，下面两个小节分别介绍两种不同的训练神经网络的方法。 二分类模型与mx.mlp 首先，我们准备一份数据，并进行简单的预处理： require(mlbench) require(mxnet) data(Sonar, package=&amp;quot;mlbench&amp;quot;) Sonar[,61] = as.numeric(Sonar[,61])-1 train.ind = c(1:50, 100:150) train.x = data.matrix(Sonar[train.ind, 1:60]) train.y = Sonar[train.ind, 61] test.x = data.matrix(Sonar[-train.ind, 1:60]) test.y = Sonar[-train.ind, 61] 我们借用mlbench包中的一个二分类数据，并且将它分成训练集和测试集。mxnet提供了一个训练多层神经网络的函数mx.mlp，我们额可以通过它来训练一个神经网络模型。下面是mx.mlp中的部分参数： 训练数据与预测变量 每个隐藏层的大小 输出层的结点数 激活函数类型 损失函数类型 进行训练的硬件（CPU还是GPU） 其他传给mx.model.FeedForward.create的高级参数 了解了大致参数后，我们就可以理解并让R运行下面的代码进行训练了。 mx.set.seed(0) model &amp;lt;- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation=&amp;quot;softmax&amp;quot;, num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9, eval.metric=mx.metric.accuracy) ## Auto detect layout of input matrix, use rowmajor.. ## Start training with 1 devices ## [1] Train-accuracy=0.488888888888889 ## [2] Train-accuracy=0.514285714285714 ## [3] Train-accuracy=0.514285714285714 ... ## [18] Train-accuracy=0.838095238095238 ## [19] Train-accuracy=0.838095238095238 ## [20] Train-accuracy=0.838095238095238 这里要注意使用mx.set.seed而不是R自带的set.seed函数来控制随机数。因为mxnet的训练过程可能会运行在不同的运算硬件上，我们需要一个足够快的随机数生成器来管理整个随机数生成的过程。模型训练好之后，我们可以很简单地进行预测： preds = predict(model, test.x) ## Auto detect layout of input matrix, use rowmajor.. pred.label = max.col(t(preds))-1 table(pred.label, test.y) ## test.y ## pred.label 0 1 ## 0 24 14 ## 1 36 33 如果进行的是多分类预测，mxnet的输出格式是类数X样本数。 回归模型与自定义神经网络 mx.mlp接口固然很方便，但是神经网络的一大特点便是它的灵活性，不同的结构可能有着完全不同的特性。mxnet的亮点之一便是它赋予了用户极大的自由度，从而可以任意定义需要的神经网络结构。我们在这一节用一个简单的回归任务介绍相关的语法。 首先，我们仍然要准备好一份数据。 data(BostonHousing, package=&amp;quot;mlbench&amp;quot;) train.ind = seq(1, 506, 3) train.x = data.matrix(BostonHousing[train.ind, -14]) train.y = BostonHousing[train.ind, 14] test.x = data.matrix(BostonHousing[-train.ind, -14]) test.y = BostonHousing[-train.ind, 14] mxnet提供了一个叫做“Symbol”的系统，从而使我们可以定义结点之间的连接方式与激活函数等参数。下面是一个定义没有隐藏层神经网络的简单例子： # 定义输入数据 data &amp;lt;- mx.symbol.Variable(&amp;quot;data&amp;quot;) # 完整连接的隐藏层 # data: 输入源 # num_hidden: 该层的节点数 fc1 &amp;lt;- mx.symbol.FullyConnected(data, num_hidden=1) # 针对回归任务，定义损失函数 lro &amp;lt;- mx.symbol.LinearRegressionOutput(fc1) 在神经网络中，回归与分类的差别主要在于输出层的损失函数。这里我们使用了平方误差来训练模型。希望能更进一步了解Symbol的读者可以继续阅读这份以代码为主的文档。 定义了神经网络之后，我们便可以使用mx.model.FeedForward.create进行训练了。 mx.set.seed(0) model &amp;lt;- mx.model.FeedForward.create(lro, X=train.x, y=train.y, ctx=mx.cpu(), num.round=50, array.batch.size=20, learning.rate=2e-6, momentum=0.9, eval.metric=mx.metric.rmse) ## Auto detect layout of input matrix, use rowmajor.. ## Start training with 1 devices ## [1] Train-rmse=16.063282524034 ## [2] Train-rmse=12.2792375712573 ## [3] Train-rmse=11.1984634005885 ... ## [48] Train-rmse=8.26890902770415 ## [49] Train-rmse=8.25728089053853 ## [50] Train-rmse=8.24580511500735 这里我们还针对回归任务修改了eval.metric参数。目前我们提供的评价函数包括“accuracy”，“rmse”，“mae” 和 “rmsle”，用户也可以针对需要自定义评价函数，例如： demo.metric.mae &amp;lt;- mx.metric.custom(&amp;quot;mae&amp;quot;, function(label, pred) { res &amp;lt;- mean(abs(label-pred)) return(res) }) mx.set.seed(0) model &amp;lt;- mx.model.FeedForward.create(lro, X=train.x, y=train.y, ctx=mx.cpu(), num.round=50, array.batch.size=20, learning.rate=2e-6, momentum=0.9, eval.metric=demo.metric.mae) ## Auto detect layout of input matrix, use rowmajor.. ## Start training with 1 devices ## [1] Train-mae=13.1889538083225 ## [2] Train-mae=9.81431959337658 ## [3] Train-mae=9.21576419870059 ... ## [48] Train-mae=6.41731406417158 ## [49] Train-mae=6.41011292926139 ## [50] Train-mae=6.40312503493494 至此，你已经掌握了基本的mxnet使用方法。接下来，我们将介绍更好玩的应用。 二、手写数字竞赛 在这一节里，我们以Kaggle上的手写数字数据集（MNIST）竞赛为例子，介绍如何通过mxnet定义一个强大的神经网络，并在GPU上快速训练模型。 第一步，我们从Kaggle上下载数据，并将它们放入data/文件夹中。然后我们读入数据，并做一些预处理工作。 require(mxnet) train &amp;lt;- read.csv(&#39;data/train.csv&#39;, header=TRUE) test &amp;lt;- read.csv(&#39;data/test.csv&#39;, header=TRUE) train &amp;lt;- data.matrix(train) test &amp;lt;- data.matrix(test) train.x &amp;lt;- train[,-1] train.y &amp;lt;- train[,1] train.x &amp;lt;- t(train.x/255) test &amp;lt;- t(test/255) 最后两行预处理的作用有两个： 原始灰度图片数值处在[0,255]之间，我们将其变换到[0,1]之间。 mxnet接受 像素X图片 的输入格式，所以我们对输入矩阵进行了转置。 接下来我们定义一个特别的神经网络结构：LeNet。这是Yann LeCun提出用于识别手写数字的结构，也是最早的卷积神经网络之一。同样的，我们使用Symbol语法来定义，不过这次结构会比较复杂。 # input data &amp;lt;- mx.symbol.Variable(&#39;data&#39;) # first conv conv1 &amp;lt;- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20) tanh1 &amp;lt;- mx.symbol.Activation(data=conv1, act_type=&amp;quot;tanh&amp;quot;) pool1 &amp;lt;- mx.symbol.Pooling(data=tanh1, pool_type=&amp;quot;max&amp;quot;, kernel=c(2,2), stride=c(2,2)) # second conv conv2 &amp;lt;- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50) tanh2 &amp;lt;- mx.symbol.Activation(data=conv2, act_type=&amp;quot;tanh&amp;quot;) pool2 &amp;lt;- mx.symbol.Pooling(data=tanh2, pool_type=&amp;quot;max&amp;quot;, kernel=c(2,2), stride=c(2,2)) # first fullc flatten &amp;lt;- mx.symbol.Flatten(data=pool2) fc1 &amp;lt;- mx.symbol.FullyConnected(data=flatten, num_hidden=500) tanh3 &amp;lt;- mx.symbol.Activation(data=fc1, act_type=&amp;quot;tanh&amp;quot;) # second fullc fc2 &amp;lt;- mx.symbol.FullyConnected(data=tanh3, num_hidden=10) # loss lenet &amp;lt;- mx.symbol.SoftmaxOutput(data=fc2) 为了让输入数据的格式能对应LeNet，我们要将数据变成R中的array格式： train.array &amp;lt;- train.x dim(train.array) &amp;lt;- c(28, 28, 1, ncol(train.x)) test.array &amp;lt;- test dim(test.array) &amp;lt;- c(28, 28, 1, ncol(test)) 接下来我们将要分别使用CPU和GPU来训练这个模型，从而展现不同的训练效率。 n.gpu &amp;lt;- 1 device.cpu &amp;lt;- mx.cpu() device.gpu &amp;lt;- lapply(0:(n.gpu-1), function(i) { mx.gpu(i) }) 我们可以将GPU的每个核以list的格式传递进去，如果有BLAS等自带矩阵运算并行的库存在，则没必要对CPU这么做了。 我们先在CPU上进行训练，这次我们只进行一次迭代： mx.set.seed(0) tic &amp;lt;- proc.time() model &amp;lt;- mx.model.FeedForward.create(lenet, X=train.array, y=train.y, ctx=device.cpu, num.round=1, array.batch.size=100, learning.rate=0.05, momentum=0.9, wd=0.00001, eval.metric=mx.metric.accuracy, epoch.end.callback=mx.callback.log.train.metric(100)) ## Start training with 1 devices ## Batch [100] Train-accuracy=0.1066 ## Batch [200] Train-accuracy=0.16495 ## Batch [300] Train-accuracy=0.401766666666667 ## Batch [400] Train-accuracy=0.537675 ## [1] Train-accuracy=0.557136038186157 print(proc.time() - tic) ## user system elapsed ## 130.030 204.976 83.821 在CPU上训练一次迭代一共花了83秒。接下来我们在GPU上训练5次迭代： mx.set.seed(0) tic &amp;lt;- proc.time() model &amp;lt;- mx.model.FeedForward.create(lenet, X=train.array, y=train.y, ctx=device.gpu, num.round=5, array.batch.size=100, learning.rate=0.05, momentum=0.9, wd=0.00001, eval.metric=mx.metric.accuracy, epoch.end.callback=mx.callback.log.train.metric(100)) ## Start training with 1 devices ## Batch [100] Train-accuracy=0.1066 ## Batch [200] Train-accuracy=0.1596 ## Batch [300] Train-accuracy=0.3983 ## Batch [400] Train-accuracy=0.533975 ## [1] Train-accuracy=0.553532219570405 ## Batch [100] Train-accuracy=0.958 ## Batch [200] Train-accuracy=0.96155 ## Batch [300] Train-accuracy=0.966100000000001 ## Batch [400] Train-accuracy=0.968550000000003 ## [2] Train-accuracy=0.969071428571432 ## Batch [100] Train-accuracy=0.977 ## Batch [200] Train-accuracy=0.97715 ## Batch [300] Train-accuracy=0.979566666666668 ## Batch [400] Train-accuracy=0.980900000000003 ## [3] Train-accuracy=0.981309523809527 ## Batch [100] Train-accuracy=0.9853 ## Batch [200] Train-accuracy=0.985899999999999 ## Batch [300] Train-accuracy=0.986966666666668 ## Batch [400] Train-accuracy=0.988150000000002 ## [4] Train-accuracy=0.988452380952384 ## Batch [100] Train-accuracy=0.990199999999999 ## Batch [200] Train-accuracy=0.98995 ## Batch [300] Train-accuracy=0.990600000000001 ## Batch [400] Train-accuracy=0.991325000000002 ## [5] Train-accuracy=0.991523809523812 print(proc.time() - tic) ## user system elapsed ## 9.288 1.680 6.889 在GPU上训练5轮迭代只花了不到7秒，快了数十倍！可以看出，对于这样的网络结构，GPU的加速效果是非常显著的。有了快速训练的办法，我们便可以很快的做预测，并且提交到Kaggle上了： preds &amp;lt;- predict(model, test.array) pred.label &amp;lt;- max.col(t(preds)) - 1 submission &amp;lt;- data.frame(ImageId=1:ncol(test), Label=pred.label) write.csv(submission, file=&#39;submission.csv&#39;, row.names=FALSE, quote=FALSE) 三、图像识别应用 其实对于神经网络当前的应用场景而言，识别手写数字已经不够看了。早些时候，Google公开了一个云API，让用户能够检测一幅图像里面的内容。现在我们提供一个教程，让大家能够自制一个图像识别的在线应用。 DMLC用在ImageNet数据集上训练了一个模型，能够直接拿来对真实图片进行分类。同时，我们搭建了一个Shiny应用，只需要不超过150行R代码就能够自己在浏览器中进行图像中的物体识别。 为了搭建这个应用，我们要安装shiny和imager两个R包： install.packages(&amp;quot;shiny&amp;quot;, repos=&amp;quot;https://cran.rstudio.com&amp;quot;) install.packages(&amp;quot;imager&amp;quot;, repos=&amp;quot;https://cran.rstudio.com&amp;quot;) 现在你已经配置好了mxnet, shiny和imager三个R包，最困难的部分已经完成了！下一步则是让shiny直接下载并运行我们准备好的代码： shiny::runGitHub(&amp;quot;thirdwing/mxnet_shiny&amp;quot;) 第一次运行这个命令会花上几分钟时间下载预先训练好的模型。训练的模型是Inception-BatchNorm Network，如果读者对它感兴趣，可以阅读这篇文章。准备就绪之后，你的浏览器中会出现一个网页应用，就用本地或在线图片来挑战它吧！ 如果你只需要一个图像识别的模块，那么我们下面给出最简单的一段R代码让你能进行图像识别。首先，我们要导入预训练过的模型文件： model &amp;lt;&amp;lt;- mx.model.load(&amp;quot;Inception/Inception_BN&amp;quot;, iteration = 39) synsets &amp;lt;&amp;lt;- readLines(&amp;quot;Inception/synset.txt&amp;quot;) mean.img &amp;lt;&amp;lt;- as.array(mx.nd.load(&amp;quot;Inception/mean_224.nd&amp;quot;)[[&amp;quot;mean_img&amp;quot;]]) 接下来我们使用一个函数对图像进行预处理，这个步骤对于神经网络模型而言至关重要。 preproc.image &amp;lt;- function(im, mean.image) { # crop the image shape &amp;lt;- dim(im) short.edge &amp;lt;- min(shape[1:2]) yy &amp;lt;- floor((shape[1] - short.edge) / 2) &#43; 1 yend &amp;lt;- yy &#43; short.edge - 1 xx &amp;lt;- floor((shape[2] - short.edge) / 2) &#43; 1 xend &amp;lt;- xx &#43; short.edge - 1 croped &amp;lt;- im[yy:yend, xx:xend,,] # resize to 224 x 224, needed by input of the model. resized &amp;lt;- resize(croped, 224, 224) # convert to array (x, y, channel) arr &amp;lt;- as.array(resized) dim(arr) = c(224, 224, 3) # substract the mean normed &amp;lt;- arr - mean.img # Reshape to format needed by mxnet (width, height, channel, num) dim(normed) &amp;lt;- c(224, 224, 3, 1) return(normed) } 最后我们读入图像，预处理与预测就可以了。 im &amp;lt;- load.image(src) normed &amp;lt;- preproc.image(im, mean.img) prob &amp;lt;- predict(model, X = normed) max.idx &amp;lt;- order(prob[,1], decreasing = TRUE)[1:5] result &amp;lt;- synsets[max.idx] 四、参考资料 MXNet是一个在底层与接口都有着丰富功能的软件，如果读者对它感兴趣，可以参考一些额外的材料来进一步了解MXNet，或者是深度学习这个领域。 MXNet on github MXNet完整文档 mxnet R包入门文档 结合Shiny&#43;MXNet搭建在线识图服务 深度学习入门 DMLC主页]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python 爬虫—破解 JS 加密的 Cookie z]]></title>
    	<url>/tech/2016/03/19/crypto-js-cookies/</url>
		<content type="text"><![CDATA[[原文地址：https://my.oschina.net/jhao104/blog/865966 如何才能使 Python 也能执行这段 JS 呢，答案是PyV8。 是 Chromium 中内嵌的 javascript 引擎，号称跑的最快。PyV8是用 Python 在 V8 的外部 API 包装了一个 Python 壳，这样便可以使 Python 可以直接与 javascript 操作。 前言 在 GitHub 上维护了一个代理池的项目，代理来源是抓取一些免费的代理发布网站。上午有个小哥告诉我说有个代理抓取接口不能用了，返回状态521。抱着帮人解决问题的心态去跑了一遍代码。发现果真是这样。 通过Fiddler抓包比较，基本可以确定是 JavaScript 生成加密 Cookie 导致原来的请求返回521。 发现问题 打开 Fiddler 软件，用浏览器打开目标站点(http://www.kuaidaili.com/proxylist/2/) 。可以发现浏览器对这个页面加载了两次，第一次返回521，第二次才正常返回数据。很多没有写过网站或是爬虫经验不足的童鞋，可能就会觉得奇怪为什么会这样？为什么浏览器可能正常返回数据而代码却不行？ 仔细观察两次返回的结果可以发现： 第二次请求比第一次请求的 Cookie 内容多了个这个_ydclearance=0c316df6ea04c5281b421aa8-5570-47ae-9768-2510d9fe9107-1490254971； 第一次返回的内容一些复杂看不懂的 JS 代码，第二次返回的就是正确的内容。 其实这是网站反爬虫的常用手段。大致过程是这样的：首次请求数据时，服务端返回动态的混淆加密过的 JS，而这段 JS 的作用是给 Cookie 添加新的内容用于服务端验证，此时返回的状态码是521。浏览器带上新的 Cookie 再次请求，服务端验证 Cookie 通过返回数据(这也是为嘛代码不能返回数据的原因)。 解决问题 其实我第一次遇到这样的问题是，一开始想的就是既然你是用 JS 生成的 Cookie, 那么我也可以将 JS 函数翻译成 Python 运行。但是最后还是发现我太傻太天真，因为现在的 JS 都流行混淆加密，原始的 JS 这样的： function lq(VA) { var qo, mo = &amp;quot;&amp;quot;, no = &amp;quot;&amp;quot;, oo = [0x8c, 0xcd, 0x4c, 0xf9, 0xd7, 0x4d, 0x25, 0xba, 0x3c, 0x16, 0x96, 0x44, 0x8d, 0x0b, 0x90, 0x1e, 0xa3, 0x39, 0xc9, 0x86, 0x23, 0x61, 0x2f, 0xc8, 0x30, 0xdd, 0x57, 0xec, 0x92, 0x84, 0xc4, 0x6a, 0xeb, 0x99, 0x37, 0xeb, 0x25, 0x0e, 0xbb, 0xb0, 0x95, 0x76, 0x45, 0xde, 0x80, 0x59, 0xf6, 0x9c, 0x58, 0x39, 0x12, 0xc7, 0x9c, 0x8d, 0x18, 0xe0, 0xc5, 0x77, 0x50, 0x39, 0x01, 0xed, 0x93, 0x39, 0x02, 0x7e, 0x72, 0x4f, 0x24, 0x01, 0xe9, 0x66, 0x75, 0x4e, 0x2b, 0xd8, 0x6e, 0xe2, 0xfa, 0xc7, 0xa4, 0x85, 0x4e, 0xc2, 0xa5, 0x96, 0x6b, 0x58, 0x39, 0xd2, 0x7f, 0x44, 0xe5, 0x7b, 0x48, 0x2d, 0xf6, 0xdf, 0xbc, 0x31, 0x1e, 0xf6, 0xbf, 0x84, 0x6d, 0x5e, 0x33, 0x0c, 0x97, 0x5c, 0x39, 0x26, 0xf2, 0x9b, 0x77, 0x0d, 0xd6, 0xc0, 0x46, 0x38, 0x5f, 0xf4, 0xe2, 0x9f, 0xf1, 0x7b, 0xe8, 0xbe, 0x37, 0xdf, 0xd0, 0xbd, 0xb9, 0x36, 0x2c, 0xd1, 0xc3, 0x40, 0xe7, 0xcc, 0xa9, 0x52, 0x3b, 0x20, 0x40, 0x09, 0xe1, 0xd2, 0xa3, 0x80, 0x25, 0x0a, 0xb2, 0xd8, 0xce, 0x21, 0x69, 0x3e, 0xe6, 0x80, 0xfd, 0x73, 0xab, 0x51, 0xde, 0x60, 0x15, 0x95, 0x07, 0x94, 0x6a, 0x18, 0x9d, 0x37, 0x31, 0xde, 0x64, 0xdd, 0x63, 0xe3, 0x57, 0x05, 0x82, 0xff, 0xcc, 0x75, 0x79, 0x63, 0x09, 0xe2, 0x6c, 0x21, 0x5c, 0xe0, 0x7d, 0x4a, 0xf2, 0xd8, 0x9c, 0x22, 0xa3, 0x3d, 0xba, 0xa0, 0xaf, 0x30, 0xc1, 0x47, 0xf4, 0xca, 0xee, 0x64, 0xf9, 0x7b, 0x55, 0xd5, 0xd2, 0x4c, 0xc9, 0x7f, 0x25, 0xfe, 0x48, 0xcd, 0x4b, 0xcc, 0x81, 0x1b, 0x05, 0x82, 0x38, 0x0e, 0x83, 0x19, 0xe3, 0x65, 0x3f, 0xbf, 0x16, 0x88, 0x93, 0xdd, 0x3b]; qo = &amp;quot;qo=241; do{oo[qo]=(-oo[qo])&amp;amp;0xff; oo[qo]=(((oo[qo]&amp;gt;&amp;gt;3)|((oo[qo]&amp;lt;&amp;lt;5)&amp;amp;0xff))-70)&amp;amp;0xff;} while(--qo&amp;gt;=2);&amp;quot;; eval(qo); qo = 240; do { oo[qo] = (oo[qo] - oo[qo - 1]) &amp;amp; 0xff; } while (--qo &amp;gt;= 3); qo = 1; for (; ;) { if (qo &amp;gt; 240) break; oo[qo] = ((((((oo[qo] &#43; 2) &amp;amp; 0xff) &#43; 76) &amp;amp; 0xff) &amp;lt;&amp;lt; 1) &amp;amp; 0xff) | (((((oo[qo] &#43; 2) &amp;amp; 0xff) &#43; 76) &amp;amp; 0xff) &amp;gt;&amp;gt; 7); qo&#43;&#43;; } po = &amp;quot;&amp;quot;; for (qo = 1; qo &amp;lt; oo.length - 1; qo&#43;&#43;) if (qo % 6) po &#43;= String.fromCharCode(oo[qo] ^ VA); eval(&amp;quot;qo=eval;qo(po);&amp;quot;); } 看到这样的 JS 代码，我只能说原谅我 JS 能力差，还原不了。。。 但是前端经验丰富的童鞋马上就能想到还有种方法可解，那就是利用浏览器的 JS 代码调试功能。这样一切就迎刃而解，新建一个 html 文件，将第一次返回的 html 原文复制进去，保存用浏览器打开，在eval之前打上断点，看到这样的输出： 可以看到这个变量po为document.cookie=&#39;_ydclearance=0c316df6ea04c5281b421aa8-5570-47ae-9768-2510d9fe9107-1490254971; expires=Thu, 23-Mar-17 07:42:51 GMT; domain=.kuaidaili.com; path=/&#39;; window.document.location=document.URL，下面还有个eval(&amp;quot;qo=eval;qo(po);&amp;quot;)。JS 里面的eval和 Python 的差不多，第二句的意思就是将eval方法赋给qo。然后去eval字符串po。而字符串po的前半段的意思是给浏览器添加 Cooklie，后半段window.document.location=document.URL是刷新当前页面。 这也印证了我上面的说法，首次请求没有 Cookie，服务端回返回一段生成 Cookie 并自动刷新的 JS 代码。浏览器拿到代码能够成功执行，带着新的 Cookie 再次请求获取数据。而 Python 拿到这段代码就只能停留在第一步。 那么如何才能使 Python 也能执行这段 JS 呢，答案是PyV8。V8 是 Chromium 中内嵌的 javascript 引擎，号称跑的最快。PyV8是用 Python 在 V8 的外部 API 包装了一个 Python 壳，这样便可以使 Python 可以直接与 javascript 操作。PyV8的安装大家可以自行百度。 代码 分析完成，下面切入正题撸代码。首先是正常请求网页，返回带加密的 JS 函数的 html import re import PyV8 import requests TARGET_URL = &amp;quot;http://www.kuaidaili.com/proxylist/1/&amp;quot; def getHtml(url, cookie=None): header = { &amp;quot;Host&amp;quot;: &amp;quot;www.kuaidaili.com&amp;quot;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Cache-Control&#39;: &#39;max-age=0&#39;, &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36&#39;, &#39;Accept&#39;: &#39;text/html,application/xhtml&#43;xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate, sdch&#39;, &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;, } html = requests.get(url=url, headers=header, timeout=30, cookies=cookie).content return html # 第一次访问获取动态加密的JS first_html = getHtml(TARGET_URL) 由于返回的是 html，并不单纯的 JS 函数，所以需要用正则提取 JS 函数的参数的参数。 # 提取其中的JS加密函数 js_func = &#39;&#39;.join(re.findall(r&#39;(function .*?)&amp;lt;/script&amp;gt;&#39;, first_html)) print &#39;get js func:\n&#39;, js_func # 提取其中执行JS函数的参数 js_arg = &#39;&#39;.join(re.findall(r&#39;setTimeout\(\&amp;quot;\D&#43;\((\d&#43;)\)\&amp;quot;&#39;, first_html)) print &#39;get ja arg:\n&#39;, js_arg 还有一点需要注意，在 JS 函数中并没有返回 cookie，而是直接将 cookie set 到浏览器，所以我们需要将eval(&amp;quot;qo=eval;qo(po);&amp;quot;)替换成return po。这样就能成功返回po中的内容。 def executeJS(js_func_string, arg): ctxt = PyV8.JSContext() ctxt.enter() func = ctxt.eval(&amp;quot;({js})&amp;quot;.format(js=js_func_string)) return func(arg) # 修改JS函数，使其返回Cookie内容 js_func = js_func.replace(&#39;eval(&amp;quot;qo=eval;qo(po);&amp;quot;)&#39;, &#39;return po&#39;) # 执行JS获取Cookie cookie_str = executeJS(js_func, js_arg) 这样返回的 cookie 是字符串格式，但是用requests.get()需要字典形式，所以将其转换成字典： def parseCookie(string): string = string.replace(&amp;quot;document.cookie=&#39;&amp;quot;, &amp;quot;&amp;quot;) clearance = string.split(&#39;;&#39;)[0] return {clearance.split(&#39;=&#39;)[0]: clearance.split(&#39;=&#39;)[1]} # 将Cookie转换为字典格式 cookie = parseCookie(cookie_str) 最后带上解析出来的 Cookie 再次访问网页，成功获取数据： # 带上Cookie再次访问url,获取正确数据 print getHtml(TARGET_URL, cookie)[0:500] 下面是完整代码： # -*- coding: utf-8 -*- &amp;quot;&amp;quot;&amp;quot; File Name： demo_1.py.py Description : Python爬虫—破解JS加密的Cookie 快代理网站为例：http://www.kuaidaili.com/proxylist/1/ Document: Author : JHao date： 2017/3/23 Change Activity: 2017/3/23: &amp;quot;&amp;quot;&amp;quot; __author__ = &#39;JHao&#39; import re import PyV8 import requests TARGET_URL = &amp;quot;http://www.kuaidaili.com/proxylist/1/&amp;quot; def getHtml(url, cookie=None): header = { &amp;quot;Host&amp;quot;: &amp;quot;www.kuaidaili.com&amp;quot;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Cache-Control&#39;: &#39;max-age=0&#39;, &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36&#39;, &#39;Accept&#39;: &#39;text/html,application/xhtml&#43;xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate, sdch&#39;, &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;, } html = requests.get(url=url, headers=header, timeout=30, cookies=cookie).content return html def executeJS(js_func_string, arg): ctxt = PyV8.JSContext() ctxt.enter() func = ctxt.eval(&amp;quot;({js})&amp;quot;.format(js=js_func_string)) return func(arg) def parseCookie(string): string = string.replace(&amp;quot;document.cookie=&#39;&amp;quot;, &amp;quot;&amp;quot;) clearance = string.split(&#39;;&#39;)[0] return {clearance.split(&#39;=&#39;)[0]: clearance.split(&#39;=&#39;)[1]} # 第一次访问获取动态加密的JS first_html = getHtml(TARGET_URL) # first_html = &amp;quot;&amp;quot;&amp;quot; # &amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;script language=&amp;quot;javascript&amp;quot;&amp;gt; window.onload=setTimeout(&amp;quot;lu(158)&amp;quot;, 200); function lu(OE) {var qo, mo=&amp;quot;&amp;quot;, no=&amp;quot;&amp;quot;, oo = [0x64,0xaa,0x98,0x3d,0x56,0x64,0x8b,0xb0,0x88,0xe1,0x0d,0xf4,0x99,0x31,0xd8,0xb6,0x5d,0x73,0x98,0xc3,0xc4,0x7a,0x1e,0x38,0x9d,0xe8,0x8d,0xe4,0x0a,0x2e,0x6c,0x45,0x69,0x41,0xe5,0xd0,0xe5,0x11,0x0b,0x35,0x7b,0xe4,0x09,0xb1,0x2b,0x6d,0x82,0x7c,0x25,0xdd,0x70,0x5a,0xc4,0xaa,0xd3,0x74,0x98,0x42,0x3c,0x60,0x2d,0x42,0x66,0xe0,0x0a,0x2e,0x96,0xbb,0xe2,0x1d,0x38,0xdc,0xb1,0xd6,0x0e,0x0d,0x76,0xae,0xc3,0xa9,0x3b,0x62,0x47,0x40,0x15,0x93,0xb7,0xee,0xc3,0x3e,0xfd,0xd3,0x0d,0xf6,0x61,0xdc,0xf1,0x2c,0x54,0x8c,0x90,0xfa,0x24,0x5b,0x83,0x0c,0x75,0xaf,0x18,0x01,0x7e,0x68,0xe0,0x0a,0x72,0x1e,0x88,0x33,0xa7,0xcc,0x31,0x9b,0xf3,0x1a,0xf2,0x9a,0xbf,0x58,0x83,0xe4,0x87,0xed,0x07,0x7e,0xe2,0x00,0xe9,0x92,0xc9,0xe8,0x59,0x7d,0x56,0x8d,0xb5,0xb2,0x6c,0xe0,0x49,0x73,0xfc,0xe7,0x20,0x49,0x34,0x09,0x71,0xeb,0x60,0xfd,0x8e,0xad,0x0f,0xb9,0x2e,0x77,0xdc,0x74,0x9b,0xbf,0x8f,0xa5,0x8d,0xb8,0xb0,0x06,0xac,0xc5,0xe9,0x10,0x12,0x77,0x9b,0xb1,0x19,0x4e,0x64,0x5c,0x00,0x98,0xc6,0xed,0x98,0x0d,0x65,0x11,0x35,0x9e,0xf4,0x30,0x93,0x4b,0x00,0xab,0x20,0x8f,0x29,0x4f,0x27,0x8c,0xc2,0x6a,0x04,0xfb,0x51,0xa3,0x4b,0xef,0x09,0x30,0x28,0x4d,0x25,0x8e,0x76,0x58,0xbf,0x57,0xfb,0x20,0x78,0xd1,0xf7,0x9f,0x77,0x0f,0x3a,0x9f,0x37,0xdb,0xd3,0xfc,0x14,0x39,0x11,0x3b,0x94,0x8c,0xad,0x8e,0x5c,0xd3,0x3b];qo = &amp;quot;qo=251; do{oo[qo]=(-oo[qo])&amp;amp;0xff; oo[qo]=(((oo[qo]&amp;gt;&amp;gt;4)|((oo[qo]&amp;lt;&amp;lt;4)&amp;amp;0xff))-0)&amp;amp;0xff;} while(--qo&amp;gt;=2);&amp;quot;; eval(qo);qo = 250; do { oo[qo] = (oo[qo] - oo[qo - 1]) &amp;amp; 0xff; } while (-- qo &amp;gt;= 3 );qo = 1; for (;;) { if (qo &amp;gt; 250) break; oo[qo] = ((((((oo[qo] &#43; 200) &amp;amp; 0xff) &#43; 121) &amp;amp; 0xff) &amp;lt;&amp;lt; 6) &amp;amp; 0xff) | (((((oo[qo] &#43; 200) &amp;amp; 0xff) &#43; 121) &amp;amp; 0xff) &amp;gt;&amp;gt; 2); qo&#43;&#43;;}po = &amp;quot;&amp;quot;; for (qo = 1; qo &amp;lt; oo.length - 1; qo&#43;&#43;) if (qo % 5) po &#43;= String.fromCharCode(oo[qo] ^ OE);eval(&amp;quot;qo=eval;qo(po);&amp;quot;);} &amp;lt;/script&amp;gt; &amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt; # &amp;quot;&amp;quot;&amp;quot; # 提取其中的JS加密函数 js_func = &#39;&#39;.join(re.findall(r&#39;(function .*?)&amp;lt;/script&amp;gt;&#39;, first_html)) print &#39;get js func:\n&#39;, js_func # 提取其中执行JS函数的参数 js_arg = &#39;&#39;.join(re.findall(r&#39;setTimeout\(\&amp;quot;\D&#43;\((\d&#43;)\)\&amp;quot;&#39;, first_html)) print &#39;get ja arg:\n&#39;, js_arg # 修改JS函数，使其返回Cookie内容 js_func = js_func.replace(&#39;eval(&amp;quot;qo=eval;qo(po);&amp;quot;)&#39;, &#39;return po&#39;) # 执行JS获取Cookie cookie_str = executeJS(js_func, js_arg) # 将Cookie转换为字典格式 cookie = parseCookie(cookie_str) print cookie # 带上Cookie再次访问url,获取正确数据 print getHtml(TARGET_URL, cookie)[0:500]]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[什么是 Generalized Method of Moments (GMM)？z]]></title>
    	<url>/prof/2016/03/19/gmm/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/41312883 慧航 既然被邀请和提到，在这里我来写一个最简单的 GMM 快速 GMM 的全名是 Generalized Method of Moments，也就是广义矩估计。只看这个名字的话，如果去掉广义这个词，可能学过本科统计的人都认识，就是矩估计。 矩估计是什么呢？简单的说，就是用样本矩代替总体矩进行统计推断的方法。一个最基础的例子是正态总体的参数估计问题。如果\(x_i\sim N(\mu,\sigma^2)\)，如何估计\(\mu\)和\(\sigma\)呢？ 本科的统计学一般会介绍两种方法：极大似然估计和矩估计。其中矩估计是我们今天的主角。观察到： \[\mathrm{E}(x_i)=\mu,\quad\mathrm{E}(x_i^2)=\mu^2&#43;\sigma^2\] 而根据大数定理，在一定的条件下，我们有： \[\overline{x_i}-\mu=o_p(1),\quad\overline{x^2_i}=\mu^2&#43;\sigma^2&#43;o_p(1)\] 也就是说，当样本量足够大的时候，样本矩与总体矩只差了一个无穷小量，那么我们是不是可以用样本矩代替总体矩得到参数的估计呢？ 按照上面的思路，我们把\(o_p(1)\)去掉，同时把未知的总体参数写成其估计值，也就是带^的形式，我们得到了： \[\hat{\mu}=\overline{x_i},\quad \hat{\sigma}^2=\overline{x^2_i}-(\overline{x_i})^2\] 如此，我们得到了两个总体矩的点估计。在这个简单的例子里面，你只要把上面的大数定理的结论带到上面两个式子里面，很容易的就可以证明出两个点估计是一致的估计量。 当然，值得注意的是，即便我使用的是矩条件，\(\sigma\)的估计也不是无偏的。一般而言，除了特殊情况，不管是 MLE 还是 MM 还是 GMM，都不一定可以得到无偏的估计量。特别是在比较复杂的应用里面，一致就很不错了，无偏性的讨论真的繁琐。 好了，上面是矩估计，非常简单是吧？但是什么又是广义矩估计呢？ 在上面的例子中，我们只使用了两个矩条件。然而我们知道，正态分布的矩是有无穷多个可以用的，那么我们是不是可以使用更多的矩条件呢？ 但是有个问题不好解决。在这个例子里面，我们有两个未知参数，如果只使用一阶矩，那么只有一个方程解两个未知数，显然是不可能的。像上面一样，我们用两个矩条件解两个未知数，就解出来了。然而，当我们用一到三阶矩，总共三个方程求解的时候，三个方程求解两个未知数，可能无解。 方程数多了，反而没有解了，为什么呢？其实很简单，用三个方程中的任意两个方程，都可以求出一组解，那么三个方程我们就可以求出三组解。所以应该如何把这些矩条件都用上呢？ 到这里我们不妨引入一些记号。还是使用上面的例子，我们把上面的三个矩条件写到一个向量里面去，记： \[g(x_i,\theta)=\left[ x_i-\mu, x_i^2-\mu^2-\sigma^2, x_i^3-\mu^3-3\mu\sigma^2\right]^\prime,\;\theta=\{\mu, \sigma^2\}\] 我们可以得到一个3*1的列向量，并且： \[\mathrm{E}[g(x_i,\theta)]=0\] 上面就是我们要用的矩条件。而根据上面的思路，用其样本矩代替总体矩： \[\begin{equation} \frac{1}{N}\sum_i g(x_i,\hat{\theta})=0 \end{equation}\] 解这个方程应该就可以得到参数\(\theta\)的估计。但是正如上面所说的，三个方程两个未知数，并不能确保这个方程有解，所以必须想一些其他办法。 一个比较自然的想法是，上面的矩条件等于\(0\)，虽然我不太可能保证三个方程同时等于\(0\)，但是仿照 OLS，我们可以让他们的平方和最小，也就是： \[\min_{\hat{\theta}} \left[ \frac{1}{N}\sum_i g(x_i,\hat{\theta}) \right] &amp;#39; \left[ \frac{1}{N}\sum_i g(x_i,\hat{\theta}) \right]\] 这样我们就能保证三个矩条件的样本矩都足够贴近于\(0\)，当然不可能同时为\(0\)。这样不就综合使用了三个矩条件的信息么？ 更一般的，由于上面的\(g\)函数是一个3*1的列向量，我们可以使用一个权重矩阵\(W\)来赋予每个矩条件以不同的权重： \[\begin{equation} \min_{\hat{\theta}} \left[ \frac{1}{N}\sum_i g(x_i,\hat{\theta}) \right] &amp;#39; W \left[ \frac{1}{N}\sum_i g(x_i,\hat{\theta}) \right] \end{equation}\] 只要这个\(W\)是一个正定矩阵，那么仍然可以保证每个样本矩都足够贴近于\(0\)。 那么问题来了，既然对\(W\)的要求只要求正定矩阵，那么使用不同的权重矩阵就有可能得到不同的结果。问题是，有没有一个最优的权重矩阵呢？当然是有的。可以证明，最优的权重矩阵应该是： \[\begin{equation} \left\{\mathrm{E} [g(x_i,\theta)g(x_i,\theta)^\prime] \right\}^{-1} \end{equation}\] 使用这个权重矩阵，就得到了最有效的估计。 比如上面的例子，用 gretl 分别估计两个矩条件、三个矩条件使用单位阵作为W、三个矩条件使用最优权重矩阵做估计： nulldata 1000 set seed 1988 series x=randgen(N,1,2) series x2=x^2 series x3=x^3 series e series e2 series e3 scalar mu=0 scalar sigma2=1 matrix W2=I(2) gmm series e=x-mu series e2=x2-sigma2-mu^2 orthog e; const orthog e2; const weights W2 params mu sigma2 end gmm matrix W3=I(3) scalar mu=0 scalar sigma2=1 gmm series e=x-mu series e2=x2-sigma2-mu^2 series e3=x3-3*mu*sigma2-mu^3 orthog e; const orthog e2; const orthog e3; const weights W3 params mu sigma2 end gmm scalar mu=0 scalar sigma2=1 gmm series e=x-mu series e2=x2-sigma2-mu^2 series e3=x3-3*mu*sigma2-mu^3 orthog e; const orthog e2; const orthog e3; const weights W3 params mu sigma2 end gmm --iterate 首先是使用两个矩条件的结果： 为什么两个矩条件的时候不使用最优权重矩阵呢？因为两个未知参数，两个矩条件，不存在过度识别的问题，存在唯一解的，所以不管使用任何的正定矩阵，得到的结果都是一样的。 三个矩条件，这个时候使用什么样的权重矩阵就不一样了。 先使用单位阵作为权重矩阵： 这里需要注意的是，即使使用了更多的矩条件，估计量的 standard error 还是变大了。感兴趣的可以做一个蒙特卡洛模拟试试，一定是会变大的。为什么呢？因为没有使用最优的权重矩阵，所以使用单位阵作为权重矩阵得到的结果不是最有效的。 那么如果使用最优的权重矩阵呢？结果： 嘿！standard error 是变小了，但是跟使用两个矩条件的好像没有什么本质变化啊？为什么呢？ 因为这里举的这个例子太特殊了，我们使用的前两个矩条件，刚好是一个充分统计量，也就是说，使用额外的矩条件不会带来附加信息的。但是如果是其他情况，一般来说更多的矩条件是可以带来更多的信息的，比如工具变量的回归。 另外如果细心观察，最后一张表格多了一个 J-test。这又是啥呢？ 这个东西就比较有意思了。直到现在，我们都是假设使用的矩条件成立，那么这些矩条件真的是成立的么？未必啊。比如，如果\(x\)本来就不服从正态分布，那么使用上面的估计显然是错的。那么是不是可以检验矩条件是否成立呢？ 一般来说，如果你有\(K\)个未知的参数，以及\(K\)个矩条件，那么矩条件是不能检验的。但是如果你有更多的矩条件，那么就有了检验的可能。这个检验的直觉很简单，比如上面的例子里面，我们有\(3\)个矩条件。我可不可以先使用前两个矩条件估计这两个参数，然后把这两个参数带入到第三个矩条件里面，看看是不是充分接近于$0$，如果充分接近，那么看来这三个矩条件彼此印证了。 实际使用的时候没有那么麻烦。可以证明，当使用了最优的权重矩阵的时候，GMM 的目标函数渐进服从卡方分布，因而只要检验这个卡方分布就可以了，也就是上面的 J-test。p-value 为0.6884，看来这三个矩条件没有矛盾的地方。 但是一定要注意，即使通过了这个检验，也不代表矩条件一定是成立的，因为有可能三个矩条件都是错的，只不过错的方向是一致的。比如这个例子里面，有可能\(x\)的分布前三阶矩跟正态分布是一样的，但第四阶就不一样了。因而通过这个检验不代表x一定服从正态分布。当然，如果通不过，可以比较自信的说，\(x\)不服从正态分布。 比如，我们把上面的数据生成过程改为 gamma 分布，得到的结果： p-value 为0.0000，拒绝了原假设，也就是说，三个矩条件不同时成立，数据很有可能不是从正态分布中生成的。 计量经济学的很多很多问题基本都可以归结为 GMM 的问题。从最简单的 OLS、2SLS 到稍微复杂一点的面板数据、动态面板等等，本质上都是在找矩条件。比如工具变量的 2SLS，可以发现矩条件不过就是： \[\mathrm{E}[(y_i-x_i^\prime\beta)z_i]=0\] 套一下上面的公式，最优权重矩阵(的逆)为： \[\mathrm{E}[(y_i-x_i^\prime\beta_0)z_i z_i^\prime(y_i-x_i^\prime\beta_0)^\prime]=\mathrm{E}[e_i^2z_i z_i^\prime]=\sigma^2\mathrm{E}[z_iz_i^\prime]\] 带入到目标函数中，就得到了 2SLS。 甚至，一些其他的估计量，比如 MLE、M-estimator 等，在一定的条件下也可以转化为 GMM，因为这些估计量的一阶条件可以看成是矩条件。所以 GMM 也就变成了一个统一的框架。 为什么 GMM 这么受欢迎呢？因为 GMM 把复杂的统计过程抽象化成为一个（看似）简单的过程：找矩条件。只要你能找到矩条件，你就能估计。GMM 把估计的繁琐细节全都抽象了，面对一个模型，你所需要做的所有事情就是找到矩条件，证明这个模型是可以识别的，然后什么也不用管，一股脑儿塞进去，结果就出来了。 所以呢如果你去看一些稍微复杂的模型，基本都可以归结为矩条件。 至于题主提到的资产定价，刚好 Gretl 提供了一个可以使用的数据集和 code。资产定价最简单的模型应该就是 C-CAPM 了，其重要结论就可以直接归结为这么一个矩条件： \[\begin{equation} \mathrm{E}\left[\delta\frac{r_{j,t&#43;1}}{p_{j,t}}\left(\frac{C_{t&#43;1}}{C_t}\right)^{\alpha-1}\Bigg|\mathcal{F}_t\right]=1 \end{equation}\] 其中\(\mathcal{F}_t\)为第\(t\)期所知道的所有信息，包括\(C_t\)、\(r_t\)等等。所以根据这个式子，如果令 \[e_t=\delta\frac{r_{j,t&#43;1}}{p_{j,t}}\left(\frac{C_{t&#43;1}}{C_t}\right)^{\alpha-1}-1\] 那么\(e_t\)跟\(C_t\)、\(r_t\)等等都是正交的，自然可以作为矩条件来用。 Gretl 自带了Hall的数据集，在 user guide 第 206 页开始给出了说明和代码，以及结果，感兴趣的可以去看看，很简单的一个程序。 我猜想上面的两个例子已经足够简单了，特别是正态分布的例子，应该不可能更简单了。 KF CHE：剛開始學 GMM，這個答案幫助很大，萬分感謝。有問題還想請教一下，希望先生有空賜教一下。如果 instrument rank 剛好就和要估計的 estimators 數目一樣，這時是 just-identified，用不著去管 j-stat。於是這時候我就只能用經濟常識去支持我的 instruments 是有效的，而沒有什麼統計工具可以幫忙嗎? 慧航：是的，没有。 Huang Zhibin GMM 简直是计量的良心，它可以涵盖几乎所有常用的 estimator，OLS, IV, 2SLS, GLS, RE, FE, SUR, 3SLS, Pooled OLS…全是它的特殊情况。所以 LZ 你说用简单的例子解释一下，我瞬间不知道该从何讲起…因为 GMM 的应用…实在广泛了。 LZ 看样子是做宏观或者金融的，那我就来根据 Hayashi 的 econometrics 来大致解释一下 GMM。GMM 是一个 framework，本质是运用矩条件，对参数进行估计。所以我们叫它广义矩估计。 我们先在线性模型 \[y_{i} =x&amp;#39;_{i}\beta &#43;\varepsilon _{i}\] 的框架下讨论，这样比较清晰。假设\(y\)是因变量，\(x\)是原自变量，\(z\)是工具自变量（可以和原自变量一致，也可以不一致），我们定义 \[g_{i}=z_{i}*\varepsilon _{i}\] 所谓矩条件，就是我们假设模型的真实参数和总体，满足这样一个条件： \[\begin{equation} \mathrm{E}[g(z,\beta)]=0 \end{equation}\] 也就是 \[ \mathrm{E}(z_{i}*(y_{i}-x&amp;#39;_{i}\beta ))=0 \] 然后在这个条件下，我们用某种方法去估计参数\(\beta\)，看上去是不是很混乱？OK 让我们做一个小小的变换，假设向量\(x_i=z_i\)，也就是说工具变量和自变量完全一样。这时候矩条件就变成了： \[\mathrm{E}(x_{i}*(y_{i}-x&amp;#39;_{i}\beta ))=0\] 回想起来这是啥了没？就是简单的线性投影条件呀！它的 sample analogue 是啥？就是 OLS！好，OLS 首先被装到了 GMM 这个框里。但是当\(z_i\)不完全和\(x_i\)一样的时候呢？那我们就得分类讨论了。 如果\(z_i\)里的变量数量小于\(x_i\)，那就是 under-identified（识别不足），这个时候我们没办法用 GMM 估计。（想想简单 IV 里最基本的估计条件就是 IV 数量比内生变量数量多） 如果\(z_i\)里的变量数量等于\(x_i\)里的，那就是 just-identified（恰好识别），这个时候我们的 sample analogue 和用样本估计参数的方法都很直接而且简单，就是用简单算术平均。定义 \[g_{n}=\frac{1}{n} *\sum_{i=1}^{n}{z_{i}*(y_{i}-x&amp;#39;_{i}\beta) }\] 估计方法就是直接让\(g_{n}=0\)，解出对应的\(\beta\)就好了，没啥花样儿。所以我们很清楚可以看到，恰好识别的时候，GMM Estimator 就是： \[\hat{\beta } _{\text{GMM}}=(\sum_{i=1}^{n}{z_{i}x&amp;#39;_{i}})^{-1}*(\sum_{i=1}^{n}{z_{i}y_{i}} )\] 是不是很熟悉？YES！就是简单的 IV Estimator，当\(z_i=x_i\)时，就直接变成 OLS Estimator 了。 如果\(z_i\)里的变量数量大于\(x_i\)里的，那就是over-identified（过度识别），这就到了 GMM 不一样的地方了。这时候我们不能直接简单用\(g_{n}=0\)的条件去求解\(\beta\)了，因为这时候我们的矩条件比未知数要多，也就是说方程组里的方程数量比未知数多，一般情况下找不到解。咋办？那我们就找一个解得出来的方程组，并且要让\(g_{n}\)尽量“靠近”零。因为\(g_{n}\)其实是空间里的一个点，所以我们这里用一个小技巧，把这种靠近，定义为最小化\(g_{n}\)这个点，和原点的空间距离。我们定义 \[J(\hat{\beta},\hat{W})=n* g&amp;#39;_{n}(\hat{\beta})\hat{W}g_{n}(\hat{\beta})\] 这个\(J\)就是我们要的距离。\(W\)是一个对称且正定的矩阵，表示我们对这个空间距离的某种度量。当\(W=I\)的时候，我们定义的这个距离就是简单的欧式空间距离。前面乘以一个\(n\)没啥别的意思，是为了某些统计量比较好算…，所以我们估计参数\(\beta\)的方法就是： \[\hat{\beta}_{\text{GMM}}=\arg\min_{\hat{\beta}}J(\hat{\beta},\hat{W})\] 取一个让距离最小的\(\hat{\beta}\)，就得到了我们要的 GMM 估计量。简单求个导，解一下一阶条件我们就有了显性表达式： \[\hat{\beta}_{\text{GMM}}=(S&amp;#39;_{zx}\hat{W}S_{zx})^{-1}S&amp;#39;_{zx}\hat{W}S_{zy}\] 其中\(S_{zx}=\sum_{i-=1}^{n}{z_{i}x&amp;#39;_{i}},\;S_{zy}=\sum_{i-=1}^{n}{z_{i}y_{i}}\)，这就是单方程 GMM 的一般解。当我们选取不同的\(W\)矩阵，也就是选择不同的空间距离度量时，GMM 会变成各种我们熟悉的 estimator，比如 2SLS 等等。 以上是关于线性模型的。 更一般的 GMM，其实差别不是很大，无非是去掉了矩条件是线性的这个假设。这时候我们有： \[\mathrm{E}[g(x,\beta)]=0\] \(x\)是自变量，\(\beta\)是真实参数，同样我们也是最小化一个空间距离： \[J(\hat{\beta},\hat{W})=n* g&amp;#39;_{n}(\hat{\beta})\hat{W}g_{n}(\hat{\beta})\] \[\hat{\beta}_{\text{GMM}}=\arg\min_{\hat{\beta}}J(\hat{\beta},\hat{W})\] 只不过在具体求解的时候，如果\(g\)是一个很复杂的非线性函数的话，那就不一定有解析解，需要用数值逼近，然后渐进方差要用 delta method 计算。（这块 general 的 GMM 具体操作方法我也不是很了解，hayashi 和 hansen 的书上也都没有太多介绍，可以咨询@慧航） 以上是最基本的 GMM 内容，从 0 开始定义。更多的重要内容，包括最优权矩阵，多方程 GMM 等等，还是看书吧。推荐 Bruce Hansen 的 Econometrics，里面关于 GMM 的章节很精练，适合快速阅读快速理解，并且是基于iid sample假设。Hayashi 的 Econometrics 对 GMM 的介绍非常全面，适合进阶阅读，基于ergodic stationary假设，偏时间序列。 参考： Hayashi, Econometrics Bruce Hansen, Econometrics J-test 是关于 orthogonal condition，也就是我们的矩条件的 test。在模型的 specification 都成立，我们假设的矩条件都真实成立的情况下，\(J\)值会收敛到一个卡方分布（其实这里就是我之前提到的为什么算\(J\)最优化前面要多乘一个\(n\)的原因！因为可以让\(J\)和卡方分布联系上），这样我们就能用\(J\)做统计检验。所以如果\(J\)太大的话，我们就得怀疑模型是不是错了，是不是有些矩条件错了。这里的直观理解很简单，我们假设了矩条件等于零，那\(n\)很大的时候，样本算出来的和零的距离就不应该太大，不然就不对了。 缄默的老橡树（补充） 今天复习 GMM 的时候想到了一个工具变量的找法很开森，于是愉快地决定强答一发 GMM，然后发现前面三位大神已经把能填的坑都填上了。 找个没填完的小坑，稍微灌点水吧，补充一下@刘澈，没讲完的具体的 GMM 提升精度的方法。 前面大神们提到了，GMM 估计相当于给不同的矩条件赋予了不同的权重，然后才能这个权重得到最小化条件，不同的权重阵其实就对不同的估计量，就像@Huang Zibin说的，“OLS, IV, 2SLS, GLS, RE, FE, SUR, 3SLS, Pooled OLS…全是它的特殊情况”。 那么结果来了，权重矩阵辣么多，要挑不过来，怎么选取最好呢，@慧航也指出了，最优权重阵这样， \[\left\{\mathrm{E}[g(x_i,\theta)g(x_i,\theta)&amp;#39;]\right\}^{-1}\] 当然了，根据 slutsky’s theorem，拿样本模拟总体一般错不了。所以样本模拟最优权重阵的结果就是这样： \[\left\{\frac{1}{n}\sum\left[g(x_i,\hat{\theta})g(x_i,\hat{\theta})&amp;#39;\right]\right\}^{-1}\] 那么问题来了，要估计最优权重阵就要估计参数，要估计参数就要知道最优权重阵（循环一二起，要估计最优权重阵就要估计参数，要估计参数就要知道最优权重阵…）。不要担心，我们有Hansen（1982）。 第一种叫 one-step GMM，玩不出来我就不玩了呗，没有胡屠夫还不吃带毛猪了，我找不到最优权重阵，我找个过的去权重阵差不多意思意思，反正满足内生性条件之后，大样本性质总归是好的，至于小样本性质，那再说吧。 一般\(W_n=\mathrm{I}_n\)（单位阵）或者\(=\mathrm{inv}(Z’Z)\)（工具变量阵乘积的逆） 第二种叫做 two-step GMM，现在不是根据第一种方法有了参数的一个估计了嘛，那往前再走一步咯，我根据参数得到最优权重阵的一个估计， \[\left\{\frac{1}{n}\sum\left[g(x_i,\hat{\theta})g(x_i,\hat{\theta})&amp;#39;\right]\right\}^{-1}\] 然后再来一次 GMM 估计嘛。 第一、二种方法有一个小小的缺陷，就是初始权重阵的选取，会影响到参数的数值（numerical value）。 第三种叫做 Iterated Efficient（迭代有效）GMM，怎么讲，2 步迭代不够那 3 步迭代，3 步不够迭代 4 步，总有一步，会得到最优的估计的。那怎么判定是不是差不多最优了呢，一般用这次迭代得到的新参数和上次的参数做差，差充分小的时候，就表示逼近已经很成功了。 第四种方法理解起来复杂，叫做 Continuous-updating （连续更新）GMM。GMM 估计是在最小化方程 \[\min_{\hat{\theta}}\left[\frac{1}{N}\sum_ig(x_i,\hat{\theta})\right]^\prime W\left[\frac{1}{N}\sum_ig(x_i,\hat{\theta})\right]\] 然后最优权重阵 \[W=\left\{\frac{1}{n}\sum\left[g(x_i,\hat{\theta})g(x_i,\hat{\theta})&amp;#39;\right]\right\}^{-1}\] 我们直接代进去嘛，这样这个估计方程里面不就没有\(W\)只有参数了，然后估计参数就好了。 第三第四种方法的解，不依赖初始权重阵。理论上说，第三第四种方法的估计应该是渐进等价的，当然小样本性质可能有所差异。 但要注意，如果矩条件不是线性的，那没啥好说的大家都是非线性参数估计；如果矩条件是个线性的，前三种就是线性估计第四种方法还是非线性估计，相比来说，计算更加繁重，但其有限样本性质要稍好些，另外如果存在弱工具变量的问题，其也相对稳健（robust）。 刘澈（金融） 之前的答案没有针对金融/Asset pricing的，补充一个。 题主看 Cochrane 的 Asset Pricing 学 GMM，是想了解宏观金融。GMM 即是 Hansen and Singleton (1982) 专门为了解决宏观金融模型的参数估计问题开发的；Hansen 因其突出贡献还与其他两位金融经济学家共同获得了 2013 年诺贝尔经济学奖。GMM 被资产定价学者开发以后，由于其泛用性，传播到了经济学的其他各个领域，成为了计量经济学中的一种典型方法。 总的来说，GMM 想解决的是复杂系统中的参数估计问题。对于一个复杂的含参系统，估计其中的参数是很困难的，因为你的估计策略不可能照顾到这个系统的所有特征。GMM 方法提出，如果你的估计策略不能面面俱到，那么退而求其次，你的估计策略至少应当考虑到这个系统最重要的特征。GMM 的精髓就是这种简化的思路。 设想你只有一个简单的含参系统，例如一个线性均值方程\(\mathrm{E}(y|X)=b_0 &#43; b_1X\)。如果你想估计整个系统，那么你只需要估计其中的参数\(b_0\)与\(b_1\)即可。假设方程真实成立，那么想用数据{y, X}估计出\(b_0\)与\(b_1\)非常简单：只需将\(y\)对\(X\)做最简单的线性 OLS 回归即可。 但是设想你的参数系统比较复杂，比如宏观资产定价里最简单的一种 Euler Equation (with power utility)： \[\begin{equation}\label{eq:zhihuGMM:Liu1} \mathrm{E}_t\left[\beta\left(\frac{C_{t&#43;1}}{C_t}\right)^{-\gamma}R_{i,t&#43;1}\right] = 1 \end{equation}\] 其中\(\mathrm{E}_t\)是\(t\)时刻的条件期望，\(R_i\)是市场中任意一种资产的毛收益率，\(C_t\)为\(t\)时刻的消费，\(\beta\)是主观折现因子，\(\gamma\)是风险厌恶系数。如果你想估计整个系统，那么你只需要估计其中的参数\(\beta\)和\(\gamma\)即可。但是很显然，假设你能拿到消费、任意资产的毛收益率等一些经济数据，而且假设经济数据（作为随机变量）真的服从等式，那么一个简单的 OLS 是不能搞定参数估计的。原因很简单，因为这个系统太复杂了。所以，想估计这个系统，就必须简化问题。这个系统复杂的原因在于： 式\eqref{eq:zhihuGMM:Liu1}对于市场上的所有资产全都成立。所以实际上式\eqref{eq:zhihuGMM:Liu1}包含了无穷多个方程。但是这个太复杂了，估计出使得式\eqref{eq:zhihuGMM:Liu1}对于所有资产都成立的\(\beta\)和\(\gamma\)很困难。所以我们退而求其次。如果有\(\beta\)和\(\gamma\)会使得式\eqref{eq:zhihuGMM:Liu1}对于所有资产都成立，那么他们也会使得式\eqref{eq:zhihuGMM:Liu1}对你认为的最重要的资产成立。比如，如果你认为一个市场中最重要的资产是市场指数与无风险债券，那么当然式\eqref{eq:zhihuGMM:Liu1}对市场指数与无风险债券均成立，亦即 \[\begin{align} &amp;amp; \mathrm{E}_t\left[\beta\left(\frac{C_{t&#43;1}}{C_t}\right)^{-\gamma}R_{m,t&#43;1}\right] = 1\label{eq:zhihuGMM:Liu21}\\ &amp;amp; \mathrm{E}_t\left[\beta\left(\frac{C_{t&#43;1}}{C_t}\right)^{-\gamma}\right]R_{f,t} = 1\label{eq:zhihuGMM:Liu22} \end{align}\] 如果估计和就能够成功得到\(\beta\)和\(\gamma\)，那么就估计更为简单的和好了，因为其得到的\(\beta\)和\(\gamma\)也能使\eqref{eq:zhihuGMM:Liu1}成立。 但是，估计式\eqref{eq:zhihuGMM:Liu21}和\eqref{eq:zhihuGMM:Liu22}也太复杂了，因为式\eqref{eq:zhihuGMM:Liu21}和\eqref{eq:zhihuGMM:Liu22}仍然用“\(t\)时刻的条件期望”写成：对于任意的时间\(t\)，\eqref{eq:zhihuGMM:Liu21}和\eqref{eq:zhihuGMM:Liu22}式都必须成立。所以式\eqref{eq:zhihuGMM:Liu21}和实际包含了无穷多个方程，这种复杂程度使得我们没法进行参数估计。所以，我们必须进一步简化，简化的方式就是将\eqref{eq:zhihuGMM:Liu21}和\eqref{eq:zhihuGMM:Liu22}中的条件期望\(\mathrm{E}_t[...]\)简化为无条件期望\(\mathrm{E}[...]\)——自然是通过期望迭代定律（Law of Iterated Expectations）实现。但是，如果我直接将条件期望简化为无条件期望，我将无穷多等式简化为两个等式，损失的信息实在太多，这样不好。所以，为了避免在简化的过程中损失过多信息，我们一般会使用一些“工具变量”（instrumental variables）来丰富信息含量。 假设你认为市场中的 Price-dividend ratio 是比较重要的经济变量，你希望在你的估计中体现它，那么你就可以用它来做一个工具变量。记\(t\)时刻的 Price-dividend ratio 为\(z_t\)。式\eqref{eq:zhihuGMM:Liu21}可以变换为：\(z_t\mathrm{E}_t[\beta(\frac{C_{t&#43;1}}{C_t})^{-\gamma}R_{m,t&#43;1} -1] = 0\)，因为\(z_t\)是时刻\(t\)的变量，所以可以进一步变换为\(\mathrm{E}_t[z_t(\beta(\frac{C_{t&#43;1}}{C_t})^{-\gamma}R_{m,t&#43;1} -1)] = 0\)。等式两边使用迭代期望定律，得到无条件期望等式 \[\begin{equation}\label{eq:zhihuGMM:Liu31} \mathrm{E}\left\{z_t\left[\beta\left(\frac{C_{t&#43;1}}{C_t}\right)^{-\gamma}R_{m,t&#43;1} -1\right]\right\}= 0 \end{equation}\] 同理，式\eqref{eq:zhihuGMM:Liu22}也可以变换为 \[\begin{equation}\label{eq:zhihuGMM:Liu32} \mathrm{E}\left\{z_t\left[\beta\left(\frac{C_{t&#43;1}}{C_t}\right)^{-\gamma}R_{f,t} -1\right]\right\} = 0 \end{equation}\] 这样，我们就进一步将复杂的\eqref{eq:zhihuGMM:Liu21}和\eqref{eq:zhihuGMM:Liu22}简化成了\eqref{eq:zhihuGMM:Liu31}和\eqref{eq:zhihuGMM:Liu32}。 采用一个\(z_t\)，我们将无穷多个式子简化为了两个式子，简化程度很大。为了避免简化程度过大，我们一般会多选用一些工具变量。每选用一个工具变量，就增加两个无条件期望等式。比如，常数变量“1”显然也是一个工具变量。重复上面的操作，我们得到 \[\begin{equation}\label{eq:zhihuGMM:Liu33} \mathrm{E}\left[\beta\left(\frac{C_{t&#43;1}}{C_t}\right)^{-\gamma}R_{m,t&#43;1} -1\right]= 0 \end{equation}\] 和 \[\begin{equation}\label{eq:zhihuGMM:Liu34} \mathrm{E}\left[\beta\left(\frac{C_{t&#43;1}}{C_t}\right)^{-\gamma}R_{f,t} -1\right] = 0 \end{equation}\] 所以，为了估计\eqref{eq:zhihuGMM:Liu1}，我们利用合理选用重要资产与工具变量&#43;迭代期望定律的策略将\eqref{eq:zhihuGMM:Liu1}式简化为了几个较为简单的等式，并且选用了多个工具变量（本例为 2 个）来避免简化过度。最终，我们得到式\eqref{eq:zhihuGMM:Liu31}、\eqref{eq:zhihuGMM:Liu32}、\eqref{eq:zhihuGMM:Liu33}、\eqref{eq:zhihuGMM:Liu34}（本例包含 4 个式子/无条件期望等式/“矩条件”）。这个系统中有两个参数，四个等式。等式个数多于待估参数个数，可以进行估计。需要的数据为消费、市场指数收益率、无风险收益率和 Price-dividend ratio。事实上，这就是你建立的 GMM 问题。 为了避免实际计算时可能出现的过度识别（Over-identification）问题，采取@慧航和@Huang Zibin答案中的策略求解\(\beta\)和\(\gamma\)的 GMM 估计量：将\eqref{eq:zhihuGMM:Liu31}、\eqref{eq:zhihuGMM:Liu32}、\eqref{eq:zhihuGMM:Liu33}、\eqref{eq:zhihuGMM:Liu34}简记成\(\mathrm{E}[\mathbf{g}(C, R_f, R_m, z; \beta, \gamma)] = \mathbf{0}\)，这是一个4*1的向量等式。用样本矩\(\bar{\mathbf{g}} = \frac{1}{T}\sum_t g_t\)（平均数）替代总体矩（期望），对于一个4*4维的正定矩阵\(W\)，求解\(\min_{\beta, \gamma} \bar{\mathbf{g}}&amp;#39; W \bar{\mathbf{g}}\)，得到的解即为估计结果。求解一般通过数值方法，另如需提升估计精度可以使用两阶段 GMM、Continuous-updating GMM 等，均数细节，不再赘述。 本例与 Hansen and Singleton(1982) 不尽相同。可补充阅读 Hansen and Singleton(1982)。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[中文文本处理简要介绍 z]]></title>
    	<url>/data/2016/01/07/intro-to-chinese-nlp/</url>
		<content type="text"><![CDATA[[原文地址：https://cosx.org/2016/01/intro-to-chinese-nlp/ 本文作者李绳，博客地址 http://a 一位文科生曾励志成为语言学家 出国后阴差阳错成了博士候选人 三年后交完论文对学术彻底失望 回国后误打误撞成了数据科学家 作为一个处理自然语言数据的团队，我们在日常工作中要用到不同的工具来预处理中文文本，比如 Jieba 和 Stanford NLP software。出于准确性和效率的考虑，我们选择了Stanford NLP software， 所以本文将介绍基于 Stanford NLP software 的中文文本预处理流程。 中文文本处理简要介绍 与拉丁语系的文本不同，中文并不使用空格作为词语间的分隔符。比如当我们说“We love coding.”，这句英文使用了两个空格来分割三个英文词汇；如果用中文做同样的表述， 就是“我们爱写代码。”，其中不包含任何空格。因而，处理中文数据时，我们需要进行分词，而这恰恰时中文自然语言处理的一大难点。 下文将介绍中文文本预处理的几个主要步骤： 中文分词 标注词性 生成词向量 生成中文依存语法树 Stanford NLP software 简要介绍 Stanford NLP software 是一个较大的工具合集：包括Stanford POS tagger等组件，也有一个包含所有组件的合集Stanford CoreNLP。各个组件是由不同的开发者开发的，所以每一个工具都有自己的语法。当我们研究这些组件的文档时，遇到了不少问题。下文记录这些问题和相对应的对策，以免重蹈覆辙。 Stanford NLP 小组提供了一个简明的FAQ——Stanford Parser FAQ 和一份详细的Java文档 ——Stanford JavaNLP API Documentation。在这两份文档中，有几点格外重要： 尽管PSFG分词器小且快，Factored分词器更适用于中文，所以我们推荐使用后者。 中文分词器默认使用GB18030编码（Penn Chinese Treebank的默认编码）。 使用 -encoding 选项可以指定编码，比如 UTF-8，Big-5 或者 GB18030。 中文预处理的主要步骤 1. 中文分词 诚如上面所言，分词是中文自然语言处理的一大难题。Stanford Word Segmenter 是专门用来处理这一问题的工具。FAQ请参见 Stanford Segmenter FAQ。具体用法如下： bash -x segment.sh ctb INPUT_FILE UTF-8 0 其中 ctb 是词库选项，即 Chinese tree bank，也可选用 pku，即 Peking University。UTF-8是输入文本的编码，这个工具也支持 GB18030 编码。最后的0指定 n-best list 的大小，0表示只要最优结果。 2. 中文词性标注 词性标注是中文处理的另一大难题。我们曾经使用过 Jieba 来解决这个问题，但效果不尽理想。Jieba 是基于词典规则来标注词性的，所以任意一个词在 Jieba 里有且只有一个词性。如果一个词有一个以上的词性，那么它的标签就变成了一个集合。比如“阅读”既可以表示动词，也可以理解为名词，Jieba 就会把它标注成 n（名词），而不是根据具体语境来给出合适的 v（动词）或 n（名词）的标签。这样一来，标注的效果就大打折扣。幸好 Stanford POS Tagger 提供了一个根据语境标注词性的方法。具体用法如下： java -mx3000m -cp &amp;quot;./*&amp;quot; edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/chinese-distsim.tagger -textFile INPUT_FILE -mx3000m 指定内存大小，可以根据自己的机器配置选择。 edu.stanford.nlp.tagger.maxent.MaxentTagger 用于选择标注器，这里选用的是一个基于最大熵（Max Entropy）的标注器。 models/chinese-distsim.tagger 用于选择分词模型。 3. 生成词向量 深度学习是目前机器学习领域中最热门的一个分支。而生成一个优质的词向量是利用深度学习处理 NLP 问题的一个先决条件。除了 Google 的 Word2vec，Stanford NLP 小组提供了另外一个选项——GLOVE。 使用Glove也比较简单，下载并解压之后，只要对里面的 demo.sh 脚本进行相应修改，然后执行这个脚本即可。 CORPUS=text8 # 设置输入文件路径 VOCAB_FILE=vocab.txt # 设置输入词汇路径 COOCCURRENCE_FILE=cooccurrence.bin COOCCURRENCE_SHUF_FILE=cooccurrence.shuf.bin BUILDDIR=build SAVE_FILE=vectors # 设置输入文件路径 VERBOSE=2 MEMORY=4.0 # 设置内存大小 VOCAB_MIN_COUNT=5 # 设置词汇的最小频率 VECTOR_SIZE=50 # 设置矩阵维度 MAX_ITER=15 # 设置迭代次数 WINDOW_SIZE=15 # 设置词向量的窗口大小 BINARY=2 NUM_THREADS=8 X_MAX=10 4. 生成中文依存语法树 文本处理有时需要比词性更丰富的信息，比如句法信息，Stanford NLP 小组提供了两篇论文： The Stanford Parser: A statistical parser 和 Neural Network Dependency Parser，并在这两篇论文的基础上开发了两个工具，可惜效果都不太理想。前者的处理格式是正确的中文依存语法格式，但是速度极慢（差不多一秒一句）；而后者虽然处理速度较快，但生成的格式和论文 Discriminative reordering with Chinese grammatical relations features – acepor中的完全不一样。我们尝试了邮件联系论文作者和工具作者，并且在 Stackoverflow 上提问，但这个问题似乎无解。 尽管如此，我们还是把两个方案都记录在此： java -cp &amp;quot;*:.&amp;quot; -Xmx4g edu.stanford.nlp.pipeline.StanfordCoreNLP -file INPUT_FILE -props StanfordCoreNLP-chinese.properties -outputFormat text -parse.originalDependencies java -cp &amp;quot;./*&amp;quot; edu.stanford.nlp.parser.nndep.DependencyParser -props nndep.props -textFile INPUT_FILE -outFile OUTPUT_FILE 结论 预处理中文文本并非易事，Stanford NLP 小组对此作出了极大的贡献。我们的工作因而受益良多，所以我们非常感谢他们的努力。当然我们也期待 Stanford NLP software 能更上一层楼。 本文原载于 https://acepor.github.io/2015/12/17/General-Pipelines/。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[数据挖掘与机器学习关系与区别 z]]></title>
    	<url>/data/2016/01/07/data-mining-machine-learing/</url>
		<content type="text"><![CDATA[[原文地址：http://blog.csdn.net/phla_han/article/details/50476406 概念定义 机器学习：广泛 “利用经验来改善计算机系统的自身性能。”，事实上，由于“经验”在计算机系统中主要是以数据的形式存在的，因此机器学习需要设法对数据进行分析，这就使得它逐渐成为智能数据分析技术的创新源之一，并且为此而受到越来越多的关注。 数据挖掘：一种解释是“识别出巨量数据中有效的、新颖的、潜在有用的、最终可理解的模式的非平凡过程”，顾名思义，数据挖掘就是试图从海量数据中找出有用的知识。 关系 关系：数据挖掘可以认为是数据库技术与机器学习的交叉，它利用数据库技术来管理海量的数据，并利用机器学习和统计分析来进行数据分析。其关系如下图1： 数据挖掘受到了很多学科领域的影响，其中数据库、机器学习、统计学无疑影响最大。粗糙地说，数据库提供数据管理技术，机器学习和统计学提供数据分析技术。由于统计学界往往醉心于理论的优美而忽视实际的效用，因此，统计学界提供的很多技术通常都要在机器学习界进一步研究，变成有效的机器学习算法之后才能再进入数据挖掘领域。从这个意义上说，统计学主要是通过机器学习来对数据挖掘发挥影响，而机器学习和数据库则是数据挖掘的两大支撑技术。 区别 数据挖掘并非只是机器学习在工业上的简单应用，他们之间至少包含如下两点重要区别： 传统的机器学习研究并不把海量数据作为处理对象，因此，数据挖掘必须对这些技术和算法进行专门的、不简单的改造。 作为一个独立的学科，数据挖掘也有其独特的东西，即：关联分析。简单地说，关联分析就是希望从数据中找出“买尿布的人很可能会买啤酒”这样看起来匪夷所思但可能很有意义的模式。 参考：《数据挖掘与机器学习》, by 周志华 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python - SDK 动态调用 z]]></title>
    	<url>/tech/2016/01/03/chain-invoke-rest-sdk-api/</url>
		<content type="text"><![CDATA[[原文地址：http://blog.csdn.net/zxvivian/article/details/50450070 现在很多网站搞 REST API SDK，给每个 URL 写一个对应的 API 太麻烦，可以利用 Python 中完全动态的 __getattr__ 完成链式调用。 如 GitHub 的 API：Get /user/:user/repos，调用时将:user替换为实际用户名。 class Chain(object): def __init__(self,user=&#39;&#39;): self.user = user def __getattr__(self, attr): return Chain(&#39;%s/%s&#39; % (self.user,attr)) def __str__(self): return self.user def __call__(self,param): return Chain(&#39;%s/%s&#39; % (self.user,param)) print(Chain().users(&#39;michael&#39;).repos) 输出：/users/michael/repos。 上面的类可以进行完善，支持所有类型的参数： class Chain2(object): def __init__(self, path=&#39;&#39;): self.path = path def __getattr__(self, attr): return Chain2(&#39;%s/%s&#39; %(self.path, attr)) def __str__(self): return self.path def __call__(self, param): return Chain2(&#39;%s/%s&#39; %(self.path, str(param))) __repr__ = __str__ print(Chain2().users(&#39;michael&#39;).age(12345).repos) 输出：/users/michael/age/12345/repos。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[北京限行政策降低了污染么？ - 慧航 - 专栏]]></title>
    	<url>/prof/2015/12/08/driving-restrictions-on-pollution/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20072406，这篇文章在评论区中有较多争论，但大多不专业，可以简单参 近年来，环境污染问题，特别是雾霾的问题，已经成为公众和政府最为关心的问题之一。据估算，2005 年中国可悬浮颗粒物造成的经济损失达 224 亿美元（Matus et al, 2012）。而为了治理污染问题，同时为了缓解城市的交通堵塞问题，北京推出了限行政策，然而这项政策对环境污染究竟是否有作用仍然是一个十分有争议的话题。 长江商学院的 V. Brian Viard 以及西南财经大学的 Shihe Fu 最近发表在 Journal of Public Economics 中的文章给出了新的证据。 顺便提一下，这篇文章被接受后，Fu老师发了一条微博： 可见经济学研究之不易。 虽然限行政策比较简单，直觉上应该是减少了出行的车辆进而减少污染，但是现实中，家庭可能会购买第二辆车，或者调整驾车时间，从而使得这项政策变的无效。 而现实中车辆的尾气排放是空气中可悬浮颗粒物的重要来源。Jiang(2006) 的研究表明，北京的 PM 10 大约有 53% 来源于机动车（其中 23% 来源于尾气排放，30% 来源于道路的灰尘）。 北京在 2008 年开始推行限行政策。2008 年 7 月 20 日开始，北京推行了“奇偶日”（OddEven）的限行政策。根据这项政策，除午夜到凌晨3点这段时间外，汽车只能隔一天出行。这项政策直到 2008 年 9 月 20 日废除。而 2008 年 10 月 11 日，政府又推出了新的限行政策（“一周一天”，OneDay69），根据这项政策，除晚上 9 点到早上 6 点这段时间之外，每辆车每周限行一天。而到了 2009 年的 4 月 11 日，政府把宽限时间调整为除晚上 8 点到早上 7 点（OneDay78）。下面一张图列举了北京与污染有关的政策执行时间： 为了解释限行政策的起作用的机制，作者还提出了一个理论模型。在该理论模型中，有两类工人：上班时间自由支配的工人、上班时间固定的工人。对于上班时间固定的工人来讲，由于限行政策，只能选择公共交通上下班；而对于上班时间自由支配的工人，则可以自由选择出行方式以及工作时间。在“扩展边际”（extensive margin）上，模型有如下的推论： 对于固定工作时间的工人，限行政策不会影响其工作与闲暇时间。 对于上班时间自由支配的工人，工作时间减少，闲暇增加。 总的机动车出行减少，污染降低 而在“集约边际”（intensive margin）上，那些上班时间自由支配的工人，如果从有从驾车到公共交通的转变，那么如果公共交通更快，则闲暇时间增加。 作者使用了北京每日的空气污染数据（加总、观测站两个层面）以及电视收看数据，使用断点回归、双重差分等方法对以上理论进行了检验。 首先作者使用加总的空气污染指数对政策施行前后做了断点回归来检验政策实施之后污染水平是不是下降了。如下图所示，在施行了“一周一天”政策之后，空气污染水平的确有了不小的降低： 当然，图形不能代替统计分析。加总水平的空气污染指数的断点回归结果如下表： 可见，不管是“奇偶日”政策还是“一周一天”政策，都显著的降低了污染水平。 以上的断点回归设计实际上只使用了时间序列上的信息。此外，作者还使用了 DID 的设计，使用了基于检测站点的空间上的信息。对于那些靠近市中心的检测站点，政策施行之后，污染水平应该降低的多，而其实证结果也证实了这个猜想（表格在此忽略）。 经过作者测算，如果汽油价格增加 2.25 元每加仑，那么达到的效果与限行政策达到的效果一致。 这篇文章最有意思的地方还在于，作者使用了电视收看的比率来观察北京市民的出行行为。由于作者的数据可以区分“按时上下班”的工人和那些自营业务的人，因而可以对其看电视（闲暇）的行为进行比较，从而确认以上政策影响的渠道。 同样使用断点回归设计，将一天分成三个时间段：早、晚以及开车受限的时间段，观察两类工人看电视的情况，结果见下表： 可见两类工人在受限的时间与非受限的时间表现出了完全不同的行为。对于时间自由安排的工人来说，出行开车受限的时间，电视的收视率明显提高，而非受限的时间则明显降低，这与理论预测一致，即这项政策使得他们增加了闲暇时间。 而对于固定工作时间的工人来讲，与理论模型的第一个预测一致，即其工作时间并没有受到影响。但是同时注意到，在 OneDay78 的政策下，受限时间电视收视率下降了 8.3%，作者的解释是由于污染降低了因而他们或他们的孩子生病的可能降低了，所以请假的时间少了。 最终，作者得到结论，即北京实行的限行政策的确降低了空气污染，然而与此同时，这项政策也降低了部分人的劳动供给。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[使用ggtree实现进化树的可视化和注释 z]]></title>
    	<url>/tech/2015/11/30/to-achieve-the-visualization-and-annotation-of-evolutionary-tree-using-ggtree/</url>
		<content type="text"><![CDATA[[原文地址： https://cosx.org/2015/11/to-achieve-the-visualization-and-annotation-of-evolutionary-tree-using-ggtree/ 本文作者：余光创，目前就读于香港大学公共卫生系，开发过多个R/Bioconductor包，包括 ChIPseeker, clusterProfiler, DOSE, ggtree, GOSemSim和 ReactomePA。 进化树看起来和层次聚类很像。有必要解释一下两者的一些区别。 层次聚类的侧重点在于分类，把距离近的聚在一起。而进化树的构建可以说也是一个聚类过程，但侧重点在于推测进化关系和进化距离(evolutionary distance)。 层次聚类的输入是距离，比如euclidean或manhattan距离。把距离近的聚在一起。而进化树推断是从生物序列（DNA或氨基酸）的比对开始。最简单的方法是计算一下序列中不匹配的数目，称之为hamming distance（通常用序列长度做归一化），使用距离当然也可以应用层次聚类的方法。进化树的构建最简单的方法是非加权配对平均法（Unweighted Pair Group Method with Arithmetic Mean, UPGMA），这其实是使用average linkage的层次聚类。这种方法在进化树推断上现在基本没人用。更为常用的是邻接法（neighbor joining），两个节点距离其它节点都比较远，而这两个节点又比较近，它们就是neighbor，可以看出neighbor不一定是距离最近的两个节点。真正做进化的人，这个方法也基本不用。现在主流的方法是最大似然法(Maximum likelihood, ML)，通过进化模型（evolutionary model)估计拓朴结构和分支长度，估计的结果具有最高的概率能够产生观测数据（多序列比对）。另外还有最大简约法和贝叶斯推断等方法用于构建进化树。 是最常用的存储进化树的文件格式，如上面这个树，拓朴结构用newick格式可以表示为： (B,(A,C,E),D); 括号最外层是根节点，它有三个子节点，B, (A,C,E)和D，而节点(A,C,E)也有三个子节点A，C和E。 加上分支长度，使用 : 来分隔： (B:6.0,(A:5.0,C:3.0,E:4.0):5.0,D:11.0); 比如A:5.0代表的是A与其父节点的距离是5.0。 内部节点也可以有label，写在相应的括号外面，如下所示： (B:6.0,(A:5.0,C:3.0,E:4.0)Ancestor1:5.0,D:11.0); 这是最为广泛支持的文件格式，很多进化树可视软件只支持newick格式。 ggtree的开发源自于我需要在树上做注释，发现并没有软件可以很容易地实现，通常情况下我们把统计信息加到节点的label上来展示，比如CodeML的dN/dS分析，输出文件里就给用户准备了newick树文本，把dN/dS ( \(\omega\) ) 加于节点label之上: codeml_file &amp;lt;-system.file(&amp;quot;extdata/PAML_Codeml/mlc&amp;quot;, package=&amp;quot;ggtree&amp;quot;) tree_text &amp;lt;-readLines(codeml_file)[375:376] tree_text # [1] &amp;quot;w ratios as labels for TreeView:&amp;quot; # [2] &amp;quot;(K #0.0224 , N #0.0095 , (D #0.0385 , (L #0.0001 , (J #0.0457 , (G #0.1621 , ((C #0.0461 , (E #0.0641 , O #0.0538 ) #0.0001 ) #0.0395 , (H #0.1028 , (I #0.0001 , (B #0.0001 , (A #0.0646 , (F #0.2980 , M #0.0738 ) #0.0453 ) #0.0863 ) #1.5591 ) #0.0001 ) #0.0001 ) #0.0549 ) #0.0419 ) #0.0001 ) #0.0964 ) #0.0129 );&amp;quot; 这种做法只能展示一元信息，而且修改节点label真心是个脏活，满满的都是不爽，我心中理想的方式是树与注释信息分开，注释信息可以方便地通过图层加上去，而且可以自由组合。于是着手开发ggtree是个简单易用的R包，一行代码 ggtree(read.tree(file)) 即可实现树的可视化。而注释通过图层来实现，多个图层可以完成复杂的注释，这得力于ggtree的设计。其中最重要的一点是如何来解析进化树。 进化树的解析 除了ggtree之外，我所了解到的其它画树软件在画树的时候都把树当成是线条的集合。很明显画出来的进化树就是在画一堆线条，但是线条表示的是父节点和子节点的关系，除此之外没有任何意义，而节点在进化树上代表物种，叶子节点是我们构建进化树的物种，内部节点是根据叶子节点推断的共同祖先。我们所有的进化分析、推断、实验都是针对节点，节点才是进化树上有意义的实体。这是ggtree设计的基础，ggtree只映射节点到坐标系统中，而线条在 geom_tree 图层中计算并画出来。这是与其它软件最根本的不同，也是ggtree能够简单地用图层加注释信息的基础。 扩展ggplot2 有很多可视化包基于ggplot2实现，包括各种 gg 打头的，号称扩展了ggplot2，支持图形语法(grammar of graphics)，我并不认同。虽然基于 ggplot2 产生的图，我们可以用theme来进一步调整细节，用scale_系列函数来调整颜色和标尺的映射，但这些不足以称之为支持图形语法，图形语法最关键核心的部分我认为是图层和映射。 像ggphylo, OutbreakTools和phyloseq这几个包都有基于ggplot2的画树函数，但其实都不支持图形语法，它们所实现的是复杂的函数，画完就完事了，用户并不能使用图层来添加相关的信息。 如果在 OutbreakTools 这个包中： if (show.tip.label) { p &amp;lt;- p &#43; geom_text(data = df.tip, aes(x = x, y = y, label = label), hjust = 0, size = tip.label.size) } 如果show.tip.label=FALSE，当函数返回p 时 df.tip 就被扔掉，用户想要再加 tip.label 就不可能了。 ggphylo 和 phyloseq 都是类似的实现，这些包把树解析为线条，所以节点相关的信息需要额外的 data.frame 来存储，并且只有极少数的预设参数，比如上面例子中的tip.label。在上面的例子中，用户连更改 tip.label 的颜色都不可能，更别说使用额外的注释信息了。 这几个包所实现的画图函数，都可以很容易地用ggtree实现，并用经过测试，ggtree运行速度比这几个包都要快。更多信息请参考ggtree的wiki页面。 ggtree是真正扩展ggplot2，支持图形语法的包。我们首先扩展ggplot支持tree object做为输入，并实现geom_tree图层来画线条。 library(ggplot2) library(ggtree) set.seed(2015-11-26) tree &amp;lt;-rtree(30) ggplot(tree, aes(x, y)) &#43; geom_tree() ggtree函数是 ggplot() &#43; geom_tree() &#43; xlab(NA) &#43; ylab(NA) &#43; theme_tree() 的简单组合。 ggtree(tree) 想要加 tip.label，用 geom_tiplab 图层，并且ggplot2的图层都可以直接应用 ggtree。 ggtree(tree) &#43; geom_tiplab() &#43; geom_point(color=&#39;firebrick&#39;) 树的操作与注释 ggtree提供了多个函数可以把clade放大缩小(scaleClade)，折叠(collapse)和展开(expand)，位置调换和旋转，以及分类(groupOTU, groupClade)。 nwk &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;sample.nwk&amp;quot;, package=&amp;quot;ggtree&amp;quot;) tree &amp;lt;- read.tree(nwk) p &amp;lt;- ggtree(tree) cp &amp;lt;- ggtree(tree) %&amp;gt;% collapse(node=21) &#43; ggtitle(&#39;collapse&#39;) ep &amp;lt;- cp &#43; expand(node=21) &#43; ggtitle(&#39;expand&#39;) hp &amp;lt;- p &#43; hilight(node=21) &#43; ggtitle(&#39;hilight&#39;) rp &amp;lt;- hp &#43; rotate(node=21) &#43; ggtitle(&#39;rotate&#39;) library(gridExtra) grid.arrange(cp, ep, hp, rp, ncol=2) 支持多种文件格式 ggtree支持的文件格式包括Newick, Nexus, NHX和jplace。 上面已经展示了Newick格式，下面的例子是NHX格式： nhxfile = system.file(&amp;quot;extdata&amp;quot;, &amp;quot;ADH.nhx&amp;quot;, package=&amp;quot;ggtree&amp;quot;) nhx &amp;lt;-read.nhx(nhxfile) ggtree(nhx, ladderize=F) &#43; geom_tiplab() &#43; geom_point(aes(color=S), size=8, alpha=.3) &#43; theme(legend.position=&amp;quot;right&amp;quot;) &#43; geom_text(aes(label=branch.length, x=branch), vjust=-.5) &#43; xlim(NA, 0.3) 支持解析多种软件的输出文件 我们知道FigTree是针对BEAST的输出设计的，可以把BEAST的统计推断拿来给树做注释，但很多的进化分析软件并没有相应的画树软件支持，用户很难把信息展示出来。 ggtree支持ape, phangorn, r8s, RAxML, PAML, HYPHY, EPA, pplacer和BEAST的输出。相应的统计分析结果可以应用于树的注释。可以说ggtree把这些软件分析的结果带给了R用户，通过ggtree的解析， 这些进化分析结果可以进一点在R里进行处理和统计分析，并不单单是在ggtree中展示而已。 RAxML bootstrap分析 raxml_file &amp;lt;-system.file(&amp;quot;extdata/RAxML&amp;quot;, &amp;quot;RAxML_bipartitionsBranchLabels.H3&amp;quot;, package=&amp;quot;ggtree&amp;quot;) raxml &amp;lt;-read.raxml(raxml_file) ggtree(raxml) &#43; geom_text(aes(label=bootstrap, color=bootstrap)) &#43; scale_color_gradient(high=&#39;red&#39;, low=&#39;darkgreen&#39;) &#43; theme(legend.position=&#39;right&#39;) multiPhylo也是支持的，所以100颗bootstrap树可以同时用一行代码展示出来。 btree_file &amp;lt;-system.file(&amp;quot;extdata/RAxML&amp;quot;, &amp;quot;RAxML_bootstrap.H3&amp;quot;, package=&amp;quot;ggtree&amp;quot;) btree = read.tree(btree_file) ggtree(btree) &#43; facet_wrap(~.id, ncol=10) 如果不分面，这100颗树会重叠画在一起，这也能很好地展示bootstrap分析的结果，bootstrap值低的clade，线条会比较乱，而bootstrap值高的地方，线条一致性比较好。 PAML 使用BaseML预测的祖先序列，ggtree解析结果的同时会把父节点到子节点的subsitution给统计出来，可以直接在树上注释： rstfile &amp;lt;-system.file(&amp;quot;extdata/PAML_Baseml&amp;quot;, &amp;quot;rst&amp;quot;, package=&amp;quot;ggtree&amp;quot;) rst &amp;lt;-read.paml_rst(rstfile) p &amp;lt;-ggtree(rst) &#43; geom_text(aes(label=marginal_AA_subs, x=branch), vjust=-.5) print(p) 不同于BaseML以碱基为单位，CodeML预测祖先序列，以密码子为单位。`ggtree`定义了一个操作符%&amp;lt;%，如果有相同的注释信息要展示，可以用tree object来更新tree view。 rstfile &amp;lt;-system.file(&amp;quot;extdata/PAML_Codeml&amp;quot;, &amp;quot;rst&amp;quot;, package=&amp;quot;ggtree&amp;quot;) crst &amp;lt;-read.paml_rst(rstfile) p %&amp;lt;% crst 像上面的例子，用crst来更新p，就是用crst画出来的树&#43;注释。对比两图，可以发现BaseML和CodeML推测的祖先序列是稍有不同的。 CodeML的dN/dS分析，我们可以直接把数据拿来给树上色。同样道理分类数据也可以拿来上色。 mlc_file &amp;lt;-system.file(&amp;quot;examples/mlc&amp;quot;, package=&amp;quot;ggtree&amp;quot;) mlc &amp;lt;-read.codeml_mlc(mlc_file) ggtree(mlc, aes(color=dN_vs_dS)) &#43; scale_color_continuous(limits=c(0, 1.5), high=&#39;red&#39;, low=&#39;green&#39;, oob=scales::squish, name=&#39;dN/dS&#39;) &#43; theme(legend.position=&#39;right&#39;) 使用用户定义数据 进化树已经被广泛应用于各种跨学科的研究中，随着实验技术的发展，各种数据也更易于获得，使用用户数据注释进化树，也是ggtree所支持的。 nwk &amp;lt;-system.file(&amp;quot;extdata&amp;quot;, &amp;quot;sample.nwk&amp;quot;, package=&amp;quot;ggtree&amp;quot;) tree &amp;lt;-read.tree(nwk) p &amp;lt;-ggtree(tree) dd &amp;lt;-data.frame(taxa = LETTERS[1:13], place = c(rep(&amp;quot;GZ&amp;quot;, 5), rep(&amp;quot;HK&amp;quot;, 3), rep(&amp;quot;CZ&amp;quot;, 4), NA), value = round(abs(rnorm(13, mean=70, sd=10)), digits=1)) ## you don&#39;t need to order the data ## data was reshuffled just for demonstration dd &amp;lt;-dd[sample(1:13, 13), ] row.names(dd) &amp;lt;- NULL print(dd) 在上面的例子中，使用一个分类数据和一个连续型数据，输入的唯一要求是第一列是taxon label。ggtree中定义了操作符%&amp;lt;&#43;%，来添加数据。添加之后，用户的数据对ggplot是可见的。可以用于树的注释。 p &amp;lt;- p %&amp;lt;&#43;% dd &#43; geom_text(aes(color=place, label=label), hjust=-0.5) &#43; geom_tippoint(aes(size=value, shape=place, color=place), alpha=0.25) p&#43;theme(legend.position=&amp;quot;right&amp;quot;) ggtree还支持用户把自己的数据和树保存为jplace格式。 更多的实例请参考vignette。 ggtree允许把不同软件的分析结果整合在一起，同时在树上展示或者比较结果。在我们提交的论文中，使用了整合BEAST和CodeML的例子，在树上展示dN/dS、时间轴、氨基酸替换、clade support values、物种和基因型 (genotype）等多维信息，6种不同的信息同时展示在一颗进化树上，这是个复杂的例子，我们在附件1中展示了可重复的代码。如果有兴趣，可以留意一下我们的文章。 🙂 其它好玩的功能 我们把树当成节点的集合，而不是线条的集合，这一点回归到了进化树的本质意义上，使这一实现成为可能。而支持图形语法，与ggplot2的无缝衔接又让注释变得更加容易ggtree为我们打开了各种注释和操作的可能性。甚至于可以创造出好玩的图，比如使用showtext来加载图形化的字体、用emoji来画树、使用图片来注释树等等。 一个比较正经又好玩的是使用PhyloPic数据库上的图形。 pp &amp;lt;-ggtree(tree) %&amp;lt;&#43;% phylopic(&amp;quot;79ad5f09-cf21-4c89-8e7d-0c82a00ce728&amp;quot;, color=&amp;quot;steelblue&amp;quot;, alpha = .3) pp &#43; geom_tiplab(align=T, linetype=&#39;dashed&#39;, linesize=.5) &#43; geom_tippoint(color=&#39;firebrick&#39;, size=2) 另一个好玩又为我们展现各种可能性的是subview函数，它使得图上加小图变得特别容易。并且已经被应用于地图上加饼图。解决这个问题的初衷在于，想要给节点加饼图注释。有了subview函数之后，这会变得很容易，当然我还没有写出给节点加饼图的函数，因为我还没有这个需求，得有一些实际的数据做参考，这样才能够设计出更易用的函数呈现给用户。 很多的功能还在开发之中，有问题/建议请及时在Github上报告(中英文都可以)。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[一行 R 代码来实现繁琐的可视化 z]]></title>
    	<url>/tech/2015/11/24/ggfortify-visualization-in-one-line-of-code/</url>
		<content type="text"><![CDATA[[原文地址：https://cosx.org/2015/11/ggfortify-visualization-in-one-line-of-code/ 本文作者：唐源，目前就职于芝加哥一家创业公司，曾参与和创作过多个被广泛使用的R和Python开源项目，是ggfortify，lfda，metric-learn等包的作者，也是 xgboost，caret，pandas等包的贡献者。（喜欢爬山和烧烤） ggfortify 是一个简单易用的R软件包，它可以仅仅使用一行代码来对许多受欢迎的R软件包结果进行二维可视化，这让统计学家以及数据科学家省去了许多繁琐和重复的过程，不用对结果进行任何处理就能以 ggplot 的风格画出好看的图，大大地提高了工作的效率。 ggfortify 已经可以在 CRAN 上下载得到，但是由于最近很多的功能都还在快速增加，因此还是推荐大家从 Github 上下载和安装。 library(devtools) install_github(&#39;sinhrks/ggfortify&#39;) library(ggfortify) 接下来我将简单介绍一下怎么用 ggplot2 和 ggfortify 来很快地对PCA、聚类以及LFDA的结果进行可视化，然后将简单介绍用 ggfortify 来对时间序列进行快速可视化的方法。 PCA (主成分分析) ggfortify 使 ggplot2 知道怎么诠释PCA对象。加载好 ggfortify 包之后, 你可以对stats::prcomp 和 stats::princomp 对象使用 ggplot2::autoplot。 library(ggfortify) df &amp;lt;- iris[c(1, 2, 3, 4)] autoplot(prcomp(df)) 你还可以选择数据中的一列来给画出的点按类别自动分颜色。输入help(autoplot.prcomp) 可以了解到更多的其他选择。 autoplot(prcomp(df), data = iris, colour = &#39;Species&#39;) 比如说给定label = TRUE 可以给每个点加上标识（以rownames为标准），也可以调整标识的大小。 autoplot(prcomp(df), data = iris, colour = &#39;Species&#39;, label = TRUE,label.size = 3) 给定 shape = FALSE 可以让所有的点消失，只留下标识，这样可以让图更清晰，辨识度更大。 autoplot(prcomp(df), data = iris, colour = &#39;Species&#39;, shape = FALSE,label.size = 3) 给定 loadings = TRUE 可以很快地画出特征向量。 autoplot(prcomp(df), data = iris, colour = &#39;Species&#39;, loadings = TRUE) 同样的，你也可以显示特征向量的标识以及调整他们的大小，更多选择请参考帮助文件。 autoplot(prcomp(df), data = iris, colour = &#39;Species&#39;, loadings = TRUE, loadings.colour = &#39;blue&#39;, loadings.label = TRUE, loadings.label.size = 3) 因子分析 和PCA类似，ggfortify 也支持 stats::factanal 对象。可调的选择也很广泛。以下给出了简单的例子： 注意 当你使用 factanal 来计算分数的话，你必须给定 scores 的值。 d.factanal &amp;lt;- factanal(state.x77, factors = 3, scores = &#39;regression&#39;) autoplot(d.factanal, data = state.x77, colour = &#39;Income&#39;) autoplot(d.factanal, label = TRUE, label.size = 3, loadings = TRUE, loadings.label = TRUE, loadings.label.size = 3) K-均值聚类 autoplot(kmeans(USArrests, 3), data = USArrests) autoplot(kmeans(USArrests, 3), data = USArrests, label = TRUE, label.size = 3) 其他聚类 ggfortify 也支持 cluster::clara, cluster::fanny, cluster::pam。 library(cluster) autoplot(clara(iris[-5], 3)) 给定 frame = TRUE，可以把 stats::kmeans 和 cluster::* 中的每个类圈出来。 autoplot(fanny(iris[-5], 3), frame = TRUE) 你也可以通过 frame.type 来选择圈的类型。更多选择请参照 ggplot2::stat_ellipse 里面的 frame.type 的 type 关键词。 autoplot(pam(iris[-5], 3), frame = TRUE, frame.type = &#39;norm&#39;) 更多关于聚类方面的可视化请参考 Github 上的 Vignette 或者 Rpubs 上的例子。 lfda（Fisher局部判别分析） lfda 包支持一系列的 Fisher 局部判别分析方法，包括半监督 lfda，非线性 lfda。你也可以使用 ggfortify 来对他们的结果进行可视化。 library(lfda) # Fisher局部判别分析 (LFDA) model &amp;lt;- lfda(iris[-5], iris[, 5], 4, metric=&amp;quot;plain&amp;quot;) autoplot(model, data = iris, frame = TRUE, frame.colour = &#39;Species&#39;) # 非线性核Fisher局部判别分析 (KLFDA) model &amp;lt;- klfda(kmatrixGauss(iris[-5]), iris[, 5], 4, metric=&amp;quot;plain&amp;quot;) autoplot(model, data = iris, frame = TRUE, frame.colour = &#39;Species&#39;) 注意 对iris数据来说，不同的类之间的关系很显然不是简单的线性，这种情况下非线性的klfda 影响可能太强大而影响了可视化的效果，在使用前请充分理解每个算法的意义以及效果。 # 半监督Fisher局部判别分析 (SELF) model &amp;lt;- self(iris[-5], iris[, 5], beta = 0.1, r = 3, metric=&amp;quot;plain&amp;quot;) autoplot(model, data = iris, frame = TRUE, frame.colour = &#39;Species&#39;) 时间序列的可视化 用 ggfortify 可以使时间序列的可视化变得极其简单。接下来我将给出一些简单的例子。 ts对象 library(ggfortify) autoplot(AirPassengers) 可以使用 ts.colour 和 ts.linetype 来改变线的颜色和形状。更多的选择请参考 help(autoplot.ts)。 autoplot(AirPassengers, ts.colour = &#39;red&#39;, ts.linetype = &#39;dashed&#39;) 多变量时间序列 library(vars) data(Canada) autoplot(Canada) 使用 facets = FALSE 可以把所有变量画在一条轴上。 autoplot(Canada, facets = FALSE) autoplot 也可以理解其他的时间序列类别。可支持的R包有： zoo::zooreg xts::xts timeSeries::timSeries tseries::irts 一些例子： library(xts) autoplot(as.xts(AirPassengers), ts.colour = &#39;green&#39;) library(timeSeries) autoplot(as.timeSeries(AirPassengers), ts.colour = (&#39;dodgerblue3&#39;)) 你也可以通过 ts.geom 来改变几何形状，目前支持的有 line， bar 和 point。 autoplot(AirPassengers, ts.geom = &#39;bar&#39;, fill = &#39;blue&#39;) autoplot(AirPassengers, ts.geom = &#39;point&#39;, shape = 3) forecast包 library(forecast) d.arima &amp;lt;- auto.arima(AirPassengers) d.forecast &amp;lt;- forecast(d.arima, level = c(95), h = 50) autoplot(d.forecast) 有很多设置可供调整： autoplot(d.forecast, ts.colour = &#39;firebrick1&#39;, predict.colour = &#39;red&#39;, predict.linetype = &#39;dashed&#39;, conf.int = FALSE) vars包 library(vars) data(Canada) d.vselect &amp;lt;- VARselect(Canada, lag.max = 5, type = &#39;const&#39;)$selection[1] d.var &amp;lt;- VAR(Canada, p = d.vselect, type = &#39;const&#39;) autoplot(predict(d.var, n.ahead = 50), ts.colour = &#39;dodgerblue4&#39;, predict.colour = &#39;blue&#39;, predict.linetype = &#39;dashed&#39;) changepoint包 library(changepoint) autoplot(cpt.meanvar(AirPassengers)) autoplot(cpt.meanvar(AirPassengers), cpt.colour = &#39;blue&#39;, cpt.linetype = &#39;solid&#39;) strucchange包 library(strucchange) autoplot(breakpoints(Nile ~ 1), ts.colour = &#39;blue&#39;, ts.linetype = &#39;dashed&#39;, cpt.colour = &#39;dodgerblue3&#39;, cpt.linetype = &#39;solid&#39;) dlm包 library(dlm) form &amp;lt;- function(theta){ dlmModPoly(order = 1, dV = exp(theta[1]), dW = exp(theta[2])) } model &amp;lt;- form(dlmMLE(Nile, parm = c(1, 1), form)$par) filtered &amp;lt;- dlmFilter(Nile, model) autoplot(filtered) autoplot(filtered, ts.linetype = &#39;dashed&#39;, fitted.colour = &#39;blue&#39;) smoothed &amp;lt;- dlmSmooth(filtered) autoplot(smoothed) p &amp;lt;- autoplot(filtered) autoplot(smoothed, ts.colour = &#39;blue&#39;, p = p) KFAS包 library(KFAS) model &amp;lt;- SSModel( Nile ~ SSMtrend(degree=1, Q=matrix(NA)), H=matrix(NA) ) fit &amp;lt;- fitSSM(model=model, inits=c(log(var(Nile)),log(var(Nile))), method=&amp;quot;BFGS&amp;quot;) smoothed &amp;lt;- KFS(fit$model) autoplot(smoothed) 使用 smoothing=&#39;none&#39; 可以画出过滤后的结果。 filtered &amp;lt;- KFS(fit$model, filtering=&amp;quot;mean&amp;quot;, smoothing=&#39;none&#39;) autoplot(filtered) trend &amp;lt;- signal(smoothed, states=&amp;quot;trend&amp;quot;) p &amp;lt;- autoplot(filtered) autoplot(trend, ts.colour = &#39;blue&#39;, p = p) stats包 可支持的stats包里的对象有： stl, decomposed.ts acf, pacf, ccf spec.ar, spec.pgram cpgram autoplot(stl(AirPassengers, s.window = &#39;periodic&#39;), ts.colour = &#39;blue&#39;) autoplot(acf(AirPassengers, plot = FALSE)) autoplot(acf(AirPassengers, plot = FALSE), conf.int.fill = &#39;#0000FF&#39;, conf.int.value = 0.8, conf.int.type = &#39;ma&#39;) autoplot(spec.ar(AirPassengers, plot = FALSE)) ggcpgram(arima.sim(list(ar = c(0.7, -0.5)), n = 50)) library(forecast) ggtsdiag(auto.arima(AirPassengers)) gglagplot(AirPassengers, lags = 4) 更多关于时间序列的例子，请参考 Rpubs 上的介绍。 最近又多了许多额外的非常好用的功能，比如说现在已经支持 multiplot 同时画多个不同对象，强烈推荐参考 Rpubs 以及关注我们 Github 上的更新。 祝大家使用愉快！有问题请及时在Github上 报告。(可以使用中文)]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[印度女人为何不嫁中国 z]]></title>
    	<url>/arts/2015/11/18/indian-girl-do-not-marry-chinese/</url>
		<content type="text"><![CDATA[[原文地址：http://www.kuoo8.com/News/55414.html 很多印度女孩很迷人的，大大的眼睛，丰厚的嘴唇，黝黑的长发。 是印度女孩家教不到位吗？我看也不是。印度女孩还保持着比较传统的生活方式，不拜金，待人有礼，善解人意，平时也能吃苦，自强，上学受教育也是一些有名大学，强势专业。是印度女孩语言能力不过关吗？我看也不是，英语确实在印度比在中国普及，印度女孩的英语其实很流利，很能准确的表达自己啊。印度不是要比中国贫穷，落后很多吗？为什么中国女人都想找老外，印度女人却很少有呢？真是想不明白啊，有人要是了解的话能给解答一下吗？ 不能这么比的，一看就不了解印度文化，在印度是包办婚姻，即便印度男女都不能自己决定伴侣，没有权利自己找，要听从家族安排，印度情侣除了大城市里，一般都不敢单独约会，印度公园里警察见了牵手的情侣就不由分说，上去一顿暴打！ 这样的社会环境下他们来自己做主叫朋友的权利都没有，会有几个敢跟外国人呢，你是不是连这也要羡慕？先了解下他们的文化背景吧，不要什么都拿出来想证明中国女孩怎么样！ 我说听说印度人不喜欢生女孩，是真的吗？她说城市里已经好多了。但是乡下的一些印度人还是不喜欢生女孩，因为传统上女孩出嫁的时候娘家要给她非常丰厚的嫁妆。所以生女孩将来要赔钱。而生男孩则在将来孩子结婚时得到一大笔钱，是赚钱之事。所以印度人喜欢生男孩，不喜欢生女孩。 我问起种姓制度在今天的印度有多大影响。她起初没听懂“种姓”（caste）这个词。我花了好大工夫给她解释了一番。然后她忙不迭地说这东西是很古老的事了，今天的影响微乎其微。只是偶尔会有些印地人不希望女儿嫁给一个别的种姓的人。（关于这一点，个人认为：锡克教主张平等，与种姓制度所源的印地人印度教大不相同；而梅妮佳来自锡克族文化圈，或许因此她才觉得种姓制度在印度已无影响，亦未可知。） 我问起关于印度的选举问题。我说，印度很多人不会读写英语或印地语，那他们在全国大选中投票会不会因为看不懂名字而投错人？梅妮佳回答：不会。选票上的名字是用印地语写的。但即使不会印地语，各个候选人的名字边上会标上他们所代表的党派的标志。即使看不懂字，也看得懂画。还有选举首相是间接选举。普通人只需要用本民族语言选出本地的代表议员。然后由议员选举产生首相。而议员通常不再是文盲。全印度有四个大党和一大堆小党。没有占绝对优势的执政党，所以往往需要组成联合政府。 我说请别介意我问一下关于普通印度人对中国的看法。她脱口而出说印度人普遍觉得中国人很勤劳，这可能是最深的印象。 最后我问了个比较敏感的问题。我说，印度和中国存在边界冲突，印度人是怎么看的？她说那些地方本来是属于印度的。但是在英国殖民期间，英国没有看管好，所以被中国占据了。我没有跟她争辩。我继续问：普通印度人是否因为边界冲突而对中国人产生反感？她想了想，说其实中印度和南印度的人，对这北方的边界问题不怎么关心。北印度的人则要看情况。她说：“比如说如果我有一个朋友，他住的地方被中国占领了，那么我就会不高兴，就会对中国有意见。但即使我对中国不满，也不会延展到对所有的个别中国人。对于各个中国人的印象，只取决于个人交往。我在这里有个姓吴的中国同学待人很友好，我和他就是好朋友。如果我碰到一个中国人对我很凶，那我就会不喜欢那个中国人。” 梅妮佳是印度学生。她是锡克族人，来自印控克什米尔地区。她在印度上的大学，法学专业，并在印度当过一段时间律师。她来美国是攻读LLM的（LLM即被国内戏称为“老流氓”的美国法学硕士学位）。梅妮佳在印度应该可以算是中上层阶层出身。她很聪明，很有礼貌，说话也很得体（从下面记录的谈话中也能看出来）。梅妮佳是个锡克族姑娘。后来我在网上查到锡克族被称为印度最漂亮的民族。锡克族人和印度主要民族印地族人普遍棕黄的皮肤不同。梅妮佳长得很白净，眼窝深陷，圆圆胖胖。初次见面时我把她当成了美国人。直到她冒出一嘴印度口音我才恍然大悟。 我问她能否概括一下她自己民族的特点。她说锡克族人主要特点是勇敢和骄傲。擅长作战是锡克族人的传统。 我说印度是多民族国家。各个民族是否都有自己的语言文字？她说大部分都有自己的文字，所有的都有自己的语言。我问：这样的话，有没有一个比较通用的官方语言？她说：英语是官方语言，在各地的教学里都有。还有印地语也是比较流行的语言。她上学的时候就学过不少语言。除了英语以外，她还会说印地语、锡克语和——那个词我用手机上的辞典查了下才知道——梵语。“你不会想学梵语的。”她心有余悸地说，“太难学了。” 我说，印度的一些少数民族希望能脱离印度，成立自己的国家。锡克人是不是也有这个要求？她承认说是有的。有些人至今还在为这个目标努力。她解释说这是殖民地时代留下的结果。英国人分而治之的政策导致了各民族的尖锐对立。 然后她随口好奇地问了我一下中国有没有做过英国殖民地。我回答说没有……-_-! 我问，历史上穆斯林入侵并统治印度，是否这也对今天印度的民族矛盾造成不良影响？她回答说最初进入印度的穆斯林是坏而邪恶的。（她用的词是形容词是bad和evil，这是很重的贬义。）他们谋杀，强奸，抢劫。但是今天的穆斯林毕竟不再是他们的祖先。只要尊重对方，还是能相处得不错的。她所在的地方就有不少穆斯林。虽然印度和巴基斯坦在克什米尔打仗，但她所在的地方穆斯林和锡克人平安无事。 我问印度是否给全民提供免费教育。她说免费教育是有的，但只给贫困线以下的人。对女性的教育则更少。我说既然对穷人有免费教育，那印度的文盲率高吗？她回答说这一点很难讲。各个民族都比较重视保留自己的民族语言。所以很多人会写自己的本民族语言，但不会读写印地语或英语。能读写英语的估计只有30%左右。但其他人算不算文盲呢？不太好说。她还说：还有以前的教育条件不如现在，所以健在的长辈中还有些不识字的，那么总体文盲率可能就比较高了。她问我：“比如说，你的爷爷奶奶能读能写吗？”我说不会……她说：“是啊。我的爷爷奶奶也不识字。他们那个年代的人没什么机会上学嘛。”]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[结构模型中约束最优化的使用——纪念 Che-Lin Su - 慧航 - 专栏]]></title>
    	<url>/prof/2015/11/15/che-lin-su/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20146048 这篇文章是之前在 Che-lin Su 在经济学上有哪些突出的研究成果？ - 慧 Che-Lin Su 最重要的一篇文章，《Constrained optimization approaches to estimation of structural models》。 这篇文章很短，正文只有 16 页，然而任何优秀的论文都不是以页数论英雄的。 在计算结构式模型的估计（structural estimation）的时候，计算的复杂度通常很大，而这种难度经常是由于各种不动点迭代（fixed-point iteration）造成的。典型的例子比如： 动态规划中求解 policy function 求解一个 game 的 Nash equilibrium 求解一个一般均衡 BLP 中通过市场份额倒推产品特征的效用 在这些例子里面都涉及到不动点的迭代。然而，考虑到当我们估计这个模型的时候，为了计算极大似然、广义矩估计，我们必须给定一个参数，做一次不动点迭代，然后计算目标函数值，再给定新的参数，再做不动点迭代，然后计算目标函数值。这个过程不停重复，最终最优化算法会收敛到最优点。 然而这个过程是非常非常耗费时间的：本身不动点迭代就是非常耗费时间的，更何况外面还套着一层最优化的迭代。因而这种结构模型一般运算时间都非常的长。 怎么办呢？Che-Lin Su 想出了一个办法：根本不需要进行不动点迭代，把他看成是一个最优化的约束就好了。而约束最优化是运筹学里面早就很成熟的方法。 数学上的描述就是，给定参数\(\theta\)、内生变量\(\sigma\)，而给定参数，模型可以通过一阶条件、贝尔曼方程、市场均衡条件等计算出内生变量。假设这两个变量之间存在如下关系： \[h(\theta,\sigma)=0\] 进而，我们观察到的数据为\(X=\{x,d\}\)，其中\(x\)为外生变量，\(d\)为内生变量。 在此之前的做法一般为不动点的迭代，即 NFXP（nested fixed-point）方法，即给定参数\(\theta\)，求解\(\hat{\sigma}\)，然后计算极大似然函数： \[\hat{\theta}=\arg\max_{\theta} \frac{1}{M}L(\theta,\hat{\sigma}(\theta);X)\] 注意这里\(\hat{\sigma}\)涉及到不动点迭代，再加上外面的最优化的迭代，迭代套迭代。 而 Che-Lin Su 提出，实际上没必要这么麻烦，只要把不动点迭代的部分作为最优化的约束就好了： \[\begin{equation} \left\{ \begin{aligned} &amp;amp; \max_{(\theta,\sigma)}\; \frac{1}{M}L(\theta,\sigma;X)\\ &amp;amp; \begin{aligned} s.t.\;\; &amp;amp; h(\theta, \sigma)=0 \end{aligned} \end{aligned} \right. \end{equation}\] 这样，虽然最优化的维数增加了，然而节省了不动点迭代的时间。而 Su 证明了，以上的两种方法都是等价的。 在这篇文章中，Su 还给出了一个 dynamic discrete choice 模型的例子。 考虑一个公共汽车公司正在考虑要不要更新汽车引擎的决策问题。其决策是一个离散变量，d=1为更换，d=0为不更换。公司在某一期的效用函数为： \[u(x,d,\varepsilon;\theta_1,RC)=\nu(x,d;\theta_1,RC)&#43;\varepsilon(d)\] \[ \nu(x,d;\theta_1,RC)=\left\{ \begin{aligned} -c(x;\theta_1),\;\text{if}\,d=0\\ -RC-c(0;\theta_1),\;\text{if}\,d=1 \end{aligned} \right. \] 其中\(x\)为里程数，\(RC\)为更换引擎的成本，\(c\)为给定里程数汽车的运营成本，\(\theta\)为参数。 公司的决策并非静态的，实际上公司必须考虑其决策对未来的影响，或者说公司应该考虑是这一期换，还是下一期换？因而公司的决策是一个动态的最优化： \[\max_{d_t,d_{t&#43;1},d_{t&#43;2},\dotsc}\mathbb{E}\left[\sum^\infty_{\tau=t}\beta^{\tau-t}u(x_\tau,d_\tau,\varepsilon_\tau;\theta_1,RC)\right]\] 而根据动态规划的方法，可以通过以上效用函数定义值函数（value function），进而得到贝尔曼方程（Bellman equation）： \[\mathrm{EV}(x,d)=\nu(x,d;\theta_1,RC)&#43;\varepsilon(d)&#43;\beta\int_{x^\prime}\mathrm{EV}(x^\prime)p_3(x^\prime&#43;x,d;\theta_3)\,\mathrm{d}x^\prime\] 这个贝尔曼方程是公司决策的核心，当更换引擎的\(\mathrm{EV}\)大于不更换引擎的\(\mathrm{EV}\)，公司就会选择更换引擎。 而当我们看到数据，需要估计其中的参数时，逻辑是这样的：首先，给定一个参数的猜测，解出贝尔曼方程，得到公司的决策，再将这个预测的决策跟实际的决策比，如果足够接近，那么参数就对了，否则，按照一定规则，给出参数的新的猜测，继续比。 然而，这里面每一次给出新的参数，都要重新计算一次贝尔曼方程，而传统上贝尔曼方程是通过 value function iteration 的方法得到的，本质上就是一个不动点的迭代，速度较慢。而使用 Su 的方法，以上的贝尔曼方程可以不用每次给定新参数就迭代计算，而是将其看成是最优化的一个约束。这样，虽然最优化的参数变多了，但是却节省了每一步不动点迭代的时间。 最后，再次为这位年轻有为的经济学家哀悼。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[解开中国房价之谜 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/11/01/housing-boom/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20192785 过去的十年见证了中国房价的飞速上涨，而在此之前的房市低迷 然而，在过去十年中，中国的房价究竟上涨了多少？在中国的不同城市，房价上涨是否形式上有所不同？高涨的房价使得低收入群体排除在房市之外么？家庭买房的负担就多大？这些问题至今仍然少有严谨的研究，而搞清楚这些问题对于理解中国的房市显然是十分重要的。 Hanming Fang, Quanlin Gu, Wei Xiong, and Li‐An Zhou 等人在 NBER 上的工作论文《Demystifying the Chinese Housing Boom》，使用中国某家商业银行的抵押贷款数据对中国房价的相关问题进行了解答。 中国的房价到底有多高？ 文章首先使用抵押贷款数据对中国 120 个主要城市的 2003-2013 年的房价进行了测算。这 120 个城市包括了四个一线城市（北京、上海、广州、深圳）、大部分二线城市以及众多三线城市。 文献中，构建房价指数有两种标准的方法。第一种是 hedonic regression 方法，即用房屋售价对房屋特征做回归从而把房屋不同特征的溢价消除掉。然而这种方法有很多局限，首先是不可观测的某些房屋特征可能会被遗漏，而更重要的是，中国城市的房地产开发多数是从中心城市向外围发散，在这种情况下，较早出售的房屋更偏向于在市中心，因而不同年份出售的房屋可比性很弱。 第二种方法是重复销售的方法，即使用那些被再次出售的房屋的价格变化估计不同年份的房价水平。然而这种方法浪费了大量观测值，而在那些被再次出售的房屋可能并不是城市中代表性的房产，因而这种方法也有其局限性。 作者将以上两种方法结合起来解决以上的问题。虽然在中国重复销售的房屋很少，但是作者观察到，在中国新开发的房地产项目通常是分期销售的，有时一个房地产项目甚至是分期开发的。控制房屋的特征之后，同一个小区在不同月份销售的房屋几乎是同质的，因而其价格可以直接进行比较。为此，作者做了如下回归： 其中lnP为房屋出售价格，X为房屋特征，DP为房产开发项目的固定效应。通过这个回归就将比较限制在相同开发项目、相似特征的房屋之中，解决了以上的问题。而时间固定效应的指数就应该等于房价指数。 根据以上方法，作者计算了 2003-2013 中国 120 个城市的房价指数。注意以上方法计算的房价指数为名义指数，而在此期间平均通胀率为 2.68%，相对于房价上涨来说相对微小。 上图是北、上、广、深四座一线城市的房价指数、人均 GDP、人均可支配收入。 首先北京市的房价上涨最为明显，房价指数从 2003 年的1上涨到 2013 年的 7.6，即在十年间房价翻了七倍，而与此同时北京市的人均可支配收入只翻了三倍。同时注意到，北京的房价出现了两次短时间的下降，第一次出现在 2008 年金融危机之后，而第二次开始于 2011 年 5 月份，然而在 2012 年 6 月份即结束。 相比较于北京而言，上海的房价上涨更加温和。十年之间上海的房价翻了 4.43 倍，远小于北京，而上海的人均可支配收入也大约翻了三倍。 而与北京不同的是，上海经历了三次房价下跌：第一次从 2005 年到 2007 年，下跌了 13%；第二次同样为金融危机之后；而第三次调整，从 2011 年六月到 2012 年三月，下跌了 25%。 广州在十年之内房价翻了 5.1 倍，而深圳则翻了 3.65 倍。与此同时，这两座城市都经历了数次房价调整，最严重的一次发生在金融危机之后的深圳，房价几乎下跌了 39%。 而对于二三线城市，房价上涨并没有一线城市那么夸张。 对于二线城市而言，十年期间二线城市的房价平均翻了 3.92 倍，与此同时房价的波动也比一线城市平缓很多。然而注意到，尽管房价上涨了很多，而人均收入的变化也几乎是同步的。作者认为人均收入的变化代表了家庭对房产需求的变化，而从二线城市的数据中，房价上涨虽然很快，但是似乎与实际需求的背离并不显著。 而对于 85 个三线城市，平均而言十年期间房价翻了三倍，然而值得注意的是，在此期间人均收入的增长甚至快于房价的上涨速度。 那么中国的房价上涨与其他亚洲国家的房价上涨有不同特点么？为此作者比较了日本和新加坡房价上涨的经验。 如上图，日本从 1955 年到 1990 年，人均 GDP 翻了大约 40 倍，然而其土地价格却翻了 80 倍。而随后的 25 年期间，日本经济增长停滞，土地价格水平也终于在 2014 年回归了与其人均收入相当的水平。上图也印证了日本战后的房地产泡沫的产生与破灭。 而新加坡房价的上涨与其经济发展水平有很高的同步性，除了两段时间出现了偏离：上世纪八十年代以及亚洲金融危机之前的 1995-1997 年。在这两段时间之前，新加坡都经历了持续的经济增长，然而当经济下行时，房价水平又回复到了经济增长水平上。 结合以上的经验，与日本的房地产泡沫相比，中国的房价上涨有其自身的特点：除了少数几个一线城市外，大量二三线城市的房价上涨与购买力的增长水平是相当的。这些国家的历史经验也许对中国的房地产市场的思考有一些历史借鉴意义。 谁在买房？ 一线城市的房产价格飞涨部分可以由地方政府有限的土地、房屋供给解释，然而，仅仅从供给方面并不能够完全解释。作者手上的抵押贷款数据可以帮助我们更详细的了解房产市场需求方的特征。 然而需要注意的是，对于那些「富人」来说，购买房屋时有可能直接全款购房，并不需要借助抵押贷款，因而作者的这部分数据对于中产、相对低收入的购房家庭更有解释力。 对于每类城市，作者主要集中于两类家庭：所有贷款家庭收入在最低的 10% 的家庭，称之为低收入家庭（bottom-income borrower group，p10）以及在 45%-55% 的中等收入家庭（middle-income group，p50）。 上面两张图分别给出了在一线城市、二线城市家庭收入的时序以及这些家庭在整个城市的收入排序（即p10、p50 样本中家庭在整个城市中收入的分位数，整个城市的收入情况由 Urban Household Survey 得出）。 通过上图可以看到，所有的城市，家庭收入在样本期间段内都经历了持续的增长。一线城市 p10 组的平均收入从 2003 年的 39000 元增长到 2012 年的 92000 元（2.4 倍），而 p50 组从 87000 元增长到 184000 元（2.1 倍）；在二线城市，p10 组的平均收入从 2003 年的 19000 元增长到 2012 年的 58000 元（3.1 倍），而 p50 组从 40000 元增长到 99000元（2.5 倍）。 而值得注意的是，样本中的 p50 组基本来自于各个城市中相对富裕的一部分人。在一线城市，样本中的 p50 组在从 2003 年处于城市中 85% 的位置下降到了 2009 年的 59% 的位置，伺候又重回了 75% 的位置。而在二线城市，p50 从 2003 年的 81.5% 的位置下降到了 2010 年 62% 的位置，此后又反弹到 2012 年 68% 的位置。 而低收入的借贷组 p10 更有意思，因为这组数据代表了买房的最不富有的一批人在这个城市中其收入究竟处于什么样的位置。基本上，一线城市 p10 在整个区间段都维持在大约 25% 的水平，而二线城市则维持在大约 30% 的水平。这个数字说明买房的人不仅仅来自于城市中收入最高的一批人，同时也来自于很大一部分低收入群体。 而更有意思的是，尽管一线城市的房价快速上涨，p10 从 2003 年的 35% 的位置下降到了 2010 年的 17.5%，之后又重新爬回了 2012 年的 26%，这说明快速增长的价格并没有把低收入家庭挡在房地产市场门外。 上图给出了样本中不同城市类别首付比例的时序，左边为 p10 组，而右边为 p50 组。可以看到中等收入组别的首付比例基本上在 35% 以上，而令人惊讶的是，相比于中等收入家庭，低收入家庭的首付比例甚至更高，基本在 40% 以上。这很大程度上是由于中国政府的政策所致，比如首套房的比例至少为 30% 等（不同时间有不同政策），而第二套房的首付比例甚至更高。 这与美国在次贷危机时几乎零首付不同，比如 Mayer, Pence and Sherlund (2009) 指出，在 2003-2006 年美国房地产泡沫中，收入几乎为 5% 甚至为 0。如此高水平的首付减小了未来房地产市场的风险，除非房价下跌超过 30%，贷款家庭不太会对他们的贷款违约。此外，在中国，银行对这部分债务有追索权，意味着一旦违约，银行可以要求贷款者使用其他资产偿付。这些原因使得中国不太可能发生类似美国的次贷危机。 房价收入比是一个非常常用的房价负担的度量。上图给出了 2003 年至 2012 年中国的房价收入比的时序图，其中左边为低收入家庭，右边为中等收入家庭。 对于低收入家庭而言，一线城市的房价收入比远远高于二三线城市，而从 2003 年至 2011 年一度从 8 左右爬升至 11 左右，之后 2012 年又降回 9.2 左右。而在二三线城市，房价收入比一直维持在 8 左右，变化幅度不大。 而对于中等收入家庭组，房价收入比比低收入家庭小很多。而同样，一线城市房价收入比高于二线城市，高于三线城市。 Cheng, Raina and Xiong (2014) 计算了 2000s 华尔街的房价收入比，这个数字大约在 3 左右。当然，这部分数据调查的是美国相对高收入的家庭，不过美国的金融咨询机构一般会建议家庭购买价格收入比在 3 左右的房产。而中国的高房价收入比可能与中国不征收房产税有关。 Noguchi (1991) 报告了日本公寓价格与收入的比，在 1989 年日本房地产泡沫时，这一数字达到了 8.6，而这一数字与中国低收入贷款者面临的房价收入比相当。 另外，样本中很多地产的购买者都是未婚，如果他们结婚，那么房价收入比可能大大降低。为了解决这个问题，作者挑出了其中已婚的家庭，其房价收入比如上图所示。比较两张图会发现，其中的差别并不大，作者解释这有可能是银行严格的审贷造成的。 中国之前持续的收入增长意味着房价的负担可能并不能单纯的按照收入计算。比如，如果假设家庭预期自己的收入年增长 10%（也就是在样本期间的平均收入增长率），那么五年之后家庭的收入将会翻 1.6 倍，而届时家庭面临的房价收入比就会大大降低。当然，这一切都是假设家庭的收入会一直增长 10% 的前提下的。 作者强调，家庭对未来收入的增长预期可能是高房价收入比的重要影响因素。如果家庭有收入增长的预期，那么就会接受购房时接近 8 甚至 10 的房价收入比。此外，家庭对于整个经济（或者等价的，其他家庭）的收入增长预期，会导致家庭对于房价一直上涨的预期，而这种预期也会使得家庭乐于接受高房价收入比。 此外，房子的大小也是决定房屋价值的重要维度。上图展示了两类家庭房屋大小的时序图。尽管总的趋势是下降的，但是中国家庭的房屋面积仍然是很宽敞的。一线城市的低收入家庭购买的了全国最小的房子，而即使这部分家庭，居住面积也有 70 多平米。对于一线城市的典型家庭，人均居住面积大约有 25 平方米，这在世界范围内都算比较高的。 从年龄分布上来看，低收入组似乎比中等收入组更年轻一点，基本上 30 的头 5 年是买房的主力军。 由上图可以看到，在三类城市中，低收入家庭组有大约 40% 的购房者为单身男女。Wei, Zhang and Liu (2014) 指出，由于性别的不平衡，是否拥有房产是中国男性在婚姻市场上竞争力的重要因素。这个论点意味着单身男性比起女性应该更渴望购买住房。这一点在二三线城市非常明显。然而在一线城市，单身男性与单身女性买房的比例大体相同，这意味着尽管婚姻市场是理解房市的重要因素，但是对于一线城市的房价，似乎并没有什么太强的解释能力。 一个经常被关注的点是，许多人拥有多套住房。上表给出了 2011-2013 年抵押贷款买房的样本中，二套房的比例。因为富人可能使用现金直接买房，因此以上的数据严重低估了二套房的比例。 在一线城市中，2011-2012 只有 5.3% 左右，然而到了 2013 年，这个数字扩大了两倍，而在二三线城市，这个数字虽然也增加了，但是并没有一线城市那么夸张。 综合起来看，尽管房价一直在涨，然而这并没有阻止低收入家庭参与到这个市场中来。然而低收入家庭承担着非常沉重的买房压力，这可能反应了大家对持续的收入增长以及未来高房价的预期。 房产作为投资工具 中国的经济增长伴随着高储蓄率。根据 Yang, Zhang and Zhou (2013)，中国的储蓄占 GDP 的百分比由 1980 年代的 35% 上升到 1990 代的 41%，并继续升高到 2000 年代的 50%。然而尽管储蓄率很高，但是在中国，家庭、企业只有有限的投资工具。银行储蓄是主要的投资工具，由于中国严格的资本控制，家庭与企业不能自由的将他们的储蓄投资到国外，而中国股市的规模又没有足够的大，债券市场就更小了。在这种环境下，房产就成了替代的投资工具。 上图给出了中国的银行储蓄的规模。中国名义的银行储蓄率 2003-2013 年大约在 2%-4% 左右，而如果扣除通胀率，一些年份收益率甚至为负。这就给中国家庭寻找其他的投资工具带来了很强的激励。 图中虚线给出了中国股市的规模，尽管在此期间快速扩张，然而规模仍然远远小于储蓄的规模。 上表总结了上海证券交易所这些年间收益与风险的情况。在这段时间，每年的平均收益大约有 7.3%，以及 51.5% 的波动率（volatility）。如此之高的波动率可能是由于 2008 年之前的股市泡沫有关，可以看到 2009-2013 年波动率只有 33.9%，当然收益率也只有 5.3%。 而对比下来，投资房产却有着更高的回报率以及更低的风险。比如一线城市在 2003-2013 这段期间，平均每年有 15.7% 的收益率，而波动率只有 15.4%。相对于股票，投资房产的收益率更高，波动性更低。 而其中，2009 年之前，投资房产的收益率特别高，达到了 20.4%，二线城市也有 17.3% 的年化收益率。而在 2009-2013 年，平均的收益率也维持在 20% 左右，相比于股票市场来说，仍然是非常有吸引力的。 一些讨论 上面给出了与房地产有关的数据，围绕这些数据，作者对于房地产市场与中国的宏观经济做了一些讨论。 房地产市场的繁荣经常与信贷扩张紧密联系在一起，最新的例证莫过于美国的次贷危机。因而中国在房地产市场繁荣过程中由于杠杆而带来的风险就成为了一个关注的焦点。然而，根据上面介绍的 30% 以上的首付比例，这大大降低了信贷扩张的风险。 另外一个大家都关心的问题是，中国的房地产市场到底有没有泡沫？作者指出，可信的识别出资产泡沫是非常困难的，这需要一个充分描述市场供需、金融系统摩擦的理论框架。不过，作者的分析提供了一些对于中国房地产市场繁荣很好的观察。 一方面，价格的大幅上涨不一定代表一定有泡沫，因为除了极个别的一线城市，房产价格的上涨同时也伴随着收入的增长。另一方面，房地产对于很多家庭来说的确非常的昂贵，然而这可能体现了这些家庭对未来收入增长的预期以及对房价增长的预期。当家庭对未来的预期发生改变时，特别是如果中国经济出现了 sudden stop，高房价收入比的确会成为一个潜在的风险。 此外这篇文章的分析强调了未来系统性分析中国房地产市场所必须包含的几个方面： 房地产市场在一、二、三线城市都提供了比股票、债券、存款更高的回报率。 在所有城市，低收入家庭也会购买房价八倍于其年收入的房子。 随着房地产市场的繁荣，中国的家庭同样有大量的银行储蓄。 考虑房地产市场应该在中国金融系统不完美的背景下进行分析。过低的存款利率是一个关键的因素。 房产除了众所周知的较差的流动性之外，风险也是房产作为一种投资品的重要维度。然而使用月度的数据很难度量当经济急剧下滑时的风险。比如在 2008-2009 年房地产是比股票更稳健的资产。 正如 Pritchett and Summers (2014) 指出，由于「向均值回归（regression to the mean）」现象的存在，中国一直维持 9%，甚至 7%、6% 的增长水平的可能性是非常小的，这意味着当经济最终放缓时的巨大风险。因此，一个系统的研究框架应该包含居民对于未来持续的增长的预期。 政府在房地产市场的作用 以上的分析只集中在房产的需求方面，而从供给方面理解中国的房地产市场同样重要。中国房地产市场的供给几乎完全由政府决定，因为政府掌握着土地的供给。 到 2007 年中国多数城市的房价都在持续上涨，因而中央政府实施了一系列的货币政策、财政政策来控制房价、抑制投机行为。比如 2007 年九月，政府提高了首付比例，从 30% 提高到 40%，同时提高了二套房的贷款利率。2008 年四月，开始对房产的利得征税。同时，政府还提高了保障性住房的建设。这些政策可能对于房价有一些影响，特别是对于广州和深圳，然而区分这些政策效应与全球经济环境的影响本身就很困难。而在 2008 年之后，政府又推出了一些列政策来刺激房地产市场。因而，从 2009 年中开始，房地产市场又开始了新一轮的繁荣。 2010 年，政府又推出了一系列的举措，包括一些传统的方法，比如提高首付比率，还有一些其他办法，比如 39 个大中城市开始实施购房限制，只有有当地户口或者在这座城市工作一段时间之后才能在这座城市购买住房。政府频繁的干预房产市场给人一种「房产市场太重要而不能倒下」的感觉。 土地出让的收益占地方政府预算的很大一部分，这是 1994 年分税制改革的结果。而地方政府处于晋升压力，有激励通过投资基础设施建设等刺激辖区内的经济增长，在财政收入收入中央的背景下，便诉诸于土地出让。 上图给出了土地出让占财政预算的比重。这一数字从 2003 年的 68% 下降到了 2008 年的 42%，而到了 2010 年左右又快速上升到 70% 左右。三类城市中，一线城市对土地出让的依赖最小，三线城市最依赖于土地出让。 此外，中央政府允许地方融资平台的存在，即用土地等国有资产的未来收益作为抵押的贷款。地方政府使用这些投资工具参与到资本市场，或者进行更大规模的基础设施建设、资本投资。这意味着如果房产价格下降，可能会导致地方政府的债务问题。因而，许多家庭相信房地产市场「too important to fall」，中央政府会想办法保证房地产市场。 面临的风险 中国经济可能已经进入了增长的新阶段，「新常态」的到来意味着中国的经济增长不能维持之前 10% 的水平。随着经济增速放缓，家庭的可支配收入增长也会放缓。高房价收入比将会持续，对于未来收入增长预期的调整将会导致居民预期到未来房价负担的恶化。就像之前介绍的那样，房价增长与收入增长在二三线城市有很好的同步，但是在一线城市并非如此，因而未来经济增速放缓可能对一线城市有更大的影响。 房屋的供给在二三线城市更有弹性，因而使得房价增速与收入增速比较同步，但是仍然是未来房产市场的风险的来源。注意新房屋的建设受到地方政府的管制，因而如果地方政府有其他的收入来源，房产供给过剩的风险是地方政府可以控制的。 另外一个重要的风险是人口的风险。由于计划生育的影响，中国的老龄化问题加剧，人口将会在2030年左右开始下降。然而，购房的主力年龄大约为 30-49 岁，这部分人口从 2005 年就已经开始减少。使用 2000 年人口普查数据，到 2030 年，这部分人口将会下降到 2000 年的 62%。当然，城市化进程对人口减少的作用有所抵消。 此外，中国此前在上海、重庆实行了房产税。房产税的开征对房地产市场可能有如下几方面影响。首先，房产税抑制了投机行为，降低了投机性需求。其次，房产税开征为地方政府增加了收入来源，从而可能会影响家庭对「too important to fall」的预期。最后，房产税以及其他财政改革使得低价有降低的压力，从而降低了房地产的成本。所有这些因素都对房地产价格有负向的影响。 另外一个潜在的可能对房地产市场有影响的改革是社会保障体系的统一。现行的社会保障体系，特别是医疗体系，激励老人更多的给后代留房产。比如，中国最好的医院都在北上广深以及各个省份的省会城市。而更重要的，现行的医疗保险体系主要基于就业或者退休之前的就业，而且处于县一级的计划框架中，这就意味着如果一个人退休回到二三线城市，那么就不能享受一线城市的医疗，同时使用他们的医疗保险业非常麻烦。这就导致老年人有更强的留在一线城市的激励。随着中国试图统一社会保障系统的结构、在不同地区平等化医疗资源，这对中国的房地产市场有一定的稳定作用。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[局部处理效应（LATE）中「排除性限制」的讨论 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/10/25/late/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20294874 在实证中，工具变量（IV）是非常常见的识别策略，而Imb and Angrist(1994)则提出了工具变量的一个『局部平均处理效应』（Local Average Treatment Effects，LATE）的解释。根据 LATE 的解释，在处理效应的语境下，工具变量识别的是那些因为工具变量的变动而导致项目参与会改变的子样本的平均处理效应，即 Compliers 的平均处理效应。然而 LATE 仍然是需要一些假设的，而对于这些假设是否能够满足，一般的文献里面似乎很少讨论。Damon Jones 的NBER working paper《The Economics of Exclusion Restrictions in IV Models》讨论了这其中『排除性限制』可能违反的情况，并给出了一些补救的策略。 我们先从 LATE 的一般设定开始。假设\(D_i\)为处理状态，\(y%为关注的结果，潜在的结果可以是一个处理状态\)D\(以及工具\)Z\(的函数。假设\)Z$也为\(0-1\)变量。那么 LATE 的假设可以总结为： 其中第一条为独立性假设；第二条为关键的『排除性限制』，即假设工具变量\(Z\)不会影响潜在的结果\(y\)；第三条假设为第一阶段回归中\(Z\)对\(D\)有影响；最后一条假设为单调性假设，即这里假设了工具变量\(Z\)对\(D\)的影响是单调的。 LATE 的估计量可以写为： 即估计的结果为那些可能因为工具变量的改变而改变自己行为的那些个体的平均处理效应。 作者认为，这其中的第三条假设在一般的效用最大化框架下是否满足需要仔细考虑。为此，作者提出了一个比较一般化的效用最大化框架。在这个模型里面，个体分两阶段最大化自己的效用，在第一阶段，个体在工具变量Z的影响下，选择是否参与项目： 其中\(v\)为第二阶段最优化的 value function，第二阶段的最优化问题为： 即个体通过在约束集\(B\)中选择\(x\)最大化效用函数\(u\)。这个模型是一个非常一般化的设定，比如常见的 Roy 模型也为这个模型的特例。 那么什么情况下会导致以上的『排除性限制』失效呢？作者提出，只有当个体所面临的约束集\(B\)不随工具变量Z变化，同时效用函数也不随工具变量\(Z\)变化时，『排除性限制』是满足的，即： 然而在某些情况下，以上条件并不满足。 作者举了一个例子。比如在研究教育回报的时候，由于上不上大学（\(D\)）是由个体自己选择的。现在有一个学费减免的政策，假设这个学费的减免是随机的，因而潜在的可以成为工具变量。假设学费为\(p\)，上大学会使得工资增加一个倍数\(\phi\)，假设\(l\)为劳动供给，\(I\)为非劳动收入，则个体面临的问题为： 在这里，结果变量为劳动的总收入，然而问题是，对于那些总是会上大学的个体（\(D(1)=D(0)=1\)），如果接受了学费的减免，那么会对个体面临的预算约束有影响，进而对其劳动供给行为造成影响： 在这里，工具变量对于个体的行为产生了收入效应，因而之前的『排除性限制』假设被违背了。 那么如何解决这一问题呢？作者提出了两种方法。 第一种是可以使用文献中的结果对 LATE 的估计结果进行校准。比如在以上的例子中，作者计算出，使用 LATE 估计出的处理效应应为： 其中： 因而如果知道了最后的三个参数，就可以对以上的结果进行校准。作者引用了 Zimmerman,（2014）的 IV 估计结果 0.22，以及 Rothstein and Rouse (2011) 对\(\eta\)的估计结果，计算出由于忽略了收入效应，使得 IV 的估计结果有大约 0.1 的偏差，应为\(0.22&#43;0.1=0.32\)。 第二种方法是，如果条件允许随机试验，那么可以增加一个随机试验来识别上式中的\(\eta\)。比如在以上的工具之外，实验设计者可以给试验对象一个额外的现金，比如\((1-\theta)p\)，那么使用这个额外的工具，潜在的收入效应规模也可以被识别。 一般说来，『潜在结果』模型的支持者对结构模型的一大批评是给出的假设太多，而现实情况经常不符合这些假设。而这篇文章则使用了一个效用最大化的框架，给潜在结果模型的假设提了一个现实的问题，是一个不错的视角。 薛熠：不做劳动经济学，但为什么不直接看工资？而是看劳动总收入？如果看工资就没这个问题了。 慧航：因为问题是劳动参与啊 薛熠：我以为是教育回报，感觉 wage 就够了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[中国的『岗位创造』与『岗位破坏』 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/09/06/job-creation-destruction/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20201591 前几天介绍了关于中国失业率的文章：中国失业率与劳动参与率 - EconPaper - 知乎专栏，而对于劳动市场的描述，除了失业率、劳动参与率之外，还有许多其他维度的指标，其中岗位创造（job creation）和岗位破坏（job destruction）是两个非常重要的维度。Hsieh and Klenow (2009) 指出中国和印度如果有和美国一样的资源配置效率，那么中国和印度的生产率将会极大的提升，因而理解劳动力的资源配置有非常重要的意义。 清华经管的 Hong Ma, Xue Qiao and Yuan Xu 等人最近在《Journal of Comparative Economics》上的文章：《Job creation and job destruction in China during 1998-2007》对这两个指标进行了系统的测算。 文章主要使用国家统计局的工业企业数据库对以上指标进行测算。作者首先定义了职位增长率： \[ g_{et}=\left\{ \begin{aligned} &amp;amp; 2(e_t-e_{t-1})/(e_t&#43;e_{t-1}), &amp;amp;&amp;amp; \text{if}\, e_{t-1},e_t&amp;gt;0\\ &amp;amp; -2, &amp;amp;&amp;amp; \text{if}\, e_{t-1}&amp;gt;0,e_t=0\\ &amp;amp; 2, &amp;amp;&amp;amp; \text{if}\, e_{t-1}=0,e_t&amp;gt;0 \end{aligned} \right. \] 其中\(e_t\)为就业人数。其中\(g=2\)代表企业进入，\(g=-2\)代表企业退出。「岗位创造」被定义为： 即那些就业人数正增长的企业的就业增长除以部门内的总就业人数。类似的，「岗位破坏」被定义为： 另外，作者还定义了「总岗位重配置率」（Gross job reallocation rate, SUM）、「净就业增长率」（net employment rate, NET）以及「超额岗位重配置率」（excess job reallocation rate, EXCESS）： 由于工业企业数据库只记录规模以上（私有企业500万以上）企业，因而如果一个企业首次出现在数据库里面并不能说明这个企业为进入企业（entry）。为了解决这个问题，作者使用企业的起始年份作为定义进入的标准，同时使用「经营状态」定义企业的退出。此外，仿照Brandt (2012)，作者除了使用ID外，还使用了企业名称、行业、地址作为不同年份企业的的识别标准。 上图是最终计算出的全国样本期间段内的岗位创造与岗位破坏的情况。平均而言，在样本期间段内，每 100 个工作有 14 个为新创造的岗位。值得注意的是在 2001 年之前，「岗位破坏」比「岗位创造」数值要大，意味着在这段时间内全国的岗位在减少，这与之前介绍的失业率的文章是相吻合的。最后，「超额岗位重配置率」（EXCESS）占「总岗位重配置率」（SUM）大约 84%，意味着工作岗位的变动最主要是由于结构性调整引起的。 上图是根据所有制划分的岗位创造与岗位破坏的情况。在样本期间内，国有企业和集体所有制企业有更低的岗位创造率、更高的岗位破坏率，因而在样本期间内创造了更少的工作岗位。值得注意的是，在样本期间段内，国有企业的岗位创造率小于破坏率，意味着国有部分的收缩，这与上世纪 90 年代的国有企业改革有密切的联系。而私有部门则在工作岗位的提供方面贡献了最多。 上图是根据企业规模分类的岗位创造与岗位破坏的情况。可以看到小企业有更高的岗位创造率。而小的国有企业有更高的岗位创造率，同时有更高的岗位破坏率，这些发现与国企改革『抓大放小』的政策是一致的。 此外，作者还对「超额岗位重配置率」（EXCESS）进行了如下组内、组间的分解： 其中前两项为组内的 EXCESS，后两项为组间的 EXCESS 变动。 上图汇报了依据不同标准进行分组，组间的 EXCESS 变动。表中可以看出，组间的变动只解释了总的变动的一小部分。值得注意的是 1999 年至 2007 年，根据所有制划分，组间变动的解释能力出现了先上升后下降的现象，说明就业从国有部门到私有部门的转移在 2002 年达到高峰，以后逐年下降。 国际贸易自由化对于国内的资源配置有重大的影响意义，在有国际贸易的条件下，国内低效率的企业会被迫退出，而高效率的企业会快速增长。为了检验贸易对岗位资源配置的作用，作者构建了行业的『贸易开放指数』，即行业的总进出口额与行业总销售额、进出口额之比。作者构建了如下动态面板模型： 并使用 Arellano-Bond 方法进行估计。 回归结果如上图所示，可见对外开放程度高的行业有更高的净岗位创造，而这个作用主要是由岗位创造而非岗位破坏带来的，即那些对外开放程度低的行业似乎并没有更高的岗位破坏率。与此同时，国有企业有更高的岗位破坏率，然而国有企业也并没有更低的岗位创造率。 梁正中：我不是学经济的，不过我想知道 2002 年到底发生了什么， 为什么私有化在 2002 年达到高峰。是否意味着政府部门有意的导向。 慧航：加入 WTO，以及国有企业改革的尾声。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[模型选择的一些基本思想和方法]]></title>
    	<url>/data/2015/08/31/some-basic-ideas-and-methods-of-model-selection/</url>
		<content type="text"><![CDATA[[0. 引言 1. 偏移、方差、复杂度和模型选择 1.1 为什么训练误差和测试误差会有波动？ 1.2 训练误差和测试误差的变化趋势说明了什么问题？ 1.3 造成预测误差变化趋势 1.4 对模型选择和评价有什么指导意义？ 2. 模型选择的方法 2.1 重复抽样思路 CV法 GCV Bootstrap法 2.2 解析解思路 2.2.1 训练误差与样本内误差 2.2.2 Cp法与AIC 2.2.3 BIC与贝叶斯 3. 总结 原文地址：https://cosx.org/2015/08/some-basic-ideas-and-methods-of-model-selection/ 0. 引言 有监督学习是日常使用最多的建模范式，它有许多更具体的名字，比如预测模型、回归模型、分类模型或者分类器。这些名字或来源统计，或来源于机器学习。关于统计学习与机器学习的区别已经有不少讨论，不少人认为机器学习侧重于目标预测，而统计学习侧重于机制理解和建模。个人更加直观的理解是，统计学习侧重于从概率分布来描述数据生成机制，除了预测之外，还关心结果（参数假设、误差分布假设）的检验，而机器学习侧重于从函数拟合角度来描述数据生成机制，基本目的就是为了拟合和预测，缺乏严谨的参数、误差的检验机制，比如下式： \[Y = f(X) &#43; \epsilon\] 统计学习目标是获取\( Pr(Y|X)\) 的条件分布，通过对数据概率生成机制的理解与建模进而获取良好的预测效果，这个过程会涉及 \(X, Y, \epsilon\)的分布假设，因此最后会衍生出对参数假设和误差分布的假设检验，以验证整个概率分布的假设的正确性，比如经典的线性模型、非参数回归等模型，预测能力并不是其主要目的； 而机器学习基本不会从概率分布的角度着手，虽然可能也会涉及\(X, Y\)的分布假设，但目的就是学习到一个能够较好描述数据生成机制的函数\(f\)，对误差的假设基本忽略，也不会涉及参数和误差的检验，模型好坏基本由预测效果来判断，同时也会提供一些比较一般的误差上界，所以机器学习中不会出现参数估计渐进性、一致性等结果的讨论，而多半最终结果的评判。比如SVM、神经网络、KNN等模型。 不过即使有上述区别，关于高维统计推断（Lasso类带正则项的线性模型）的理论也逐渐完善，但相对于传统的生物制药、生物实验、社会调查、经济分析等领域，当前图像、文本、推荐系统等应用领域中，人们更关心模型的预测能力，而不是解释能力甚至是模型的可靠性，主要原因即这些领域模型预测能力相比于模型的假设检验要重要得多，因此如何根据模型预测能力来选择最优模型变得越来越重要。本文下面就逐步介绍模型选择的思路和方法，主要参考 ELS这本书。 1. 偏移、方差、复杂度和模型选择 模型的预测能力通常也被称作模型的泛化能力，表示模型在新的、独立的测试数据上的预测能力。在很多关于模型泛化能力的介绍中，我们总会看到这样一幅图：模型在训练集上的训练误差与在测试集上的测试误差的变化趋势对比。 var_bias_small 图上横轴表示模型的复杂度大小（比如线性模型中特征维度大小），纵轴表示预测误差，衡量预测值与真实值间的平均损失大小\(E(L(Y,\hat{f}(X)))\)，损失函数根据分类、回归问题做合适的选择，比如0-1损失、负似然函数、平方损失、对数损失、指数损失、交叉熵损失、Hinge损失等。平均损失大小在训练集上预测误差称作训练误差，在测试集上称作测试误差。图中每一条线都表示同一个训练集（浅蓝色）和测试集（浅红色）上的预测误差表现，从图上可以看到两个现象 训练误差（浅蓝色）和测试误差（浅红色）都有波动，并不是一个稳定的值，并且随着模型复杂度的增加，训练误差（浅蓝色）波动越来越小，而测试误差（浅红色）波动则越来越大； 随着模型复杂度增加，训练误差（浅蓝色）和平均训练误差（粗蓝线）越来越小，但测试误差（浅红色）和平均测试误差（粗红线）先降低后减小，在相对中间的位置有一个最小值。 看到这上面的现象，我们的脑中可能会冒出下面几个问题： 为什么训练误差和测试误差会有波动？ 训练误差和测试误差的变化趋势说明了什么问题？ 造成这种变化趋势的原因是什么？ 这种趋势对模型的选择和评价有什么指导意义？ 这四个问题由浅入深，最终目的就是想获取泛化能力最好和最稳定的预测模型。在回答这四个问题前，我们首先需要做一个假设：模型能够较好的预测，说明预测集与训练集有较好的相似性，更严格来说，很可能来源于同一分布，下文做分析时均假设来源于统一总体分布。如果测试集相对于训练集发生了巨大的变化，那么从训练到预测的思路将不可行。下面我们逐步来解答这四个问题。 1.1 为什么训练误差和测试误差会有波动？ 现假设我们有个研究项目，想根据学生的平时表现和性格的\(p\)个指标\(X\)来预测最终该学生的期末综合成绩\(Y\)。为达到这个目的，我们在一个学校中我们随机抽了一批学生，选用某个模型训练，估计其模型参数，然后找一批新的学生预测来评价模型好坏，发现预测误差的相对误差非常小。但是通常人们会问，这个模型预测能力真的那么好么？我咋不相信根据学生平时表现和性格就可以得到这么好的预测效果呢？是不是你们特意挑选的一批学生呐？为了让他们相信这个模型还不赖，我们从这个学校重新抽了好几批学生，分别根据这些学生的指标去训练模型和估计模型参数。随后我们发现，换了几批学生样本，估计的模型参数确实有些差别，而且预测效果也是时好时坏，整体而言，平均的预测误差也还差强人意，能够控制预测误差的相对误差在20%以内，此时再把结果拿给其他人看，估计很多人对你的模型可能就不会有那么大疑问了。 对应到上文的图，其实上图的波动产生的原因也和该例子操作是一样的，有多条线就意味着重复抽了多组训练集来分别训练，因此训练误差和测试误差的波动是由训练样本的变化带来的。在理想的实验条件下，为了能公正地衡量模型的预测能力，通常需要多换几组训练集和测试集来综合评价模型的预测能力，这样的结果才可能让人更信服模型的预测能力，而不是偶然结果。 但是实际情况中，我们却可能仅有一个训练集和一个测试集。用数学化语言描述，新的测试集中，目标变量\(Y^0\)和预测变量\(X^0\)均从两者的联合分布中抽取，独立于训练集\(\mathcal{T} = (\mathbf{X}, \mathbf{Y})\)，此时我们获取的测试误差（泛化误差）为某个训练集\(\mathcal{T}\) 上测试集上损失的条件期望 $$\\Err_{\mathcal{T}} = E_{X^0, Y^0}(L(Y^0, \hat{f}(X^0))|\mathcal{T})$$ 这里训练集 \(\mathcal{T}\) 是固定的，即仅有一个训练集。而从统计分析（重复试验）上来说，我们可能更关注测试误差的期望，即去掉训练集随机性所带来的影响： $$\\Err = E(L(Y^0, \hat{f}(X^0))) = E_{\mathcal{T}}[\text{Err}_{\mathcal{T}}]$$ 这个目标即上图预测误差波动想要表达的含义，想要通过多个训练集训练来获取平均的预测误差，抹平训练集变动带来的影响，这是评价模型预测能力最理想方法，可以防止某个训练集上训练所得模型表现过好而夸大模型的预测能力。但是实际情况中，我们手边通常可能只有一个训练集，实际的需求是在此训练集上模型做到最好，所以\(\text{Err}_{\mathcal{T}}\)又是我们最关心的目标，即希望在当前训练集下获取最佳的预测能力，也就是说我们想获取上图的一条线的趋势中最小的测试误差的点，如下 var_bias_sample 换句话说，很多时候人们给你一个训练集就希望你能够给他一个相对最稳定的预测模型，这个目标相对获取平均预测误差来说更难，后续模型选择方法比如CV法、bootstrap法、Cp法等其实都是估计测试误差的期望，即第一幅图中的红色均线。 1.2 训练误差和测试误差的变化趋势说明了什么问题？ 图上反映的两个现象一句话表示即：随着模型复杂度增加，训练误差波动降低，平均训练误差降低趋向于0，而测试误差波动上升，平均测试误差先降低后升高。这个现象说明训练误差不能代替测试误差来作为模型选择和评价的手段。随着模型复杂度变化，训练误差与测试误差并不是一个良好的正相关关系，而是呈现较为复杂的非线性关系。用数学式子表示即 $$\\frac{\text{Err}_{\mathcal{T}}}{\bar{\text{err}}} \underset{df \rightarrow \infty}{\neq} Const$$ 用更通俗的话说，复杂的模型可能在训练集上拟合的很好，但是面对新的测试集，预测误差不降反升，发生了所谓的“过拟合”现象。如果一个模型在不同的测试集上测试结果不仅波动性大，而且预测误差也比较大，就要警惕发生了过拟合现象，此时不妨将模型的复杂度降低些（关于模型的复杂度含义下文会做更细致的说明），即使用变量更少的简单模型，比如线性模型。 过拟合的原因有很多，其中一个很可能的原因是，随着模型复杂度升高，对于训练数据刻画的很细，但是训练数据中可能某些特征仅出现过一次或者很少，信息不足，而测试集中该特征却出现了很多其他的值，虽然模型在训练集上刻画的足够细致，但是由于测试集的变动，模型反而往测试机上的迁移性能下降，训练误差变化并不正比于测试误差。 1.3 造成预测误差变化趋势的原因是什么？ 那究竟是什么原因导致了随着模型复杂度增加，训练误差与测试误差不呈现良好的正比关系呢？为何同样都是预测误差，训练误差很小的模型反而预测能力很差呢？下面我们以线性模型为例来阐释原因，假设 $$y = f(x) &#43; \epsilon, \quad E(\epsilon) = 0, Var(\epsilon) = \sigma_{\epsilon}^2$$ 如果用线性函数\(f_p(x) = x^T\beta\)去近似\(f(x)\)，其中\(p\)表示特征个数，损失函数取平方损失，最小化 \(\frac{1}{N}\sum_i(y_i; x_i^T\beta)^2\)，则在训练集\(\mathcal{T} = (\mathbf{X}, \mathbf{Y})\)下得到参数估计为\(\hat{\beta}\)，同时记\(\beta_{*}\)是\(f(x)\)最佳线性近似的估计参数 $$\\beta_{*} = \arg\min_{\beta}E_X(f(X); X^T\beta)^2$$ 在某个新样本点\(X = x_0\)处预测误差为 $$\begin{split} \text{Err}(x_0) = &amp;amp; E[(y_0 ; \hat{f}_p(x_0))^2 | X = x_0] \\ = &amp;amp; \underbrace{\sigma^2_{\epsilon}}_{Irreducible Error} &#43; \underbrace{[f(x_0) ; E\hat{f}_p(x_0)]^2}_{Bias^2} &#43; \underbrace{E[E\hat{f}_p(x_0)- \hat{f}_p(x_0)]^2}_{Variance} \\ \end{split}$$ 如果\(x_0\)此时取所有训练集中的\(x_i\)，则其平均预测误差（该误差也被称作样本内(in-sample)误差，因为\(x_0\)都是来自与训练样本内。不过\(y_0\)都是新的观测，且真实的\(f(x_i)\)仍未知，因此该预测误差不是训练误差，后续会AIC等准则中会详细讲解）： $$\begin{split} \frac{1}{N}\sum^N_{i=1}\text{Err}(x_i) = &amp;amp; \underbrace{\sigma^2_{\epsilon}}_{Irreducible Error} &#43; \underbrace{\frac{1}{N}\sum^N_{i=1}[f(x_i); E\hat{f}(x_i)]^2}_{Ave(Bias^2)} &#43; \underbrace{\frac{p}{N}\sigma^2_{\epsilon}}_{Variance} \\ = &amp;amp; \underbrace{\sigma^2_{\epsilon}}_{Irreducible Error} &#43; \underbrace{\frac{1}{N}\sum^N_{i=1}[f(x_i) ; x_i^T\beta_{*}]^2}_{Ave[Model Bias]^2} &#43; \underbrace{\frac{1}{N}\sum^N_{i=1}[x_i^T\beta_{*} ; Ex_i^T\hat{\beta}]}_{Ave[Estimation Bias]^2} &#43; \underbrace{\frac{p}{N}\sigma^2_{\epsilon}}_{Variance} \end{split}$$ 对于普通线性模型易知其“估计偏移（Estimation Bias）”为0（最小二乘估计也是线性估计类中的最佳估计），易知随着特征个数\(p\)增加，方差（注：第一个等式根据线性回归很容易推导方差（Variance）为\(\frac{p}{N}\sigma^2_{\epsilon}\)）逐步增大，而对于真实\(f(X)\)却近似越好，模型偏误（Model Bias）越小，但预测误差是这两者的综合，则整体变化趋势如下图 bias_var 这与上图测试集误差变化一致。另外，之所以特地提到还有“估计偏移”，因为对于线性模型类，还有其他诸如岭回归、Lasso等受限的回归类别，他们都属于线性模型类，相比纯线性模型，他们由于对回归系数做了不同程度的压缩，因此相比于最佳线性估计\(\beta_{*}\)会有些差距，产生“估计偏移”，进而整体上导致“模型偏移”增加，但是他们限制了参数个数和压缩了回归系数，模型可能更加简单，因此直观上这类模型估计平均而言会稳定些（模型方差小），用图形来表示他们的关系即如下图 scheme_bias_var 箭头组合长短即表示了平均预测误差，可以看到在受限模型空间中由于较小的模型估计方差，可能使得整体的平均预测误差更小。 1.4 对模型选择和评价有什么指导意义？ 从“偏移-方差”分解可以看到，在有限的模型空间中，对某个模型类控制好模型的复杂度非常重要，否则不容易获取较好（包含稳定与预测误差小两方面）的预测模型，这便是模型选择阶段的工作。可能不少人觉得此处获取较好模型是指模型评价，但是模型评价与模型选择是两个不同的概念，代表两个不同的阶段： 模型选择：根据一组不同复杂度的模型表现，即从某个模型空间中挑选最好的模型； 模型评价：选择一个（最好）模型后，在新的数据上来评价其预测误差等评价指标。 从定义看，两者的目标不同，模型评价是模型选择的后一步。换句话说，模型选择是在某个模型类中选择最好的模型，而模型评价对这个最好的模型进行评价。模型评价可以比较多个模型类中的最佳模型，然后从中挑选出最佳模型，亦或者进行模型融合再进行评价。在模型选择阶段，常见的指标有AIC准则、BIC准则、CV值、结构风险上界等比较普适的准则，而在模型评价阶段，我们可以根据分类、回归、排序等不同问题关心的问题选择不同的评价指标，多与模型选择时的损失不同：（1）分类：ROC、AUC、TPR、FPR、F1 score；（2）排序：DCG、NDCG；（3）回归：RMSE、MAE、Deviance。根据具体业务，实际的评价指标有很多种，最好的方式当然是模型选择时即设计其损失函数即为评价指标，但是通常而言这些指标包含了某些非线性变化，优化起来难度颇大，因此实际模型选择仍是选用经典的那些损失函数，而模型评价则会与其略有不同。 随着机器学习普及，大家都有了“训练-验证-评价”的思维，这其实就是完整重现模型选择、模型评价的过程。如下图我们将数据集分成三个不相交的集合来做模型选择和模型评价： model_proc 训练集：获得模型及其训练误差，用来训练不同模型； 验证集：与训练集相对独立，获取训练模型在该集上的预测误差，用来做模型选择； 测试集：与训练集和验证集独立，获得真实的测试误差和其他模型评价指标，用来评价已选择出的模型。 使用训练集、验证集目的就是做模型选择，测试集自然是做模型评价。这三个集合的划分，并没有严格的准则，根据样本大小不同而做不同的选择，但是一个原则是测试集需要保持未知和与训练集、验证集的独立性。在数据挖掘比赛的时候，主办方通常会给我们一个训练集，然后自己持有一个未知的测试集。实际上这个测试集并不是真正的“测试集”，更应该称作“验证集”。因为随着参赛选手不断提交结果，他们在这个数据集也做了很多探索和尝试，能够逐渐发现这个所谓的“测试集”上的规律，模型选择和模型评价都依赖该数据集进行调整，因此从模型评价的独立性角度来说，这并不能当做最终的测试集，往往会低估预测误差，最好的做法是重新更换几组未知的数据集来当做新的“测试集”做模型评价，这也是秉承统计随机的思想——仅仅在某个“测试集”好是不够的（最近ImageNet事件也确实存在这样的问题）。 所以结合文章开始的方差-偏移图，对模型选择和模型评价的指导可以凝缩为一句话：根据已知的训练集和验证集在特定模型空间中进行模型选择，获取合适复杂度的模型，然后在多种模型空间做模型选择获取多种模型，最后的最优模型需要通过多个独立未知的测试集来做模型评价决定，否则很容易导致模型过拟合。（这实际上就是一个完整而规范的机器学习过程。） 2. 模型选择的方法 模型选择核心思想就是从某个模型类中选择最佳模型。注意，它与一般的“调参”意义不同，调参很多时候可能是针对优化算法中的某些参数进行调整，比如步长（学习速率）、mini-batch大小、迭代次数等，也会涉及到模型\(f(x, \alpha)\)中调整参数（也称正则参数）\(\alpha\)的选择，但是模型选择不涉及算法中的参数，仅涉及模型目标函数中的调整参数\(\alpha\)。 从上面叙述可得知模型选择阶段，最标准的方法自然在训练集\(\mathcal{T}=(\mathbf{X}, \mathbf{Y})\)上训练模型，然后在验证集上获取预测误差\(\text{Err}_{\mathcal{T}} = E_{X^0, Y^0}(L(Y^0, \hat{f}(X^0))|\mathcal{T})\)，该误差也被称作“样本外（extra-sample）误差”，可真实反映出模型的样本外的预测能力，最后选择最小预测误差所对应的模型作为最佳模型即可。但通常而言，独立的验证集我们也没有，手头仅有的信息就是训练集，那么要想估计测试误差或者其期望曲线，就只能在训练集上做文章，一般而言可能仅有两种思路： 从训练集划分点数据出来形成验证集来近似测试误差； 对训练误差进行某种转化来近似测试误差。 第一种思路是非常自然的思路，只要对训练集进行合适的划分，我们就有可能近似出预测误差\(\text{Err}_{\mathcal{T}}\)。但是对原始训练集\(\mathcal{T}\)划分为新的训练集\(\mathcal{T}_{new}\)和验证集，不同的划分比例可能使得新训练集与原训练集相差较大，进而使得\(\text{Err}_{\mathcal{T}}\)差异很大，因此用这种划分的方式来估计条件期望形式的预测误差\(\text{Err}_{\mathcal{T}}\)比较困难。那么此时我们可以不估计\(\text{Err}_{\mathcal{T}}\)转为估计其期望，即平均预测误差\(\text{Err}\)，通过重复抽样的方式来多次估计预测误差\(\text{Err}_{\mathcal{T}}\)，然后取其平均即可，这种方式我们可以称其为“重复抽样法”：通过训练集多次切分、抽样来模拟训练集、验证集，计算多个“样本外误差”，然后求其平均预测误差，这是一种密集计算型方法，比如交叉验证（Cross Validation）、自助法（bootstrap）等。 第二种思路相比第一种思路更加考虑计算效率，因为重复抽样需要计算多次估计，因此做一次模型选择可能需要花费不少时间，如果单单从训练集的训练误差就可以近似出测试误差\(\text{Err}_{\mathcal{T}}\)，那么模型选择效率便会大大提高。这种方式以统计学中的AIC、BIC等为代表，深刻剖析训练误差与之前提到的“样本内（in-sample）误差”、预测误差\(\text{Err}_{\mathcal{T}}\)间的关系，给出了预测误差估计的解析式，因此第二种思路我们可以称之为“解析法”。 这两种思路在统计学习和机器学习中都有大量应用，相比较而言，统计学习更喜欢第二种解析法，这样容易计算，并且会较好的理论性质（似然角度）；而机器学习则更喜欢第二种重复抽样法和从VC维衍生出来的结构风险最小化法，不需要计算基于分布的似然，普适性更好。 一般而言模型选择准则有如下几种： 重复抽样与预测稳定性角度：CV、GCV、Boostrap 似然与模型复杂度角度：AIC、AICc、BIC、EBIC VC维与风险上界控制角度：SRM 首先我们从更加普适的重复抽样法入手来介绍这些模型选择的方法和思路。 2.1 重复抽样思路 CV法 交叉验证法（CV法）是最自然的重复抽样法，过程如下图所示 cv 将一个训练集随机分成K份（图中所示为5份），然后选择第K份作为验证集（图中为第3份），然后剩余的K-1份作为训练集训练模型，这样便可以得到K个“预测误差”，求其平均值即为所谓的“CV值”，所以常说的CV值实际上是预测误差期望\(\text{Err}\)的一个估计值。数学语言叙述如下：记\(\tau:\{1, \ldots, N\} \rightarrow \{1, \ldots, K\}\)是一个划分函数，表示随机地将第\(i\)个观测分配\(\{1, \ldots, K\}\)中某个指标；记\(\hat{f}^{-k}(x)\)表示去除第\(k\)部分数据训练所得的模型，则预测误差的交叉验证估计（CV值）为 $$CV(\hat{f}) = \frac{1}{N}\sum^N_{i=1}L(y_i, \hat{f}^{-\tau(i)}(x_i))$$ 如果该模型有调整参数\(\alpha\)来调整模型的复杂度，记该模型集为\(f(x, \alpha)\)，则CV值为 $$CV(\hat{f}, \alpha) = \frac{1}{N}\sum^N_{i=1}L(y_i, \hat{f}^{-\tau(i)}(x_i, \alpha))$$ 此时通过调整参数\(\alpha\)可以获得不同模型复杂度下的CV值，然后选择最小的CV时所对应的模型\(\hat{f}(x, \alpha_{cv-min})\)作为最后的预测模型即可，这便是利用CV值来做模型选择的思路。 从CV估计定义可以看到，划分的份数\(k\)粒度大小不同时，CV值对平均预测误差\(\text{Err}\)的估计也有好坏之分，这里将涉及“偏误”、“方差”以及“计算效率”的权衡，主要是“偏误”与“计算效率”的权衡。以两个极端例子为例： \(K=N\)时即“留一法（Leave-One-Out, LOO）”，表示每次切分的验证集仅为一个观测，其余的都是训练集，此时新训练集与原始训练集非常接近，则所得的CV值是平均预测误差的近似无偏估计，但是由于这N个训练集相关性也很高，训练的模型则也很相似，而且每次仅评价了一个观测，使得每个验证集上的预测误差相差较大，CV估计的方差较大，另外，N折交叉验证要估计N个模型，计算消耗巨大导致模型选择很慢； 如果\(k\)取得较小，比如2，相当于一半数据用来训练一半用来验证，很显然此时新训练集与原始训练集差异较大，则CV值相对于真实平均预测误差会有较大偏误，导致会高估了平均预测误差，不过两个新训练集完全独立，训练出来的两个模型相关性很低，而且验证集数据比较充足，模型评价相对充分，在数据分布没有太大变化的大前提下，两个验证集上的预测误差会比较接近，使得CV估计的方差比较小，而此时计算消耗很少，模型选择效率高。 实际中，训练集切分带来的估计偏误与计算量才是我们真正关心的量。权衡偏误与效率的得失，由于CV对于预测误差的估计与训练样本大小有关，如果本身样本量就不大，交叉验证切分将导致训练样本更少，这会引起更大的估计偏差，所以实际折数\(k\)经验是： 样本量大时，5折交叉验证对预测误差估计便足够，并且计算快速； 样本量小时，10折甚至LOO都可以，在损失计算效率的情况下优先考虑预测误差估计的准确性。 另外，由于5折或者10折CV估计有偏误，实际模型选择中还使用“one standard error”规则，即不选择CV值最小的模型，而是选择高于最小CV值一个标准差之内的最简模型，比如glmnet通常推荐lambda.1se，即这个准则。原因仍是5或10折CV很可能会低估平均测试误差，所以要保守选择平均预测误差略高于最小CV值得简单模型。 对于交叉验证法的实际操作，我们多半可能还会涉及变量筛选等预处理。对于这类预处理步骤，如果后续使用CV来做模型选择便需要考虑使用顺序的问题，一个使用原则是： 如果预处理涉及联合使用自变量\(X\)与\(Y\)的关系，比如利用\(X\)与\(Y\)的相关性等有监督方式来做变量选择，则需要在预处理前就需要对数据进行切分，数据预处理成了模型选择的一部分，而不能先变量筛选，然后在新数据进行交叉验证来做模型选择； 如果预处理不涉及自变量\(X\)与\(Y\)的关系，仅仅利用\(X\)自身的信息，比如\(X\)的方差或者熵来做变量选择，这种无监督方式的预处理无需对数据进行切分来交叉验证，可以直接先无监督预处理完再利用交叉验证方法来做模型选择。 目前变量筛选的方法有很多，传统的有监督包含相关性筛选、熵增筛选等都是有监督方法，由于这种筛选已经利用了全体\(X\)和\(Y\)间的关系信息，如果利用这种筛选后的数据来做交叉验证将会导致模型低估预测误差，高估了模型的泛化效果，因此实际使用时尤其需要注意这种筛选方法与交叉验证的联合使用，防止犯错。 另外，在分类问题中，特别是对于类别不平衡问题，由于CV法可能会导致每折中的类分布不一致，使得训练不稳定，因此实际中分层CV（stratified CV）也会使用。其相比较CV的不同之处就是不使用经典CV的均匀随机抽样方法来切分样本，而是根据总体类别比例，对每折都使用依赖于类别比例的分层抽样，保证每折的类别分布与原始数据差不多。学习过分层抽样的同学可能知道，分层抽样可以降低估计量方差，因此实际使用分层CV相比经典CV选择模型可能更稳定些。 GCV 由于计算CV是一个密集计算的模型选择法，即使可以利用并行计算来提高模型选择的效率，但是如果能够找到无需重复计算的替代方法，那么实际应用中，人们可能更倾向于使用这种模型选择方法。对于线性模型，如果使用平方损失，广义交叉验证（GCV）是LOO法解析形式的近似估计，可以避免计算N个模型来快速做模型选择。对于线性模型，对于目标变量的估计可以写成如下投影形式 $$\hat{\mathbf{y}} = \mathbf{Sy}$$ 其中\(\mathbf{S}\)是投影阵，仅与\(\mathbf{X}\)有关，与\(\mathbf{y}\)无关，则线性模型的LOO值为 $$\frac{1}{N}(y_i ; \hat{f}^{-i}(x_i))^2 = \frac{1}{N}\sum^N_{i=1}(\frac{y_i ; \hat{f}(x)}{1 ; S_{ii}})$$ 其中\(S_{ii}\)是\(\mathbf{S}\)的对角元素，则GCV近似为 $$GCV(\hat{f}) = \frac{1}{N}\sum^N_{i=1}(\frac{y_i ; \hat{f}(x)}{1 ; \text{trace}(\mathbf{S})/N})$$ 此时不需要重复计算，只需要计算线性模型的投影阵\(\mathbf{S}\)的迹（后面也称作**自由度**）即可，极大降低了交叉验证的计算量，并且使得平均预测误差偏误更小，关于线性模型的GCV详细推导可参考此处。不过GCV仅适用于线性模型，包含带正则项的普通线性模型、非参线性模型，比如LASSO、岭回归、样条回归、多项式回归等模型，其余比如树模型、神经网络模型都不适合。 关于CV的衍生方法比较新的时ES-CV，由Yu Bin在2013年提出，不过实际上这种方法对于核心是估计稳定性的定义，CV法只是来改进估计稳定性的一种方式而已，感兴趣的同学可以参 考Yu老师的论文。 Bootstrap法 对于bootstrap，不管是统计还是机器学习的同学，可能对这个名词以及实施方式都比较熟悉。bootstrap法由Efron于1979年提出，随后在统计中得到了大量的应用，主要用于解决复杂统计量的置信区间等估计问题；而在机器学习中，由Breiman在94年提出bagging方法（全称为bootstrap aggregating）实际上就是bootstrap的直接应用，它是一种模型组合方法，主要用于分类问题中以获取比较稳定的结果。bootstrap的思路和操作都非常简单，如下图 bootstrap 假设有样本\(\mathbf{Z}\)，则不断重复随机抽同样大小的B个样本集\(\mathbf{Z}^{*b}\)，这些样本被称作bootstrap样本，随后用着B个样本分别训练模型，得到B个目标估计量\(S(\mathbf{Z}^{*b})\)。然后可以用这些统计量求某些指标，比如统计量的均值、方差、偏移等。对于分类问题，用这些bootstrap样本训练多个分类器，比如决策树或神经网络，然后将这B个分类模型对新的样本做预测，把B个分类结果做一个投票获取最终的结果即可，这边是所谓的bagging。 不过上述都是对于估计量或者模型而言，那么如何利用bootstrap来做模型选择呢？如果我们用着B个模型对每个观测都进行评价，然后求其平均误差 $$\hat{\text{Err}}_{boot}(\alpha) = \frac{1}{B}\frac{1}{N}\sum^B_{b=1}\sum^N_{i=1}L(y_i, \hat{y}^{*b}(x_i, \alpha))$$ 看起来似乎可行，但仔细一思考就可以发现这并不是一个好的平均预测误差的估计，主要原因是bootstrap样本即被训练又被评价，与CV不同训练集被重复分割为独立的训练集与验证集相比，数据评价存在重合，所以\(\text{Err}_{boot}(\alpha)\)肯定与训练误差可能比较接近，而与平均预测误差有较大偏差，那么用该估计来调模型复杂度参数\(\alpha\)显然更不是个好选择。那如何利用bootstrap法来更好的近似平均预测误差呢？我们可以借助于CV的分割思想。 我们知道，bootstrap样本的获取其实就是重复有放回的N次抽样，那么对于观测\(i\)属于该bootstrap样本，至少被抽中一次的概率即 $$P_{boot} = 1 ; (1 ; \frac{1}{N})^N \overset{N \rightarrow \infty}{\longrightarrow} 1 ; 1/e \sim 0.632$$ 换句话说，每个bootstrap样本中，总有些观测没被抽到，那么根据CV法的思路，这部分观测就可以拿出来作为验证集来作为平均预测误差的估计。 熟悉随机森林或者Bagging的同学对于OOB（out of bag）这个词肯定不陌生。OOB其实就是这种思路，不过只是对未抽中的样本再次做了投票然后再估计预测误差，对于此处我们则不做投票，直接取那些没出现过\(i\)观测的bootstrap样本训练的模型分别估计\(i\)的误差，然后求个平均即可 $$\hat{\text{Err}}^{(1)}_{boot}(\alpha) = \frac{1}{N}\sum^N_{i=1}\frac{1}{C^{-i}}\sum_{b \in C^{-i}}L(y_i, \hat{f}^{*b}(x_i,\alpha))$$ 其中\(C^{-i}\)即不包含\(i\)观测的bootstrap样本集。你可能想万一有个观测所有bootstrap都出现了怎么办？直接去掉就好了嘛，不过你可以算算B个bootstrap样本都出现的概率有多小。实际而言，B大点便很容易保证观测\(i\)很难在所有bootstrap样本集中出现了。 下面在思考下，这种估计是对平均预测误差估计是个好估计吗？虽然不会像第一个估计量那样低估平均预测误差，但是这种估计量也很容易高估平均预测误差，主要原因是每个bootstrap样本中仅有差不多63.2%的不同观测用来建模，这样使得\(\hat{\text{Err}}^{(1)}_{boot}(\alpha)\)估计量表现得很像2折或3折交叉验证，分割太少导致可能偏差很大，特别是对于样本量不够多的训练集。如果改进，直观想法便是将训练误差\(\text{Err}_{train}\)与该估计量\(\hat{\text{Err}}^{(1)}_{boot}(\alpha)\)按照某种比例加权\((1 ; w)\text{Err}_{train} &#43; w\hat{\text{Err}}^{(1)}_{boot}(\alpha)\)来纠正这种偏移，具体细节可以看ESL的阐述，实际中由于bootstrap计算量过大，所以用来做模型选择不多，所以此处不再详述。 不过在大数据时代，分布式思维逐深入统计学家和计算机学家脑中，由于bootstrap具备良好的可并行性，以及良好的统计性质和估计稳定性，Jordan在2012便提出了基于bootstrap的 BLB(Bag of Little Bootstraps) ，能够给出较稳定的估计量以及估计量的区间估计，这是其他方法不具备的特点。比如能告诉你预测误差大小，同时可以告诉你预测误差的偏误以及方差，那这是不是一件更令人安心的事情呢？在现在这种环境下，与其不停做实验等待结果，不妨考虑下bootstrap这类有可靠性估计的方法的好处。BLB的算法思路很清晰，简单来说：subsampling &#43; bootstrap &#43; average；先无放回抽样，然后bootstrap抽样，获取参数bootstrap估计量，以及其置信区间、偏移、预测误差等估计量，最后将这些估计量平均起来即可。细节可以参考其论文，只要有多机可并行环境便可很容易实施该方法。 bootstrap思想是一种非常重要思想，后来著名的random forest便充分利用了该思路。而且相比目前的数据并行、模型并行的分布式算法思路，我觉得可以从bootstrap抽样角度获取更加稳定的估计量，当然这些都是题外话，与本文话题不相符合，以后可以再谈谈抽样与并行算法之间的感想，实际上都会在“计算效率”与“精度”之间做些权衡。 2.2 解析解思路 根据上述重复抽样法可知，CV等方法直接来估计“样本外误差”，并求其期望，而解析解思路由于不像CV法通过原始训练集切分构建验证集，仅仅从训练集出发，构建训练误差与“样本内误差”间等式关系，因此需要深刻理解训练误差、“样本内误差”、模型复杂度这几个概念，才能较好的理解为何解析解思路的那几个准则是有效的。 2.2.1 训练误差与样本内误差 在本文第一节提到，实际应用中通常只有一个训练集\(\mathcal{T} = \{(x_1 y_1), \ldots, (x_n, y_n)\}\)，此时我们希望能在该训练样本上获得最好的预测模型\(\hat{f}\)，在新样本点\((X^0, Y^0)\)能得到最小的泛化误差\(\text{Err}_{\mathcal{T}} = E_{X^0, Y^0}[L(Y^0, \hat{f}(X^0))| \mathcal{T}]\)。这是训练集\(\mathcal{T}\)上预测误差的条件期望，随着样本集变动而变动。但从统计角度来说，抹掉训练集带来的变动，估计预测误差的期望更符合统计口味\(\text{Err}_ = E_{\mathcal{T}}E_{X^0, Y^0}[L(Y^0, \hat{f}(X^0))| \mathcal{T}]\)，这也是上述CV、bootstrap采取的方式。 在第一部分关于\(\bar{err}\)和\(\text{Err}_{\mathcal{T}}\)与模型复杂度走势的图形中，可以看到\(\bar{err}\)低估了\(\text{Err}_{\mathcal{T}}\)，并且复杂度高的模型容易过拟合，导致\(\text{Err}\)很小，而\(\text{Err}_{\mathcal{T}}\)很大，所以用\(\bar{err}\)来评测模型就显得过于乐观了。但是，能不能建立\(\bar{err}\)与\(\text{Err}_{\mathcal{T}}\)间的关系呢？ 由于\(\text{Err}_{\mathcal{T}}\)要引入新\(X^0, Y^0\)有难度，那么退一步看\(X\)不变动而仅引入新的\(Y^0\)的预测误差 $$\text{Err}_{in} = \frac{1}{N}\sum^N_{i=1}E_{Y^0_i}[L(Y^0_i, \hat{f}(x_i))|\mathcal{T}]$$ 这种误差称作“样本内误差”（in-sample误差），虽然也是一种预测误差，但是\(X\)没有变动，因此对于模型泛化估计能力还是不够。不过样本内误差与样本外误差与模型复杂度的关系走势类似，对于模型选择而言，更关心误差的相对值而不是其绝对值，因此实际模型选择中，我们也常常关注“样本内误差”，它也是一种有效且更方便的思路，并且此时建立\(\text{Err}_{in}\)与\(\bar{err}\)间的关系相对更容易了。 由于\(\text{Err}_{in}\)与\(\bar{err}\)都拥有相同的\(X\)，两者唯一的区别便是\(\text{Err}_{in}\)是对\(Y\)求了期望，而\(\bar{err}\)则直接使用了某个\(Y\)值（训练样本），两者的差便是 $$\text{op} \equiv \text{Err}_{in} ; \bar{err}$$ 为更便于理解，以平方损失和线性模型为，且预测值\(\hat{\mathbf{y}}\)是原始值\(\mathbf{y}\)的线性变换\(\hat{\mathbf{y}} = \mathbf{Sy}\)，则 $$\text{op} \equiv \frac{1}{N} \sum^N_{i=1}[E_{Y^0_i}(Y^0_i ; \hat{y}_i)^2 ; (y_i ; \hat{y}_i)^2)]$$ 与预测误差类似，这其实是关于训练样本点\(y_i\)的条件期望，这种条件期望不好估计和分析，因此消除训练样本中\(y_i\)变异带来的影响，我们再次对所有\(\mathbf{y} = \{y_i\}^N_{i=1}\)求期望（注意这其中样本点与随机变量间概念的转化，此处将\(y_i\)看做是样本点还是随机变量并不做严格区分，所以\(E_{y_i}y_i = E_{Y_i}Y^0_i\)） $$\begin{split} \omega \equiv E_{\mathbf{y}}(\text{op}) &amp;amp; \equiv \frac{1}{N}\sum^N_{i=1}[E_{y_i}E_{Y^0_i}(Y^0_i ; \hat{y}_i)^2 ; E_{y_i}(y_i ; \hat{y}_i)^2] \\ &amp;amp; = \frac{1}{N}\sum^N_{i=1}2(E_{y_i}(y_i \hat{y}_i) ; E_{y_i}y_iE_{y_i}\hat{y}_i) \\ &amp;amp; = \frac{2}{N}\sum^N_{i=1} \text{Cov}(y_i, \hat{y}_i) \end{split} $$ 于是我们便得到了如下非常重要的关系式 $$ E_{\mathbf{y}}(\text{Err}_{in}) = E_{\mathbf{y}}(\bar{err}) &#43; \frac{2}{N}\sum^N_{i=1} \text{Cov}(y_i, \hat{y}_i) $$ 因此从消除训练集变动的影响（期望）角度来看，我们找到了训练误差与“样本内误差”间的数量关系，这个关系式推广到0-1损失或者似然形式都成立。对于加性误差的线性模型\(y = f(x) &#43; \epsilon\)，易知 $$ \sum^N_{i=1}\text{Cov}(y_i, \hat{y}_i) = \sum^N_{i=1}\text{Cov}(y_i, \mathbf{Sy}_i) = \text{trace}(\mathbf{S})\sigma_{\epsilon}^2 = \sum^N_{i=1} d \sigma_{\epsilon}^2 $$ 其中\(\mathbf{S}\)与\(d\)便是线性模型的有效参数个数，也称作自由度，注意仅对于线性模型如此，控制着模型复杂度。可以看到，“样本内误差”与训练误差间的桥梁就是模型复杂度！当模型复杂度过高时，很可能“样本内误差”不降反升。 2.2.2 Cp法与AIC 借助上述训练误差与样本内误差的关系式，实际中我们便可以这样来对“样本内误差”做这样的估计 $$\\hat{Err}_{in} = \bar{err} &#43; \hat{\omega}$$ 训练误差与“样本内误差”都不是期望形式，看起来有些混合，不过由于实际情况无法求\(Y\)的期望，所以我想这也是没办法的“最佳”估计了吧，统计中常会遇到这种知难而相对随意的形式- -! 对于平方损失下的线性模型（注意此时的损失限制），所谓的Cp准则即为 $$C_p= \bar{err} &#43; 2 \cdot \frac{d}{N}\hat{\sigma}_{\epsilon}^2$$ 其中\(\sigma_{\epsilon}^2\)用包含所有变量的回归模型的残差估计所得。乍一看过去，这完全就是给“样本内误差”估计换了个名称而已嘛:) AIC准则与之略有差异，训练误差选用似然函数的负数来代替，而\(\hat{\omega}\)没有噪音方差的估计\(\hat{\sigma}_{\epsilon}^2\)，为如下形式 $$ \text{AIC} = -\frac{2}{N}\cdot \text{loglike} &#43; 2 \cdot \frac{d}{N} $$ \(d\)仍是模型的参数个数，用来衡量模型复杂度。对于非线性模型和更复杂的模型，此处\(d\)需要用其他形式来代替。 对于误差为已知方差的高斯模型，化简似然函数便可知AIC就等价于Cp准则。对似然函数化简，可以得到对应的不同的各类损失，比如高斯模型与平方损失的对应，logistic模型与cross entropy损失的对应等，所以相比仅只适用于平方损失线性模型的Cp准则，AIC适用范围更广。实际使用中，AIC做模型选择更倾向于选择比真实模型更多参数的模型，容易低估“样本外误差”，有**过拟合的倾向**。 另外AIC准则还与KL距离有紧密联系，可以从KL距离来推出AIC准则，感兴趣的同学可以参考这篇文档中关于AIC的介绍。而关于AIC的校正版AICc准则，实际中也有使用，关于其介绍可直接 参考wiki。 2.2.3 BIC与贝叶斯 BIC准则形式项与AIC很像，同样还是似然负数作为损失，然后加上一个关于自由度与样本相关的项。 $$\text{BIC} = -2 \cdot \text{loglike} &#43; (\log N) \cdot d $$ 对于方差已知的高斯模型，化简似然函数即可得 $$ \text{BIC} = \frac{N}{\sigma_{\epsilon}^2}[\bar{err} &#43; (\log N) \cdot \frac{d}{N}\sigma^2_{\epsilon}] $$ 忽略比例项，此时BIC与Cp和AIC基本一致，只是第二项的因子2换成了\(\log N\)，相比AIC和Cp准则，BIC对于参数更多、复杂度更高的模型惩罚更重。虽然看起来相似，但是BIC原始想法并不是类似AIC这种思路，而是从贝叶斯角度出发得到的。 从贝叶斯角度来看，模型选择无非就是依托于当前样本数据\(\mathbf{X}\)，从候选模型集合\(\mathcal{M}_m, m = 1, \ldots, M\)中选择后验概率最大的模型即可（所谓后验概率即从数据反推可能模型的概率，\(\mathcal{M}_m\)可以看做是所有变量\((1, \ldots, p)\)中得的某个变量子集），当然每个模型都有相应自己的参数\(\theta_m\)，且这些参数也有先验概率分布\(P(\theta_m | \mathcal{M}_m)\)，根据贝叶斯公式可知给定数据\(\mathbf{X}\)时模型后验概率为 $$ \begin{split} P(\mathcal{M}_m | \mathbf{X}) &amp;amp; = \frac{P(\mathcal{M}_m) \cdot P(\mathbf{X}|\mathcal{M}_m)}{P(\mathbf{X})} \\ &amp;amp; \propto P(\mathcal{M}_m) \cdot P(\mathbf{X}|\mathcal{M}_m) \\ &amp;amp; = P(\mathcal{M}_m) \cdot \int P(\mathbf{X}|\theta_m, \mathcal{M}_m) P(\theta_m|\mathcal{M}_m) d\theta_m \end{split} $$ 对于模型选择而言，我们并不需要作上述复杂的积分，只需要比较模型后验概率的相对大小即可，这样的好处是忽略常数项是的计算简便了很多 $$ \frac{P(\mathcal{M}_m | \mathbf{X})}{P(\mathcal{M}_l | \mathbf{X})} = \frac{P(\mathcal{M}_m)}{P(\mathcal{M}_l)} \cdot \frac{P(\mathbf{X}|\mathcal{M}_m)}{P(\mathbf{X}|\mathcal{M}_l)} $$ 具备最大后验概率的模型将与其他所有模型的后验概率比值都大于1。进一步，如果我们事先对模型情况一无所知，模型先验\(P(\mathcal{M}_m)\)服从均匀分布均相等，那么寻找最大后验概率的模型就演变成了寻找最大概率密度\(P(\mathbf{X}|\mathcal{M}_m)\)的模型即可，实际上取对数即求最大似然\(\log P(\mathbf{X}|\mathcal{M}_m)\)。由于\(\theta_m\)被积分掉，求解\(P(\mathbf{X}|\mathcal{M}_m)\)很困难，所以实际中我们可以采用 Laplace近似 即可，只要样本量足够大，该近似效果便很好，通过Laplace近似，似然\(\log P(\mathbf{X}|\mathcal{M}_m)\)变成了 $$ -\text{BIC} \approx \log(\mathbf{X} | \mathcal{M}_m) = \log P(\mathbf{X}|\hat{\theta}_m, \mathcal{M}_m) ; \frac{d_m}{2}\cdot \log N &#43; O(1) $$ 其中，\(\theta_m\)为极大似然估计，\(d_m\)为模型\(\mathcal{M}_m\)的自由参数个数。很显然，只需要将似然函数\(\log P(\mathbf{X}|\hat{\theta}_m, \mathcal{M}_m)\)作相应替换，去掉常数项，求解模型的最大后验概率就等价于最小化BIC，因此在样本量很大使得lalapce近似效果很好时，BIC准则便渐进选择最佳模型。 （注：一句话阐述Laplace技巧即，对于复杂概率函数的似然求解，我们可以将其在参数的极大似然估计处做二阶泰勒展开，根据似然函数在MLE估计处的一阶导为0的性质，原始的概率函数可凑成正态密度函数的形式，于是复杂概率函数就可以用均值和方差均可求的正态分布来近似。） 有了BIC值，我们也可以直接给出每个模型\(\mathcal{M}_m\)的后验概率 $$ P(\mathcal{M}_m | \mathbf{X} ) = \frac{\exp(-\frac{1}{2}\cdot \text{BIC}_m)}{\sum^M_{l=1}\exp(-\frac{1}{2}\cdot \text{BIC}_l)} $$ 这对于模型选择将更加直观，只是计算所有模型的BIC值计算量太大了，实际中我们多半不会如此操作。 虽然在样本量大且变量维数固定时，BIC准则有模型渐进一致性，但是面对实际有限的样本，BIC相比AIC的过拟合又会欠拟合，对模型复杂度控制太严格，导致选择相对过于简单的模型。另外，BIC在模型特征维度\(p\)大时，估计模型更加会有偏差，因此关于BIC的改进版EBIC准则便应运而生。 EBIC法主要的改进思路是对模型先验分布\(P(\mathcal{M}_m)\)的均匀分布进行合理改进。当变量维数比较大时，含不同变量数的模型个数是不同的，比如\(\mathbf{X}\)现为1000维变量,则包含1个变量的模型\(s\)有1000个，模型集合即为\(S_1\)，而包含2个变量的模型\(s\)有\(1000\times 999/2\)，模型集合记为\(S_2\)，可以看到\(S_2\)的大小将近是\(S_1\)的500倍。按照BIC的均匀分布假设，所有模型\(s\)被选择概率相同\(P(s)=1/M\)，这将导致\(P(S_j)\)被选择的概率与之模型集合大小\(\pi(S_j)\)成正比，换句话说，差不多含一半变量数的\(S_{p/2}\)模型集合中的模型\(s\)被选择概率最大，这显然与变量数可能比较小（稀疏）的假设精神相违背，特别在更加高维的情况中，BIC将会更加倾向在较大的模型空间选择，导致选择的变量数过多。 为了改进这种所有BIC所有模型等同视之的思路，EBIC略作了改动，对于属于同一个模型集合\(S_j\)的模型\(s\)自然仍等同视之，概率为\(P(s|S_j) = 1/\pi(S_j)\)，但对于不同的模型集合\(S_j\)整体的先验被选概率则与之大小\(\pi^{\xi}(S_j), 0 \leq \xi \leq 1\)成正比，而不是直接与其大小\(\pi(S_j)\)成正比，这种简单的幂次方式可以平滑不同大小的模型集合被选择的概率，使之差异不那么大，于是模型\(P(s)\)被选择的概率就正比于\(\pi^{\gamma}(S_j), \gamma = 1-\xi\)，带入原来的推导式子，便可得到如下的EBIC准则 $$ \text{EBIC}_{\gamma} = -\log P(\mathbf{X}|\hat{\theta}_m, \mathcal{M}_m) &#43; \frac{d_m}{2}\cdot \log N &#43; \gamma\log \pi(S(\mathcal{M}_m)) $$ 其中\(S(\mathcal{M}_m)\)表示\(\mathcal{M}_m\)所属的模型集合，模型集合中变量个数相同。所以EBIC相比于BIC增加的惩罚更大，一般经验是“**如果维度\(p\)增长速度相比样本量为\(O(n^k)\), 若\(k &amp;lt; 0.5\)维度增长很慢，则\(EBIC_{0}\)即BIC准则下的模型选择具备一致性；而如果\(k &amp;lt; 1\)维度较快时，则\(EBIC_{0.5}\)模型选择具备一致性，只要\(p\)不是随样本量\(n\)指数级增长，\(k\)接近1，则\(\text{EBIC}_1\)模型选择是一致的**”。实际建模中，考虑实际数据维度增加与样本增加的关系，选择合适的\(\gamma\)即可，关于EBIC的论文和讲义可以分别参考这里和这里。 3. 总结 其他模型选择方法还有“最小描述长度（MDL）”和“基于VC维的最小结构风险法（VC-SRM）”。这两种方法一个从最优编码的角度，一个从数据可分性角度分别来阐述模型选择，感兴趣同学可以学习这两种思想，不过由于方法各有缺陷，实际应用较少。 总之，对于模型选择方法，实际中由于CV法、GCV法的通用性，不管在计算机还是统计领域中都大量应用。而其他的BIC等法则，由于计算快速且有良好的理论性质，统计领域的研究者可能更加喜欢这类方法。不过由于他们基于似然并不是通用框架，并且对于预测来说，根植于样本内误差做的模型选择，模型预测能力很可能不如预期，因此实际应用，我们也更加推荐CV法、GCV法来做模型选择，毕竟现在计算能力如此强大，并行处理也比较方便，而且方法也比较灵活，可以模型选择，也可以模型组合。通过学习这些方法，我们可以充分感受到不同学科的思考方式，通常而言，简单的方法会比较可靠，但是可能需要大量计算来验证、评价；而需要绕些脑子的方法，多半是为了能够进一步简化运算而推演的，这样可能会认识到更深刻的规律，但也可能会使用不少近似的方法来损失精度。对于现实计算而言，“效率-精度”总是一个绕不开的话题，以后有机会我们可以再谈谈这个问题。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[传统统计与贝叶斯方法：难以逾越的鸿沟？ - 慧航 - 专栏]]></title>
    	<url>/prof/2015/08/25/bayesian-freq/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20180632 先验学派（apriorius），实用学派（pragmat 我想每个人都有这样的认识：统计学有两大『学派』：传统的频率学派和贝叶斯学派。正如本文的题图一样，不少人认为频率学派是落后的、原始的，贝叶斯的方法更现代（酷炫）。然而，频率学派和贝叶斯学派之间真的有难以逾越的鸿沟么？ 前菜 在介绍正文之前，我们先来一碟前菜，熟悉一下贝叶斯公式热热身。在实践中我们经常会碰到二元选择变量的问题，比如家庭是不是要买车、是否选择外出打工等等问题。令\(d\)为一个二元变量，\(d=1\)或者\(d=0\)。我们的问题是，给定一些特征，\(d=1\)的概率是多少？ 用条件概率的描述，我们想知道\(\mathrm{P}(d=1|x)\)。一个很自然的想法是使用贝叶斯公式，我们可以写出： \[\mathrm{P}(d=1|x)=\frac{f(x|d=1)\mathrm{P}(d=1)}{f(x|d=1)\cdot \mathrm{P}(d=1)&#43;f(x|d=0)\cdot \mathrm{P}(d=0)}=\frac{f(x|d=1)\mathrm{P}(d=1)}{f(x)}\] 现在我们从两个方面解读这个等式。 首先看第一个等号，如果对等号右边做一个变换，可以得到： \[\mathrm{P}(d=1|x)=\frac{f(x|d=1)\mathrm{P}(d=1)}{f(x|d=1)\cdot \mathrm{P}(d=1)&#43;f(x|d=0)\cdot \mathrm{P}(d=0)}=\frac{\frac{f(x|d=1)\mathrm{P}(d=1)}{f(x|d=0)\mathrm{P}(d=0)}}{1&#43;\frac{f(x|d=1)\mathrm{P}(d=1)}{f(x|d=0)\mathrm{P}(d=0)}}\] 现在令： \[g(x)=\log(\frac{f(x|d=1)\mathrm{P}(d=1)}{f(x|d=0)\mathrm{P}(d=0)})\] 那么上式就变成了： \[\mathrm{P}(d=1|x)=\frac{\exp(g(x))}{1&#43;\exp(g(x))}\] 这是什么呢？Logistic（Logit）回归。如果给定\(g(x)\)一些光滑性的假设，可以将\(g(x)\)用 sieve 逼近，从而计算\(\mathrm{P}(d=1|x)\)。注意这里虽然得到了 Logistic 的形式，但是没有做任何分布的假设的。 另一方面，我们来看第二个等号。为了求\(\mathrm{P}(d=1|x)\)，必须求出\(f(x|d=1)\)和\(f(x)\)。如果用 kernel 来估计两个密度函数，可以得到： \[\mathrm{P}(d=1|x)=\frac{f(x|d=1)\mathrm{P}(d=1)}{f(x)}=\frac{\frac{1}{N_dh}\sum{d_i\cdot K(\frac{x-x_i}{h})}\cdot \mathrm{P}(d=1)}{\frac{1}{Nh}\sum{K(\frac{x-x_i}{h})}}=\frac{\sum{d_i\cdot K(\frac{x-x_i}{h})}}{\sum{K(\frac{x-x_i}{h})}}\] 大家看这是啥？对了，这不就是个\(d\)对\(x\)的非参数的回归吗！ 用贝叶斯公式得到频率学派的两个方法，是不是很有趣呢？ 贝叶斯作为计算方法 当然，以上的前菜仅仅是使用了贝叶斯公式，并不是严谨意义上的贝叶斯「学派」。实际上，更多的人理解的「贝叶斯学派」是一种哲学：与频率学派不同的是，贝叶斯学派把未知的参数看成是一个随机变量。 如果我们有数据X，希望得到参数\(\theta\)的分布，那么根据贝叶斯学派，\(\mathrm{P}(\theta|x)\propto\mathrm{P}(x|\theta)\pi(\theta)\)。熟悉贝叶斯统计的人都应该知道，贝叶斯方法与极大似然估计有千丝万缕的联系。 然而，对于频率学派，除了极大似然估计之外，仍然有很多其他估计方法。比如最为经济学家所喜欢的广义矩估计（GMM），以及 M-estimator 等等。这些方法，与极大似然估计一样，都要最大化某个目标函数，比如GMM的目标函数为： \[L_n(\theta)=-\left( \frac{1}{\sqrt{n}}\sum{m_i(\theta)} \right)&amp;#39;W_n \left( \frac{1}{\sqrt{n}}\sum{m_i(\theta)} \right)\] 那么这些方法是不是潜在的也可以和贝叶斯学派有些许的相通之处呢？答案是有的。 Victor Chernozhukov 和 Han Hong 2003 年在 JoE 上发表的文章《An MCMC Approach to Classical Estimation》就讲了这么一个故事。 他们提出，如果定义一个『拟后验分布』（Quasi-posterior dsitribution）： \[\mathrm{P}_n(\theta)=\frac{e^{L_n(\theta)}\pi (\theta)}{\int_\Theta e^{L_n(\theta)}\pi (\theta) d\theta}\] 那么后验分布的均值： \[\hat{\theta}=\int_\Theta \theta p_n(\theta)d\theta\] 在一些相对宽松的条件下，会收敛到真值。注意以上的后验分布的均值可以通过使用 MCMC 方法在后验分布中抽样，继而通过简单对抽样计算平均得到。 注意这里这个方法仍然是传统方法而非贝叶斯方法。在这个方法里面，仍然是有真值的，未知参数并没有像贝叶斯方法那样被假设为一个随机变量。 但是以上的『拟后验分布』非常有意思。试想一下，如果目标函数为极大似然的目标函数，那么以上的『拟后验分布』是不是就变成了贝叶斯统计的『后验分布』了呢？ 此外，作者还讨论了 MCMC 作为一个抽样、计算积分的算法在这里与『模拟退火』算法的相通之处，所以在频率学派看来，贝叶斯方法提供了一个计算最优值的不错的算法。 贝叶斯的频率方法 上面介绍了频率方法的贝叶斯视角，那么反过来，是不是贝叶斯方法也有频率的视角呢？答案是有的。 一般来讲，传统的贝叶斯方法很少讨论样本量对统计量的影响情况，那么当样本趋向于无穷时，贝叶斯统计量究竟有何种表现呢？ Jiti Gao在一篇会议论文《A Frequency Approach to Bayesian Asymptotics》中讨论了这一问题。证明过程这里略过，说一下结论，他证明出，在一些比较宽松的条件下，当模拟次数M、样本量N同时趋向于无穷时，贝叶斯统计量与传统方法（如 MLE）的统计量有相同的渐进分布。 结论 对于『贝叶斯学派』和『频率学派』有很多争议，然而最新的这些发现却给了我们一些新的视角，这两个学派之间并没有我们之前想象的巨大的鸿沟。是不是很有趣呢？]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[比较优势理论与农业生产率 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/08/18/comparative-advantage/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20159778 之前的一篇文章里面，我们介绍了从比较优势的角度来看贸易的 比较优势理论可以上溯至大卫·李嘉图。然而虽然比较优势理论历史悠久，而且长期以来在经济学中一直是解释分工、贸易的有力工具，但是这个理论却非常难以验证。最大的难点是，比如如果法国生产葡萄酒有比较优势，而瑞士生产手表有比较优势，那么法国会专业化生产葡萄酒而不会去生产手表，因而法国生产手表、瑞士生产葡萄酒的生产率是观察不到的。这有点类似于 Roy 模型：生产何种产品是一个自选择的结果，而那些没有被选择的结果是观察不到的。 如何解决这个问题呢？一种方法是给出一些函数形式的假设，比如 Costinot, Donaldson, and Komunjer 就基于 Eaton-Kortum 模型，通过假设生产率服从 Fréchet 分布，得到了检验李嘉图模型的推论的方法。 而 Arnaud Costinot 和 Dave Donaldson 发表在《American Economic Review: Papers &amp;amp; Proceedings》上的文章，《Ricardo’s Theory of Comparative Advantage: Old Idea, New Evidence》却使用了其他的方法，在不假设函数形式的情况下检验了李嘉图的比较优势理论。 如何做到的呢？作者单独拿出最为传统的行业——农业作为特例进行研究。农业的一大好处是，其生产率是非常容易预测的——如果知道了一块土地的土壤状况、降水、气温等条件，那么任意一种作物种植在这块土地上的生产率也就是高度可预测的了。 首先来看李嘉图的比较优势理论是怎样的呢？假设有\(f=1...F\)种生产要素，要素不能跨国流动，但是在一国之内的不同部门可以自由流动。在这篇文章里面，生产要素\(f\)即为一个国家的不同区块的土地。假设生产函数为： \[Q^g_c=\sum^F_{f=1}A^g_{cf}L^g_{cf}\] 其中\(g=1...G\)代表商品，\(c=1...C\)代表国家，\(A\)代表生产率，而\(L\)代表生产要素\(f\)在\(c\)国生产商品\(g\)的供给。生产率\(A\)的差异是国家之间比较优势的来源。如果在国家\(c\)的两种要素\(f_1\)、\(f_2\)，有： \[A^{g_2}_{cf_2}/A^{g_1}_{cf_2}&amp;gt;A^{g_2}_{cf_1}/A^{g_1}_{cf_1}\] 那么可以说对于国家\(c\)，要素\(f_2\)在生产\(g_2\)上有比较优势。 假设完全竞争，并且每个国家、每个部门的要素分配都是有效的，那么生产行为就是最大化： \[\max_{L^g_{cf}}\left\{\sum^C_{c=1}\sum^G_{g=1}p^g_cQ^g_c\Bigg|\sum^G_{g=1}L^g_{cf}\leqslant L_{cf}\right\}\] 进而，假设最有效的配置是唯一的，那么以上问题的解为： \[Q^g_c=\sum_{f\in\mathcal{F}^g_c}A^g_{cf}L_{cf}\] 其中 \[\mathcal{F}^g_c=\left\{f=1,\dotsc,F|\frac{A^g_{cf}}{A^{g^\prime}_{cf}}&amp;gt;\frac{p^{g^\prime}_c}{p^g_c},\;\; \text{if}\,g^\prime g\right\}\] 为在国家\(c\)，要素\(f\)被分配给商品\(g\)的集合。换句话说，以上的集合描述了，这个国家哪块土地会种什么作物。 为了验证以上模型，作者通过 Food and Agriculture Organization (FAO)的 FAOSTAT 收集了各种作物的产量、价格信息。此外，为了计算各种作物在不同国家、土地上的生产率，作者使用了 Global Agro-Ecological Zones (GAEZ)项目，该项目可以使用地理信息预测特定作物的产量。 在有了以上数据之后，实证就变的相对简单了。使用 GAEZ 项目给出的生产率情况，通过以上模型预测每个国家生产某种作物的产量，再跟实际观察到的产量进行对比。如果李嘉图的比较优势模型能够解释这些专业化，那么观察到的产量和预测的产量应该是高度正相关的。作者得到的结果如下： 可以发现，在各种设定下，预测的产出与实际观察到的产出都是高度正相关的，因而李嘉图的比较优势假说一定程度上得到了验证。 然而，虽然高度正相关，但是似乎解释能力远远不够。作者认为比较优势模型没能完全解释观察到的产出是非常正常的，理由如下： 首先，预测的产量只考虑了土地作为生产要素，而现实中除了土地还有其他的生产要素 empirical 部分的解释能力不仅仅依赖于李嘉图理论，而且取决于 GAEZ 项目的预测能力，而这个项目潜在假设所有国家的农业生产技术是一样的 对土地的划分，虽然解析度已经很好，但是还不够 不管怎样，尽管有以上的缺憾，但是从现有的结果来看，李嘉图的比较优势理论仍然有很强的解释能力。 最后，男神 Donaldson 的文章真的太风骚了！]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[中国失业率与劳动参与率的长期趋势 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/08/14/unemployment/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20174356 失业率是描述一个国家宏观经济的最重要的指标之一，然而长久 上图是各个国家失业率的比较，其中最下方的黑色虚线为中国官方公布的失业率数字。除去中国，可以发现收入越高的国家，其失业率水平越高。这一现象虽然初看起来非常不符合直觉，因为收入高的国家通常其劳动力市场更有效率，然而这一现象仍然可以解释，即所谓的『奢侈的失业假说』（「luxury unemployment hypothesis」）。该假说认为，收入低的国家通常失业保障也相对匮乏，因而如果一个工人失业，很难维持生活，所以在这些国家，保留工资都非常低，从而出现了低收入国家失业率低的现象。 然而，虽然中国已然成为中高收入国家，但是失业率却非常之低，这不免让人怀疑中国失业率数据的准确性。 实际上，中国的失业率被严重低估已经不是什么新鲜话题。中国目前官方公布的失业率为『注册失业率』，即总的注册失业人口与总劳动力之比。然而，由于许多外来务工人员在工作地没有户口，因而不能进行登记失业，而对于那些有资格进行登记失业的人来说，失业补偿非常低以至于这部分人没有激励进行失业登记。与此同时，由于失业率数据是从底层政府逐级向上汇报，失业率数据也可能存在被操纵的问题。 上海财经大学冯帅章、约翰霍普金斯大学胡颖尧男神以及 Robert Moffitt 合作的文章：《Long run trends in unemployment and labor force participation in China》使用了国家统计局的城镇入户调查（UHS）对中国的失业率、劳动参与率进行了估计。 UHS 数据是我国唯一一个全国性的代表性家庭数据，包含了 1980s 以来的中国大陆所有省份的调查。由于抽样可能是非随机的，因而作者使用人口普查数据，根据年龄、省份、性别、教育程度对UHS数据的权重进行校准。 由于官方的退休年龄男性为 60 岁，女性为 55 随，因而作者将样本限制在男性 16-60 岁，女性 16-55 岁的区间内。此外，由于直到 2002 年 UHS 才包含了非本地户口居民，因而作者将样本限制为有本地户口的居民。此外，由于非本地户口的居民可以回到农村家乡，或者去另外的城市，因而有户口的居民在经济和政治上更为重要。 在UHS数据中，就业状态共分为 15 类，作者将其中『待业人员』和『待分配者』视为『失业』，而将『离退休人员』『丧失劳动能力者』『家务劳动者』『在校学生』『待升学者』『其他非就业者』视为『不在劳动力市场』。注意这与通常 OECD 国家的定义有略微差别，比如『在校学生』在 OECD 国家中将其定义为就业，而在这里定义为『不在劳动力市场』。 那么经过了这么多处理，中国的失业率究竟是什么样的呢？首先是从全国层面来看的时间序列数据： 作者将从 1980s 到现在大体分为三个阶段： 1995 年以前，虽然经历了改革开放，但是国有经济仍然占主要地位，经济还在计划经济的影响下，国有部门员工端着『铁饭碗』，因而失业率水平波动非常小。到 1995 年，大约有 60% 的员工仍然身处国有企业。 1995-2002 年，失业率迅速提高。在这段时间，首先是 1995 年 1 月，劳动法开始正式实施，确立了劳资双方的契约关系。其次，随着城市部门工业的扩张，开始出现了农村到城市的「移民」，即「农民工」。1995 年，中央政府开始实施暂住证政策。最后，也是最重要的，由于国有企业绩效不佳，1995年开始，中央政府开始『抓大放小』的国有企业改革，这立即引起了国有企业员工下岗潮。 2002 年以后，中国加入 WTO，开始了出口导向型的经济扩张，伴随着城市工业部门的迅速扩张，失业率略有下降。而 2002 年开始的高校扩招更是为城市部门提供了大量高教育的劳动者。 与此同时，在整个样本期间，劳动参与率一直处于下降的趋势： 而其中劳动参与率下降最快的为 1995-2002 期间段，随后劳动参与率稳定在了约 74% 的水平。 而随着 1995-2002 期间段的国企改革，国有企业员工的比例大幅下降。 此外，作者还从各个角度横向比较了失业率、劳动参与率。 首先是，那些没有学历、年轻人、妇女的失业率更高： 而年轻人的劳动参与率在 1995-2002 期间段内大幅下降，这可能是因为 25 岁以下年轻人更多的接受教育有关，当然这也可能意味着年轻人参与工作的成本更高，或者收益更低。 而从地区层面来看，尽管所有地区的失业率走势都是相似的，然而中国的中南部、东北、西南地区在 1995-2002 年期间失业率增长更快， 劳动参与率方面，值得注意的是，东北在 1995-2002 年期间经历了劳动参与率的急剧降低。 此外，原文还有更加丰富的细节，感兴趣的读者可以从 NBER 找到原文阅读，在此不再赘述。 Liu Rio：对结论保留看法。一个国家失业率超过 10% 以上竟然还能穿凿世界第一的增长率？这好像缺乏基础吧？失业率失真的问题作者提到了，那么就业率的失真问题呢？比如无良老板不给员工加金的情况，小时工，短工的情况等等？ 慧航：这并不矛盾，如果你考虑城乡二元经济结构，把那些农村的声誉劳动当做失业的话，90 年代 50% 的失业率我也信，而且这种失业率越高，经济增长潜能越高。 九零：那个…我不是想故意找茬。但是我觉得人口普查数据恐怕都不靠谱。易富贤在《大国空巢》里说过(也结合我自己的生活经验)，我国的人口普查至少是存在着修改数据的现象。就是实际上生育率绝对是低的危险了，报到市一级被改到 1.8。 另外一个就是，有很多汉人被人工修改成少数民族，这个连知乎都有过帖子的。某个少数民族建国好像也就 100 多人，几十年时间数量翻到比老鼠下崽子还多，是有领导为了突出自己对少数民族建设贡献把很多汉人改成了少数民族。 慧航：这不是人口普查数据，另外一个数据靠不靠谱，要看应用场景。 俞晚：请问创业者算参与劳动吗？ 慧航：self-employed，算。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[工业化的后果：水污染与癌症 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/08/14/water-pollution/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20163237 提到水污染，现在的讨论似乎没有空气污染激烈。根据世界银行 亿的中国人仍然主要依赖地表水，所以在没有全面进行水处理的情况下，水质对健康的影响非常大。在中国，三分之二的癌症发生于消化系统，且死亡率很高。在 2012 年的 Review of Economics and Statistics 文章中，Ebenstein 识别了水污染对消化系统癌症死亡率的影响，并且计算了可能的处理方式和成本。 医学已经证明，水质与婴儿的死亡率和某些特定疾病如腹泻之间关系很大，但是水质和癌症之间的关系尚不明确。现有的研究局限于医学专业，去寻找某种具体的化学污染的影响和机制。癌症的影响因素太多，在医学上很难确认，于是这也成为了经济学分析的一个领域。文章主要分析了消化道癌症的情况，一方面是由于这种癌症很有代表性，另外这种癌症和抽烟这样的生活习惯关系不大，所以更容易因果识别。 从这张图可以看出，水质和癌症死亡率之间有一定的相关性，水质好的地方，消化道癌症的死亡率较低。 作者长期采用中国数据研究中国问题（没错他是陈玉宇教授那篇著名的淮河文章的合作者），这一组数据也比较好。 和其他的中国数据不同，中国 80 年代以来的婴儿死亡率数据和水质数据都很健全，而且完整经历了从水质好到水质差的整个过程。 由于选择居住地和水质有关系，造成了内生性问题。但是作者认为，中国死亡率的数据并不完全公开，所以居民可能没有办法去选择，而是完全暴露于这个水质之下。另外，在数据的区间（1990-2002年），人们很少关注水质问题，由此产生的迁徙更是少之又少，内生性问题就没那么严重。 中国很大，有一些水质的问题是完全由于降雨量不同造成的，为识别提供了一个方案。 作者采取的数据是环保的年鉴和 DSP(Disease Surveillance Point)，覆盖的地区和人数都非常好，同时采取了一些卫星和地理数据。关注变量为水质的数据，由1-6逐渐变差。作者控制了一系列可能影响癌症死亡率的因素如在农业和工业就职的人数、平均受教育水平、空气质量。由于癌症死亡率和卫生条件、工业发展等影响都很大，作者还控制了地区固定效应和收入水平。在识别方面，确实很难厘清同时影响水质和癌症死亡率的非观测因素，但是作者认为这种偏误并不大。为了解决内生性问题，文章采用了两个工具变量： 降雨量会影响水质，进而影响癌症死亡率，且这种影响只通过水质发生，而不会直接影响癌症死亡率 距离河流源头的距离会影响水质（主要是流速不同的原因），支流往往受污染更加严重。而在前面假设这段时间内迁徙很少，这个变量相对外生，距离通过水质影响癌症死亡率。 iv 的回归结果比 OLS 的更大，水质每恶化一个等级会造成癌症死亡率增加27%。作者还做了一个更加粗略的水质分类，结果仍然是稳健的。 文章的结论是： 自来水普及率低的地方，消化道癌症更容易受到水质的影响。 水质每降低一级，消化道癌症死亡率增加 9.7% 对污染企业的惩罚一定程度上会“救命”。 根据前面估计的参数，可以得到水质和工业排放之间的关系（作者选择了最为保守的 OLS 估计结果），结合对工业企业调查做出的罚款和工业排放的关系，可以得到水质治理的成本。再反过来推算，如果把罚款的力度翻番，未经处理的工业排放会减少 82%，水质会提高18%，消化道癌症的死亡会减少 17000 人。根据环保年鉴中治污费用的数据估算，企业的治污费为 5 亿美元，相当于每个生命三万美元。这个结果还没有考虑到治污带来的其他作用，所以作者认为提高惩罚明显是值得的。 之前的很多有关污染的研究都是基于地区样本的，这是第一篇全国范围内的水质研究。最重要的是，量化了污染治理的费用和收益，很多人认为中国的罚款和治理是然并卵的，然而这篇文章的结论证明，惩罚是有意义的。 周强：指出一点疑问，在中国高发的癌症主要是肺癌、肝癌、血癌，消化道真不可能达到三分之二那么多，因为中国人的饮食习惯还是较西方健康的。与水质相关性强的癌症恰恰是肝癌及肾癌，这文章让人很惊讶。 Sido Chen：三分之二是直接根据文章说的。另外，我理解文章中消化系统包括了食道、胃、肝，总和的和对每种癌症的效应都分析了。文章指的主要是死亡率的问题，而不是发病率，所以概念是有点不同的。抱歉我真的不太懂医学，欢迎讨论~ 周强：死亡率消化道癌症也不是高的，胰腺癌、肺癌等存活率都低，癌症很少用死亡率这个指标，因为治愈率很低所以基本上都是等死，出了早期发现的乳腺、前列腺、胃癌几个个别的，以及有特效药物的如间质瘤、白血病等癌症存续期长可以算部分治愈外，很少用死亡率这个概念的。实话实说手机看小图太费劲就没看你引用的东西。 消化系统癌症本身除转移外是不包括肝这个脏器的，国内这方面的专家主要是季院、顾院、沈院等。 我虽不是学医的，但接触的癌症患者还算不少，环境污染容易引发肺癌及肝癌、肾癌还是有比较明显的相关性的。消化系统癌症还主要跟饮食习惯饮食卫生和幽门螺旋杆菌等因素相关，你的文章参考方向有问题。 Sido Chen：谢谢，这些细节我也是第一了解。所以你的意思是，消化系统癌症和水污染的研究相对不那么重要，和饮食关系更大？论文主要是识别因果关系，所以我觉得从方法来说是没什么问题的。饮食结构不同是通过地区固定效应控制。至于这个研究的意义，自然是作者自己讲了一个故事，读者判断啦。 文章控制了空气污染的。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[在社会科学研究中，研究者巧妙地运用过哪些「自然实验」？z]]></title>
    	<url>/prof/2015/08/13/quasi-experiment/</url>
		<content type="text"><![CDATA[[1 Zenan Wang 1.1 Peter Koudijs(2014) The boats that did not sail 1.2 Richardson and Troost(2009) Quasi-Experimental Evidence from a Federal Reserve District Border 2 李金源 (宏观经济学中的自然实验, 柏林墙倒塌) 3 鱼非鱼 (班级规模对学生学习效果的影响) 4 狼宝 (心理学, 贫穷削弱认知功能) 5 肥头喵 (Finance) 6 Owl of Minerva (1978年的中国改革开放, 中国留学生) 7 匿名用户 (越战兵役) 8 Reinhardt Jin (受教育年限对一生收入的影响) 9 其蔚先生 (白人退役老兵收入) 10 知乎用户 (中国供暖政策, 煤炭供暖带来污染对生命期望值的影响) 11 Zhi Li (双胞胎) 12 匿名用户 (灾难对人的心理影响) 13 匿名用户 (911, 女性沮丧与危险性行为) 14 吴玥 (两篇政治传播学，一篇市场营销) 15 Chinhogo 16 苏白 (书) 17 知乎用户 (89) 18 李三火 (纳粹德国解雇科学家) 19 蒲Pu (中文维基) 20 思凡 (参考《基本无害的计量经济学》) 21 匿名用户 (波黑战争前的ethnic diverse和战争后ethnic homogenous) 22 liu jun (兰德实验、抓阄实验) 原文地址：https://www.zhihu.com/question/30470194 1 Zenan Wang 看到巧妙使用「自然实验」的经济学论文时，我往往会有两种反应： 一种是，我去，这种事情还能从这个角度想！我从来没有这么想过，但是现在这样想，确实是研究这个问题的完美的自然实验。 另一种是，逗呢，还能有这么合理的事情发生，这作者运气太好了，让他给发现了。 虽然我是稍稍更佩服第一种论文，但是两种论文都非常难得，没有足够的积累，刻苦的钻研与阅读，运气再好也是写不出使用「自然实验」的论文的。 我下面就两种论文分别举一个例吧。因为看的时间比较久了，而且不是我擅长的领域，我就简单介绍一下，如有错讹，请多指正。 1.1 Peter Koudijs(2014) The boats that did not sail 作者利用18世纪的一个自然实验分析了信息对股票短期价格的影响1。从经典经济学理论我们知道，股价应该是对公司的信息的反映，但是想做实证研究非常困难，因为现在是信息爆炸的时代，研究者很难从浩瀚的信息海洋中筛选出关键信息并追踪其来源与去向。（虽然，也有一些论文例如高频交易的记录研究过这个问题，但是总的来说是非常困难的。） 作者巧妙利用了18世纪信息传播方式单一，缓慢，不稳定的特点，对这一重要问题做了研究。18世纪的时候，伦敦有股票交易所，阿姆斯特丹也有交易所。很多英国公司同时在伦敦和阿姆斯特丹挂牌交易。当时的通信主要是通过两国之间的定期的横穿海峡的邮船完成的，所以在那个时代背景下邮船就变成了公司信息的化身。邮船会带来关于英国公司的最新消息，并在交易所公开宣读。简单的分析可以发现，每次邮船到到港的时间都对应着阿姆斯特丹交易所英国公司股价的大幅变动。作者还进行了很多稳健性检验，证明信息是单向的从伦敦到阿姆斯特丹，反向并没有影响。他还证明了其他方式的信息交流对股价影响不大。 因为海况难以预测，船的出发时间虽然很有规律，但是到港日期基本可以看成是是随机的，一般情况两国之间的航程是几天，但遇上坏天气有的船好几周都到不了。这时候最有意思的事就是，在没有船到达的日子里，阿姆斯特丹的股价也不是一成不变的，而是大致遵循着伦敦股价的趋势。因为作者已经论证过，其他的通讯方式对股价几乎没有影响，他认为在无船期间的股价变动主要来自邮船带来的个人信件（私人信息）。比方说，可能英国某公司的高管写信给荷兰的朋友提前告诉他下周公司可能会有人事变动等等，然后等到下周时该公司真的有了人事变动，但公开的信息还没有被邮船带过来，只有有私人信息的人在进行交易。所以通过对无船期的股价波动情况，作者成功地测算私人信息对股价的影响。 1.2 Richardson and Troost(2009) Quasi-Experimental Evidence from a Federal Reserve District Border 美联储有12个分区，每个分区都有相当的自治权，可以采用一些独立的政策（至少在1930是，现在我不清楚）。而这个分区也很奇怪，有很多州被腰斩，分到不同的美联储辖区。这篇文章作者研究的密西西比就是一个典型的例子2。密西西比州北部被分到了圣路易斯辖区，而南部被分到了亚特兰大辖区。 在1930年美国金融危机时，亚特兰大辖区和圣路易斯辖区分别采取了不同的（几乎是两个极端的）货币政策。当时亚特兰大辖区奉行政策是，给资金紧缺的银行注资，所以采取了一系列措施给密西西比南部的银行提供贷款。而而圣路易斯辖区采取的是”real bill“政策，他们认为经济不景气的时期就应该减少货币供应，所以密西西比北部的银行几乎没有得到什么资金支持。对金融危机之后几年的经济数据分析，作者发现北部的各项指标与南部有明显差距，然而在危机前南北几乎是相似。作者通过研究这个自然实验得出结论，央行给困难的银行注资可以缓解金融危机。我说这个作者非常幸运，是因为密西西比的这个州非常理想，南北都没有特大城市，各方面指标在危机前都很相似，却偏偏被分到持有截然不同货币政策的两个辖区。另外，因为同属一个州，作者可以控制地方财政政策以及司法政策的影响，单独分理出货币政策来研究。 结语：生活中不缺少「自然实验」，只是缺少发现「自然实验」的眼睛。 Terry Zhang：密西西比州这个比方有点问题。由于整个州的经济是一体的，南部获得了资金，同时也获得了通货膨胀，而北部只获得了通货膨胀，明显吃亏了。由此判断，扩大银行注资是正确的，不太靠谱。 顺便说明：我支持在经济危机情况下给银行注资，但是要有节制，而且要有退出机制，整个社会不能因此被银行绑架了。 Zenan Wang：具体的逻辑请看文章本身，我只是想介绍使用自然实验的想法，具体论证非常不严谨。 美国整个经济体量很大，我觉得仅仅因为一个州的行为就造成通货膨胀不太现实。不过，我不做宏观，你觉得有问题可以先看一下原文。 Terry Zhang：(1) 我没觉得你的文章本身有问题，只是觉得例子举得有待商榷。(2) 给银行注资可不是一个州的行为，其他州也有。(3) 即使没有造成通货膨胀，但两个“半州”并不是独立的经济体，被注资的半个州肯定会影响另半个州，这个无法比较注资的行为的优劣。 2 李金源 (宏观经济学中的自然实验, 柏林墙倒塌) 经济学，特别是宏观经济学研究者对「自然实验」运用颇多。上面回答中提到的几篇经济学论文，有些零散。如果想要深入了解自然实验，比较好的阅读材料是Nicola Fuchs-Schündeln 和 Tarek A. Hassan所著的Natural Experiments in Macroeconomics，《宏观经济学中的自然实验》，其中总结了大量运用自然实验的论文，并对自然实验的操作与方法论有详细介绍，自己读完之后获益颇多。 目录如下(https://pan.baidu.com/share/init?shareid=498960388&amp;amp;uk=405332923, code: 7xid)： 我个人比较心水的自然实验的例子是对柏林墙倒塌的研究。柏林墙宛如设置自然实验的“天降之物”，将统一的国家和人民随机地分成了处理组和对照组。 在2007年发表在AER上的论文中，哈佛大学的Alesina教授及其合作者考察了两德统一后前东西德居民在政治态度上存在的区别，发现东德居民更加相信政府应在社会保障事业中扮演重要的角色。 在2009年的一篇论文中，普林斯顿大学的Redding教授利用两德的分裂和统一检验了城市经济学理论中的一个经典问题：禀赋和运气，哪个对于城市经济活动的繁荣更为重要？ 在2009年发表于Political Analysis的一篇政治学文章中，斯坦福大学的Hainmueller以及耶鲁大学的Kern通过自然实验方法，研究了西德电视节目是否会影响东德居民政治态度这一问题。在考察了民调数据之后，研究者发现能收看到西德节目的东德年轻人反而更加支持共产政权。 在2013年发表于《经济学季刊》的一篇论文中，芝加哥商学院的Hassan教授和斯德哥尔摩大学的Burchardi教授，试图通过柏林墙的倒塌巧妙地识别出社会联系对于经济发展所起的作用。 政见团队观察员王也曾著文引介——《柏林墙废墟上的经济学革命》，文中也概论了自然实验及其对经济学发展的影响。对自然实验有兴趣的可以一读。 顺便，强烈推荐政见CNPolitics.org。(@方可成) 谢谢 @Krau Alan 提醒，@王也 此文已发知乎专栏，请移步阅读：柏林墙废墟上的经济学革命 - 政见CNPolitics 3 鱼非鱼 (班级规模对学生学习效果的影响) 在教育政策研究领域，一直有一个问题，就是班级规模对学生学习效果的影响。 在美国，很多家长都从自身带孩子的经验出发，认为班级规模小教学质量一定会提高，所以一直有声音推进政府增加教育预算缩减公立中小学的班级规模。拿加州来说，自1996年以后，每年州政府会提供10亿美元的资金用于缩减加州中小学的平均规模。美国结果就是美国的中小学平均班级规模远小于其他工业化国家 （Educational Testing Service, The International Assessment of Educational Progress, Learning Mathematics (1992). For Japan, U.S. Department of Education, Japanese Education Today (Washington, D.C.: 1987) (data provided by Japanese Ministry of Education, Science, and Culture).） 但是如此大的投资，当然需要证据证明它确实有效，而且效果非常显著，因为毕竟同样的钱如果不花在缩减班级规模，增加师资、教室数量上，就可以用于： 增加课外活动，计算机机房，实验室，图书馆的资源； 增加教师工资，雇佣更优秀的教师。比如如果美国数学教学质量常为人诟病，如果将把班级规模缩减一半的资金用于将数学教师的薪水提高一倍，在美国可以雇到大学副教授级别的人 所以很多缩减班级规模的投资项目，也都伴随着对其效果的研究。 第一类研究直接不同班级规模的学校/地区的学生学业成绩。但这种方法的漏洞是显而易见的。一个学校、地区会选择减少班级规模，往往与家长、社区对孩子的教育的重视水平相关，所以导致学业水平上升的，未必就是班级规模的变化。事实上，在这类研究中，班级规模的不同也确实跟其他社会经济的因素相关。 第二类研究采用随机分配实验，在一个学校内将学生随机分配到普通班级和小班，并跟踪不同班级学生的表现。著名的田纳西州的STAR项目就是如此。但这种随机实验仍然无法消除偏误。 从学校的管理者来说，如果实验的结果说明缩减班级规模效果有限，就不会收到政府下一步的资金。所以在很多学校出现管理者并没有真正随机分配的情况，好的学生和老师被看似『随机』地分配到小班，也自然造成小班成绩更好。而另一些学校管理者则可能为了平等，将学习成绩较差的学生送到小班去。 从学生和教师的角度来看，被分配到小班的人可能会潜意识里觉得自己被『选中』了，所以会格外努力表现。这被称为『Hawthorne效应』，也会导致小班的效应被高估。 这些长期困扰美国教育政策学者的问题，被当时哈佛大学的教授Caroline Hoxby巧妙的用自然实验解决了。这篇文章名为The Effects of Class Size on Student Achievement: New Evidence from Population Variation，发表在经济学季刊上3。 在这篇论文中，Hoxby使用的第一个Identification Strategy是每一年出生人口的自然变化导致的班级规模的变化。因为美国对儿童入学年龄有严格限制，但每个学校的教室数量是固定的，不同年份一个地区出生儿童数量的随机变化就会变成班级规模的变化。既然人口变化是自然实验，就不应该受到上述人为因素的影响。 Hoxby的第二个Identification Strategy是因为美国很多州对于班级的规模上限和下限有硬性规定，当某一年某一学校的入学儿童数量超过一个阈值以后（比如26人，超过了25人的上限），当年班级的规模会出现突变（在这个例子中当入学人数从25人变为26人时，班级规模从25人突变为13人）。本身如果入学总人数跟学业成绩相关，其关系也应该是平滑的，但这一规则却导致班级规模的变化不与入学总人数直接相关，也就进一步完善了自然实验的可靠性。 Hoxby分别使用了长面板数据和面板数据分离班级规模的影响。最后的结果，是班级规模对学生成绩的影响在统计上不显著。应该说，有了这篇论文以后，美国再讨论缩减班级规模的资金支持项目的话，应该要考虑一下其他选项了。 4 狼宝宝 (心理学, 贫穷削弱认知功能) 说一个心理学的研究吧！ 2013年，Science上发表了一篇挺震撼的文章，题目是贫穷削弱认知功能。 关于贫穷与认知功能的关系，其实已经有大量的研究了。然而，可以说，那些研究基本上都是相关法，而这篇文章则是用实验的方法证明了贫穷与认知功能之间的因果关系。 整个研究分为5个子研究，其中实验1-4都是实验室实验，这里就不展开说了。我们来重点看看第5个研究，是一个田野准实验研究。 在前面4个实验中，研究者已经证明了贫穷的个体在面对经济困难的窘境的条件下，完成一些认知任务时会表现更差（e.g., 瑞文推理测验，认知控制测验）。那么，这样的发现是否能推广到实验室外呢？ 研究者考察了生活在印度的54个村庄里的464个农民。研究者采用了前后测的技术。可是这样一来，结果就可能受到日历效应（calendar effect）的干扰。为了克服这样的干扰变量，研究者结合当地的实际情况想出了一个很绝妙的点子——以当地种甘蔗的农民为被试进行研究。在收获甘蔗的前后可以看做是贫穷与富足的两种条件。果然，实验所得结果与在实验室中的结果是一样的。在贫穷的状态下，农民的认知表现要更差。 这项研究无疑是一篇影响深远的研究。一是其“实验室&#43;自然实验”的模式，保证了研究的内部效度与外部效度——证明了贫穷与认知功能之间的因果关系；二是研究者从非常新的角度思考了关于贫穷的问题。很多心理学研究总是考察我们有了什么，导致我们怎么样，而这项研究恰恰从反方向来看，我们没有什么，导致我们怎样。 引申一点说，这里的贫穷并非仅仅是金钱是贫穷，其实包含很多方面，比如时间的贫穷（时间不够用）等等。 具体详见稀缺（穆来纳森、沙菲尔创作的行为经济学书籍） 参考：Poverty Impedes Cognitive Function, Science, 341, 976 (2013); Anandi Mani et al. DOI: 10.1126/science.1238041 5 肥头喵 (Finance) 关于Finance这块，最近在读的几篇： 使用Finland男性服兵役中的IQ类数据追踪其后续投资行为：IQ, trading behavior, and performance, JFE, 2012, Ginblatt, Keloharju, Linnainmaa 使用Chile pension funds的投资者数据研究noise trader对其financial market影响：Price pressure from coordinated noise trading: Evidence from pension fund reallocations, 2014, Zhi Da, et al. 感觉如果有这种大规模统一规范登记，拿到数据后是很好展开natural experiment方面的研究。想到了再补充。 美国SEC： SHO Pilot Program Kecsecs, et al., 2013, The Accounting Review, Are short sellers informed? Evidence from the bond market Alexander, Peterson, 2008, The effect of price tests on trader behavior and market quality: An analysis of Reg SHO Two-year Piolt Program that widen tick sizes for stocks of smaller firms. Approved May, 2015, begin by May 2016 SEC在201５年公布的Pilot Test研究tick size对small company 关于transaction cost, price impact的影响。包含市值低于30亿美元，或每日均交易量低于1m，或每日交易量权重均价低于$2.00，整个pilot包含约1400只股票和400只其余股票构成的三个test组，具体的项目信息可以参照SEC提供的说明4，其相关数据将为研究市场微观结构提供了很好的自然实验。 韩国：Seongkyu Gilbert Park, 2015, The Power of Asking Questions: Resolving Financial Market Rumors throught Public Inquiries, working paper 在韩国证券交易所，当市场中出现对上市公司的“rumor”时，公司需要向市场监管部门报告有关传闻的相关信息，予以确顶或是否认，并向市场公开。当出现inqury-disclosure事件时，通过研究事件对股票交易环境的影响，可以用于研究informed trader v.s. uninformed trader的投资行为及比例。这一独特的监管流程为研究市场微观结构和信息不对称问题提供了较好的自然实验数据，相关数据可以通过KOSCOM获得。 台湾：Xiaohui Gao, Tse-Chun Lin, 2014, Do Individual Investros Treat Trading as a Fun and Exciting Gambling Activity? Evidence from Repeated Natual Experiments, Review of Financial Studies, forthcoming 6 Owl of Minerva (1978年的中国改革开放, 中国留学生) 我要提的这篇文章是 Borjas, George J., Kirk B. Doran, and Ying Shen. Ethnic Complementarities after the Opening of China: How Chinese Graduate Students Affected the Productivity of Their Advisors. No. w21096. National Bureau of Economic Research, 2015. 论文一作是劳动经济学领域高产的研究者 George J. Borjas. 这篇论文研究的内容是1978年的中国改革开放导致了涌入美国的中国留学生数量从0开始暴增，而这种自然的条件变化对美国大学教授的产出产生了一定的影响. 为了更进一步的控制变量，作者选用了数学系导师的产出数据，因为这个专业在数十年中的招生数量是较为一致的。中国数学专业学生的涌入，会导致其他裔学生的减少。作者只需比较数学系导师在1978年前后的论文产出就能说明问题，即中国学生的到来刺激了美国导师的产出。 另一方面，由于数学专业招生数量的恒定，而中国学生更倾向于找华裔导师，因此这种产出的增加在华裔导师上非常明显。而数据结果又证明，华裔导师的产出增加量，跟非华裔导师的产出减少量是一致的。更加验证了归因的正确性，也一定层度上验证了老外对华裔的刻板印象：数学好/学霸。 7 匿名用户 (越战兵役) @Zhi Li提到了双胞胎。这是一个极佳的自然实验，因为它可以解释Nature(先天因素)和Nurture(后天因素)的关系。 计量经济学里最好的自然实验就是越战兵役，没有之一。这有多个原因 1. 越战中美国用抽签系统征兵几百万。因为抽签的随机性，这给了社会科学家很自然的control/treatment group。 2. 越战被征兵者在参军时填了很详细的个人资料，包括身体素质，家庭背景，甚至智商。这些资料让社会科学家有很多可以随便玩的变量。 3. 因为越战老兵可以通过各种政府项目得到经济补贴，政府可以通过经济补贴的申请表track(跟踪？)这些老兵的资料，比如婚姻状况，经济状况。这些可以让社会科学家不用多花研究经费取得follow up study。 云折夜：是用参加越战的和没参加的对照吗？没参加的怎么统计资料？ 知乎用户：用抽中的（实验组）和没抽中的（对照组）比较。抽中的也有部分未入伍甚至逃兵役的，但总体来说服役概率大大高于未抽中的。 王小非：不能再赞同！中计和高计都会提 8 Reinhardt Jin (受教育年限对一生收入的影响) 研一学劳动经济学时听老师讲过一篇经典文章，经评论中 @Zenan Wang 指出是Angrist, J. D., &amp;amp; Krueger, A. B. (1991). Does Compulsory School Attendance Affect Schooling and Earnings?. Quarterly Journal of Economics, 106(4), 979-1014。研究的问题是受教育年限对一生收入的影响，使用数据（感谢 @容哲 补充）是美国1970和1980年人口统计数据的一部分变量，包括了所有州。数据：MIT Economics : Angrist Data Archive 这个问题的难点在于不同教育水平的人可能具有不同的“内在能力”（intrinsic ability）。所以直接按照受教育年限分组来回归，得出的结果颇有争议。 老师讲的这篇文章里利用的分组工具是出生月份。一般认为，人出生的月份和个人能力的关系可以忽略。但这（些）州的法律规定，学生未满17岁（貌似）前不得辍学离校。所以有辍学意愿的学生，如果出生的月份较晚，将不得不在学校多呆一年。这样一来，州法和出生月份就为劳动经济学家提供了一个自然实验：一组是高中肄业生，一组是高中毕业就进入社会工作的学生。通过一些整理和调整，就能得到近似满足如下实验条件的数据： 实验对象：能力分布相似的两组青少年。\ 分组标准：受教育年限——11年或12年。\ 对比标的：两组青少年参加工作后X年的收入水平（X是多少忘了）。 在这个研究中，研究者并未人为干涉实验对象的选择。影响其选择的主要因素被认为是州法律和出生月份。这就构成了所谓“自然实验”。而且这个“自然实验”代替了做这个研究本来需要的，有违科研伦理的人工实验。 辍学与否的选择其实背后也会有家庭状况等原因，但这些原因原作者应该是解释了，似乎也不重要。原文的结论（感谢 @容哲 补充）是受教育年限越长收入越高。使用工具变量估计的结果和OLS估计的结果差不多。这种巧妙的研究方法令我印象深刻。 9 其蔚先生 (白人退役老兵收入) 好像并没有很多为了社会科学研究而进行的社会实验，在我理解社会实验都是由于学者去寻找并赋予了意义。 好像有一系列巨型的“社会实验”对我们有很大意义。例如在《为什么有的国家穷、有的国家富》一文中，奥尔森对比了三八线南北和柏林墙左右的制度是如何使地理条件相似的国家走向不同道路。这大概就对社会科学研究很重要。 再举个有意思的例子，之前上计量课的时候，老师给我们看过一篇文章，由非常著名的Angrist于1990年发表——《Lifetime earnings and the Vietnam era draft lottery: evidence from social security administrative records》。文章想解决的问题是为何白人退役老兵收入比没当过兵的白人少15%，而这到底是否受到参军本身影响。解决该问题面临的主要难点是参军是具有自选择性的，即参军的人都具有一定特质。于是angrist寻找到了越战时期美国实行的一种draft lottery征兵制，即按照生日进行随机选择。这实际上就是一个社会实验，作者依靠这些数据得出了越战服役经历对白人而言相当于减少了两年工作经验的影响。 后附参考文献：Angrist J D. Lifetime earnings and the Vietnam era draft lottery: evidence from social security administrative records[J]. The American Economic Review, 1990: 313-336. 10 知乎用户 (中国供暖政策, 煤炭供暖带来污染对生命期望值的影响) 北大清华MIT共同成果，Pnas文章，利用中国供暖政策（淮河分界），研究煤炭供暖带来污染对生命期望值的影响。Evidence on the impact of sustained exposure to air pollution on life expectancy from China’s Huai River policy，还有一篇研究了世界杯对垒两国股市与足球比赛结果的关系。 11 Zhi Li (双胞胎) 双胞胎。因为双胞胎的生理基础类似，所以非常适合用计量经济学的固定效应模型。比如这篇NBER Working Paper From the Cradle to the Labor Market? The Effect of Birth Weight on Adult Outcomes，作者就是用双胞胎数据讨论了出生体重的短期和长期影响。 12 匿名用户 (灾难对人的心理影响) 灾难对人的心理影响：(Cohn, Mehl, &amp;amp; Pennebaker, 2004) 研究者下载并分析了1084名某个网路平台博客使用者在2001年9月11日前后2个月（即总时间跨度4个月）写的博客文字，呈现了他们随这个灾难而来的（透过文字表现的）心理反应 不同基因表现对灾难反应的不同影响：(Fletcher, 2014) 这个其实不太算社会科学，觉得设计不错还是想放上来。纵轴大致可以看成是悲伤程度，竖线对应的0表示2001.9.11当天，横轴负数表示9/11发生前，正数表示发生后。拟合线实线和虚线的区别在于负责编码多巴胺受体D4的基因DRD4 分析显示，基因DRD4多态性的主要效应体现在延长个体从悲伤恢复过来的过程，而不是体现在对于灾难的即时反应上，即：在相同时间段内，右边实线的悲伤程度变化不大，而虚线在下降。这有助于进一步理解环境和基因的交互作用 还有一个是9/11对英国市民心理健康影响的研究 (Metcalfe, Powdthavee, &amp;amp; Dolan, 2011)，比较复杂，有兴趣可以自己找来看看 参考： Cohn, M. A., Mehl, M. R., &amp;amp; Pennebaker, J. W. (2004). Linguistic markers of psychological change surrounding September 11, 2001. Psychological Science, 15(10), 687-693.Fletcher, J. M. (2014). Enhancing the Gene-Environment Interaction Framework Through a Quasi-Experimental Research Design: Evidence from Differential Responses to September 11. Biodemography and social biology, 60(1), 1-20. Metcalfe, R., Powdthavee, N., &amp;amp; Dolan, P. (2011). Destruction and Distress: Using a Quasi‐Experiment to Show the Effects of the September 11 Attacks on Mental Well‐Being in the United Kingdom*. The Economic Journal, 121(550), F81-F103. 13 匿名用户 (911, 女性沮丧与危险性行为) 我读到过做准自然实验最神的当属AER的这篇： Averett, Susan L., and Yang Wang. 2012. “Identification of the Effect of Depression on Risky Sexual Behavior: Exploiting a Natural Experiment.” American Economic Review, 102(3): 570-74. 以911为treatment，探讨女性沮丧与危险性行为之间的关系，发现沮丧的女性更喜欢危险性行为，比如口交，无套性交等。评论：美国佬的数据真是全啊，而且稀奇古怪，但是我比较怀疑survey的真实性。链接：https://www.aeaweb.org/articles.php?doi=10.1257/aer.102.3.570 胡玄韬：我想问一下受试者的基因数据是怎么采集的啊？如果是随机的1000多人不是很难么？ 匿名用户：那个样本是原来因为别的研究项目就存在的，然后突然发生了9/11。 14 吴玥 (两篇政治传播学，一篇市场营销) 我想到了两篇政治传播学，一篇市场营销（Marketing），与一篇不知道该如何归类的论文，它们都应该属于 natural experiment 。若有错误，请不吝指教。 第一篇，政治传播学：King, G., Pan, J., &amp;amp; Roberts, M. E. (2013). How censorship in China allows government criticism but silences collective expression. American Political Science Review, 107(02), 326-343. “哈佛大学政治学者 Gary King 和同事想出了一个方法：建立了一个封闭的中文社交网站，联系中国公司咨询如何使用中国网站相同的审查技术，选择100家流行的中国社交网站，创建帐号，向对方网站上递交含有敏感关键词的帖子，了解正反观点和审查策略，逆向工程中国社交网站的审查机制。”以上摘自：Solidot | 哈佛研究人员逆向工程中国社交网站的审查机制 第二篇，政治传播学：Al-saqaf, W. (2014). Breaking digital firewalls: analyzing internet censorship and circumvention in the arab world. 这一篇是博士论文。作者免费分发科学上网软件，之后去观察用户如何使用这款软件，都去访问哪些网站等等。我的疑虑是，作者是否已经明确告诉用户，这款软件会实时记录他们的上网踪迹。 第三篇，市场营销：Goldstein, N. J., Cialdini, R. B., &amp;amp; Griskevicius, V. (2008). A room with a viewpoint: Using social norms to motivate environmental conservation in hotels. Journal of consumer Research, 35(3), 472-482. 客人若将酒店里的毛巾扔在地上，就意味着需要工作人员清洗。为了检测社会规范（social norms）的作用，作者联系一家酒店，印制一些提示比如：“住在这间客房的大部分客人都选择重复使用毛巾。”然后观察客人是否也会遵从，减少清洗次数，有助于环保。 第四篇，不知道如何归类：Wang, X., Xu, S., Peng, L., Wang, Z., Wang, C., Zhang, C., &amp;amp; Wang, X. (2012). Exploring scientists’ working timetable: Do scientists often work overtime?. Journal of Informetrics, 6(4), 655-660. “大连理工大学王贤文等人通过监测Springer上科技论文的下载情况，分析了各国科学家的工作时间。主要结论： 1. 论文的下载次数基本上正比于该国发表的SCI文章数。一分耕耘，一分收获。 2. 科学家基本上没有周末。美国、德国、中国科学家的周末工作强度分别是平时的68%，64%，77%。 3. 科学家基本上不分上下班。 4. 美国人是夜猫子（会不会是作者没把IP与时区的对应算对？）；中国人与美、德的生活习惯差别非常明显，工作时间呈现三峰结构，即中午要休息，吃饭的时间比较规律（也比较认真 ^_^）。而西方人一般中午不休息，吃饭大概也没准点。” 以上摘自：科学网—科学家的工作时间 15 Chinhogo 懂英文的童鞋可移驾：http://www.stata-journal.com/sjpdf.html?articlenum=st0136，Nichols, Austin. 2007. Causal Inference with Observational Data. Stata Journal 7(4): 507-541. 附带STATA的使用哦 16 苏白 (书) 17 知乎用户 (89) 最经典的是89年的美国绿卡问题… 18 李三火 (纳粹德国解雇科学家) 用纳粹德国解雇科学家（数学物理化学领域）这个事件来看high-quality peer会不会促进增长（在科学界就是发表和引用），也就是peer effect的一个衡量。 Waldinger F. Peer effects in science: Evidence from the dismissal of scientists in Nazi Germany[J]. The Review of Economic Studies, 2011: rdr029. http://media.wix.com/ugd/0d0a02_234ad2d219734d05a9438a68dcbb9c03.pdf 利用苏联解体这个事件。 前提是苏联的数学家和美国的数学家在数学研究上侧重的领域不完全相同，苏联的数学家研究微分方程，概率论、几何等比较多，而美国数学家更专注于与生物和计算机领域相关的统计和逻辑学的研究。而在苏联解体之前，由于各种原因，两个国家的数学家合作极少，苏联的数学家也基本只在苏联的刊物上发表文章。解体后，苏联的数学家有部分前往美国并增加了学术合作。 作者认为这些苏联移民数学家对美国数学家在顶尖刊物发表的影响是两方面的： 知识溢出的正效应。 竞争带来的负效应。（在学术刊物的数量和发行容量不变的前提下，“Maybe the effect was negative for the average mathematician but more top papers could have been produced by people in Soviet fields”） 作者用这个背景研究了High-Skilled Migrants的影响。 Borjas G J, Doran K B. The collapse of the Soviet Union and the productivity of American mathematicians[R]. National Bureau of Economic Research, 2012. https://dash.harvard.edu/bitstream/handle/1/8160722/RWP12-004-Borjas.pdf 19 蒲Pu (中文维基) 有一篇蛮符合，Zhang and Zhu (2010) Group size and Incentives to Contribute: a natural experiment at Chinese Wikipedia。研究中文维基在大陆block后用户group size减小（自然形成被block和不被block两个对照）对用户contribution的影响。 20 思凡 (参考《基本无害的计量经济学》) 柏林墙、轰炸越南、非洲国家的国界、班级规模、突然出台的政策、重大自然灾害等等，可参考《基本无害的计量经济学》，里面有系统的介绍，当然主要是针对经济学 21 匿名用户 (波黑战争前的ethnic diverse和战争后ethnic homogenous) 来一篇新的吧， Swee(2015)研究了关于ethnic diversity和public goods provision的关系，用波黑战争前的ethnic diverse和战争后ethnic homogenous 作为natural experiments得出结论，挺有趣的有兴趣的人可以去看看。 22 liu jun (兰德实验、抓阄实验) 卫生经济学里最经典的是兰德实验。按下不表。今天要说的是俄勒冈的抓阄实验。 美国的Medicaid是针对低收入者的医疗保险项目。俄勒冈州决定扩大Medicaid，但是政府的财政吃紧，所以采取了这样的办法：符合条件的都可以报名；政府抓阄来决定让谁入选。 结果这就成就了著名的Oregon Health Study，http://www.nber.org/oregon/。 有无医保成了一个完全随机分配的事件，因此有医保的和没医保的成了完美的实验组和控制组。 哈佛大学公共卫生学院的Katherine Baicker和MIT的Amy Finkelstein是主持人。她们的论文发在了The New England Journal of Medicine，世界上影响因子最高的刊物上。你不要问我是多高，我只能告诉你两倍于Nature或者 Science。其他的刊物例如Science, Health Affairs, QJE, AER就不用说了。BTW，上过的Katherine Baicker课，伊气场非常了得。 原文地址：https://www.gsb.stanford.edu/sites/gsb/files/publication-pdf/peter_boats.pdf。↩ 原文地址：http://www.jstor.org/stable/pdf/10.1086/649603.pdf。↩ 原文地址：http://www.hernando.cl/educacion/Bibliografia/Insumos/Hoxby_QJE2000.pdf。↩ 下载地址：http://www.sec.gov/divisions/marketreg/tick-size-pilot-plan-final.pdf。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[比较优势理论与贸易的利益：来自日本的证据 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/08/09/comparative-advantage/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20152417 国际贸易对贸易双方都有利，这是几乎所有的经济学家都会承认 在讨论证据之前，我们不妨先给出两个定理的简单证明。首先是自由贸易比起自给自足的经济能够带来更高的福利： 其次是比较优势的 DDN 定理： 最后这个关系式描述了自给自足条件下的价格向量与自由贸易条件下净出口向量之间的关系。这个结论意味着，本国在自给自足条件下价格相对较高的产品，在有自由贸易的条件下会更倾向于进口；而相反，本国在自给自足条件下价格相对较低的产品，在有自由贸易的条件下会更倾向于出口。 比较优势理论虽然占据了贸易理论的重要位置，然而在长期以来并没有非常令人信服的证据证明比较优势所带来的益处有多大。其中的一个挑战是，我们无法观察到“自给自足”条件下的价格。通过以上的 DDN 定理我们可以看到，其价格向量为“自给自足”条件下的价格，而T却是贸易条件下的贸易量，而这两个量我们是无法同时观察到的。 而 Bernhofen和Brown 2005年的AER文章：《An Empirical Assessment of the Comparative Advantage Gains from Trade: Evidence from Japan》，使用了日本的对外开放作为自然实验，估测了贸易对日本的福利的影响。 日本的历史这里不再赘述，经历了 1853 年的“黑船事件”，日本最终于 1859 年实现了对外开放。 而有趣的是为什么日本是一个理想的自然实验呢？作者认为主要有以下几个原因： 1850 年的日本经济情况比较满足新古典模型的完全竞争市场、产品同质化、世界市场的价格接受者这三条假设。 日本的对外开放是外生的、突然的，使得其国内的价格在短时间内有剧烈的变化。 由于突然的开放，其生产技术短时间内没有受到太大冲击。 综合以上，作者认为这个时期的日本是一个绝佳的研究贸易的收益的样板。 那么如何估测贸易对日本带来的益处呢？如果学过《中级微观经济学》，你肯定对所谓的“等价变化（equivalent variation）”还有印象。如果使用支出函数，我们可以把等价变化表示为： 以上\(\Delta W\)就是所谓的“等价变化”。其中第一个支出函数代表了在自给自足的条件下达到自由贸易条件的消费需要的支出，而第二个支出函数则代表了如果1850年代日本是自由贸易的，那么其实际支出是多少。从而，这个\(\Delta W\)的意思就是：在自给自足的价格下收入需要增加多少才能获得跟自由贸易下一样的消费。（注意这里的\(T\)跟最初的T定义是反的，所以\(pT&amp;gt;0\)）。 而根据收益函数的性质，可以知道最后一项是非负的，因而有： 以上不等式右边就是作者需要进行估测的。为了估测这个数字，作者需要： 自给自足条件下的价格 反事实的贸易量 1850 年的 GDP 而这里面最关键的是反事实的贸易量。作者认为既然日本的开放是突然性的，其面临世界市场也是突然性的，者对其本身的价格向量有个冲击，但是对技术等并没有太大冲击，所以可以使用开放之后的一小段时间内的贸易向量代替自给自足时期的反事实的贸易向量。由于数据的原因，作者最终使用了 1868-1875 年的贸易向量，通过GDP增长率的反向调整作为 1850 年代的贸易向量的 counter facturals。 此外，对于 GDP 数据，由于缺乏 1851-1853 年的数据，作者使用了两种方法：由于 1840 年代的 GDP 数据是有估测可用的，因而可以用 1840 年的数据，假设不同的增长率，估测 1850 年代的；此外，1870 年代的 GDP 数据也可用，因而可以倒推回 1850 年代的 GDP 数据。 最终，作者得到了如下对于贸易的收益的估测： 经过作者的估测，日本的对外开放，使得其由于比较优势而获得的收益不高于当时日本 GDP 的 8%-9% 的水平。 不知彼岸：所以其实益处并不大么？ 自由贸易，在中长期来看，对于技术落后国，不利于本国产业资本的壮大，不利于形成完整的产业技术，落后国倾向于沦为廉价劳动力提供地，原料出口地，和他国商品倾销对象。这一负面作用会不会更大？ 大锴：楼上这到底怎么看的文章。 不知彼岸：占日本当时 GDP 的 9% 很大么？ 首先，日本当时 GDP 绝对量比现在小得多。 其次，即使是现在中国的经济水平，比 19 世纪的日本要好得多，但以技术进步的推动能力，让 GDP 增长 9%，连两年都不需要。可见经济增长的能力才是最为压倒性的因素。 如果不适当的自由贸易损害了产业的长期发展能力，完全有可能，损失的增长率大大超过那 9% 的上浮。请@慧航有时间的话就这种可能性发表一下看法吧。 大锴：你拿现在中国的经济增长来举例，可是中国过去的高速增长很大部分就是自由贸易带来的呀。 不知彼岸：1. 经济增长并不必然需要自由贸易。地球作为封闭的经济体一直在增长，这是技术进步推动的。历史上许多国家在采用贸易保护措施的同时经济高速增长，比如曾经的美国。此外，即使中国加入 WTO 以后，也有 15 年的保护期，对于发展中国家 WTO 通常都会给予一些贸易上的保护。你说中国过去的高增长中有很大部分归功于自由贸易，我认为这个观点值得商榷，至少需要进一步论证。2. 我提中国的经济增速，只是为了说明经济增长的能力相对于 9% 的 GDP 上浮是更重要得多的东西。如果要论证自由贸易人人受益，就必须要论证自由贸易对经济增长的影响，慧航给大家分享的静态的模型在这方面是无力的。3. 迄今也没有完全的自由贸易，多自由的贸易最合适刺激经济增速？一些保护是不是必要的，最优的？自由贸易的时机会不会非常重要？自由贸易对经济增速的影响是很复杂且充满争议的问题。我说这么多，就是想多听些关于这一问题的讨论。 慧航：9%很大么？不小。你要这么想，这篇文章识别的是：仅仅比较优势所带来的直接利益，而不包括技术外溢、间接造成的资本积累等等。也就是说，对于日本来说，对外开放之后单单比较优势就有 9% 的增长，这个数字在那个时候的世界经济来看是非常 amazing 的，那个时候除了英国这种经历工业革命的国家以及英国荷兰等对外急速扩张的国家，其他的国家，比如中国，增长 9% 需要多少年呢？另外，关于“贸易不利于本国资本扩大”这个观点，我们观察到的是日本明治维新之后进入了资本化的道路，而日本仍然保持着自由贸易，似乎这并不矛盾。 不知彼岸：对于开始走上工业化的国家来说，增长 9% 用不了 3 年，现今中国只要一年半。至于那个时候的中国要很久是因为还没有走上工业化的道路。可见，是否能够工业化是更关键的因素。因此，我说 9% 相对于能否走上工业化，走好工业化是＂小＂。 自由贸易的确未必会完全扼杀工业化，但他们之间大致会是什么关系呢？可能是削弱，可能是增强，也可能是看时机和程度。只看到二者共同存在并不能说明改变自由贸易政策对工业化就是好的影响，因为我们不知道如果不是这个贸易政策，而是别的，会怎样。日本明智维新走上工业化道路了，可是这期间日本的贸易政策到底是怎样的，发生过什么变化么，真得能称得上很自由么？ 技术的溢出等效应可能是存在的，也可能导致长期居于低附加值的产业链，而对技术有抑制。也许不那么自由的开放和贸易会能够更好？ 总之，我想强调的观点是贸易的自由度和经济增长之间的关系并不是那么简单的单调正相关。而这个关系到底是什么是比 9% 的比较优势重要得多的东西。 慧航：跟你说了，那个识别的仅仅是比较优势的收益，而这个 9% 的收益是在开放了贸易之后任何时间都存在的，不是一个时间序列上的增长的概念。也许你得再仔细看看原文，搞清楚他要识别的是什么东西，什么叫 counter-factural。 不知彼岸：我并没有说原文识别的东西是不存在的，也没有说原文识别错了，也没有说 9% 是一次性的，更没有说原文在讨论时间序列上的东西。也许你应该再看看我的评论，到底要说的是什么东西。 慧航：既然你自己也清楚自己的论点跟这篇文章没有什么关系，我觉着讨论可以到此为止了。 不知彼岸：100 的 GDP 上浮 9%，但却损失了 3% 的增长率，另一个也是 100 的 GDP，没有上浮，但却每年多增 3%。若你能再活 60 年，你选哪个？这就是我说的 9% ＂小＂。 慧航：你还是混淆了时间序列上的“增长”，以及比较优势带来的“收益”。 不知彼岸：我讨论的论点和原文不同且不矛盾，并不代表就和原文没有关系。我们都是在讨论自由贸易的利弊。 我混淆了？能不能请指正，谢谢！ 慧航：这个 9% 的意思是说，任何时间开放贸易比不开放贸易，单单从比较优势理论的收益是 9%，这个 9% 是每年都存在的，跟增长率没关系。 不知彼岸：对啊，我也是这么认为的。请比较：109 109 109 109 109... 100 103 106 109 112..假设前者自由贸易 后者不自由的贸易。我举例了，若还有误解请你也举例。如果你懒得做，或没兴趣那就算了吧，我们就讨论到这里。 墨子蔚：&amp;quot; 长期以来并没有非常令人信服的证据证明比较优势所带来的益处有多大&amp;quot; 对日本 1850年 前后的影响，是 9% 以下。那么，现代全球高度分工协作模式下的国际贸易如果中断，产生的 GDP 损失是多大 ? 如果所有行业国际贸易品 都有替代性，并且方便人员在不同行业间转岗，及方便行业横向扩张，那么不大，也就 9% 以内，但，对于难以替代的零部件，原料(如石油和一些芯片)，和不大量输出产品保证产能就严重折旧赔本(如钢铁、汽车行业)的行业，那“中断国际贸易” 的损失可就大了。所以，我们才一方面说：地球村谁也离不开谁；一方面按郎咸平先生提到的“产业链完整性”，不断研发替代外国在国际贸易中高利润的部件、商品，确保一旦“中断贸易”时的生产损失。这一点美国战略也很明确。 Pats Chen：这 counter factural 稍微有点不够优雅… 慧航：233，不过也算得上靠谱，结论也没有很夸张，不管怎样都要做假设的，要么只能写很复杂的结构，要么搞个自然实验，反倒让人喜欢 唐朝：长篇大论就是为了说明，自由贸易人人得益。 王振宇：233，你这么说贬低了作者的工作。人家算出了精确的贡献率呀！ Yuting Chen：慧航，日本的 open to trade 是 exogenous 的，为什么 technology 短期内不会有太大冲击呢？难道不是会有较大的 technological shock么？ 慧航：我觉着他说技术上没有太大的冲击是有道理的。黑船事件之后日本在其出口领域并没有立即学习国外的技术，那要到明治维新前后才开始全面学习西方的。类比于，鸦片战争前后几年，中国似乎也没有什么技术冲击，真正的技术冲击应该是洋务运动前后了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Hello R Markdown]]></title>
    	<url>/tech/2015/07/23/hello-r-markdown/</url>
		<content type="text"><![CDATA[[1 R Markdown 2 Including Plots 1 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit &amp;lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 2 Including Plots You can also embed plots. See Figure 2.1 for example: par(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;), col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;), init.angle = -50, border = NA ) Figure 2.1: A fancy pie chart.]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[一个数据科学家的这些年]]></title>
    	<url>/data/2015/07/20/years-as-a-data-scientist/</url>
		<content type="text"><![CDATA[[原文地址：https://cosx.org/2015/07/years-as-a-data-scientist/ 简介：李舰先生现任堡力山（PMI）集团副总，曾任 Mango Solutions 中国区数据总监。专注于数据科学在行业里的应用。擅长R语言的工程开发与分析建模，是 Rweibo、Rwordseg、tmcn 等 R 包的作者。与肖凯合著了《数据科学中的R语言》，参与翻译了《R语言核心技术手册》、《机器学习与R语言》。李舰先生也曾有多篇文章在统计之都主站上发表。个人主页：http://jianl.org/ 引言：这篇文章来自于我和肖凯的新作《数据科学中的R语言》的前言。原书受篇幅和语言风格所限，前言经过了一些删减，在这里将全文和增补的内容发布出来。为了尽量避免为新书打广告的嫌疑，先提前声明我们会把书中一些自认为比较有价值的经验和见解发布到统计之都上，只是希望对数据科学和R语言有个基本的认识或者了解一些作者的建议和感悟的读者大可不必去买这本书，只有对具体案例和技术感兴趣并且愿意实际操作才值得去购买。 僭称科学家我本来是不敢的，不过如今人们对数据的研究和应用的主战场在业界，“数据科学家”通常指的是一个职位的名称。现在很多公司（包括我自己的）招聘的职位都流行写“Data Scientist”，所以我自称数据科学家应该还好。从我本科进入中国人民大学学习统计学专业开始到现在的10多年时间里，我所有的求学经历和职业生涯都在和数据打交道，在数据应用的最前线感受到了业界对于数据价值理解的巨大变化。也亲身经历了从数据被冷遇到如今“大数据”成为显学这一激动人心的变革。这些年的很多经验都化成了这本书中的内容。在这里，我回顾自己在数据科学家道路上的一些经历，用自己的视角来总结这个数据时代的变化，也作为这本书的前言。 我少年时的志向和很多无名的儒生一样，“为天地立心，为生民立命，为往圣继绝学，为万世开太平”，结果也一样，就是越长大越失望、越难有新的目标，对什么事情都不执着，常被推着走。当然也不会否定自己，习惯顺其自然。就这样不小心走上了数据科学家的道路，在这条路上我经历了很多次对知识和技术的被动接受与主动融合。 我们那时高考是先估分再报志愿，很适合我，我对自己的估分很有把握，所以敢于填报一直心仪的人大，最后果然实际得分和估分只差两分，可是离最低录取线也只差两分。于是我被人大录取了，但是专业是我填报的第五志愿，也就是当时还算冷门的统计学，前面四个专业我就不提了，怕被骂黑子。 入学时是2001年，刚进入新的世纪。我对统计最初的看法和很多人一样，以为就是算GDP和物价指数。后来我在很多报告中都会讲一个段子，“搞计算机的人最烦的就是被叫去修电脑，搞统计最烦的就是一桌人吃完饭后被要求算一下账单是否对”，即使到现在场下都还有不少人笑，说明人们对统计的误解还没完全消失。遥想我自己当年，大一时去系里主办的“资料采矿研讨会”当志愿者，听到谢邦昌老师介绍资料采矿时我还在纳闷“难道台湾也搞采掘业？”后来看到数学公式才明白不是我想的那样。陈希孺老师出来时我和同学都以为可以听懂，因为报告题目是基尼系数，这个词在经济学教材中我们都懂，结果幻灯片出来后没一个人懂，陈老师开口后我比同学多懂了一点点，因为我能听懂一点点的长沙话。从此以后我知道了统计不是以前想的那样。 大二的时候，吴喜之老师给我们上了统计计算的课，这是我求学阶段最庆幸的事。当时赶上非典，这门课被拆成了两部分。一半的人逃回家了所以需要暑期回来补课，其中就包括我。当时我带了一大包北美精算师考试的习题逃回武汉打算用来打发时间，结果有一天突然在官网看到中国的考点都被取消了。于是在家期间我以研究吴老师教的R语言为乐，回来后考试得了100分，这是我学生生涯中期末考试的唯一满分，当时高兴了好久，完全不会想到现在R成为了我工作中的主要工具。我们系师资很强大，大三时就会安排自己的导师，我很庆幸吴老师收下了我，我跟着他从学年论文开始写起。算起来我学R的起点还是很高的，当年把吴老师那本《非参数统计》中的例子全部用R代码实现了一遍，开始尝到了编程的甜头，因为之前觉得像天书般的非参数统计在我写完程序后就觉得什么都懂了，而且对吴老师的引言里的内容有了更深刻的理解，直到今天，我这本书的引言里都还借鉴了好多吴老师的思想。我从当年的程序中选了一个符号检验放在本书附带的R包中，在“假设检验”一节中还用到了的，这个函数的源代码完全可以当作编程规范的反面例子，但是我丝毫不害怕丢脸，因为即使当年这么弱的水平，也可以写R程序，而且有用，直到今天还能用，这就是R最大的价值，当然也是数据科学的价值所在。从那时起我就发现了一条学统计的捷径，遇到任何不懂的地方，拿到数据后写程序算个结果再来看书。 大四的时候有门专业课叫做“数据挖掘”，这在当时是个极热门的概念，在世纪之交的时候，数据挖掘被各路专家钦点为新世纪最重要的技术。当时谷歌刚刚上市，国内的数据电子化和企业信息化也差不多成熟了，人们的问题由数据不够变成了数据过多，需要借助搜索引擎和数据挖掘才有可能获取有价值的信息。当时比较流行的说法是“知识爆炸”，各路英雄都瞄准了这个激动人心的方向。当时甚至还有数据挖掘和统计学谁主谁次的争论，也有很多认为两者就是一回事的观点。而当时的我对数据挖掘的理解只是关联规则、聚类、分类、神经网络等具体算法的实现。按照我当时的认知水平，我感觉数据挖掘只是对统计方法的补充以及对大数据量的实现，在思维方式上并没有什么不同。当然，那时我对数据挖掘的理解完全是根据其自行描述的理想状态来判断的。 由于统计专业当时仍然不热门，我考研时报了热门的北大光华，当年有个数理金融的方向，我很看好它，可惜考试结果不看好我，不过成绩不算太差，正赶上那时候软件学院刚开张，所有科目和总分都过线的话就可以调剂一个金融信息化方向的双证，于是我又被推着朝数据科学家的方向近了一步。我很喜欢自己的一个优点就是从不抱怨，为了让之前的努力完全没有白费，我就迅速地找到了自己的专业和软件的结合点，在软件学院苦练数据结构和编程技术，也多学了几种编程语言，曾经也怀疑过这样苦练的意义，因为不可能超过同资质下计算机出身的人，后来在我的研究生导师杭诚方老师教的课程中找到了自信，那是一个OLAP的练习，我感觉那些立方体实际上就是R中的多维数组，于是自己用R写了个OLAP的工具，从分析结果来看，不比其他同学用商业软件做的差，从此我开始从自己以前的专业中找到了存在感，也可以以一个更轻松的心态来随意学习自己想要的，而不是跟在编程高手后面追赶。 研究生入学的时候是2005年，数据挖掘正如日中天，但业界更喜欢热炒的一个词是“商业智能”，简称BI。BI主要是厂商提的概念，按道理应该包含数据仓库和数据挖掘。当时大的企业都有ERP（企业资源规划）系统，小的企业至少也有MIS（管理信息系统），这些系统都能采集数据，再加上其他各类应用系统，使得数据的内容过于丰富，快速而直观地发现数据中的规律是企业非常现实的需求。数据仓库的思路是将所有系统中的数据存入一个数据库，但这个数据库的设计范式与业务系统是不同的，因为其目的是数据分析而不是操作，所以数据的增删改并不重要，而数据的查询非常重要。这样的数据库就是数据仓库，可以从数据层面实现企业内所有数据的整合，同时能够快速地访问所需要的数据。所有的数据仓库都包含OLAP（联机分析处理）系统，基于数据仓库对数据的各个维度进行展现。维度就是统计中分类变量的概念，在行业中也常被写成“纬度”，这和把“阈值”写成“阀值”的是一个门派的。一般来说，BI项目的核心就是建数据仓库，而建仓库时最大的工作量是ETL（数据清洗、抽取、转换、加载等），基本上仓库建好后靠OLAP就能解决一般企业绝大多数的分析需求，因此很多时候BI都不包含数据挖掘。对厂商来说，以ETL为主导的BI项目可以用比较便宜的人工，同时也容易复制，因此慢慢地在行业内BI这个词就变味了，变成了OLAP和报表可视化的代称。我那个时候还没有深入业界，虽然有这样的怀疑，但是不敢真往这方面想。 我的实习和第一份工作是在西门子医疗，一个财务相关的部门。同事大多是会计背景，但是他们用Excel的能力让我惊叹。我的工作是分析财务数据，但是实际的内容主要是操作SAP然后用VBA写自动化报告的程序，工作的过程中我也感受到了Excel和VBA的强大，最重要的领悟就是任何语言都有可能解决任何的问题。网络上喷来喷去的只是弱点，可能影响到效率，但实际工作中，人们最关注的是能与不能，而不是好与不好。用Excel的过程中我解决了同事提出的所有问题，有些和交互协作相关的问题就用JSP来写，不过当时公司的服务器上没有Tomcat，于是自学ASP也都解决了。毕业后我留在了西门子，并随公司搬到了上海。 我很庆幸我在西门子的工作经历，可能当时入职时最吸引我的只是五百强的虚荣。但在这样的大企业形成的工作习惯是可以受用一辈子的。虽然效率不是很高，但是任何的工作细节决定了所有的努力不会白费也不会起相反作用，这里不需要个人英雄主义，只需要所有人的合力。在自己的位置上完成本职工作就是成功。工作的节奏对我这样的急性子来说太慢了，但是慢下来之后和大家的节奏契合之后常常能出一些我之前想象不到的成果，这都是我自行摸索学习不到的东西。 在感到已经没有可学的东西之后，两年过去了，当时已是2009年。继续呆在这里只需要深入学习会计和熬资历，一步步升职加薪就能变成真正的外企人，一直成为有用的螺丝钉，我之前的专业和兴趣就要白费了。于是我选择了另一个极端。源略数据是一个当时的创业公司，其理念是融合IT和统计，打动了我，一看Logo就喜欢，八卦变来的。即使现在我也佩服老板们的远见，当时要搞的就是今天的数据科学。各种类型的项目都做，不限行业不限内容，从满意度调查到BI，从运筹学到文本挖掘，都是我们的解决方案。在源略的两年是最开心的时间，一群人可以在办公室里搞烧烤，装个卫星锅看世界杯，还一起自驾千里去搞户外，有过这样的经历后现在对创业就没那么向往了。公司在这段时间靠项目过得很不错，但是最终没能迎来大家期望的对数据需求的爆发。可能在很多人看来只是个没成功的理想主义公司，但是这段经历对我来说非常重要。我作为一个资历尚浅的人可以担当很多重要的角色，很多之前的想法在实际项目中一一得到了印证，纯粹地做任何喜欢的事情，以前不确定的地方靠本事也能找到自信。当我想离开的时候，有一种出山的感觉。 在源略数据的两年时光里，数据开始慢慢变得热门。R语言也开始走入人们的视线，中国R语言会议也办起来了。记得08年的时候，一群青涩的少年与青年在人大举办了第一次中国R语言会议。当时仗着谢益辉从大多特回来的余勇，一伙当时算是R的狂热分子的人就聚到了北京。会议当天谢益辉一直在紧张调试设备，导致很多他的粉丝找他不着，还有粉丝拍着他的肩膀问谢益辉在哪里。当时的大家完全没有什么演讲水平，很多内容还留在网上今天见了也会汗颜，不过那次会议点燃了R的火种，像魏太云、邱怡轩这些当年的年轻人现在就成了绝对的大佬。在那个时代，行业里数据的应用仍然是以BI为主，但是很多新的应用已经开始兴起。除了具体的技术和工程实践，我开始意识到对数据的理解其实是最重要的能力。纯粹的技术能解决的问题很少，很多时候问题错综复杂，涉及到多个系统之间的复杂关系甚至人与人之间的复杂关系，数据散布其中形成一个又一个难解的结，再前沿的技术也难以成为一把斩断乱麻的刀，只能靠人来抽丝剥茧，然后在不同的阶段和环节选择最有效的或者自己最擅长的工具一个个地解决问题。如果之前没有一个清晰的总体的理解，那么很容易就陷入到局部的死胡同去硬撼各种难题，反之，如果找到了一条正确的道路，就可以用最经济的方式来解决问题。直到今天我都认为这些是数据科学家最重要的技能，实际上也是最容易被忽视的。 我的下一站是Mango Solutions。2011年离开源略后我对自己不再怀疑，开始坚定地向数据应用的巅峰挑战。选择无非只有两条路，读个博士搞学术或者在业界找个更专业的地方搞技术。无论哪种选择，最现实的出路就是找个狭小的领域寻章摘句或者找个狭小的圈子千锤万凿。我选择了后者，因为我信仰数据的价值，但并不执着于方法。在数据应用的领域，学术界和业界的差别不大，总之数据为王，能更多见到数据的地方就是好地方。Mango是个专业用R的公司，与我的专长非常匹配，更重要的是它可以深入业界去解决一些和数据相关的具体问题，无论大小、无论难易，客户高兴就是最好的度量，这样简单的评价方式是我喜欢的。 在Mango一呆就是四年多，已经超过了人大，是我在一个组织内呆的时间最长的了。这四年里，我接触到了欧美很多顶尖的公司和顶尖的人，从他们的项目中学到了很多东西，也帮助他们解决了不少问题，看着自己曾贡献的努力出现到了人们的日常生活中是一种很好的体验，感觉自己的价值得到了实现。这四年的时间也使我从一个青年人变成了中年人，在专业的道路上越走越远，也牺牲了很多原来的兴趣。在这个阶段，我感受到了自己之前所有的技能融会贯通了，统计、编程和沟通能力自不用说，这是基本的技能，即使是会计、市场、销售的能力也感觉很有用武之地。更重要地，我体会到了行业的差异、东西方的差异、文化的差异并没有想象的那么大。能帮到别人，就会是受欢迎的人，能解决难题，就会是令人佩服的人。 这段时间随着互联网行业的成功，“云计算”迅速成了热点。我非常欣赏这种模式，因为“云”是可以对抗传统厂商的绑架的。通过廉价开源的个体聚集成庞大的系统，这就是互联网的精神。但是发起这个概念的人更多的是计算机专家而不是数据的专家，并不是所有分析算法都可以轻松部署到云上的，因此业界的云计算大部分沦为云存储平台。正如之前的数据挖掘变成了关联规则和分类算法、商业智能变成了OLAP一样，都是很好的概念被厂商狭义化了。 很快，“大数据”的概念崛起了，迅速占据了最热门的位置，其热度是之前任何时代的热炒概念所不能比拟的。对于大数据，虽然仍然存在很多跟风炒作的，但是不得不承认它确实开创了一个全新的时代。大数据的概念完全是应运而生，因为数据的来源有了翻天覆地的变化，数据的规模完全足够，计算的能力也得到了长足的发展，新的机器学习方法也不断涌现，终于赢来了数据应用的黄金时代。社会上也开始广泛地关注数据的价值和大数据的应用，随后也产生了“数据科学”这一理性的概念。这是所有数据从业者的好时代。 这四年里，数据的价值在国内得到了认可，R语言也越来越火。工作之余，我和统计之都一群志同道合的伙伴们也时常探讨数据的价值，也闲聊各类八卦，还组织了规模越来越大的中国R语言会议，我们逐渐发现，数据已经融入到了自己的生活和价值观中。理解问题、相信数据、慎用方法、尊重需求，这就是数据科学家的思维方式。数据科学家不是拯救蒙昧的传道者，不是秀智商优越感的“理科生”，不是曲高和寡的“专业人士”，而是真正能用数据来解决问题的实干派。这在本质上与R语言是一致的，也是如今大数据时代下这两者越来越火的原因。记得2012年北京的R语言会议结束之后，郁彬老师给我们作了一次印象深刻的报告，郁老师强调的统计应该跟上现代的节奏、要主动去和计算机结合、要深入到应用领域的观点让我感觉自己做的事情很有意义。 现在我又离开了Mango，加入了PMI，开始迎接新的挑战。这些年看着身边的朋友一个一个地投身到互联网行业，在这个最激动人心的行业弄潮。我比较怕和这么多高手竞争，就仍然坚持在传统行业。我喜欢李商隐和李贺，讨厌李白，我喜欢周邦彦和史达祖，不喜欢苏轼，我喜欢用大神力于空际转身或者带着镣铐跳舞，所以虽然传统行业有太多阻碍和脑残的东西，我还是坚持用数据科学在这里做出点东西来，毕竟这里的资源实在太丰富，数据也太好。虽然传统行业里节奏太慢做事风格也太Low，可能不适合纯粹的技术大牛，但我觉得这是数据科学家的最好去处，数据科学家如果没有深入行业的能力以及无论任何条件都能全天候地从数据中挖出价值的本事，那还不如直接去搞机器学习挑战巅峰算了。 最初有写这本书的想法是在2012年上海R语言大会时，西安交大出版社的李颖找到我和肖凯开始谋划一本基于R语言与数据实战的原创书。肖老师是圈内我最崇拜的博主，李颖是统计科班出身的专业编辑，我们很快一拍即合。当时肖凯提议起名数据科学时我还从来没听说过这个词，没想到短短两年多的时间后，这个词会变得如此火热。当然，从另一面来看，我们这本书居然写了两年多还没写完。当时我还担心数据科学的书名让人摸不着头脑，不过在读了肖凯写的博客和推荐的链接之后，觉得这个词可以非常精确地描述我们的工作。我们从数据出发，介绍各种方法的原理、在R中的实现以及在具体领域中的应用。书中的内容全部来自于我们平时工作中的经验和对R语言的感悟，与传统的统计学、R语言编程或行业实战类书籍都有所不同，命名数据科学是再合适不过了。 感谢中国人民大学的吴喜之老师，从我当年开始学习R语言到现在从事专业的数据分析工作，都离不开吴老师悉心的指点，对于本书吴老师也提了很多宝贵的意见，帮助我们改正了不少错误。感谢统计之都的伙伴们，很庆幸有这样一群志趣相投的朋友，大家利用业余时间一同为统计学的普及和应用而努力，平时各类专业问题的讨论和各种各样的八卦是这本书的重要动力和源泉。感谢浙江大学软件学院金融数据分析技术专业2013级和2014级的全体同学，我在讲授“金融数据分析基础”和“R（语言）及其应用”课程的时候用到了本书中大部分的例子，同学们的参与和反馈为本书的不断完善提供了很大的帮助。 以上部分结束了书的前言，但没有结束统计之都的文章。后续我们还会把书中一些比较有趣的见解发布到统计之都，敬请期待。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[岭回归和最小二乘法的区别是什么？z]]></title>
    	<url>/prof/2015/07/05/ridge-regression/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/28221429 说明：\(X&amp;#39;X\)在相关程度高的时候，\( 0，这导致\(|X&amp;#39;X|^{-1}\)很大，从而不一样的样本数量对应的结果之间差别会非常大，模型的拟合结果非常不稳定，参数估计量的方差也增大，对参数的估计会不准确。 子元 关于稳定性再补充一下。当回归变量\(X\)不是列满秩的时候，我们固然需要通过正则化来获得唯一解： \[\min_{\pmb{\beta}} \|y-X\pmb{\beta}\|^2\Rightarrow\min_{\pmb{\beta}} \|y-X\pmb{\beta}\|^2&#43;\lambda\|\pmb{\beta}\|^2\] 但即使\(X\)列满秩，我们来看看当有其中两列相关程度很高时，会发生什么。 比方说一个自变量是身高\(x_1\)，一个自变量是体重\(x_2\)，假设因变量y是某种性激素的水平（或者别的什么跟身体发育相关的东西，随便举的例子）。虽然我们拟合后能得到唯一解\(\hat{y}=ax_1&#43;bx_2&#43;c\)，但由于\(x_1\)和\(x_2\)高度相关，所以\(a\)和\(b\)之间存在互相抵消的效应：你可以把\(a\)弄成一个很大的正数，同时把\(b\)弄成一个绝对值很大的负数，最终\(\hat{y}\)可能不会改变多少。这会导致用不同人群拟合出来的\(a\)和\(b\)差别可能会很大，模型的可解释性就大大降低了。怎么办？最简单就是给一个限制，令\(a^2&#43;b^2\leqslant t\)，这正好就是岭回归。 The Elements of Statistical Learning 第 63 页有提到这一点： When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin. By imposing a size constraint on the coefficients, as in (3.42), this problem is alleviated. 我觉得这个才是岭回归的 motivation，正态先验充其量只是一个概率解释。 司马木：确实如此，相比之下，LASSO 就差些了。 子元：没有什么差不差。LASSO 进一步追求稀疏性，不像岭回归那样按某个固定因子来收缩。 fresh：谢谢楼主的举例。加上了正则化 L2 范数，其实就是认为参数服从先验分布（高斯分布），然后就把系数给限制住了，这样就不会出现某个系数可大可小即不稳定。对吧？但是我对二乘法不了解，尤其是不是列满秩与否所带来的问题更不了解？请指教如何有针对的解决啊？ 亲爱的龙哥 最小二乘法是对普通线性回归参数估计的一种方法，目标是是 loss function 达到最小，而此时的 loss function 是误差平方和。 岭回归和普通线性回归的区别，我们可以从三种方式来看。 最优化问题的不同 \[\hat{\pmb{\beta}}_{\text{ridge}}=\mathop{\arg\,\min}_{\pmb{\beta}}\left\{\sum^N_{i=1}\left(y_i-\beta_0-\sum^p_{j=1}x_{ij}\beta_j\right)^2&#43;\lambda\sum^p_{j=1}\beta^2_j\right\}\] 对于岭回归，我们的最优化问题多了后面这些\(\beta\)的平方和。 多元线性回归的 OLS 回归不仅仅可以看成是对 loss function 的最小化，得出的结果也是\(Y\)在\(X\)的线性空间上的投影。 从多变量回归的变量选择来说，普通的多元线性回归要做的是变量的剔除和筛选，而岭回归是一种 shrinkage 的方法，就是收缩。这是什么意思呢， 比如做普通线性回归时候，如果某个变量 t 检验不显著，我们通常会将它剔除再做回归，如此往复（stepwise)，最终筛选留下得到一个我们满意回归方程，但是在做岭回归的时候，我们并没有做变量的剔除，而是将这个变量的系数\(\beta\)向\(0\)“收缩”，使得这个变量在回归方程中的影响变的很小。与普通的多元线性回归相比，岭回归的变化更加 smooth，或者说 continuous。从这点上来说活，岭回归只是 shrinkage methods 中的一种，大家常说的 LASSO 回归（貌似叫套索回归）其实也属于这种方法。 从计算的角度，有人提到了多元线性回归的 OLS 估计是 \[\hat{\pmb{\beta}}=(X&amp;#39;X)X&amp;#39;\pmb{y}\] 当存在很强的多重共线性时\(X&amp;#39;X\)是不可逆（或者接近不可逆）的，但是岭回归系数估计是 \[\hat{\pmb{\beta}}_{\text{ridge}}=(X&amp;#39;X&#43;\lambda I)^{-1}X&amp;#39;\pmb{y}\] 此时虽然对系数的估计是有偏的，但是提高了稳定性。 bh lin 先从优化的角度讲讲这个问题。 最小二乘回归求解的最小化问题是：\(\min||y-Ax||^2\)，这个问题解存在且唯一的条件是\(A\)列满秩： \[\mathrm{rank}(A)=\mathrm{dim}(x)\] 当此条件不满足时，你需要添加一些额外的假设来达到唯一的解。比如岭回归在 cost function 中加\(L^2\)的测度项。 而\(A\)不满足列满秩这个条件在回归上的可以简单理解为你所有的样本没有办法提供给你足够的有效的信息1。这时候，你就需要一些额外的假设。从 Bayesian 的角度，比如你假设\(x\)应该是服从多元正态分布\(\mathrm{N}(0,\Sigma_x)\),那么根据 Bayes theorem，你可以推知岭回归的结果就是MAP（maximum a priori）的估计。 少吃多有味 简单说，岭回归是带二范数惩罚的最小二乘回归。ols 方法中， \[b=(X&amp;#39;X)^{-1}X&amp;#39;Y\] \(X&amp;#39;X\)不能为 0。当变量之间的相关性较强时，\(X&amp;#39;X\)很小，甚至趋于 0。 岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于 OLS。本质是在自变量信息矩阵的主对角线元素上人为地加入一个非负因子。即： \[b(\lambda)=(X&amp;#39;X&#43;\lambda I)^{-1}X&amp;#39;Y\] 当\(\lambda=0\)时，\(b(\lambda)=b\)。\(b(\lambda)\)中各元素\(b_i(\lambda)\)的绝对值均趋于不断变小（由于自变量间的相关，个别\(b_i(\lambda)\)可能有小范围的向上波动或改变正、负号)，\(\lambda\)增大时，它们对\(b_i\)的偏差也将愈来愈大；如果\(\lambda\to\infty\)，则\(b(\lambda)\to 0\)。\(b(\lambda)\)随\(\lambda\)的改变而变化的轨迹，就称为岭迹。 应用场景就是处理高度相关的数据。画出岭迹图，选取稳定的那一段的 lambda 就好了。 Kernel ridge Regression 白羽：计量经济学的文章很少见到岭回归。我觉得，这里就出现了统计学和计量经济学的一个很大的区别了。岭回归我理解它的本质在于牺牲无偏性和一致性来换取有效性，最近听的一个讲座中统计研究者也会通过它来估计高维模型。但是在计量经济学里面，一致性始终是第一位的，不一致的估计方法造成的偏差在实证中造成的偏差很可能会带来严重后果。因此，计量经济学对共线性问题更多的则是“无为而治”。 路路 昨天做模型刚看到这个问题，来自推酷的一篇文章写得挺明白： 当\(X\)不是列满秩，或者某些列之间的线性相关性比较大时，\(X&amp;#39;X\)的行列式接近于\(0\)，即\(X&amp;#39;X\)接近于非奇异，计算\((X&amp;#39;X)^{-1}\) 时误差会很大。此时传统的最小二乘法缺乏稳定性与可靠性。 岭回归是对最小二乘回归的一种补充，它损失了无偏性，来换取高的数值稳定性，从而得到较高的计算精度。当\(X&amp;#39;X\)的行列式接近于\(0\)时，我们将其主对角元素都加上一个数\(k\)，可以使矩阵为非奇异的风险大降低。于是：随着\(k\)的增大，\(B(k)\)中各元素\(b_i(k)\)的绝对值均趋于不断变小，它们相对于正确值\(b_i\)的偏差也越来越大。\(k\)趋于无穷大时，\(B(k)\)趋于\(0\)。\(b(k)\)随\(k\)的改变而变化的轨迹，就称为岭迹。实际计算中可选非常多的\(k\)值，做出一个岭迹图，看看这个图在取哪个值的时候变稳定了，那就确定k值了。X不满足列满秩，换句话就是说样本向量之间具有高度的相关性（如果每一列是一个向量的话）。 技巧：遇到列向量相关的情形，岭回归是一种处理方法，也可以用主成分分析 PCA 来进行降维。 guosc 岭回归就是在最小二乘法的后面加上正则项，正则项是对待求系数的惩罚以避免过拟合的发生。可以从好几个方面来解释正则项的作用。 当训练数据较少时，\(X^TX\)不是满秩的，所以可能有很多可行解，没法找到最优的解。此时加上正则项能使矩阵满秩，也就有最优解了； 直观上来说训练数据较少时容易发生过拟合，过拟合曲线会尽可能拟合所有数据点，包括噪音点，此时由于函数的导数较大，因此系数较大，为了避免过拟合需要减小系数，正则化就是通过对系数进行惩罚以减小系数。这样得到的会是一条光滑的曲线，会有较好的泛化性能； 具体到岭回归，其正则项是二范数，从贝叶斯的角度看，相当于对系数 omega 添加了一个先验信息，所以会有更好的表现。所以，我认为训练数据较少时（具体多少我也不知道）岭回归会更好一点。 这句感觉不是太恰当，因为也可以理解为信息不足，但显示不是信息不足，而是信息的冗余问题较严重。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[12 步轻松搞定 Python 装饰器 z]]></title>
    	<url>/tech/2015/07/04/decorator/</url>
		<content type="text"><![CDATA[[原文地址：http://www.jianshu.com/p/d68c6da1587a、https://dzone.com/articles/ 作为一名教 Python 的老师，我发现学生们基本上一开始很难理解 Python 的装饰器，也许因为装饰器确实很难懂。理解装饰器需要你了解一些函数式编程的概念，还需要理解在 Python 中定义和调用函数相关语法的一些特点。我没法让装饰器变得简单，但是通过一步步的剖析，也许能够让你在理解装饰器的时候更自信一点。因为装饰器很复杂，这篇文章将会很长。 提醒：这篇文章中用的是 2.X 的语法。 函数 在 Python 中，函数通过def关键字、函数名和可选的参数列表定义。通过return关键字返回值。举例来说明如何定义和调用一个简单的函数： &amp;gt;&amp;gt;&amp;gt; def foo(): ... return 1 &amp;gt;&amp;gt;&amp;gt; foo() 1 函数定义主体部分和所有的多行 Python 代码类似，需要有缩进，定义好函数后，在方法名的后面加上双括号()就能够调用函数。 作用域 在 Python 中，函数会创建一个新的作用域。Python 开发者可能会说函数有自己的命名空间，差不多一个意思。这意味着在函数内部碰到一个变量的时候函数会优先在自己的命名空间里面去寻找。我们写一个简单的函数看一下本地作用域和 全局作用域有什么不同： &amp;gt;&amp;gt;&amp;gt; a_string = &amp;quot;This is a global variable&amp;quot; &amp;gt;&amp;gt;&amp;gt; def foo(): ... print locals() &amp;gt;&amp;gt;&amp;gt; print globals() {..., &#39;a_string&#39;: &#39;This is a global variable&#39;} &amp;gt;&amp;gt;&amp;gt; foo() # 2 {} 内置的函数globals()返回一个包含所有 Python 解释器知道的变量名称的dict（方便起见，这里省略了 Python 自行创建的一些变量）。在#2我调用了函数foo()把函数内部本地作用域里面的内容打印出来。我们能够看到，函数foo()有自己独立的命名空间，虽然暂时命名空间里面什么都还没有。 变量解析规则 当然这并不是说我们在函数里面就不能访问外面的全局变量。在 Python 的作用域规则里面，创建变量肯定是在当前作用域中进行，但是访问或者修改变量时会先在当前作用域查找指定的变量，没有找到匹配变量时才依次向上在闭合的作用域里面进行查找。所以如果我们修改函数foo()的实现让它打印全局的作用域里的变量也是可以的： &amp;gt;&amp;gt;&amp;gt; a_string = &amp;quot;This is a global variable&amp;quot; &amp;gt;&amp;gt;&amp;gt; def foo(): ... print a_string # 1 &amp;gt;&amp;gt;&amp;gt; foo() This is a global variable 在#1处，Python 解释器会尝试查找变量a_string，当然在函数的本地作用域里面（即foo()里面）是找不到的，所以接着会去上层的作用域里面去查找。但是另一方面，假如我们在函数内部给全局变量赋值，结果却和我们想的不一样： &amp;gt;&amp;gt;&amp;gt; a_string = &amp;quot;This is a global variable&amp;quot; &amp;gt;&amp;gt;&amp;gt; def foo(): ... a_string = &amp;quot;test&amp;quot; # 1 ... print locals() &amp;gt;&amp;gt;&amp;gt; foo() {&#39;a_string&#39;: &#39;test&#39;} &amp;gt;&amp;gt;&amp;gt; a_string # 2 &#39;This is a global variable&#39; 可以看到： 全局变量能够被访问到； 如果是可变数据类型（如list, dict这些，甚至能够被更改），可能准确来讲并没有改变list或dict，而是改变了指向的具体list或dict内部的某个元素； 不可以赋值，这里的赋值相当于在foo()内部创建一个名称为a_string的变量，因此只能在foo()的作用域内进行。 在函数内部的#1处，我们实际上新创建了一个局部变量，隐藏全局作用域中的同名变量。可以通过打印出局部命名空间中的内容得出这个结论，显然在#2处打印出来的变量a_string的值并没有改变。 变量生存周期 值得注意的一个点是，变量不仅是生存在一个个的命名空间内，他们都有自己的生存周期，请看下面这个例子： &amp;gt;&amp;gt;&amp;gt; def foo(): ... x = 1 &amp;gt;&amp;gt;&amp;gt; foo() &amp;gt;&amp;gt;&amp;gt; print x # 1 Traceback (most recent call last): ... NameError: name &#39;x&#39; is not defined #1处发生的错误不仅仅是因为作用域规则导致的（尽管这是抛出了NameError的错误的原因）它还和 Python 以及其它很多编程语言中函数调用实现的机制有关。在这个地方这个执行时间点并没有什么有效的语法让我们能够获取变量x的值，因为它这个时候压根不存在！函数foo()的命名空间随着函数调用开始而开始，结束而销毁。 函数参数 Python 允许我们向函数传递参数，参数会变成本地变量存在于函数内部。 &amp;gt;&amp;gt;&amp;gt; def foo(x): ... print locals() &amp;gt;&amp;gt;&amp;gt; foo(1) {&#39;x&#39;: 1} 在 Python 里有很多的方式来定义和传递参数，完整版可以查看 Python 官方文档。我们这里简略的说明一下：函数的参数可以是必须的位置参数或者是可选的命名[默认]参数。 &amp;gt;&amp;gt;&amp;gt; def foo(x, y=0): # 1 ... return x - y &amp;gt;&amp;gt;&amp;gt; foo(3, 1) # 2 2 &amp;gt;&amp;gt;&amp;gt; foo(3) # 3 3 &amp;gt;&amp;gt;&amp;gt; foo() # 4 Traceback (most recent call last): ... TypeError: foo() takes at least 1 argument (0 given) &amp;gt;&amp;gt;&amp;gt; foo(y=1, x=3) # 5 2 在#1处我们定义了函数foo(),它有一个位置参数x和一个命名参数y。 在#2处我们能够通过常规的方式来调用函数，尽管有一个命名参数，但参数依然可以通过位置传递给函数； 在调用函数的时候，对于命名参数y我们也可以完全不管就像#3处所示的一样。如果命名参数没有接收到任何值的话，Python会自动使用声明的默认值也就是0； 不能省略第一个位置参数x，否则的话就会出现类似#4处所示发生错误； 目前还算简洁清晰吧， 但是接下来可能会有点令人困惑。Python支持函数调用时的命名参数（个人觉得应该是命名实参）。 观察#5处的函数调用，我们传递的是两个命名实参，这个时候因为有名称标识，参数传递的顺序也就不用在意了； 相反的情况也是正确的：函数的第二个形参是y，但是我们通过位置的方式传递值给它。比如在#2处的函数调用foo(3,1)，我们把3传递给了第一个参数，把1传递给了第二个参数，尽管第二个参数是一个命名参数； 简单来讲，我们可以给只定义了位置参数的函数传递命名参数（实参），反之亦然！如果觉得还理解不了，可以查看官方文档。 嵌套函数 Python 允许创建嵌套函数。这意味着我们可以在函数里面定义函数而且现有的作用域和变量生存周期依旧适用。 &amp;gt;&amp;gt;&amp;gt; def outer(): ... x = 1 ... def inner(): ... print x # 1 ... inner() # 2 ... &amp;gt;&amp;gt;&amp;gt; outer() 1 这个例子有一点儿复杂，但是看起来也还行。想一想在#1发生了什么：Python 解释器需找一个叫x的本地变量，查找失败之后会继续在上层的作用域里面寻找，这个上层的作用域定义在另外一个函数里面。对函数outer()来说，变量x是一个本地变量，但是如先前提到的一样，函数inner()可以访问封闭的作用域（至少可以读和修改）。 在#2处，我们调用函数inner()，非常重要的一点是，inner()也仅仅是一个遵循 Python 变量解析规则的变量名，Python 解释器会优先在outer()的作用域里面对变量名inner()查找匹配的变量。 函数是 Python 世界里的一级类对象 函数作为参数 显然，在 Python 里函数和其他东西一样都是对象。 &amp;gt;&amp;gt;&amp;gt; issubclass(int, object) # all objects in Python inherit from a common baseclass True &amp;gt;&amp;gt;&amp;gt; def foo(): ... pass &amp;gt;&amp;gt;&amp;gt; foo.__class__ # 1 &amp;lt;type &#39;function&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; issubclass(foo.__class__, object) True 你也许从没有想过，你定义的函数居然会有属性，比如foo就有一个名称为__class__的属性。没办法，函数在 Python 里面就是对象，和其他的东西一样，也许这样描述会太学院派太官方了点：在 Python 里，函数只是一些普通的值而，已和其他的值一样。这就是说你可以把函数像参数一样传递给其他的函数或者给函数一个函数类型的返回值！如果你从来没有这么想过，那看看下面这个例子： &amp;gt;&amp;gt;&amp;gt; def add(x, y): ... return x &#43; y &amp;gt;&amp;gt;&amp;gt; def sub(x, y): ... return x - y &amp;gt;&amp;gt;&amp;gt; def apply(func, x, y): # 1 ... return func(x, y) # 2 &amp;gt;&amp;gt;&amp;gt; apply(add, 2, 1) # 3 3 &amp;gt;&amp;gt;&amp;gt; apply(sub, 2, 1) 1 这个例子对你来说应该不会很奇怪。add()和sub()是两个非常普通的 Python 函数，接受两个值，返回一个计算后的结果值。在#1处能看到apply()第一个参数虽然只是一个普通的位置变量，但实际上在#2处func最终对应一个特定的函数，整个过程中函数的名称只是根其他变量一样的一个标识符而已。Python 把频繁要用的操作变成函数作为参数进行使用，比如传递一个函数给内置排序函数sorted()的key参数来自定义排序规则。 函数作为返回值 &amp;gt;&amp;gt;&amp;gt; def outer(): ... def inner(): ... print &amp;quot;Inside inner&amp;quot; ... return inner # 1 ... &amp;gt;&amp;gt;&amp;gt; foo = outer() #2 &amp;gt;&amp;gt;&amp;gt; foo &amp;lt;function inner at 0x...&amp;gt; &amp;gt;&amp;gt;&amp;gt; foo() Inside inner 在#1处我把作为函数标识符的变量inner作为返回值返回出来。在#2处我们捕获住返回值：函数inner()，将它存在一个新的变量foo里。我们能够看到，当对变量foo进行求值时，它确实包含了函数inner()，而且我们能够对他进行调用。 闭包 我们先不急着定义什么是闭包，先来看看一段代码，仅仅是把上一个例子简单的调整了一下： &amp;gt;&amp;gt;&amp;gt; def outer(): ... x = 1 ... def inner(): ... print x # 1 ... return inner &amp;gt;&amp;gt;&amp;gt; foo = outer() &amp;gt;&amp;gt;&amp;gt; foo.func_closure (&amp;lt;cell at 0x...: int object at 0x...&amp;gt;,) 在上一个例子中我们了解到，inner()作为一个函数成为outer()的返回值，保存在一个变量foo中，并且我们能够对它进行调用：foo()。不过它会正常的运行吗？我们先来看看作用域规则。 所有的东西都在 Python 的作用域规则下进行工作：x是函数outer()里的一个局部变量。当函数inner()在#1处打印x的时候，Python 解释器会在inner()内部查找相应的变量，当然会找不到，所以接着会到封闭作用域里面查找，并且会找到匹配。但是从变量的生存周期来看，该怎么理解呢？我们的变量x是函数outer()的一个局部变量，这意味着只有当函数outer()正在运行的时候才会存在。根据我们已知的 Python 运行模式，我们没法在函数outer()返回之后继续调用函数inner()，在函数inner()被调用的时候，变量x早已不复存在，可能会发生一个运行时错误。 然而事实上返回的函数inner()居然能够正常工作。Python 支持一个叫做函数闭包的特性，用人话来讲就是，嵌套定义在非全局作用域里面的函数能够记住它在被定义的时候它所处的封闭命名空间。这能够通过查看函数的func_closure属性得出结论，这个属性里面包含封闭作用域里面的值（只会包含被捕捉到的值，比如x，如果在outer()里面还定义了其他的值，封闭作用域里面是不会有的）。 记住，每次函数outer()被调用的时候，函数inner()都会被重新定义。现在变量x的值不会变化，所以每次返回的函数inner()会是同样的逻辑，假如我们稍微改动一下呢？ &amp;gt;&amp;gt;&amp;gt; def outer(x): ... def inner(): ... print x # 1 ... return inner &amp;gt;&amp;gt;&amp;gt; print1 = outer(1) &amp;gt;&amp;gt;&amp;gt; print2 = outer(2) &amp;gt;&amp;gt;&amp;gt; print1() 1 &amp;gt;&amp;gt;&amp;gt; print2() 2 从上面例子可以看到闭包（被函数记住的封闭作用域）这一特性可被用于创建自定义的函数，本质上来说是一个硬编码的参数。事实上我们并不是传递参数1或者2给函数inner()，我们实际上是创建了能够打印各种数字的各种自定义版本。 闭包单独拿出来就是一个非常强大的功能， 在某些方面，你也许会把它当做一个类似于面向对象的技术：outer()像是给inner()服务的构造器，x像一个私有变量。使用闭包的方式也有很多：你如果熟悉 Python 内置排序方法的参数key，你说不定已经写过一个lambda方法在排序的时候重新指定一个新的排序规则。现在你说不定也可以写一个itemgetter方法，接收一个索引值来返回一个完美的函数，传递给排序函数的参数key。 不过，我们现在不会用闭包做这么 low 的事，我们要写的是一个高大上的装饰器！ 装饰器 装饰器其实就是一个闭包，把一个函数当做参数然后返回一个替代版函数，这个新的替代版的函数在保持原有函数功能的前提下会加入一些新的功能。 &amp;gt;&amp;gt;&amp;gt; def outer(some_func): ... def inner(): ... print &amp;quot;before some_func&amp;quot; ... ret = some_func() # 1 ... return ret &#43; 1 ... return inner &amp;gt;&amp;gt;&amp;gt; def foo(): ... return 1 &amp;gt;&amp;gt;&amp;gt; decorated = outer(foo) # 2 &amp;gt;&amp;gt;&amp;gt; decorated() before some_func 2 仔细看看上面这个装饰器的例子。我们定义了一个函数outer()，它只有一个some_func的参数，在函数里面我们定义了一个嵌套的函数inner()，inner()会打印一串字符串，然后调用some_func()，在#1处得到它的返回值。在outer()每次调用的时候some_func的值可能会不一样，但是不管some_func的之如何，我们都会调用它。最后，inner()返回some_func() &#43; 1的值：我们通过调用在#2处存储在变量decorated里面的函数能够看到被打印出来的字符串以及返回值2，而不是期望中调用函数foo()得到的返回值1。 我们可以认为变量decorated是函数foo()的一个装饰版本，一个加强版本。事实上如果打算写一个有用的装饰器的话，我们可能会想愿意用装饰版本完全取代原先的函数foo()，这样我们总是会得到我们的加强版的foo()。想要达到这个效果，压根不需要学习新的语法，直接将foo当成outer()的参数，然后将函数outer()的结果赋值给foo即可： &amp;gt;&amp;gt;&amp;gt; foo = outer(foo) &amp;gt;&amp;gt;&amp;gt; foo # doctest: &#43;ELLIPSIS &amp;lt;function inner at 0x...&amp;gt; 注意：这里是将outer(foo)赋值给变量foo，由于outer()的返回值是一个函数inner()，因此现在foo实际上对应于inner()，即针对原始foo()函数在功能上加强后的新的foo()。不可以写成foo = outer，这样的话，前面的decorated就需要改写成decorated = foo(foo)，这样两个foo的处理会非常混乱。 现在，任何怎么调用都不会牵扯到原先的函数foo()，都会得到新的装饰版本的foo()，现在我们还是来写一个有用的装饰器。 想象我们有一个库，这个库能够提供类似坐标的对象，也许它们仅仅是一些x和y的坐标对。不过可惜的是这些坐标对象不支持数学运算符，而且我们也不能对源代码进行修改，因此也就不能直接加入运算符的支持。我们将会做一系列的数学运算，所以我们想要能够对两个坐标对象进行合适加减运算的函数，就需要在原有的基础上用装饰器方法来加强原有坐标对象的功能： &amp;gt;&amp;gt;&amp;gt; class Coordinate(object): ... def __init__(self, x, y): ... self.x = x ... self.y = y ... def __repr__(self): ... return &amp;quot;Coord: &amp;quot; &#43; str(self.__dict__) &amp;gt;&amp;gt;&amp;gt; def add(a, b): ... return Coordinate(a.x &#43; b.x, a.y &#43; b.y) &amp;gt;&amp;gt;&amp;gt; def sub(a, b): ... return Coordinate(a.x - b.x, a.y - b.y) &amp;gt;&amp;gt;&amp;gt; one = Coordinate(100, 200) &amp;gt;&amp;gt;&amp;gt; two = Coordinate(300, 200) &amp;gt;&amp;gt;&amp;gt; add(one, two) Coord: {&#39;y&#39;: 400, &#39;x&#39;: 400} 如果不巧我们的加减函数同时也需要一些边界检查的行为那该怎么办呢？搞不好你只能够对正的坐标对象进行加减操作，任何返回的值也都应该是正的坐标。所以现在的期望是这样： &amp;gt;&amp;gt;&amp;gt; one = Coordinate(100, 200) &amp;gt;&amp;gt;&amp;gt; two = Coordinate(300, 200) &amp;gt;&amp;gt;&amp;gt; three = Coordinate(-100, -100) &amp;gt;&amp;gt;&amp;gt; sub(one, two) Coord: {&#39;y&#39;: 0, &#39;x&#39;: -200} &amp;gt;&amp;gt;&amp;gt; add(one, three) Coord: {&#39;y&#39;: 100, &#39;x&#39;: 0} 我们期望在不更改坐标对象one, two, three的前提下one减去two的值是{x: 0, y: 0}，one加上three的值是{x: 100, y: 200}。与其给每个方法都加上参数和返回值边界检查的逻辑，我们来写一个边界检查的装饰器！ &amp;gt;&amp;gt;&amp;gt; def wrapper(func): ... def checker(a, b): # 1 ... if a.x &amp;lt; 0 or a.y &amp;lt; 0: ... a = Coordinate(a.x if a.x &amp;gt; 0 else 0, a.y if a.y &amp;gt; 0 else 0) ... if b.x &amp;lt; 0 or b.y &amp;lt; 0: ... b = Coordinate(b.x if b.x &amp;gt; 0 else 0, b.y if b.y &amp;gt; 0 else 0) ... ret = func(a, b) ... if ret.x &amp;lt; 0 or ret.y &amp;lt; 0: ... ret = Coordinate(ret.x if ret.x &amp;gt; 0 else 0, ret.y if ret.y &amp;gt; 0 else 0) ... return ret ... return checker &amp;gt;&amp;gt;&amp;gt; add = wrapper(add) &amp;gt;&amp;gt;&amp;gt; sub = wrapper(sub) &amp;gt;&amp;gt;&amp;gt; sub(one, two) Coord: {&#39;y&#39;: 0, &#39;x&#39;: 0} &amp;gt;&amp;gt;&amp;gt; add(one, three) Coord: {&#39;y&#39;: 200, &#39;x&#39;: 100} 这个装饰器能像先前的装饰器例子一样进行工作，返回一个经过修改的函数，但是在这个例子中，它能够对函数的输入参数和返回值做一些非常有用的检查和格式化工作，将负值的x和y替换成0。 显而易见，通过这样的方式，我们的代码变得更加简洁：将边界检查的逻辑隔离到单独的方法中，然后通过装饰器包装的方式应用到我们需要进行检查的地方。虽然直接在计算的开始处和返回值之前调用边界检查的方法也能够达到同样的目的，但是使用装饰器能够让我们以最少的代码量达到坐标边界检查的目的。事实上，如果我们是在装饰自己定义的方法的话，我们能够让装饰器应用的更加有逼格。 使用 @ 标识符将装饰器应用到函数 Python 2.4 支持使用标识符@将装饰器应用在函数上，只需要在函数的定义前加上@和装饰器的名称。在上一节的例子里我们是将原本的方法用装饰后的方法代替: &amp;gt;&amp;gt;&amp;gt; add = wrapper(add) 这种方式能够在任何时候对任意方法进行包装。但是如果我们自定义一个方法，我们可以使用@进行装饰： &amp;gt;&amp;gt;&amp;gt; @wrapper ... def add(a, b): ... return Coordinate(a.x &#43; b.x, a.y &#43; b.y) 这样的做法和先前简单的用包装方法替代原有方法是一样的， Python 只是加了一些语法糖让装饰的行为更加的直接明确和优雅一点。 *args 和 **kwargs 我们已经完成了一个有用的装饰器，但是由于硬编码的原因它只能应用在一类具体的函数上，以上一节的checker()为例，它接收两个参数并传递给闭包捕获的函数add()或sub()。如果我们想实现一个能够应用在任何函数上的装饰器要怎么做呢？再比如，如果我们要实现一个能应用在任何函数上的类似于计数器的装饰器，不需要改变原有方法的任何逻辑。这意味着装饰器能够接受任意形式的函数作为自己的被装饰函数，同时能够用传递给它的参数对被装饰的方法进行调用。 非常巧合的是 Python 正好有支持这个特性的语法。可以阅读 Python Tutorial 获取更多的细节。当定义函数的时候使用了*，意味着那些通过位置传递的参数将会被放在带有*前缀的变量中， 所以： &amp;gt;&amp;gt;&amp;gt; def one(*args): ... print args # 1 &amp;gt;&amp;gt;&amp;gt; one() () &amp;gt;&amp;gt;&amp;gt; one(1, 2, 3) (1, 2, 3) &amp;gt;&amp;gt;&amp;gt; def two(x, y, *args): # 2 ... print x, y, args &amp;gt;&amp;gt;&amp;gt; two(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;) a b (&#39;c&#39;,) 第一个函数one()只是简单地讲任何传递过来的位置参数全部打印出来而已，在代码#1处我们只是引用了函数内的变量args, *args仅仅只是用在函数定义的时候用来表示位置参数应该存储在变量args里面。Python 允许我们制定一些参数并且通过args捕获其他所有剩余的未被捕捉的位置参数，就像#2处所示的那样。 *操作符在函数被调用的时候也能使用。意义基本是一样的。当调用一个函数的时候，一个用*标志的变量意思是变量里面的内容需要被提取出来然后当做位置参数被使用。同样的，来看个例子： &amp;gt;&amp;gt;&amp;gt; def add(x, y): ... return x &#43; y &amp;gt;&amp;gt;&amp;gt; lst = [1,2] &amp;gt;&amp;gt;&amp;gt; add(lst[0], lst[1]) # 1 3 &amp;gt;&amp;gt;&amp;gt; add(*lst) # 2 3 #1处的代码和#2处的代码所做的事情其实是一样的，在#2处，Python 为我们所做的事其实也可以手动完成。这也不是什么坏事，*args要么是表示调用方法的时候额外的参数可以从一个可迭代列表中取得，要么就是定义方法的时候标志这个方法能够接受任意的位置参数。 接下来提到的**会稍多更复杂一点，**代表着键值对的参数字典，和*所代表的意义相差无几，也很简单： &amp;gt;&amp;gt;&amp;gt; def foo(**kwargs): ... print kwargs &amp;gt;&amp;gt;&amp;gt; foo() {} &amp;gt;&amp;gt;&amp;gt; foo(x=1, y=2) {&#39;y&#39;: 2, &#39;x&#39;: 1} 当我们定义一个函数的时候，我们能够用**kwargs来表明，所有未被捕获的关键字参数都应该存储在kwargs的字典中。如前所诉，args和kwargs并不是 Python 语法的一部分，但在定义函数的时候，使用这样的变量名算是一个不成文的约定。和*一样，我们同样可以在定义或者调用函数的时候使用**。 &amp;gt;&amp;gt;&amp;gt; dct = {&#39;x&#39;: 1, &#39;y&#39;: 2} &amp;gt;&amp;gt;&amp;gt; def bar(x, y): ... return x &#43; y &amp;gt;&amp;gt;&amp;gt; bar(**dct) 3 更通用的装饰器 有了这招新的技能，我们随随便便就可以写一个能够记录下传递给函数参数的装饰器了。先来个简单地把日志输出到界面的例子： &amp;gt;&amp;gt;&amp;gt; def logger(func): ... def inner(*args, **kwargs): #1 ... print &amp;quot;Arguments were: %s, %s&amp;quot; % (args, kwargs) ... return func(*args, **kwargs) #2 ... return inner 请注意我们的函数inner()，它能够接受任意数量和类型的参数并把它们传递给被包装的方法，这让我们能够用这个装饰器来装饰任何方法。 &amp;gt;&amp;gt;&amp;gt; @logger ... def foo1(x, y=1): ... return x * y &amp;gt;&amp;gt;&amp;gt; @logger ... def foo2(): ... return 2 &amp;gt;&amp;gt;&amp;gt; foo1(5, 4) Arguments were: (5, 4), {} 20 &amp;gt;&amp;gt;&amp;gt; foo1(1) Arguments were: (1,), {} 1 &amp;gt;&amp;gt;&amp;gt; foo2() Arguments were: (), {} 2 随便调用我们定义的哪个方法，相应的日志也会打印到输出窗口，和我们预期的一样。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[消失的长期投资：为什么没有更多的早期抗癌药物？（Innovation系列） - 慧航 - 专栏]]></title>
    	<url>/prof/2015/07/02/underinvest-in-long-term-research/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20095600 引言 许多技术创新，不仅仅能为企业带来丰厚的利润，也具有极 时任哈佛商学院教授的 Jeremy Stein 提出市场短视理论，即在自由市场中，企业会选择短期获利大但价值不高的项目进行投资。由于长期投资项目不能准确被市场估值，企业管理层为了避免潜在的股东财富损失，更加倾向于短期效益高，容易被市场估值的项目。 这种短视行为对于研发型企业意义深远。研发型企业，例如制药业，经常面临短期项目和长期项目的选择。长期项目的劣势对于制药业显而易见，从最开始的新药研发，到获得专利批准，再到临床实验结束，新药获得药监局审批上市，商品化滞后周期可以长达十年。过长的商品化周期意味着效益很难在短期内收回，投资项目也难以被估值。 另外一个被忽略的难题是现行的专利制度。在现有的规定下，专利保护期限由申请日开始算起，固定为年限二十年。研发性企业通常在新产品上市前申请专利，如果专利保护成果有一定的商品化滞后期，那么实际的专利保护是少于二十年的。因此在这种由上市前起开始算固定年限的专利保护下，当商品化滞后期越长，实际专利保护年限越短，从而进一步降低了企业对长期项目研发的估值。 在 American Economic Review 七月刊登的一篇论文《Do firms underinvest in long-term research?Evidence from cancer clinical trials》构造了一个理论模型1，估算了短视行为和专利制度对于制药业投资行为的影响。 这篇文章以癌症药物研发为背景，实证检验了专利制度对于长期投资的消极作用。制药企业，在新药研发成功获得专利后，还需通过国家药监局规定的临床试验。癌症药物的临床试验通常使用总生存期为试验终点，及比较从药程开始到病人死亡之间的时间。由于所有药物都必须通过临床实验，临床实验的长度决定了药品实际专利保护的年限。相比早期抗癌或癌症预防药物，晚期癌症病人的总生存期短，因而新药研发的商品化滞后期短，较高的短期效益让制药公司偏重投资。 作者收集了美国所有癌症药物的临床试验纪录和的投资金额，提供充足的证据，指出由于制药企业过多偏重于短期获利大的晚期癌症药物，我们没有在早期抗癌和癌症预防方面作出足够的药物研发。作者提出了三种政策弥补措施，1) 在临床实验中使用代理终点 (surrogate endpoint)，例如按肿瘤是否变小，而非总生存期作为试验终点。代理终点可以大幅缩短临床实验的时间，2) 政府补贴或者主导早期抗癌和癌症预防的研究项目，3)改革固定年限的专利制度。 癌症药物背景 从癌症药物市场作为切入点有两点优势： 药物可以按照恶性肿瘤产生的部位（例如，肺部）和癌症病程（例如，原位期，局部期和转移期）系统分类。美国癌症中心（US National Cancer Institute）管理的癌症存活率监控署（Surveillance, Epidemiology, and End Results Program）收录了这些药物的临床试验纪录。临床试验的次数可以显示制药业对于该药物所属的癌症－病程的投资力度。如果直接比较临床试验次数，转移期和复发性的癌症药物有更多的临床实验。 投资周期则可以用药物对照的癌症五年生存率来测量。生存率越高，临床实验周期越长，投资时间越久。 本文样本一共有 80 种癌症，再细分到病程，有 201 个癌症−病程。平均五年生存率为 37.7%。 1973−2011 年中，平均每种癌症—病程有将近 1000 个临床实验。平均每一种癌症－病程有 0.5 个药物被批准。见文章表一。 理论模型 （不感兴趣请移步章节四） 本文使用的模型，刻画了新药研发-专利获准-临床试验-新药过审上市的全过程。 主要时间点: t_invent = 研发成功，可以归为 0； t_comm ＝ 从研发成功开始到审批上市时间(commercialization)，也就是商品化滞后期；t_patent = 专利保护，通常在t_invent后不久。因为会在t_invent后马上开始临床实验，这时候申请专利，那么新药会被认为是已有技术（prior art)，而不会得到专利保护。但由于企业对药物的垄断期从是上市开始计算，所以实际的专利保护时间是max(t_patent-1-t_comm,0)。因此，滞后期越短，实际的专利保护时间就越长。相反，如果滞后期长于专利保护期，亦即临床试验在专利保护失效前仍未结束，那么实际专利保护为 0 年。 风险管理： \(p\in[0,1]\)：临床实验结果显示有疗效，获得上市许可； \(\gamma\in[0,1]\)：每年不被淘汰的机率(试验药物可能被疗效更好的药物取代)； \(1/(1&#43;r)\)：每年折现率； \(\eta\in[0,1]\)：年度企业短视率，对长期投资项目估值较低的一个折扣率； 社会整体无长期或短期之分，因而社会折现系数为\(\delta=\gamma/(1&#43;r)\)。而对于企业折现系数为\(\delta=\eta\gamma/(1&#43;r)\)。 价值考量 \(c\)：临床实验的支出，可视为总投资资金； \(\pi\)：每年的垄断利润； \(v_{\mathrm{mono}}\)：垄断价格下的社会价值； \(v\)：非垄断价格下的社会价值，应该大于等于\(v_{\mathrm{mono}}\)； 实际垄断期和实际使用期（可以参考笔者总结的图一(a)） 在t_invent，药物研发成功这一时刻，企业可以按如下函数来估测这个药物的实际垄断期(effective monopoly life) \[\mathrm{EML}=p\sum^{t_{\mathrm{patent}}-1}_{t_{\mathrm{comm}}}(\eta\delta)^t=p\frac{(\eta\delta)^{t_{\mathrm{comm}}}-(\eta\delta)^{t_{\mathrm{patent}}}}{1-\eta\delta}\] 实际垄断期的参量为：能否获得批准上市，作为专利药开始销售后享有的t_patent-1-t_comm专利保护，企业短视程度，和折现系数。 所以企业的回报为\(\mathrm{EML}*\pi\)。 而对于社会总体而言，垄断期结束后，该药物仍被使用，因而依然有社会价值。 所以相应的实际使用期(effective total life。) \[\mathrm{ETL}=p\sum^{\infty}_{t_{\mathrm{comm}}}(\delta)^t=p\frac{\delta^{t_{\mathrm{comm}}}}{1-\delta}\] 和实际垄断期函数不同的是，药物使用期会药物使用期会持续到药物被淘汰时才结束。同时，社会没有短视行为，对短期和长期投资一视同仁。因此总体社会价值为\(\mathrm{ETL}*v\)。 什么情况下企业和市场的最优化投资决定不同? （preposition 1.1） 当实际垄断期小于实际使用期（\(\mathrm{EML}&amp;lt;\mathrm{ETL}\)) 时，或者垄断利润小于社会价值(\(\pi&amp;lt;v\))时，企业会因回报不足而放弃对社会福利高的项目的投资。 因为企业得到的回报不够。要使得\(\mathrm{EML}*\pi=\mathrm{ETL}*v\)，必需条件为： 垄断期和使用期同样长 \(\pi=v\)，即垄断回报与社会价值等同。一个极端的例子是张悟本的《把吃出来的病吃回去》里的绿豆疗法，假设某制药公司考虑把绿豆汤作为癌症药物上交药监局审批上市，绿豆汤的垄断期和回报基本为零，因为大家都知道怎么煮绿豆汤，所以不会有制药公司来投资关于绿豆汤的临床实验。 在\(\mathrm{EML}/\mathrm{ETL}\)和\(\pi/v\)这两个比值中，政策可调控的更多是\(\mathrm{EML}/\mathrm{ETL}\)，即实际垄断期与实际使用期的比值。比值越高，企业享受的相对垄断期就越长，从而对其投资越有利。但我们可以推出相对垄断期和市场滞后期成反比（preposition 2）滞后期越长，企业投资越少，这和引言里的观察一致。 实践检验结果 因为临床实验的量度为次数，作者用 quasi-maximum likelihood Poisson models 估测生存率对 RD 的影响。每 10% 生存率的增长，对应 8.7% RD 的减少，见下图。 读到这里你也许会质疑，制药公司在开发新药之前，肯定会做相应的市场调查，来估测市场需求。市场需求越大的新药，制药公司肯定会优先考虑投资。如何来估算不同癌症药物的市场需求量呢？作者用不同癌症所导致的预期寿命缺失来调整病人对于药物的需求量，比如说，一个中年男性预期寿命是70岁，他有可能在45岁时由于晚期前列腺癌死亡，那么该病人的预期寿命缺失为25年。如果他也有可能于60岁死于晚期胃癌，晚期胃癌预期寿命缺失要比晚期前列腺癌要短，由此推出晚期前列腺癌药物的需求比较大。但即便在考虑了药物需求量后，癌症生存率对于RD还是有显著的负作用。(附录里有具体从生存率到滞后期的换算，有生物统计的相关知识。概括而言生存率越高，临床试验所需人数越多，时间也越长。） 政策调整 作者提出了三种政策调整措施，1）在临床实验中使用代理终点 (surrogate endpoint)，例如肿瘤是否变小，而非总生存期作为试验终点。代理终点可以大幅缩短临床实验的时间，2）政府补贴或者主导早期抗癌和癌症预防的研究项目，3）改革固定年限的专利制度。虽然没有足够数据来证明，模型和已有的案例证明了这些政策的合理性。 代理终点。本文的模型告诉我们，缩短滞后期会使得相对垄断期增长，从而企业投资的可能性更大。使用代理终点是一个缩短滞后期的有效方法。如同模型所预测的，假设临床试验采用替代终点，癌症药物的研发周期就不再与存活率挂钩。目前美国药监局已经批准了一些替代终点，主要是治疗血癌的药物，如对于白血病药物 可以用癌症是否完全缓解（Complete Response, CR，及体表肿块完全消失，血象和骨髓象也基本恢复正常）作为替代终点。在这些药物中，存活率于 RD 力度没有显著影响。但是替代终点不是对于所有癌症药物都适用。 另外一个方法是考虑政府补贴。实际上所有的癌症预防药物不是使用了替代终点，就是政府提供的资金。虽然一共也只有 6 个，其中之一是要跑去香港注射的HPV疫苗。。 最为大胆的措施是专利制度改革，根据滞后期的长度调整专利保护期。这是很有研究价值的一项措施。现行制度下，所有专利的保护期都是二十年。如果专利保护期应该和该发明所带来的社会福利挂钩，那么癌症预防药物应该比晚期癌症药物享有更多的保护期。但事实恰好相反，癌症预防的临床实验比晚期癌症药物长，于是实际专利保护期要短得多。在最优化社会福利的假设下，癌症预防的专利保护期应该被延长。 延长专利期有可能导致制药公司想要研发更多的药物。但公司还是会选择最有价值的药物，所以于社会而言有益无害。那么，价值低的药物同样享受了延长的专利期，会不会推后了更便宜的非专利药物的上市呢？事实上，延长越久，制药公司的支出越大，因而它们并不会因专利期延长而选择投资价值低的药物。 可喜的是，FDA 已经有相关的法令，虽然具体规则值得商榷。 Hatch Waxman Act（在两位参议会议员 Hatch 和 Waxman 倡导下于 1984 年通过）就规定当药监局审批过长，制药企业可以申请延长专利。（但这个法令有意思的是，延长专利期的同时，也让非专利药更早的进入市场，从而缩短了一些药物的专利期。以前所有非专利药必须等到专利过期后开始研发，但现在只要有一个非专利药能够打赢官司，专利无效，那这个非专利药就有半年的垄断期。这样也避免了其他生产非专利药的公司不劳而获地享受第一个非专利药生产商打官司的成果。）一个更为简单粗暴的改革方式，是将专利生效期定在药物上市，如图一（b）。这样改革，完全消除了商品化滞后期对整体垄断期的影响。 结言 本文指出由于专利制度和短视行为，制药企业过多投资于晚期癌症药物的研发。对于这种市场扭曲，用代理终点而非死亡终点可以缩短临床试验的长度，因而帮助早期癌症药物的研发。但对于专利制度改革，作者持保守态度，毕竟更加根本的问题是，专利制度是否还是最能鼓励企业投资的方式？ 中国目前专利制度与美国相似，因此本文对中国专利制度研究也有借鉴作用。另外一个值得研究的问题是进口癌症药物会不会也有相似的市场扭曲问题？希望有志于相关研究的学者阅读该文。 还有笔者很明显对制药行业很陌生，有制药领域的朋友欢迎交流想法！ talich：国会众院刚刚通过了新 21st Century Cures Act（https://www.congress.gov/bill/114th-congress/house-bill/6），应该很快参议院会过。所谓的 breakthough medical device 不一定需要 clinical trial 了。对药上也有一些改变。 nautilus sun：谢谢您指出这个议案，非常有帮助。不过议案里还有“The marketing exclusivity period is extended by six months for a drug approved for a new indication that is a rare disease or condition.”如果一些癌症算成 rare disease 的话，会变相延长专利期的吧？一种癌症药物的 new indication 有可能很多吗？ talich：抗癌药物用在其他癌症的情况很常见，这种把现有药用于其他病的情况叫 repositioning。不过很多癌症不一定能叫 rare disease。再不过，orphan disease act 让每个制药公司只要通过一个 rare disease 药，公司的所有药都能享受 7 年的 exclusive 保护。这个怎么用上去，我也不知道。 反正各种条款很多，各种关系我也不好说清楚。可能是指生物合成的药物，以前被认为无法仿制。但是今年 FDA 通过了第一个生物合成仿制药，所以…… Biosimilar Klaith：代理终点是个非常棘手的问题，因为它未必能够真正代表疾病的转归。仅以糖尿病为例，一直以来，我们都将血糖或糖化血红蛋白作为临床研究的终点，然而文迪亚事件后，我们发现血糖、糖化血红蛋白虽然改善，但患者因主要心血管事件的发生率也许并不改善。所以代理终点的选择现在慎之又慎，同样需要大量的前瞻性临床研究和数据来支持。在这样的背景下，为了尽快发生终点事件，制药企业只能拖上晚期患者，既迅速，伦理学风险又小，最后的决策权又交给临床医生（off-label），何乐而不为？ nautilus sun：原来 Avandia 是文迪亚。会不会有临床研究一部分使用代理终点，一部分使用“比较硬”的终点的情况？ talich：文迪亚的问题好像是它故意隐瞒了不利于自己的结果。这种慢性病的药，药厂还是喜欢的，因为一旦得病，终生服药，细水长流流的买卖。你可以同时观察几个终点，但只有一个是主要终点（primary endpoint），也只有这个主要终点，它的统计学意义是充分的。生物仿制药比化学仿制药监管复杂很多，因此门槛高很多。 Klaith：文迪亚的根本问题是马里兰有位心内科医生，使用了一些非个体病例级别的大型临床研究数据，作了一个 meta 分析，结果得出一个增加心血管事件死亡率的结论。去年还是今年，GSK 自己的前瞻性研究结果出来了，摘掉了帽子，可惜整个类型已经废掉了。 talich：嗯，是去掉了。但是 GSK 被司法部告也是真的：In 2012, the U.S. Justice Department announced GlaxoSmithKline had agreed to plead guilty and pay a $3 billion fine, in part for withholding the results of two studies of the cardiovascular safety of Avandia between 2001 and 2007. Klaith：去掉用药限制，而仅处罚这一行为，也表明这两项研究的数据并未对文迪雅整体的获益-风险评估造成实质性影响。 talich：这两个之间没有逻辑关系。司法部在 12 年了结，一定是基于之前的事实：2001 到 2007 年；而 FDA 在 2013 年通过，是基于以后的随机比照结果。应该考虑的是，如果当初 GSK 没有隐瞒结果，Avandia 能否通过 FDA 的审批？后来药解限了，不等于公司以前的非法行为就不严重了。我强调的，是公司的操守问题，是这件事上的大问题。 Klaith：文迪亚是 1999 年 FDA 批准上市的，2001 年到 2007 年的研究数据均为上市后研究数据，因此不会影响其 FDA 审批情况。GSK 在该案中存在确凿无误的程序错误，操守当然有问题，但这并不代表其产品文迪亚存在相应风险，而仅罚款不退市，足以说明瞒报的数据，尚不影响该产品的获益—风险评价。 talich：审批这个是我搞错了。我想我们没有本质矛盾。只是应该回到原点，重新沟通一下： 我认为 Avandia 一事的最重要问题，「是它故意隐瞒了不利于自己的结果」。 而你认为「文迪亚的根本问题是马里兰有位心内科医生，使用了一些非个体病例级别的大型临床研究数据，作了一个 meta 分析，结果得出一个增加心血管事件死亡率的结论。」 我们之后的讨论都基本此，而我没看出来你说的这个为什么是根本问题。这个当然是 Avandia 销售过程中的重要传折点，但是作为整个事件，我指的重要问题，是它的 legacy，对后人的启示。 你觉得在 NEJM 上的这篇文章的研究方法既然是个问题，那它的 legacy 具体是什么呢。 我本人没读过这篇文章，如果说这篇文章本身是有问题的，那它应该被 Retract，如果它没问题，只是后来的研究证明了它的结论是错的，那我觉得 FDA 当时对 Avandia 的冷处理也很正常。因为这种情况其实在其他研究中也常出现。就好像最近的关于胆固醇和鸡蛋的事。 严凌霄：不是 health economics 专业的，可能问题有点小白。为什么总体社会价值一直是\(\mathrm{ETL}*v\)而不考虑\(v_{\mathrm{mono}}\)呢？ nautilus sun：不会的，\(v\)是 the social value of the product if it were priced by a social planner instead of a monopolist, and it is greater than v_mono if the product were priced by a monopolist. Z Abigail：还有一个问题，美国医疗保险的 coverage 是以 treatment 为标准的（有病来治），即使是 Obamacare 也是这样，preventative 的东西长期以来投入很少，也有这方面的原因。癌症早期的症状相对不明显，要发现的话一般是有比较好的保险 coverage（很完善的体检）还有有这个意识的人。近年来保险公司有在医疗保险里包括 gym membership 什么的，也算是朝这方面努力吧？ nautilus sun：是的，要让保险的 coverage 以身体情况为标杆，就会有依据过往病史来定价的情况出现。 作者：芝加哥大学的 Eric Budish，麻省理工 Sloan 商学院的 Ben Roin，和麻省理工经济系的 Heidi Williams，题目为《公司在长期研究中投资不足吗？来自癌症临床试验的证据》，该论文获得了2015斯隆研究基金会奖。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[飘逸的 Python - 单例模式乱弹 z]]></title>
    	<url>/tech/2015/06/28/single-instance/</url>
		<content type="text"><![CDATA[[原文地址：http://blog.csdn.net/handsomekang/article/details/46672047 这篇的内容暂时 Metaclass 资料时从其它页面跳转来的。 装饰器 利用“装饰器只会执行一次”这个特点： def singleton(cls): instances = []# 为什么这里不直接为None,因为内部函数没法访问外部函数的非容器变量 def getinstance(*args, **kwargs): if not instances: instances.append(cls(*args, **kwargs)) return instances[0] return getinstance @singleton class Foo: a = 1 f1 = Foo() f2 = Foo() print id(f1), id(f2) 基类 利用“类变量对所有对象唯一”，即cls._instance class Singleton(object): def __new__(cls, *args, **kwargs): if not hasattr(cls, &#39;_instance&#39;): cls._instance = object.__new__(cls, *args, **kwargs) return cls._instance class Foo(Singleton): a = 1 metaclass 利用“类变量对所有对象唯一”，即cls._instance class Singleton(type): def __call__(cls, *args, **kwargs): if not hasattr(cls, &#39;_instance&#39;): cls._instance = super(Singleton, cls).__call__(*args, **kwargs) return cls._instance class Foo(): __metaclass__ = Singleton Borg 模式 利用“类变量对所有对象唯一”，即__share_state class Foo: __share_state = {} def __init__(self): self.__dict__ = self.__share_state 利用 import 利用“模块只会被import一次” #在文件mysingleton中 class Foo(object): pass f = Foo() 然后在其它模块，from mysingleton import f，直接拿f当作单例的对象来用。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python 模拟登陆知乎和 CSDN z]]></title>
    	<url>/tech/2015/06/25/spider-csdn-login/</url>
		<content type="text"><![CDATA[[原文地址：http://blog.csdn.net/natsuyu/article/details/46641295 HTTP 协议方面现在懂得还很少 socket 的时候好多了，有个更加立体的了解！ 模拟登陆的思路很简单： 登录的时候有重要数据肯定是用 post 方法提交的，用各种方法找到 post 中的请求数据 用各种方法获取请求数据中的内容 处理头部和 cookie，并带着请求数据 post 给网址 截获 post 包 Windows 上用 fiddle，很好用的貌似。Mac上。其实我现在还没有 get 到优雅的方法。所以是。输入一个错误的密码，然后在开发者控制台中找到刚刚 post 出去的包，里面也会有请求数据。因为如果正确登录的话会出现自动跳转。然后刚刚 post 的包都没有了。 其实对于 cookie 处理这一块并不熟。这里不多做说明。 初级版本：用urllib，urllib2来处理 这时候需要处理头部，就是找到 quest 请求后，把内容复制过来作为自己的 head。 例子：知乎登录。先上代码。 # !/usr/bin/env python # -*- coding:utf-8 -*- import re import urllib, urllib2 import cookielib def get_head(head): cj = cookielib.LWPCookieJar() cookie_support = urllib2.HTTPCookieProcessor(cj) header = urllib2.build_opener(cookie_support, urllib2.HTTPHandler) urllib2.install_opener(header) # 以上这一段是cookie处理 li = [] for key, value in head.items(): tmp = (key, value) li.append(tmp) #添加头部 header.addheaders = li return header url = &#39;http://www.zhihu.com&#39; quest = urllib2.Request(url) page = urllib2.urlopen(quest).read() pat = re.compile(r&#39;name=&amp;quot;_xsrf&amp;quot; value=&amp;quot;(\w&#43;?)&amp;quot;&#39;) code = re.search(pat, page) code =code.group(1) print code name = &amp;quot;**********&amp;quot; password = &amp;quot;*********&amp;quot; postdict = {&amp;quot;xsrf&amp;quot;:code, &amp;quot;email&amp;quot;:name, &amp;quot;password&amp;quot;:password, &amp;quot;rememberme&amp;quot;:&#39;y&#39;} postdict = urllib.urlencode(postdict).encode() head = {&amp;quot;Accept&amp;quot;: &amp;quot;*/*&amp;quot;, &amp;quot;Accept-Language&amp;quot;: &amp;quot;zh-CN,zh;q=0.8&amp;quot;, &amp;quot;Connection&amp;quot;: &amp;quot;keep-alive&amp;quot;, &amp;quot;Host&amp;quot;:&amp;quot;知乎 - 与世界分享你的知识、经验和见解&amp;quot;, &amp;quot;Referer&amp;quot;:&amp;quot;知乎 - 与世界分享你的知识、经验和见解&amp;quot;, &amp;quot;User-Agent&amp;quot;: &amp;quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36&amp;quot;} opener = get_head(head) url &#43;= &#39;/login&#39; uop = opener.open(url, postdict) page = uop.read() print page 有一点要说。虽然现在也不是很明了。本来在请求数据时有一个_xsrf是需要从页面中用正则获取的。本来以为是动态的，但是其实好像。每次抓取发现都是同一个值。所以模拟知乎登录的时候直接交用户名和密码上去就好了。（摔！这给我造成了多大的困扰！）然后 cookie 的处理和添加头部那一部分其实并不是很了解。还需 get 技能。 升级版本：用requests模拟登陆，十分方便！ 例子：CSDN登录 CSDN 登录的时候，通过网页源码给我们的提示。那个lt值绝壁是很重要的！而且每次都不一样的！所以！用登录知乎的方法就不管用了！因为之前爬了_xsrf实际上是没有用的，并且我们两次登录了那个页面，如果_xsrf是动态的，那么！爬到的_xsrf也是没有用的！之前看了一个爬虫教程！用了这种错误的方法！给了我血淋淋的教训。。也可能是我还不知道怎么用urllib用保持状态访问。。所以。这里就说说request方法吧。 requests的session()函数可以生成保持状态的对象，用这个对象获取页面并且 post 数据，妥妥的。 #!/usr/bin/env python # -*- coding:utf-8 -*- import requests import re url = &#39;https://passport.csdn.net/account/login&#39; s = requests.session() page = s.get(url).text pat = re.compile(r&#39;name=&amp;quot;lt&amp;quot; value=&amp;quot;(.*?)&amp;quot;.*[\s\S]name=&amp;quot;execution&amp;quot; value=&amp;quot;(.*?)&amp;quot;&#39;,re.S) ret = re.findall(pat, page) print ret lt = ret[0][0] exe = ret[0][1] print lt, exe submit = &#39;submit&#39; id = &#39;natsuyu&#39; password = &#39;**********&#39; postdict = {&#39;username&#39;:id, &#39;password&#39;:password, &#39;lt&#39;:lt, &#39;execution&#39;:exe, &#39;_eventId&#39;:submit} page = s.post(url, data = postdict) print page.text 代码量剧减有没有！requests真的是 or human！]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[MetaClass 与 ORM z]]></title>
    	<url>/tech/2015/06/01/metaclass-orm/</url>
		<content type="text"><![CDATA[[原文地址：http://blog.csdn.net/langqing12345/article/details/46318503 type 与 MetaClass type 实例 Python 默认的元类是type（缺省元类），就连object也是type创造出来的类，type是一切类型的缺省元类，所以object类也是一种type；但同时type也是object的类，构造出的世间万物都是object的实例。所以就算声明一个类不继承自object，它也依然是object的实例，只是不继承自object而已，虽然没有从object继承它的一些方法，但是从构造器object. __new__()中获得了作为一个对象所需要的数据结构和参数。 &amp;gt;&amp;gt;&amp;gt; isinstance(object, type) True &amp;gt;&amp;gt;&amp;gt; isinstance(type, object) True &amp;gt;&amp;gt;&amp;gt; class Ok: ... pass ... &amp;gt;&amp;gt;&amp;gt; o=Ok() &amp;gt;&amp;gt;&amp;gt; isinstance(o, ok) True &amp;gt;&amp;gt;&amp;gt; isinstance(Ok, object) True &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; isinstance(o, object) True 用type来动态创建类： &amp;gt;&amp;gt;&amp;gt; def fn(self, name=&#39;world&#39;): # 先定义函数 ... print(&#39;Hello, %s.&#39; % name) ... &amp;gt;&amp;gt;&amp;gt; Hello = type(&#39;Hello&#39;, (object,), dict(hello=fn)) # 创建Hello class 使用type创建新的类的语法如下： class type(name, bases, dict) return a new type object。 &amp;gt;&amp;gt;&amp;gt; x=type(&#39;y&#39;, (object,), dict(a=1)) &amp;gt;&amp;gt;&amp;gt; x &amp;lt;class &#39;__main__.y&amp;gt; &amp;gt;&amp;gt;&amp;gt; x() &amp;lt;__main__.y object at 0x021FD210 &amp;gt;&amp;gt;&amp;gt; h=x() &amp;gt;&amp;gt;&amp;gt; h &amp;lt;__main__.y object at 0x021FD270 &amp;gt;&amp;gt;&amp;gt; y Traceback (most recent call last): File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt; NameError: name &#39;y&#39; is not defined &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; class x_1: ... pass ... &amp;gt;&amp;gt;&amp;gt; x_1 &amp;lt;class __main__.x_1 at 0x01D1EA40&amp;gt; &amp;gt;&amp;gt;&amp;gt; h_1=x_1() &amp;gt;&amp;gt;&amp;gt; h_1 &amp;lt;__main__.x_1 instance at 0x01D68328 &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; class x_1(object): ... pass ... &amp;gt;&amp;gt;&amp;gt; x_1 &amp;lt;class &#39;__main__.x_1&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; x_1() &amp;lt;__main__.x_1 object at 0x021FD1D0&amp;gt; &amp;gt;&amp;gt;&amp;gt; 由上可知，用type()和class创建类，type()中的y和class中的x_1是等同的，都是类名，赋值给__name__属性，但是type()创建的类却不能用类名y来创建实例，只能用type的返回值x来创建h = x()，为了避免混淆，我们最好把类名和type()的返回值设置成相同的字符串，如X=type(&#39;X&#39;, (object, ), dict(a=1))。 MetaClass 参考资料：深刻理解python中的元类（没看）。MetaClass 翻译过来就是元类，什么叫元？元者，源也，根也，本也！所谓元类就是能够创造出其他类的类，即原始类。下面看看怎么用 MetaClass 来动态创建类。 # -*- coding:utf-8 -*- # metaclass是创建类，所以必须从 &#39;type&#39; 类派生 class ListMetaclass(type): def __new__(cls, name, bases, attrs): print &#39;attrs:%s\n&#39; %attrs attrs[&#39;add&#39;] = lambda self, value: self.append(value) print &#39;cls: %s, \n name: %s, \n bases: %s, \n attrs: %s\n&#39; %(cls, name, bases, attrs) print type.__new__(cls, name, bases, attrs) return type.__new__(cls, name, bases, attrs) #用type申请内存空间(并返回用metaclass创建好的MyList对象) class MyList(list): __metaclass__ = ListMetaclass #指示使用ListMetaclass来定制类 结果： E:\shizhan&amp;gt;python ok5.py attrs:{&#39;__module__&#39;: &#39;__main__&#39;, &#39;__metaclass__&#39;: &amp;lt;class &#39;__main__.ListMetaclass&#39;&amp;gt;} cls: &amp;lt;class &#39;__main__.ListMetaclass&#39;&amp;gt;, name: Mylist, bases: (&amp;lt;type &#39;list&#39;&amp;gt;,), attrs: {&#39;__module__&#39;: &#39;__main__&#39;, &#39;__metaclass__&#39;: &amp;lt;class &#39;__main__.ListMetaclass&#39;&amp;gt;, &#39;add&#39;: &amp;lt;function &amp;lt;lambda&amp;gt; at 0x020401B0&amp;gt;} &amp;lt;class &#39;__main__.MyList&#39;&amp;gt; 值得注意的是，attrs是dict类型的数据，它存储着 MetaClass 调用者的所有属性和方法，其中属性包括类属性和实例的属性，所以我们可以向用 Metaclass 创建的类中添加方法 ，这实际上就是给attrs添加key - value值对。给attrs赋值时要用dict的方式attrs[ &#39; &#39; ] = ?。 关于类属性和实例属性：直接在class中定义的是类属性： class Student(object): name = &#39;Student&#39; 实例属性必须通过实例来绑定，比如self.name = &#39;xxx&#39;。来测试一下： &amp;gt;&amp;gt;&amp;gt; # 创建实例s： &amp;gt;&amp;gt;&amp;gt; s = Student() &amp;gt;&amp;gt;&amp;gt; # 打印name属性，因为实例并没有name属性，所以会继续查找class的name属性： &amp;gt;&amp;gt;&amp;gt; print(s.name) Student &amp;gt;&amp;gt;&amp;gt; # 这和调用Student.name是一样的： &amp;gt;&amp;gt;&amp;gt; print(Student.name) Student &amp;gt;&amp;gt;&amp;gt; # 给实例绑定name属性： &amp;gt;&amp;gt;&amp;gt; s.name = &#39;Michael&#39; &amp;gt;&amp;gt;&amp;gt; # 由于实例属性优先级比类属性高，因此，它会屏蔽掉类的name属性： &amp;gt;&amp;gt;&amp;gt; print(s.name) Michael &amp;gt;&amp;gt;&amp;gt; # 但是类属性并未消失，用Student.name仍然可以访问： &amp;gt;&amp;gt;&amp;gt; print(Student.name) Student &amp;gt;&amp;gt;&amp;gt; # 如果删除实例的name属性： &amp;gt;&amp;gt;&amp;gt; del s.name &amp;gt;&amp;gt;&amp;gt; # 再次调用s.name，由于实例的name属性没有找到，类的name属性就显示出来了： &amp;gt;&amp;gt;&amp;gt; print(s.name) Student 因此，在编写程序的时候，千万不要把实例属性和类属性使用相同的名字(如果碰到这种情况)。 创建一个实例时调用了其类的构造器__new__()，当解释器看到类声明时，就调用type的构造器__new__()，把类声明中写的东西作为name, bases, attrs参数传递进去，然后__new__()就申请内存创建了一个类的对象（实例），并返回它。cls参数是 metaclass 简称，指它调用的metaclass。 用type()创建一个类，是调用了type()的构造器__new__()，从而创建了一个type的新实例，也就是一个新的类。如果解释器在类声明中读到了__metaclass__属性，那就会用这个自定义元类的构造器__new__()来造这个类，如果没有，那就会用type的构造器__new__()来造这个类，也就是说type就是缺省元类。 有了自定义元类，就可以使真实造出来的类和类声明中写的有所不同，只要覆盖元类的构造器就行了，当然，它需要调用type的构造器来申请空间。 这样，我们就用 Metaclass 创建了list的派生类MyList，并添加了一个方法add()。关于lambda的用法疑问，可以用下面例子解释： &amp;gt;&amp;gt;&amp;gt;x = lambda a,b : a*b &amp;gt;&amp;gt;&amp;gt;x(2,3) 6 看来调用lambda函数就是用括号里穿参数的形式。这样就可以解释下面调用add()方法的疑问 &amp;gt;&amp;gt;&amp;gt;from ok5 import MyList &amp;gt;&amp;gt;&amp;gt;L=MyList() &amp;gt;&amp;gt;&amp;gt;L.add(1) # self 就是 L，调用add，就是调用 lambda 函数 &amp;gt;&amp;gt;&amp;gt;L [1] 调用add()方法，就是调用 lambda 函数。 定制类：__str__用作自定义打印实例 我们先定义一个Student类，打印一个实例： &amp;gt;&amp;gt;&amp;gt; class Student(object): ... def __init__(self, name): ... self.name = name ... &amp;gt;&amp;gt;&amp;gt; print Student(&#39;Michael&#39;) &amp;lt;__main__.Student object at 0x109afb190&amp;gt; 打印出一堆&amp;lt;__main__.Student object at 0x109afb190&amp;gt;，不好看。怎么才能打印得好看呢？只需要定义好__str__()方法，返回一个好看的字符串就可以了： &amp;gt;&amp;gt;&amp;gt; class Student(object): ... def __init__(self, name): ... self.name = name ... def __str__(self): ... return &#39;Student object (name: %s)&#39; % self.name ... &amp;gt;&amp;gt;&amp;gt; print Student(&#39;Michael&#39;) Student object (name: Michael) 这样打印出来的实例，不但好看，而且容易看出实例内部重要的数据。 ORM 为什么 ORM 一定要用 Metaclass 来创建？很简单，假设我们已经写好了 ORM，用户会怎么调用呢？ class User(Model): id_key = IntegerField(&#39;uid&#39;) name_key = StringField(&#39;username&#39;) email_key = StringField(&#39;email&#39;) password_key = StringField(&#39;password&#39;) 这就是用户调用 ORM 的方式，但是我们并不知道用户会用哪些类属性，在这里的类属性是id_key、name_key、email_key、password_key，但是下一次可能用户定义的就是id、address、cellphone、name、email、password等，所以我们必须把用户创建的类属性动态加入，这就要用到 Metaclass，总结一句话，Metaclass就是给动态创建的类【添加属性和方法的】。拿上面的例子来说， attrs: {&#39;id_key&#39;: &amp;lt;__main__.IntegerField object at 0x020ED8F0&amp;gt;, &#39;__module__&#39;: &#39;__main__&#39;, &#39;password_key&#39;: &amp;lt;__main__.StringField object at 0x020ED950&amp;gt;, &#39;email_key&#39;: &amp;lt;__main__.StringField object at 0x020ED930&amp;gt;, &#39;name_key&#39;: &amp;lt;__main__.StringField object at 0x020ED910&amp;gt;} 在User类调用的 MetaClass 中打印attrs得到上面的结果，可见在User中定义的类属性变成了其所调用的 MetaClass 的attrs的key，类属性的值则作为attrs的value，类属性的值可以是任何类型的数据，这里是IntergetField或StringField类的实例。 另外，利用上面的__str__()方法，我们在打印类属性的值由于其是实例(object)类型的数据时，所以我们可以自己选择打印格式，而不是类似&amp;lt;__main__.IntegerField object at 0x0220D8D0&amp;gt;这样冷冰冰的东西。（当然不设置__str__()也并没有影响） ORM 代码： # -*- coding: utf-8 -*- &#39; Simple ORM using metaclass &#39; class Field(object): def __init__(self, name, column_type): self.name = name self.column_type = column_type def __str__(self): #定义__str__()方法的目的：一旦打印Field实例就调用 return &#39;&amp;lt;%s:%s&amp;gt;&#39; % (self.__class__.__name__, self.name) class StringField(Field): def __init__(self, name): super(StringField, self).__init__(name, &#39;varchar(100)&#39;) class IntegerField(Field): def __init__(self, name): super(IntegerField, self).__init__(name, &#39;bigint&#39;) class ModelMetaclass(type): def __new__(cls, name, bases, attrs): if name==&#39;Model&#39;: return type.__new__(cls, name, bases, attrs) print(&#39;Found model: %s&#39; % name) print &#39;-------------------------------------------------&#39; print &#39;cls: %s, \n name: %s, \n bases: %s, \n attrs: %s\n&#39; %(cls, name, bases, attrs) print &#39;-------------------------------------------------&#39; mappings = dict() for key, value in attrs.iteritems(): if isinstance(value, Field): print(&#39;Found mapping: %s ==&amp;gt; %s&#39; % (key, value)) #打印v时调用__str__()方法 mappings[key] = value for key in mappings.iterkeys(): attrs.pop(key) attrs[&#39;__mappings__&#39;] = mappings &#39;&#39;&#39; 以上那么做的目的很明显，就是为了让attrs的结构更清晰，让所有的类属性都集中到mappings dict中,再赋值给attrs的__mappings__ 这样类属性就整合为一个整体作为attrs的__mappings__的 value &#39;&#39;&#39; attrs[&#39;__table__&#39;] = name # 假设表名和类名一致 return type.__new__(cls, name, bases, attrs) class Model(dict): __metaclass__ = ModelMetaclass #User和Model都会调用ModelMetaclass，不过Model调用ModelMetaclass会什么都不做就返回(if name==&#39;Model&#39;) def __init__(self, **kw): super(Model, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r&amp;quot;&#39;Model&#39; object has no attribute &#39;%s&#39;&amp;quot; % key) def __setattr__(self, key, value): self[key] = value def save(self): fields = [] params = [] args = [] &#39;&#39;&#39; print &#39;-------------------------------------------------&#39; print &#39;__mappings__:%s,\n&#39; %self.__mappings__ print &#39;-------------------------------------------------&#39; &#39;&#39;&#39; #self是u，u是User的实例,但是 u 没有属性__mappings__，__mappings__是类的属性 #u继承类的属性，所以这里的self.__mappings__是User.__mappings__ for k, v in self.__mappings__.iteritems(): #print k,v fields.append(v.name) #v本身就是实例(object)，所以可以有自己的属性 params.append(&#39;?&#39;) args.append(self[k]) sql = &#39;insert into %s (%s) values (%s)&#39; % (self.__table__, &#39;,&#39;.join(fields), &#39;,&#39;.join(params)) print(&#39;SQL: %s&#39; % sql) print(&#39;ARGS: %s&#39; % str(args)) 测试代码： class User(Model): id_key = IntegerField(&#39;uid&#39;) #括号里的是数据库表的列名，因为 fields.append(v.name)，而v.name就是uid，uid等作为sql的 values name_key = StringField(&#39;username&#39;) email_key = StringField(&#39;email&#39;) password_key = StringField(&#39;password&#39;) &#39;&#39;&#39; print &#39;-------------------------------------------------&#39; print &#39;User.__mappings__:%s,\n User.__table__:%s,\n User.__module__:%s\n&#39; %(User.__mappings__,User.__table__,User.__module__) print &#39;-------------------------------------------------&#39; &#39;&#39;&#39; ######################################################################################### u = User(id_key=12345, name_key=&#39;zhuma&#39;, email_key=&#39;test@orm.org&#39;, password_key=&#39;zhangpan&#39;) #括号里的全部是**kw 关键字参数(dict), self 是 u . id, name, email, password变成了实例u的属性，而非User的类属性 #根据User的类属性key，找到实例u的对应的value(u的key一定要和User类属性的名称保持一致). &#39;&#39;&#39; print &#39;-------------------------------------------------&#39; print &#39;u.__mappings__:%s,\n u.__table__:%s,\n u.__module__:%s\n&#39; %(u.__mappings__,u.__table__,u.__module__) print &#39;-------------------------------------------------&#39; &#39;&#39;&#39; u.save() u=User()会调用User的父类Model初始化参数，而Model的父类又是dict，所以u = User(id_key=12345, name_key=&#39;Michael&#39;, email_key=&#39;test@orm.org&#39;, password_key=&#39;my-pwd&#39;)括号里的参数会被dict初始化，同时Model又定义了__getattr__()和__setattr__()方法，所以初始化后可以用u[id_key]和u.id_key来访问u的属性。 注意很重要的一点：save()方法中 args.append(self[k]) self是u，而k是User定义的id_key、name_key、email_key、password_key，所以要想用self [k]找到u对应的value，就必须u的key一定要和User类属性(即上面的k)的名称保持一致。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[NumPy、SciPy 和 Pandas z]]></title>
    	<url>/tech/2015/05/05/numpy-scipy-pandas/</url>
		<content type="text"><![CDATA[[NumPy、SciPy 和 Pandas 原文地址：http://blog.csdn.net/u012654283/article/details/4551 NumPy 和 SciPy 都是开源的函数库，由于 Python 中是没有数组这个数据类型的，所以涉及到数学计算时会变的非常吃力。Numpy 和 Scipy 正好帮我们解决了这个难题。 Numpy 是 Python 基于 Python 语言的一个数据处理的基础函数库，里面主要是一些矩阵的运算等，重在数值计算； Scipy 则是基于 Numpy，提供了一个在 Python 中做科学计算的工具集，也就是说它是更上一个层次的库，主要包含一下模块： statistics optimization numerical integration linear algebra Fourier transforms signal processing image processing ODE solvers special functions Pandas 是基于 NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。Pandas 提供了大量能使我们快速便捷地处理数据的函数和方法。 类似于 Numpy 的核心是 ndarray，Pandas 也是围绕着 Series 和 DataFrame 两个核心数据结构展开的 。Series和 DataFrame 分别对应于一维的序列和二维的表结构。Pandas 约定俗成的导入方法如下： from pandas import series, DataFrame import pandas as pd 参考 Numpy 和 Scipy 的关系 Numpy 和 Scipy 的前世今生 Pandas 简介 知乎 原文地址：https://www.zhihu.com/question/38353562 Coldwings Numpy 是以矩阵为基础的数学计算模块，纯数学。 Scipy 基于 Numpy，科学计算库，有一些高阶抽象和物理模型。比方说做个傅立叶变换，这是纯数学的，用 Numpy；做个滤波器，这属于信号处理模型了，在 Scipy 里找。 Pandas 提供了一套名为 DataFrame 的数据结构，比较契合统计分析中的表结构，并且提供了计算接口，可用 Numpy 或其它方式进行计算。 李粤强 NumPy：N 维数组容器 SciPy：科学计算函数库 Pandas：表格容器]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[严刑峻法能减少犯罪吗？ - 慧航 - 专栏]]></title>
    	<url>/prof/2015/05/05/punishment-and-deterrence/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20021208 引言 这是标题党。这篇专栏其实只是想介绍今年发表在 AER 上的一 and Deterrence: Evidence from Drunk Driving。下文中涉及的资料，若无特别说明，均来自这篇文章。 从标题就可以看出，这篇文章旨在从酒驾行为入手，研究犯罪行为的事后惩戒与未来犯罪的关系。也就是本文的标题，严刑峻法，能减少犯罪吗？ 现代经济学研究犯罪行为可以追溯到加里贝克尔 1968 年 JPE 上的 Crime and Punishment: An Economic Approach. 他认为犯罪行为也可以用经济学解释：理性人权衡预期的收益和成本，从而决定要不要犯罪。这个解释很符合经济学直觉，迄今仍是经济学用以解释犯罪行为的主要框架。 从这个框架出发的话，犯罪的主要成本由被抓住的可能性和被抓之后预期可能受到的惩罚决定，也就是所谓的 enforcement 和 punishment。因而根据这一逻辑，加强执法力度，或是提高惩罚强度，都可以增加犯罪成本，从而减少未来犯罪。所以更强的 punishment，似乎理应带来更低的犯罪率。 但实证却没这么简单。经济学家不是法律制定者，不可能去调整法律来看接下来犯罪率会发生什么变化。我们只能设法去收集现有的数据，从中寻找规律。 而这样得来的数据几乎都会存在一个摆脱不了的问题：内生性。比如作为关键解释变量的惩罚严厉程度就是个内生变量，它与很多因素，特别是犯罪活动本身的严重程度相关。而严重犯罪的犯罪者，显然在很多特征上与一般人群存在差异。因而如果我们直接对比受到法律制裁严厉程度不同的犯罪行为的发生率，得到的结果就很有可能是存在问题的。用计量术语说，就是存在跟主要解释变量相关的遗漏变量，导致估计值有偏。 此外，严刑峻法还可能通过另外一条渠道影响犯罪率。它可能直接改变受惩罚者此后的犯罪率。这种影响无法由传统的加里贝克尔框架解释。显然，在传统框架下，影响犯罪行为决策的是犯罪行为的预期收益和成本，而是否受过惩罚，与这预期是无关的。不过如果我们有限理性或不完备信息的角度考虑的话，受到惩罚可能会让犯罪者对犯罪的成本有新的认识，俗话说不见棺材不落泪嘛。按照这一逻辑，更严厉的惩罚应当会减少犯罪。 不过严厉的惩罚，很有可能会给受惩罚者带来惩罚之外的影响。比如长时间的坐牢会带来人力资本的贬值，使坐牢者在出狱之后难以通过合法途径维持生存，进而提高其犯罪收益，反而提高其犯罪率。所以要知道惩罚本身对犯罪行为的影响，我们还得排除掉这一影响。 数据和方法 回到这篇文章。 就如统计学和计量经济学有什么区别？ - 经济学这个问题下几个高票答案所强调的那样，经济学家在应用统计学方法时，更关注变量间的因果性，通常并不试图用一个模型完全地描述某件事物。具体到这个研究上，就是经济学家并不试图给出犯罪率的完整决定模型，更不会预测特定情况下的犯罪率水平，而更关注一个具体的问题：对犯罪者的惩罚，有助于降低他未来再犯罪的可能性吗？ 作者用的数据是美国华盛顿州从 1999/1/1 到 2011/12/31 所有的司机血液酒精含量（BAC, Blood Alcohol Content）测试记录。在这段时间内，华盛顿州规定 BAC 达到 0.08 就被视作酒后驾车（DUI，Drive Under the Influence），而达到 0.15 则被视作醉酒驾车（Aggravated DUI）。华盛顿州对未成年人饮酒采取了所谓零容忍的政策，因而作者排除掉了这部分数据。 这里面不仅有达到酒驾或者醉酒驾驶标准的测试结果，也包含那些未达标准的测试结果。研究针对的是再犯罪的可能性，因而作者所用的被解释变量是一个标志是否再犯的虚拟变量，如果某人在第一次被测试后 4 年内都再没被查到，这个变量就被设定为 0 ，否则就被设定为 1。 如下表所示，按照华盛顿州法律规定，BAC 超标不仅会使得司机立刻面临罚款乃至牢狱之灾，也会提高他下次违法时可能会承担的成本。 因而无论是依据传统的加里贝克尔框架，还是从有限理性和不完备信息出发的判断，我们都有理由认为遭受惩罚会降低被惩罚的酒驾者未来再犯的可能性。 酒驾数据的另一个漂亮之处在于，它事后的惩处不算太严厉，大部分时候不会带来长时间的牢狱之灾。因而酒驾之后的惩罚对犯罪率的影响会相对比较干净，不容易受到其他因素的干扰。 给定数据和研究目的，这样的研究当中我们最容易想到的方法当然就是用是否遭受惩罚作为解释变量，对比那些达到酒驾标准的和未达标准的司机在未来一段时间内的犯罪情况，如果前者犯罪率显著低于后者，我们就有理由认为法律制裁减少了犯罪。 不过显然，这样的结果会因为内生性而变得有偏：在检测中BAC很低的人和很高的人可能是完全不同的，这种差异可能会出现在家庭背景、工作、偏好、文化乃至对酒精的代谢速度上，而后面这些因素显然都会影响到这人此后再犯酒驾的概率，而且无法完全被控制。于是这就又是个遗漏变量与解释变量相关导致结果有偏的故事。 所幸数据和酒驾本身的特殊性，为我们提供了另外一种方法。 司机可以决定喝不喝酒，可以决定喝多少酒，但一旦喝酒，他是无法精确控制被检查时的BAC的。也就是说，你有没有喝酒，这不是个随机事件，但你喝完酒之后在被检查时，BAC 究竟是 0.079 还是 0.08，是 0.149 还是 0.15，却基本上是个随机事件。换句话说，虽然 BAC 0.01、0.08 和 0.15 的人可能是十分不同的，但我们有理由认为 BAC 0.079 和 0.08 的人，0.149 和 0.15 的人是相似的，他们之间最大差异，只是有些人运气好，在测试时逃过一劫。而这运气本身显然与未来的行为没有关系。 而且目前的BAC测试都由设备自动完成，即使是警方也几乎没有上下其手的空间，这也是一个我们应当认为那些BAC在越界边缘的人是否越界只是运气问题的理由。 因而如果 BAC 0.08 的人相比 0.079 的人在此后酒驾的概率更低，或者 BAC 0.15 的人相比 0.149 的人此后酒驾的概率更低，我们就能比较有把握地说，针对酒驾的法律制裁减少了未来的酒驾行为。 要完成这样的对比，就需要 RD（Regression Discontinuity）方法。所幸作者所获得的数据当中，不仅有是否被测试者 BAC 是否超标的记录，也包含了确切的 BAC 数字；不仅有那些 BAC 超标者的数据，也有未超标者的数据。因而这一方法的运用就成为可能。 主要结果 RD 回归的结果在上图中一目了然。在 0.08 那条竖线附近，再犯罪率的拟合线出现了明显的不连续下降。相比 BAC 0.079 的人，因 BAC 0.08 而遭受制裁者在此后四年中再犯的概率降低了 2%。 我们仍然无法区分犯罪率的降低是由预期犯罪成本提高导致的，还是只是制裁本身的结果，或者两者兼而有之。我们无法直接回答这一问题，不过，数据仍然提供了一条间接的观测渠道。 华盛顿州法律会加大对累犯者的惩罚力度，但在判定时，它并不区分累犯者上次违法究竟是酒驾（DUI）还是醉驾（Aggravated DUI）。换句话说，从 BAC 0.079 到 0.08 的跨越会带来预期犯罪成本的上升，但从 BAC 0.149 到 0.15 的跨越不会。因而如果这些人的未来犯罪率也会下降，那就意味着传统的加里贝克尔框架在解释犯罪率时，还是不够的。 同样在上图中，我们可以看到，在 0.15 那条竖线附近，虽然再犯罪率的拟合线仍然出现了明显的不连续下降，这可能说明传统的加里贝克尔框架是不够的；但下降的幅度要明显小于 0.08 竖线附近，这或许说明加里贝克尔的框架仍然是重要的。 除了罚款、坐牢等一般意义上的惩罚，美国的法庭经常还会判决酒驾者接受各种戒酒疗程。如果这些措施是有效的，那酒驾者的酒精摄入而酒驾行为可能就会随之减少。不过这一机制显然无法推广到其他犯罪上，因而也不是我们最希望看到的。于是我们不免要问，排除掉这一机制影响后，惩罚措施对减少酒驾仍然有效吗？对这一质疑，作者从两个方面进行了回答。一方面，他发现在 BAC 0.15 这个门槛上，法庭要求戒酒疗程的可能性并没有显著增加，因而他认为，至少在这个门槛上酒驾行为的减少无法由戒酒疗程解释。 另一方面，酒精的摄入通常不仅与酒驾有关，还会与其他各种犯罪相关，这不难理解，喝大了难免冲动。因而如果戒酒疗程对减少酒驾是十分有效的，那它应该还会降低其他犯罪行为的犯罪率。 但如上图所示，作者发现，无论是在 0.08 还是 0.15 门槛上，其他犯罪的犯罪率都没有显著下降。 含义解读 到此为止这篇文章主要的部分已介绍完毕。这无疑是个很漂亮的研究，它用很简单的方法得到了很可靠的结论。不过，在读完这篇文章之后，我们依然要继续问，这个结果能说明严刑峻法有助于减少犯罪吗？我们应当像一些人所认为的那样，提高刑罚的严厉程度，乃至于恢复剥皮实草连坐九族来减少犯罪吗？ 很遗憾，这篇文章不能回答这样的问题。它的结果固然是漂亮而相当可靠的，但这是一个典型的所谓 Local Average Treatment Effect，在一定范围内，它很可靠，但当超出这个范围，一切就很难说了。 确切地说，这篇研究只能近乎完全可靠地说明在它的样本范围内，如果对 BAC 0.079 的人施以当前 0.08 的人所遭受的制裁，则他们未来再犯酒驾的概率会降低，我们甚至可以相当精确地估计出整体上犯罪率降低的幅度。 这个结论当然不是完全没法推广的。比如在没有其他证据的情况下，我们完全有理由认为华盛顿州之外，甚至美国之外一些制度环境差异不大的地区，对酒驾行为施以惩罚都能有效减少这一行为的出现频率。 我们也可以比较自信地说，如果降低酒驾标准，比如将 BAC 0.05 以上的都视为酒驾，那酒驾行为会相应减少。但如果要问将标准进一步降到 0.01，或是在 0.15 之上再设定一个 0.3 的超醉驾门槛，对超标的人施以更严厉的惩罚，那么酒驾行为是否还会继续减少，我们可能就没那么自信了。 单从上面的 Fig.3 结果图上，我们就能看到那些BAC 含量特别高以至于受到了最严厉惩罚的人，其事后的再犯罪率并不比 BAC 最低的人低；再对比 Panel A 和 Panel C 的纵轴，我们还会发现，累犯者平均再犯可能性要显著高于一般人。对这些人，惩罚力度再加大，效果多半也不会太显著吧。 所以如果要将这结果继续推广，推广到其他犯罪行为，其他人群，其他地区，那我们就务必要更加谨慎了。 参考文献 Hansen, Benjamin, Punishment and Deterrence: Evidence from Drunk Driving, American Economic Review, 2015, 105(4): 1581-1617. 枣严：在 repeat offenders 里惩罚的“边际效益”递减真是厉害，0.15 两边的拟合线几乎就是连续的…感觉这些人貌似就破罐子破摔了…… 姬行川：这也是传统反重刑刑法思想的主要论点，对于潜在的轻罪犯施以过重的刑法，可能会使这些轻罪犯的犯罪行为变得更恶劣，因为既然已经犯了，大罪小罪惩罚都一样，那么不如捞一笔大的爽爽。这个论点的政策结果就是刑法的罪刑适度原则。 Charles.H：实验经济学前景广啊！ SlowMover：这个研究是 observational 的，所以你看他只识别出了一个局部的影响。类似于犯罪这样的问题一般没法做实验。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[陈志武：清代妻妾价格研究——传统社会里女性如何被用作避险资产？ - 慧航 - 专栏]]></title>
    	<url>/prof/2015/04/28/wives-qing/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20015259 以下内容可能造成部分女权主义者感到不适，如果不能平静的读 这里讲的是一个悲惨的故事。 传统农业社会极易受到诸如洪涝、干旱等灾害的影响，这些灾难经常造成大量的死亡，因而对于个人和家庭来讲，如何规避这种风险，是关乎生存的命题。 看过《1942》的朋友应该对其中卖儿鬻女的场景记忆犹新。那么，在这种极端情况下，子女、妻妾是不是也变成了一种可以避险的资产用于交易呢？ 耶鲁大学教授陈志武等人的文章：《清代妻妾价格研究 ——传统社会里女性如何被用作避险资产?》1讨论了这个问题。由于是中文文章，所以这里只做简要介绍，感兴趣的读者请参考原文。 传统社会由于金融系统不完善，所以面对巨大的风险，必然需要一些其他非金融的手段来规避风险。这篇文章讨论了在灾荒发生时通过嫁卖妻女、用变现所得缓冲生存挑战。 现代人可能难以接受把妻妾、女儿当避险工具用的行为，在清代的法律也明文禁止妻妾买卖(岸本美绪,2001)1，但现实中，不仅中国长期有买卖妻妾、租妻、典妻的传统(潘光旦,2010;夏明方,2004;赵晓华,2007)，而且印度、泰国、非洲、美 洲、英国、法国、德国及其它欧洲国家也都曾经有买卖妻子的习俗(Gray,1960;Goldschmidt, 1974;Menefee,1981;汤姆逊,2002)，就像其它商品和资产一样把妻子拍卖交易。早在公元前 1700 年的古巴比伦《汉谟拉比法典》中，就制定“为了还丈夫欠债，可以卖妻”(Levine, 1997-2001)，而近到 2009 年的印度，还因为长年旱灾、欠收而负债累累被迫卖妻(Sidner, 2009)。潘光旦(2010)甚至写道：“荒年来了,家里的老辈便向全家打量一过，最后便决定 说，要是媳妇中间最年轻貌美的一个和聪明伶俐的十一岁的小姑娘肯出卖的话，得来的代价就可以养活其余的大小口子，可以敷衍过灾荒的时期。”这种买卖妻妾以化解生存风险的“自我保险”行为3，成为男权家长制传统中国抗争粮食风险的一种手段(夏明方,2004)。在这里，女性被用作灾荒时期寻求活路过程中可变现的资产。 由于灾荒时期死亡率很高，因而直接观测灾荒时期买卖妻妾的数量非常不准确。为了解决这个问题，作者转而使用妻妾的价格作为被解释变量。 这篇文章使用的数据： 本文所使用的清代刑科题本档案来自中国第一历史档案馆(简称“一史馆”)。这些档案记录了中国各地从 1736 年到 1895 年间的严重刑事案件，通常是命案。根据一史馆电子检索系统的分类，刑科题本分为“内阁刑科题本土地债务类” 和“内阁刑科题本婚姻奸情类” 两类。本文采用“内阁刑科题本婚姻奸情类”涉及婚嫁、买婚卖婚、性出轨等以及日常家庭纠纷引发的命案。题本一般记录了案件在各级政府的审判过程，包括谁在何时报案、谁接案、 仵作验尸过程和结果，以及证人和嫌疑人等等的户籍、生活地、性别、年龄、职业、家庭构成及情况、与死者的关系等。关于刑科题本档案的详细介绍，参见步德茂(2008),倪道善 (1992),黄才庚(1987),中国第一历史档案馆(1985)。为了表述方便，下文将本文所收集到的题本数据简称为“妻妾命案数据”或“妻妾数据”。 为了检验（死亡的）风险对于妻妾价格的影响，还需要风险的度量。这篇文章使用了粮食价格作为风险的度量，即：当粮食价格变高时，人们面临更大的粮食短缺的风险，因而更有可能卖妻以求生存。 为了解决内生性问题，作者还是用了气候（降水）作为粮食价格的工具变量。 首先看 OLS 的回归结果： 可以发现，在所有的回归中，粮价对妻妾价格都有显著的负向影响，即：粮价越高-&amp;gt;风险越高-&amp;gt;卖妻妾越多-&amp;gt;妻妾价格越低。 以上的回归包括了婚嫁、寡妇再嫁以及妻妾买卖等所有样本。但是妻妾买卖是“风险资产”的最直接证据，因而作者也用妻妾买卖的字样本作了同样但回归： 可以发现，粮价对妻妾价格的影响更大了。 最后，作者使用了干旱作为工具变量： 可以看到，工具变量回归的结果在数值大小上更大，为之前估计结果的三倍。考虑到工具变量估计结果实际上反映的是，由于干旱导致的粮价上涨，进而导致妻妾价格的变化，在极端灾害的情况下，这样的结果仍然是十分符合直觉的。 改之理zcw：哈哈。我猜他写这个文章目的是证明金融让人更有尊严了，更像个人而不是一个工具了，例如他提到古代没有金融工具所以养儿防老，这样亲情就成了一种金融工具，现代人则能享受纯粹的亲情。但是由于这个题目太过惊悚，很容易让人作出相反的联想，所以天然就带着抵触。 李一鸣：最后一张表里面第一阶段挺弱啊 慧航：也不算很弱吧，系数显著而且 R2 不低，看的过去的。 金浩峰：个人认同最后一位提问的同学的观点，即妻妾价格随着粮价变动不能证明妻妾是避险资产。理由有三。其一，粮价不仅是灾害的度量，同时也是物价水平的度量（粮食是古代中国重要的交易媒介与价值储藏）：粮价上涨，则用粮价衡量的妻妾价格下降，也就是内生性问题。其二，妻妾价格与灾害等风险存在负相关反映了妻妾的供需波动，但并不意味着妻妾就是避险资产，事实上陈教授在讲座中并未对避险资产给出定义。其三，将妻妾作为避险资产需要主客观的一致。陈教授只证明了存在买卖妻妾的客观交易，没有证明妻妾所有人在当初娶妻妾时主观上就是将妻妾作为避险资产来看待的，较之“养儿防老”，“娶妻避险”更缺乏心理动机。 链接：http://www.econ.sdu.edu.cn/advance/uploadfile/2015010316441981.pdf.↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python 中 __init__ 和 __call__ 的区别 z]]></title>
    	<url>/tech/2015/04/19/python-call/</url>
		<content type="text"><![CDATA[[原文地址：https://stackoverflow.com/questions/9663562/what-is-difference-be 背景：廖雪峰的教程中关于 Class 的__call__()方法的讲解简略，读过以后无法理解该方法设计的目的，事实上大部分中文站点上的分析对这个方法的交待都不够透彻。 结论：__init__()和__call__()的区别就在于前者用于创建一个类的实例，而后者使得该实例变得像函数一样 callable，并且这种处理不会影响对象的生命周期，即__call__()不影响类的构造和析构过程，只影响介于构造和析构之间的中间过程时对象内部某些成员的具体状态。 Python 可调用对象__call__方法的用法分析：http://blog.csdn.net/networm3/article/details/8645185 Python 2.7 中的__call__方法在实际环境中有什么实用的地方？(cached_property, Pipline)：https://segmentfault.com/q/1010000006113393?_ea=1020343 Paolo Maresca ***** 在 Python 中，函数是一类对象，也就是说，Python 的函数（A）或者方法可以接收某个函数引用（函数名 B-Name）作为参数，并且在 A 的内部可以执行 B。 类的实例——对象可以被当成函数进行处理，即可以作为某个函数或方法的参数，要让对象成为某个函数或方法的参数，就需要为对象对应的类实现一个特定的__call__()方法。 def __call__(self, [args ...])接受若干参数，现在假定$x$是类$X$的一个实例，那么调用x.__call__(1, 2)与调用x(1,2)等同，这个实例本身在这里相当于一个函数。 Python 中的__init__()实际上就是其它语言中的构造函数（__del__()对应于其它语言的析构函数），而__init__()和__call__()的区别就在于前者用于创建一个类的实例，而后者使得该实例变得像函数一样 callable，并且这种处理不会影响对象的生命周期，即__call__()不影响类的构造和析构过程，只影响介于构造和析构之间的中间过程时对象内部某些成员的具体状态。 class Stuff(object): def __init__(self, x, y, range): super(Stuff, self).__init__() self.x = x self.y = y self.range = range def __call__(self, x, y): self.x = x self.y = y print &#39;__call__ with (%d,%d)&#39; % (self.x, self.y) def __del__(self): del self.x del self.y del self.range &amp;gt;&amp;gt;&amp;gt; s = Stuff(1, 2, 3) &amp;gt;&amp;gt;&amp;gt; s(7, 8) __call__ with (7,8) noisy **** &amp;gt;&amp;gt;&amp;gt; class A: ... def __init__(self): ... print &amp;quot;init&amp;quot; ... ... def __call__(self): ... print &amp;quot;call&amp;quot; ... &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; A() init &amp;gt;&amp;gt;&amp;gt; A()() init call Cat Plus Plus **** 下面的代码接收参数并初始化类的某个实例： class foo: def __init__(self, a, b, c): # ... x = foo(1, 2, 3) # __init__ 下面的代码使得类的实例变得可以像函数一样进行调用或者作为其它函数（方法）的参数： class foo: def __call__(self, a, b, c): # ... x = foo() x(1, 2, 3) # __call__ 简言之，初始化类的实例时调用__init__()方法，像调用函数一样调用实例时用__call__()方法。 mattkang (先 new，再 init) 这部分的解释比较乱，只有一点是需要注意的，就是：先__new__()，而后__init__()。 根据 mattkang 的解释，__new__()、__init__()、__del__()、和__call__()四者之间的关系比较密切，因此有必要先梳理一下四者的关系。 __new__(cls, )： 对象的创建，是一个静态方法，第一个参数是cls。(想想也是，不可能是self，对象还没创建，哪来的self) __init__(self, )： 对象的初始化， 是一个实例方法，第一个参数是self。 __call__()： 对象 callable，注意不是类，是对象。 先有创建，才有初始化。即先__new__()，而后__init__()。 class Bar(object): pass class Foo(object): def __new__(cls, *args, **kwargs): return Bar() print Foo() &amp;lt;__console__.Bar object at 0x02066F70&amp;gt; 可以看到，输出来是一个Bar对象。__new__()方法在类定义中不是必须写的，如果没定义，默认会调用object.__new__()去创建一个对象。如果定义了，就是override，可以 custom 创建对象的行为。 聪明的读者可能想到，既然__new__()可以 custom 对象的创建，那我在这里做一下手脚，每次创建对象都返回同一个，那不就是单例模式了吗？没错，就是这样。可以观摩飘逸的python - 单例模式乱弹。定义单例模式时，因为自定义的__new__()重载了父类的__new__()，所以要自己显式调用父类的__new__()，即object.__new__(cls, *args, **kwargs)，或者用super()，不然就不是 extend 原来的实例了，而是替换原来的实例。 class Foo(object): def __call__(self): pass f = Foo() # 类 Foo 可 call f() # 对象 f 可 call 总结，在 Python 中，类的行为就是这样，__new__()、__init__()、__call__()等方法不是必须写的，会默认调用，如果自己定义了，就是 override，可以 custom。既然 override 了，通常也会显式调用进行补偿以达到 extend 的目的。这也是为什么会出现“明明定义def _init__(self, *args, **kwargs)，对象怎么不进行初始化”这种看起来诡异的行为。事实上，这里_init__()少写了个下划线，因为__init__()不是必须写的，所以这里不会报错，而是当做一个新的方法_init__()，换言之，Python 根本就不会调用_init__()进行类的初始化，而是调用了默认的__init__()来完成这个过程。 Mudit Verma 在 Python 中，__init__()方法类似于其它语言中的构造函数1，该方法在创建对象时由类的__new__()方法调用，用于初始化一个实例。 __call__()方法使得类的实例变得callable，那什么时候可能会用到呢？考虑如下的场景：已经创建了类的实例——某个对象，现在需要在不销毁的前提下对这个已经创建好的对象进行重新定义，就需要用到__call()__方法。 不理解：这里的重定义(redefine)与修改对象的某个属性有什么区别？ __del__()则类似于析构函数。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[古时欧洲好像很强悍，四处征讨却从未踏向中国？ z]]></title>
    	<url>/arts/2015/04/10/ancient-europe/</url>
		<content type="text"><![CDATA[[原贴地址：https://www.zhihu.com/question/29437029 记得看了不少欧洲史诗电影，感觉欧洲在公元以前及以后早期很强悍； 历史上好多横跨亚欧非的超级帝国建立，应该很强悍； 中国数次陷入内部纷争，很混乱，内耗如此之大； 我知道我国古时候很强大，但我觉得这个理由不足以说服； 希望熟读历史的智者能较完整全面的解惑； 在此先行拜谢。 jo jo 远征有多难？第一次十字军东征就是一个缩影，道尽远征各种苦。 在正规十字军出发之前，有一支山寨十字军率先进入他们要援救的对象——东罗马的地盘。由于严重缺乏补给，平民十字军开始沿途劫掠，最后攻占并洗劫了贝尔格莱德，然后东罗马军队过来讨伐，于是所谓的异教徒一个都还没碰到，援助者和被援助者先大打出手，血流成河。 而正宗十字军虽然尽其可能带足了粮草和金钱，但是在进入了安纳托利亚后，这些补给也全部耗尽。他们一路忍饥挨饿到了安条克城下，然后发生严重的饥荒，导致了大量的减员，饿死的比打死还要多。 这就是远征第一难，后勤永远跟不上。 接下来山寨十字军刚进入安纳托利亚就被突厥人设伏全歼，1 万多人暴尸荒野。而正宗十字军一路奋战至安条克，和人数占优补给良好的穆斯林军队血战连场，目的地耶路撒冷还没个影子，已经损失过半。 这就是远征第二难，到达目的地之前，先要清理无数拦路虎，而且谁清理谁还不知道。 十字军在无数的战斗中发现，自己那套为中西欧洲多山多水多林地形量身打造的军事体系并不太适合西亚平原荒原地形。重骑无脑猪突由于地形过于广阔很容易被穆斯林轻骑兵躲过并受到侧击，装备也过于笨重难捱炎炎烈日，必须改良战术装备才能提高胜率。 这就是远征第三难，被迫在不熟悉的地形使用不熟悉的战术去对抗在本地玩了千百年的对手。 当然，西欧的重骑兵战斗力惊人毋庸置疑，在整个东征期间无数次出现几百重骑兵向几千上万穆斯林骑兵冲锋的场景，大胜也不少。爆表的战斗力加上爆表的好运气，十字军如愿以偿攻占了耶路撒冷。但是大部分骑士和士兵在杀完人磕完头后就心满意足回老家吹牛逼去了。军队主力一下子散了大半，剩下几百骑兵上千步兵顿时陷入了穆斯林的汪洋大海中。 这就是远征第四难，大部分人出来混都是简单地想开个眼，捞一把就心满意足了。像亚历山大这种不计成本的偏执狂少之又少，再有才的老大，小弟想家不干了照样干瞪眼。 虽然摊子比较烂，好在历代耶路撒冷国王都还算聪明努力爱上进，尤其鲍德温一世四世都很能打，手下的重骑兵也很能打，遇到危险了就玩几百骑兵猛冲几千上万穆斯林骑兵的老套路，勉勉强强也活了下来。但是新的问题出现了，西亚的基督徒就这么点，合格能打更是凤毛麟角，精兵强将死一个少一个，欧洲的后援又少又不靠谱。而对手穆斯林有着无穷无尽的人力资源，输了喘口气分分秒秒卷土重来，从穆斯林角度，应了爱尔兰共和军的一句谚语：你们必须每次都走运才行，而我们只要走运一次就够了。所以基督徒在哈丁一个惨败，王国立马衰败；拉夫比又一个惨败，王国立马垂死，赢过多少次都没用。 这就是远征第五难：对手主场作战，后援团永远比你强大一百倍，要么赢要么死。 所以古代欧亚互爆基本无法打进对方的核心地区，而中国位于旧世界尽头那就更不可能了。 罗夏 建议把那些欧洲史诗电影丢在一边，虽然我想破头也没想到哪些又算史诗还拍了电影的，然后去图书馆租本科普版的世界上下五千年来读一读，先搞清楚罗马人、希腊人、日耳曼人、维京人、斯拉夫人的区别，啥是查理曼帝国，啥是东西罗马帝国，啥是奥斯曼土耳其帝国，分清楚凯撒、屋大维、狮心王查理、彼得大帝等著名君王，之后再看这个问题下面的各个答友的回答就不会一头雾水了。 地跨亚非欧三大洲的帝国有罗马帝国、马其顿帝国、阿拉伯帝国，印象中就这仨，如果我漏下了请提醒我。这三个国家的领土鼎盛时期也未必比同期的中国大，更别说元。 欧洲纷争时期远比中国长，统一个小小的亚平宁半岛都如此吃力，一个比利时还分德语区。 中国古代确实很强大，这个谁不服都不行，但是与欧洲之间的波斯、阿拉伯、印度、巴比伦、东罗马帝国没一个是省油的灯，十字军东征是想一直打到东方的，但是到土耳其就被人打尿了。 如果有空建议你去中国西北旅游一趟，见识下中国有多大，汽车跑在笔直的国道上都十分漫长，如果长途跋涉那简直是噩梦，游牧的蒙古人突厥人还有可能，奴隶社会和封建社会的欧洲都别想。当年唐朝阿拉伯灭波斯，波斯王子向唐玄宗求助，唐派了远征军援助，但是失败了，这仗没法打，现在打都难，补给根本跟不上。 近代中国被列强侵略主要是从海上，在有苏伊士运河之前，欧洲人想从海上打到中国，简直跟穿越伟大航路一样艰难，更别说船还不如中国的结实。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Bootstrap 的一些注意事项 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/04/08/bootstrap/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19996637 自从 bootstrap 被发明出来，就受到来统计学家、计量经济学家的极大关 当然，在计量经济学领域，这个方法主要用在假设检验上，实际应用中做 bias reduction 的至少我没怎么见过。 但是由于对统计理论的不熟悉，有些人使用 bootstrap 的方法其实是错的或者至少是不太好的，而自己也没有察觉。最近刚好在做关于 bootstrap 的东西，之前我也不是很懂，做了这个项目之后才发现自己之前的一些理解是错的。姑且做一些小的总结，也希望能吸引各位计量经济学、统计学大神深入讨论。 Bootstrap 的思想是，我们已经得到了从未知总体\(F_0\)中抽样的样本，假设为\(s\)，那么为了获得更好的估计或者进行假设检验，我们可以把这个样本\(s\)作为总体，在\(s\)的分布函数\(F_1\)里面进行抽样。由于\(s\)的经验分布函数（EDF）的性质，在\(F_1\)中抽样即在\(s\)中有放回的抽样。Peter Hall 在他的著作《The bootstrap and Edgeworth Expansion》中把 bootstrap 比作俄罗斯套娃，我觉着再形象不过。 Peter Hall 书中略恐怖的俄罗斯套娃，还是我找的图可爱 但是在实际使用中，有一些细节需要格外注意。 Horowitz 在 Ch52 The Bootstrap, Handbook of Econometrics 中提出了几条 bootstrap 的原则： bootstrap 的统计量一定要是 pivotal 的，也就是说，这些统计量的渐进分布不依赖于未知的总体参数； 在一个过度识别的系统，做 bootstrap 的时候一定要中心化（centering）； 不要对非参数、半参数或者不平滑的估计量使用 bootstrap。 第一条，统计量需要时 pivotal 的。比如我们做最平常的 t 检验。我们当然可以对参数\(\hat{\beta}\)进行 bootstrap，得到 bootstrap 的\(\hat{\beta}\)的分布，然而这个统计量的渐进分布是依赖于未知总体参数的（方差），因而我们需要对\(\frac{\hat{\beta}}{s.e.(\hat{\beta})}\)进行 bootstrap，这个统计量的渐进分布如果是\(\mathrm{N}(0,1)\)，那么就是 pivotal 的了。 第二条，比如我们在做 2SLS 的时候，如果使用了多于内生变量个数的工具变量，可以想象，GMM 的目标函数\(\sum_i (y_i-x_i&amp;#39;\hat{\beta}_{IV})z_i&amp;#39;(Z&amp;#39;Z)^{-1}z_i(y_i-x_i&amp;#39;\hat{\beta}_{IV})\neq0\)，所以如果我们直接使用放回抽样，因为对于原先的抽样，以上不等式成立，所以 bootstrap 抽样的 2SLS 矩条件不成立，所以 bootstrap 估计量是不一致的。解决的办法就是使用\(\hat{\beta}_{IV}\)对矩条件 recentering（详见 Handbook of Econometrics pp. 3186-3187）。 第三条，很多统计量是不能用 bootstrap 的，比如常见的非参数 kernel 回归，以及一些目标函数不是非常平滑的估计量，例如 quantile 回归、maximum score estimators 等等。 另外还有需要提醒的是精炼（refinement）。比如有些人（比如刚学 bootstrap 时候的我），以为做假设检验只需要 bootstrap 出\(s.e.(\hat{\beta})\)就可以了。但是我们发现，\(s.e.(\hat{\beta})\)不是 pivotal 的。正确的做法是对\(t=\frac{\hat{\beta}}{s.e.(\hat{\beta})}\)进行 bootstrap，记为\(\{t_1,t_2...t_m\}\)，然后找出\((1-\alpha)\)th quantile of \(\{|t_1|,|t_2|...|t_m|\}\)作为 critical value，与\(\frac{\hat{\beta}}{s.e.(\hat{\beta})}\)进行比较，如果\(t&amp;gt;t_{1-\alpha}\)则拒绝原假设。 另外 bootstrap 的抽样方法除了最简单的有放回抽样之外，还有各种其他的抽样方法，有参数的、非参数的，有不放回懂，有 bolck，有 residual-based。这些方法如果扩展起来就有点复杂了。如果是要做 test，那么不同的抽样方法会导致不同的 size 和 power。在这方面可以写一本书了，我也不是完全了解所有的抽样方法，因而在这里只做个提醒，欢迎讨论。 之前两个星期一直在做 bootstrap，但是一直是错的。昨晚翻 handbook，有一段话给了我灵感，就把正确的写出来了。所以做研究没事还是要翻 handbook 啊！ 细雨平湖：有放回抽样，目的是为弥补样本不足造成的随机性欠缺。不过从信息论角度讲，样本不足的缺陷实际上是信息含量低，bootstrap 多少遍也弥补不了这个问题。 talich：bootstrap 不建议用来做 bias reduction。Efron 当年提这个 Bootstrap 时就说用来做 Bias reduction 虽然能得到一个 unbiased estimator，但方差太大，应该比原来的 estimator 还差。 不过他自己后来搞了一个 bootstrap 635&#43;，在不少情况下做错误估计效果还不错，缺点是理论支持很弱。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Raj Chetty 谈作为科学的经济学 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/03/23/economics-science/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19724219 Yes, Economics Is a Science Published: October 20, 2013 CAMBRIDGE, Mass. — THERE’S an old lament about my profession: if you ask three economists a question, you’ll get three different answers. This saying came to mind last week, when the Nobel Memorial Prize in Economic Science was awarded to three economists, two of whom, Robert J. Shiller of Yale and Eugene F. Fama of the University of Chicago, might be seen as having conflicting views about the workings of financial markets. At first blush, Mr. Shiller’s thinking about the role of “irrational exuberance” in stock markets and housing markets appears to contradict Mr. Fama’s work showing that such markets efficiently incorporate news into prices. What kind of science, people wondered, bestows its most distinguished honor on scholars with opposing ideas? “They should make these politically balanced awards in physics, chemistry and medicine, too,” the Duke sociologist Kieran Healy wrote sardonically on Twitter. But the headline-grabbing differences between the findings of these Nobel laureates are less significant than the profound agreement in their scientific approach to economic questions, which is characterized by formulating and testing precise hypotheses. I’m troubled by the sense among skeptics that disagreements about the answers to certain questions suggest that economics is a confused discipline, a fake science whose findings cannot be a useful basis for making policy decisions. That view is unfair and uninformed. It makes demands on economics that are not made of other empirical disciplines, like medicine, and it ignores an emerging body of work, building on the scientific approach of last week’s winners, that is transforming economics into a field firmly grounded in fact. It is true that the answers to many “big picture” macroeconomic questions — like the causes of recessions or the determinants of growth — remain elusive. But in this respect, the challenges faced by economists are no different from those encountered in medicine and public health. Health researchers have worked for more than a century to understand the “big picture” questions of how diet and lifestyle affect health and aging, yet they still do not have a full scientific understanding of these connections. Some studies tell us to consume more coffee, wine and chocolate; others recommend the opposite. But few people would argue that medicine should not be approached as a science or that doctors should not make decisions based on the best available evidence. As is the case with epidemiologists, the fundamental challenge faced by economists — and a root cause of many disagreements in the field — is our limited ability to run experiments. If we could randomize policy decisions and then observe what happens to the economy and people’s lives, we would be able to get a precise understanding of how the economy works and how to improve policy. But the practical and ethical costs of such experiments preclude this sort of approach. (Surely we don’t want to create more financial crises just to understand how they work.) Nonetheless, economists have recently begun to overcome these challenges by developing tools that approximate scientific experiments to obtain compelling answers to specific policy questions. In previous decades the most prominent economists were typically theorists like Paul Krugman and Janet L. Yellen, whose models continue to guide economic thinking. Today, the most prominent economists are often empiricists like David Card of the University of California, Berkeley, and Esther Duflo of the Massachusetts Institute of Technology, who focus on testing old theories and formulating new ones that fit the evidence. This kind of empirical work in economics might be compared to the “micro” advances in medicine (like research on therapies for heart disease) that have contributed enormously to increasing longevity and quality of life, even as the “macro” questions of the determinants of health remain contested. Consider the politically charged question of whether extending unemployment benefits increases unemployment rates by reducing workers’ incentives to return to work. Nearly a dozen economic studies have analyzed this question by comparing unemployment rates in states that have extended unemployment benefits with those in states that do not. These studies approximate medical experiments in which some groups receive a treatment — in this case, extended unemployment benefits — while “control” groups don’t. These studies have uniformly found that a 10-week extension in unemployment benefits raises the average amount of time people spend out of work by at most one week. This simple, unassailable finding implies that policy makers can extend unemployment benefits to provide assistance to those out of work without substantially increasing unemployment rates. Other economic studies have taken advantage of the constraints inherent in a particular policy to obtain scientific evidence. An excellent recent example concerned health insurance in Oregon. In 2008, the state of Oregon decided to expand its state health insurance program to cover additional low-income individuals, but it had funding to cover only a small fraction of the eligible families. In collaboration with economics researchers, the state designed a lottery procedure by which individuals who received the insurance could be compared with those who did not, creating in effect a first-rate randomized experiment. The study found that getting insurance coverage increased the use of health care, reduced financial strain and improved well-being — results that now provide invaluable guidance in understanding what we should expect from the Affordable Care Act. Even when such experiments are unfeasible, there are ways to use “big data” to help answer policy questions. In a study that I conducted with two colleagues, we analyzed the impacts of high-quality elementary school teachers on their students’ outcomes as adults. You might think that it would be nearly impossible to isolate the causal effect of a third-grade teacher while accounting for all the other factors that affect a child’s life outcomes. Yet we were able to develop methods to identify the causal effect of teachers by comparing students in consecutive cohorts within a school. Suppose, for example, that an excellent teacher taught third grade in a given school in 1995 but then went on maternity leave in 1996. Since the teacher’s maternity leave is essentially a random event, by comparing the outcomes of students who happened to reach third grade in 1995 versus 1996, we are able to isolate the causal effect of teacher quality on students’ outcomes. Using a data set with anonymous records on 2.5 million students, we found that high-quality teachers significantly improved their students’ performance on standardized tests and, more important, increased their earnings and college attendance rates, and reduced their risk of teenage pregnancy. These findings — which have since been replicated in other school districts — provide policy makers with guidance on how to measure and improve teacher quality. These examples are not anomalous. And as the availability of data increases, economics will continue to become a more empirical, scientific field. In the meantime, it is simplistic and irresponsible to use disagreements among economists on a handful of difficult questions as an excuse to ignore the field’s many topics of consensus and its ability to inform policy decisions on the basis of evidence instead of ideology. Raj Chetty is a professor of economics at Harvard. 谢了不送：经济学要贴近科学，就要有实验，自然实验可遇不可求，于是 Even when such experiments are unfeasible, there are ways to use “big data” to help answer policy questions. 接着说了一个自己如何剥离出好老师对学生影响的例子。最后说 These examples are not anomalous. And as the availability of data increases, economics will continue to become a more empirical, scientific field. 随着数据的可获得性可用性日益改善，经济学将…巴拉巴拉…所以俺说“大数据”。 解释完了这跟大数据到底有什么毛线关系之后。我还是想问原先的问题。 慧航：他这里仅仅说的是自己的研究，他用的也不是互联网意义上的“大”数据。经济学的核心问题是识别问题，跟数据多大没有什么关系。Chetty 个人做的东西数据都“大”，这倒是真的。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[你的选择是？Roy 自选择模型及前苏联的数学家们（Labor系列） - 慧航 - 专栏]]></title>
    	<url>/prof/2015/03/16/soviet-american-mathematicians/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/20009505 前苏联解体后有将近 300 多位苏联数学家移民到了美国。经济学家 George Borjas 和诺特丹大学 Kirk Doran 2012 在 Quarterly Journal of Economics 发表的《苏联解体对于美国数学家的影响》The collapse of the Soviet Union and the productivity of American mathematicians。 激发 Borjas 写这篇文章的可能是他对 Roy 自选择的理解。最早提出自选择概念的是英国经济学家 A.D.Roy。他于 1951 在 Oxford Economics Paper 发表了题为《针对收入分配的一些想法》Some Thoughts on the Distribution of Earnings 的论文。文章讨论了为什么不同职业的收入是不一样的。Roy 最重要的想法是，我们不能假设每种职业的收入都完全由外因决定， 在自由就业的情况下，人们会根据自身偏好和特长作出不同的选择。这种自我选择行为于是被称为 Roy 自选择模型。也许这种想法当下看来明显不过，但过去的很长一段时间里职业并不是个人选择而定的，于是经济学家也想当然得认为职业报酬“好像是历史偶然因素造成的” －“developed by the process of historical accident”。但对于现在很多 Labor Economics 问题，自选择模型都非常有用。其中 George Borjas 于 1987 发表的Self-selection and the earnings of immigrants最为经典（原文在 American Economic Review 上，推荐一读。）他用 Roy 模型来分析移民这种二元选择 。曾经人们普遍认为移民只对于高收入工种有促进作用，但鉴于这几年美国共和党和民主党对于非法移民改革争议连连，我们也知道移民对于低收入工种也是意义深远的。Borjas 对 Roy 模型的应用能很好的解释高收入移民和低收入移民（还有第三种不常发生的政治移民。）我们先来看一下 Borjas 对 Roy 模型的提炼（其实模型可以简单概括为人往高处走，水往低处流。不知各位看法如何？） 以下我们标记本国为 country 0，移民国为 country 1。首先假设收入在国家\(j\)可以表达为\(\ln w_{j} = \mu_j &#43; \epsilon_{j}\)。同时标记协方差为\(\sigma_{0,1}= cov(\epsilon_{0},\epsilon_{1})\)。假设\(\epsilon_{0}\)和\(\epsilon_{1}\)是联合正态分布，也就是说 \[\left(\begin{smallmatrix} \epsilon_{0} \\ \epsilon_{1} \end{smallmatrix} \right) \sim N \left( \left( \begin{smallmatrix} 0 \\ 0 \end{smallmatrix} \right), \left(\begin{smallmatrix} \sigma_0^2 &amp;amp; \sigma_{0,1} \\ \sigma_{0,1} &amp;amp; \sigma_1^2 \end{smallmatrix} \right) \right)\] 这里关于收入的假设是：一部分收入由 observable characteristics 可观察特征决定，如教育水平(\(\mu_0\)和\(\mu_1\))；另一部分则由不可观察特征决定 unobserved characteristics (\(\epsilon_{0}\)和\(\epsilon_{1}\))。 标记\(\rho_{0,1}\)为\(\epsilon_{0}\)和\(\epsilon_{1}\)的相关系数，那么 \[\rho_{0,1}=\frac{\mathrm{cov}(\epsilon_0,\epsilon_1)}{\sigma_0 \sigma_1} = \frac{\sigma_{0,1}}{\sigma_0 \sigma_1}\] 假定移民的开支\(C\)是在本国收入的一部分，即\(C = \pi w_0\)，这样后面我们可以简化1\(\ln(w_0 &#43; C) \approx \ln(w_o) &#43; \pi\)。 接下来我们便能导出移民决策啦。即根据以下\(I\)的正负来决定。正的时候移民后的收入更高，会选择移民；反之则留在本国，即 \[ \begin{aligned} I = \ln \left( \frac{w_1}{w_0 &#43; C} \right) &amp;amp;= \ln(w_1) - \ln(w_0(1 &#43; \pi)) \\ &amp;amp; \approx (\mu_1 - \mu_0 - \pi) &#43; (\epsilon_1 - \epsilon_0) \end{aligned} \] 接着标记\(v \equiv \epsilon_1 - \epsilon_0\)，我们知道\(v\sim N(0, \sigma_0^2&#43; \sigma_1^2-2\sigma_{0,1})\)。这样的话一个人准备移民的可能性可以写作 \[ \begin{aligned} \mathrm{Pr}(I&amp;gt;0) &amp;amp; =\mathrm{Pr} [ \epsilon_1 - \epsilon_0 &amp;gt; - (\mu_1 - \mu_0 - \pi)] \\ &amp;amp; = \mathrm{Pr} [v &amp;gt; - (\mu_1 - \mu_0 - \pi)] \\ &amp;amp; = \mathrm{Pr} \left[ \frac{v}{\sigma_v} &amp;gt; \frac{\mu_0 - \mu_1 &#43; \pi}{\sigma_v} \right]\\ &amp;amp; = 1 - \mathrm{Pr} \left[ \frac{v}{\sigma_v} \le \frac{\mu_0 - \mu_1 &#43; \pi}{\sigma_v} \right]\\ &amp;amp; = 1 - \Phi \left( \frac{\mu_0 - \mu_1 &#43; \pi}{\sigma_v} \right) \\ &amp;amp; = 1 - \Phi(z) \end{aligned} \] 其中我们标记\(z = \frac{\mu_0 - \mu_1 &#43; \pi}{\sigma_v}\)。如果\(z\)值更高，那么就越不可能移民，影响\(z\)值的有本国收入，移民国收入，移民所需费用和\(z\)的标准差。前面三项都是比较直观的因素，接下来我们更多的探讨这个标准差的影响。 对于已经准备移民的人，我们也想知道他们在本国是处于什么样的收入情况，是高收入还是低收入，即\(\mathrm{E}(\ln w_0 | I &amp;gt; 0)\) 和 \(\mathrm{E}(\ln w_1 | I &amp;gt; 0)\)相对本国平均收入的大小。相关推导可以查看原文，以下我们直接看自选择结果啦。 假设两国的平均收入相等，那么对于移民的人们，比较他们在本国和在移民国的收入分布主要看\(Q_0\)和\(Q_1\)的正负： \[ \left\{ \begin{aligned} Q_0 \equiv \mathrm{E}(\epsilon_0|I&amp;gt;0) &amp;amp;= \frac{\sigma_0 \sigma_1}{\sigma_v} \left(\rho_{0,1} - \frac{\sigma_0}{\sigma_1} \right) \left( \frac{\phi(z)}{1-\Phi(z)}\right) \\ Q_1 \equiv \mathrm{E}(\epsilon_1|I&amp;gt;0) &amp;amp;= \frac{\sigma_0 \sigma_1}{\sigma_v} \left(\frac{\sigma_1}{\sigma_0} - \rho_{0,1} \right) \left( \frac{\phi(z)}{1-\Phi(z)} \right) \end{aligned}\right. \] 有以下三种情况： 正向选择移民\(Q_0&amp;gt;0,Q_1&amp;gt;0\)（\(\rho_{0,1}&amp;gt; \frac{\sigma_0}{\sigma_1}\)而且\(\frac{\sigma_1}{\sigma_0}&amp;gt;1\)），即在本国收入和移民国收入都很高，但是移民国的收入分配比本国更加分散。比如欧洲的好医生移民到美国，因为美国医生的收入相对平均收入要高很多。 负向选择移民\(Q_0&amp;lt;0,Q_1&amp;lt;0\)（\(\rho_{0,1}&amp;gt; \frac{\sigma_1}{\sigma_0}\)而且\(\frac{\sigma_0}{\sigma_1}&amp;gt;1\)），即在本国收入和移民国收入都很低，但是移民国的收入分配比本国更加集中。比如墨西哥的劳工移民到美国，因为能享受更好的社会福利。 \(Q_0&amp;lt;0,Q_1&amp;gt;0\)（\(\rho_{0,1}&amp;lt;\min(\frac{\sigma_0}{\sigma_1},\frac{\sigma_1}{\sigma_0})\)）移民是负向选择，但收入在两国没有相关性。Borjas 说此类为难民式移民或曰政治性移民，比如人们逃离专制国家。 关于第一种（高收入自我正向选择）和第二种（低收入自我负向选择）都有很多研究，而第三种情况较少。但就在这篇文章发表后不久，苏联解体了，Borjas 当然没有放过这个机会。虽然搜集数学家的 panel data 很费事2，但还是有了开篇提到的这篇文章。 来到美国的数学家们都是同行中的佼佼者，其实也是正向选择移民。意外（不意外？）的是美国的数学家并没有因为和苏联学术交流，而提高自身水平，Borjas 和 Doran 估算苏联数学家的涌入在 1992 到 2008 间减少了美国数学家近三成的发表量，而且 1992 年左右的美国本土博士生们都就业难了。 虽然是从移民的自选择问题出发，Borjas 和 Doran 的意图更在于讨论学术人才涌入对于本国学术人才的影响。学术圈一方面是在教职上竞争，另一方面是发表论文数上竞争。当然论文发表量并非是一个好的测量方法，我们都知道钱学森归国对我国航空事业还是有巨大贡献的（如题图。） 所以想问大家对于学术圈 productivity 有什么好的测量方法吗？你觉得自选择模型还能解释哪些别的现象呢？ 李一鸣：Moser, Voena, and Waldinger (2014) 用专利申请数目来探究二战期间犹太裔德国化学家到美国以后的影响。 nautilus sun：“In the second step of our data collection, we hand-match émigrés and other German chemists with U.S. patents between 1900 and 1970.” 好同情挖数据的小朋友们。不过你记得文章有看多少专利是化学家从 industry 里申的还是从 academia 里申的？ 李一鸣：我没读过论文，只听 Voena 讲过一次。我记得他们搜集的专利好像是教授的专利。 Loser Loser：人们会根据自身偏好和特长作出不同的选择为什么会造成不同职业的收入是不一样的？还是没明白… nautilus sun：假设劳工市场是 perfect competition，那么工人的 wage 等于他们的 marginal product of labor。因而 high productivity worker 选择的职业会是高收入的职业。至于为什么不同国家同一职业的收入分布会不一样，除了 productivity 不一样之外，我想还有就是社会福利政策。寻租行为也会导致非 perfect competition，例如开出租车要有营业执照。 当然这个假设有待商榷，但简化是用\(f(x) = \ln(1&#43;x)\)在\(0\)处的泰勒展式。↩ 用到了 The Mathematics Genealogy Project 呢！↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[市场会腐化企业的社会责任么？ - 慧航 - 专栏]]></title>
    	<url>/prof/2015/03/16/csr/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19950028 之前看过一部电影，叫做《战争之王》。电影根据五位军火商的 电影中有几句令人深思的台词： There are over 550 million firearms in worldwide circulation. That’s one firearm for every 12 people on the planet. The only question is…how do we arm the other 11? How many car salesmen talk about their work, huh? How many cigarette salesmen? Both their products kill more people every year than mine. At least mine has a safety switch. 军火商卖军火，只为了卖出更多的军火，而买家拿这些军火去做什么，似乎与军火商并没有什么关系。就好像卖车的人不会告诉你每年死在交通意外中的有多少人，卖烟的也不会告诉你每年死于肺癌的有多少人：商人只负责买卖，至于其他的事情，与我无关。 这个逻辑非常迷惑人，但是总感觉哪里有什么不对。也许他们认为，有需求就会有供给，正如主人公说的一样，这件事我不做，照样有其他人去做。但是难道商人就是商人，只负责买卖而不需要去理会社会责任么？ 也许，这在一定程度上也许是对的。杜蕾丝不会因为其对避孕的贡献而被称为负有社会责任感的企业，而烟草商也不需要为吸烟致死的烟民负责：卖烟草是我的事，吸不吸就是你自己的事了。我们总不能因为钢铁、发电厂排出废气导致雾霾，就把相关的疾病完全算在这些厂商头上吧，毕竟最终消费这些的是消费者本身。 然而，我相信在市场中，社会责任是非常必要的。电影里面有两个角色似乎诠释了什么是责任感。主人公的对手，告诉主人公，你至少要有立场。而主人公的弟弟，则为了避免眼皮底下难民被屠杀，干脆搞毁了一半的军械，最终死在军队的枪下。你不能把武器卖给那些滥杀无辜的人，比如萨达姆。而片中主人公解释了为什么没有把武器卖给萨达姆：因为这家伙信用不好——主人公的道德底线暴露无遗。 奥，不好意思，今天我是来介绍论文的，不是写观影报告，我也写不了。带着这一问题，University of Zurich 的 Björn Bartling、Roberto A. Weber 以及上海财经大学的 Lan Yao(姚澜)最近在 QJE 上的文章：Do Markets Erode Social Responsibility?通过实验的方法，讨论了市场经济中企业的社会责任问题。另外上海财经大学经济学院也凭借这篇文章完成了国际经济学期刊 Top 5 的大满冠，这在国内非常罕见，祝贺一下。 文章从亚当斯密“看不见的手”出发，指出市场的有效性是在假设没有外部性的前提之下的。而当交易的社会成本不全部由企业承担时，那么市场就会失灵。而在市场失灵的情况下，政府可能会通过税收等方式解决社会成本的问题（比如对于污染），或者企业可能自发的内部化这些外部性。然而企业自发的解决外部性的问题究竟是出于对社会影响的考虑，还是为了企业的名誉以及长期的利润？ 比如经常看到产品标签上标有“低碳”、“cruelty free”等标签，这些产品一般来说伴随着高生产成本和高价格。那么这些企业之所以生产这些产品，是出于社会责任感的考虑，还时另有动机？ 这篇文章通过实验的方法研究了在存在负外部性的情况下企业的行为。 当然，拿现实中的企业做实验是非常困难的，所以这篇文章仍然是使用了自然人（非经济系的学生）来做实验的。首先先将实验对象分组，一组是厂商，一组消费者，以及一组第三方。消费者和厂商交换两种产品，一种“公平”的产品，和一种“不公平”的产品。公平的产品不会造成任何的外部性，而不公平的产品会对第三方造成损失。 对于消费者来说，每种产品都价值 50 元。不公平产品的生产成本为 0，然而如果交换了不公平的产品，会对第三方造成 60 块的损失。而公平的产品生产成本为\(c \in (0,50)\)。总的来说，不公平的产品交易会造成社会总体净福利为\(-10\)，而公平的产品产生了\(50-c\)的社会剩余。 在基准实验中，市场上存在 6 个企业、5 个消费者和 5 个第三方。每个消费者有 100 块，每个厂商提供一单位的产品，厂商可以选择生产“公平”的产品或者“不公平”的产品。企业生产公平的产品成本为 10。在所有企业选择了生产何种产品以及产品价格之后，消费者进入市场，选择购买什么产品，只能买一份，当然也可以不买。由于企业比消费者多一个，因而最后一个进入市场购买的消费者也可以面临两个选择。简单来收，三类行为人的收益可以如下描述： 实验对象要进行 24 轮的实验。由于所有实验在电脑上进行，而且实验对象的 ID 都是被隐藏的，因而不存在两轮之间有任何相互影响。 此外，实验设计者还设计了一个非市场的实验设计，即选一个参与人作为 dictator，选择合适的配置。 经典的经济理论会预测，对于自利的行为人，所有消费者都会选择不公平的商品，其交易价格为 0，而所有的厂商都会生产不公平的商品。至此，这个市场完全失灵了，达到了最差的配置。 但是实验结果是这样的么？下图中蓝色实线是基准实验中公平产品被交易的比率，横轴为实验的轮数（1-24）。 令人惊奇的是，大约有不太到一半的“公平的”产品被交易了。而下图给出了每一轮交易，两种产品的价格差异： 可以发现，公平的产品的价格比不公平的产品价格大约高出了大约 4 元左右，小于 10 元的成本差异。 以上的证据可以看出，在基准的市场实验中，消费者和生产者都表现出了对于社会福利的关心。 此外，作者还通过改变企业的数目（市场的竞争性）、改变消费者的信息（消费者可能不知道产品的种类）以及公平产品的成本等等，做了重复的实验，发现了同样类似的结果。 其实下面的结果才是最有意思的。以上的实验都是在瑞士做的。之前提过，这篇文章的作者有上海财经大学的老师，那么这个实验在中国做结果是不是类似呢？ 首先作者做了调查问卷，看中国的学生（其实是上海财经大学的学生）对企业社会责任的看法。结果显示，中国的学生更普遍认同企业应该担负社会责任。 然而，实验结果呢？我们来看下图： 奥，为啥公平的物品，在中国的实验，交易率这么低呢？这样就意味着，在中国的实验中，似乎中国的实验对象的社会责任感更低。 然而，通过对非市场实验设定的对比，发现中国和瑞士之间似乎没有显著的不同。也就是说，两个国家的行为人对于社会福利的偏好似乎是相同的。那么市场行为的差别来源自哪里呢？作者认为可能是市场上的规范不同，导致了中国人对于不公平或者不道德的行为有更多的容忍。 以上只是对这篇文章做了一个大致的介绍，其实文章里面有大量的细节我没有介绍。如果感兴趣，可以去读原文。不过虽然这篇文章使用实验的方法得到了一些结果，企业社会责任的问题我觉着仍然是一个 open question，这些证据还不能提供给我们更加深入的理解。这也是我介绍这篇文章的目的，希望大家对此有广泛而深刻的讨论。 最后声明，文章所有数据都来自于实验，并没有任何民族自卑等的倾向。虽然开放评论，但请就事论事。 沐风草洛：企业承担更多的社会责任，是因为能转化为更大的社会利益。而目前国内消费者对社会责任认同感不高，所以企业也自然就没有动力去承担社会责任。额，这是我看了之后总结的，不知道对不对。 慧航：不对，上面说了，中国人比瑞士人更认为企业要有社会责任，而且对社会福利的偏好也跟瑞士差不多，但是结果却不一样。 沐风草洛：恩，就是中国的学生更普遍认同企业应该承担社会责任，但是实际上却对企业道德的要求比较低。这也就造成了在华企业没有动力去承担社会责任了。 Alex屁话多：通过调查问卷来推测 preference 恐怕不是科学的方法。 慧航：所以设计了非市场的实验。 Alex屁话多：所以，可能原因就是 preference 不同。 慧航：实验显示 preference 似乎是相同的。 你没有懂我啊，作为 social planner 和作为 consumer 的 preference 不同是可能的~很多情况下 representative household 是不合适的假设。 慧航：sp 也是随机选的啊，同样的样本，所以应该不存在你说的问题。 栗子：为什么要做那么复杂而且得了结论也半信半疑的实验，直接调研一下版权市场应该能得到更具参考价值的答案吧？ 慧航：受控实验。 陈名曦：去 SSRN 看了原文，结论倒是简单一个”是“。但果然模型好弱——复杂市场主体作简单市场决策连点外生因素都不考虑的……答主最后扯中国实验也跑题了，“作者认为可能是市场上的规范不同，导致 XXX”翻译一下就是：影响决策的变量全在外部，跟我的实验没什么关系，“为什么市场会这样了”的原因是“市场就长这样”。跟论文结论没关系扯它干嘛。——还是觉得模型好弱。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[经济学家的优越感 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/01/31/superiority-of-economists/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19945042 Fourcade et al. 所作的 Superiority of Economists（经济学家的优越感） Winter 2015 期的《经济视角期刊》(Journal of Economic Perspectives) 之上。此文共有三位合作者，但不按姓氏的首字母顺序署名。第一作者 Marion Fourcade 是加州大学伯克利的社会学教授，第二作者 Étienne Ollion 亦是社会学研究人员，因此文章的视角更多来自社会科学中姐妹学科的专业人士，比较公允。 文章主要围绕经济学在美国社会科学领域处于主导学科地位这一现象展开了讨论。文章指出，在经济学家眼中，他们自己就是处在社会科学研究中最顶层的。一项 2005 年的调查表明，有 77% 的顶级经济学系研究生认同“经济学是社会科学中最科学的学科”这一说法。文章引用了哈佛大学经济学教授 Richard Freeman 的观点： 相比经济学家，社会学家和政治学家缺乏有力的分析工具，并且学识不广。从 GRE 分数和其他条件上看，经济学吸引了更好的生源，并且对数学能力的要求更高。 从就业上看，经济学家的优越感并非毫无依据。在那些对数学要求很高的高校教职中，经济学家要比数学家和物理学家拥有更好的职业前景，并且在工资收入方面仅次于计算机科学家和工程师。除去工资收入，许多知名的经济学家往往还能从企业中获得咨询费和董事费，甚至在华尔街上拥有一席之地。经济学家收入高、数学好（被认为是智力水平高的表现），似乎可以解释他们为什么有底气瞧不起其他社会科学家。但文章指出，经济学在社会科学中的独特地位，以及经济学家的各种“奇葩”，其实远不止如此。 首先是学科中从业者的构成。经济学长期由男性主导，这与其他所有“文科”甚至是某些“理科”都不一样。从下图中不难看出，在博士毕业生的性别构成上与经济学走得最近的既不是政治学，也不是社会学，更不是心理学，而是物理科学。 文章认为，经济学与其他社会科学之间在很大程度上是孤立的。造成这一现象的主要原因是当代经济学家普遍持有的一种观点，即经济学家试图通过高度技巧化的方式（精确的数学公式、明确的因果关系）来理解复杂的社会过程，因此与那些喜欢东拉西扯的社会科学是格格不入的。文章引用了斯坦福大学经济学教授 Edward Lazear 在《经济学帝国主义》一文中的著名论断： 经济学拥有一套严密的语言。这套语言可以把复杂的概念用简单、抽象的方式表达出来，从而使经济学家免于复杂现象的羁绊。复杂的现象虽然可以增添描述上的丰富程度，却不利于分析者看穿问题背后的本质。 众所周知，经济学的这套严密的分析语言主要建立在数学和统计学之上。自二战以来，经济学家就致力于通过一系列正规而简洁的公式来建立其科学化的体系，这与物理学的做法十分相似。在此之上，自上世纪 90 年代“经验革命”以来，经济学更是在对待因果关系的问题上表现出不屈不挠的精神。这一批文献中涌现出了大量研究设计和推断问题，并且用到了许多随机对照试验。尽管后者在经济学内部尚有争议，但不可否认的是，经济学家并非死抱教条而不顾现实。相反，他们相当脚踏实地，而且研究的问题往往涉及到了社会科学的其他领域。 然而，经济学家广泛的研究兴趣并没有促使他们与其他社会科学家达成更多合作。2013 年的一项统计表明：平均而言，对于一篇经济学论文，其引用的文献中有 81% 是来自学科内部的，而这个比例在社会学中是 52%，在人类学中是 53%，在政治学中是 59%。下表显示了美国经济学会、美国政治学会和美国社会学会各自旗舰期刊的自/跨领域引用情况。在不考虑专著的情况下，《美国政治学评论》引用经济学领域 25 本顶尖期刊的频率是《美国经济评论》引用政治学领域 25 本顶尖期刊频率的 5 倍还多。而对于《美国社会学评论》而言，尽管引用的绝对数量较小，这个数字却是 7 倍还多。 文章指出，在 2000 至 2010 年期间，美国社会学领域内被引用最多的社会学家是 Pierre Bourdieu，而这个名字在《美国经济评论》中仅被提及了 1 次；与之形成鲜明对比的是经济学家 Gary Becker，其在《美国社会学评论》中竟被引用了 41 次。在同一时期，社会学家 Max Weber 和 Mark Granovetter 在《美国经济评论》中各被提及了 4 次，而经济学家 James Heckman 和 Oliver Williamson 在《美国社会学评论》中则被引用了 25 次和 13 次。经济学和其他社会科学之间权力的不平等似乎是显而易见的。 在经济学内部，这种权力与等级的划分甚至更加清晰。尽管经济学家之间存在着政治上的分歧，但从局外人的角度看，经济学家的观点却是高度集中且一致的。换句话说，经济学家之间达成的共识要比其他社会科学家之间都要高，而且经济学精英手里集中了更多权力。例如，经济学家都广泛认同他们的那套工具和准则，因此与其他社会科学不同，经济学研究生课程高度依赖于标准化的教材，而这些教材往往出自顶尖经济学系的教授之手。根据一项 1990 年的调查，几乎所有经济学博士生课程都“惊人的相似”。 更有意思的是，经济学家对于排名几乎到了痴迷的程度。如下图所示，美国经济学会中超过 72% 的非指定管理层人员被推选自排名前 5 的经济学系，而这一比例在美国政治学会和美国社会学会中还不到 20%。 根据文章的计算，美国历史最悠久的社会学期刊《美国社会学期刊》有 22.3% 的文章出自排名前 5 的社会学系，而美国历史最悠久的两本经济学期刊《政治经济学期刊》和《经济学季刊》则分别有 28.7% 和 37.5% 的文章出自排名前 5 的经济学系。如果把从这些系取得博士学位的作者都算进来，那么社会学的这个比例为 35.4%，而经济学的这个比例则为 45.4% 和 57.6%。经济学中各种与排名有关的论文以及 RePEc 等排名服务在其他学科中几乎是找不到的。 不过，经济学家与其他社会科学家之间的巨大差异并不代表他们找不到朋友。下图展示了过去 60 多年来经济学领域 5 本顶尖期刊的跨学科引用情况。从趋势上看，数学和统计学对于经济学的影响日趋式微，政治学有所增强（政治学部分采纳了理性选择理论），而金融学作为从经济学中独立出去的分支与经济学之间的联系日益紧密。 文章认为，两者之间的密切联系主要源于经济学方法在金融学研究中所占据的主导地位，从而使后者在发展过程中可以反哺前者。下图便呈现了这样一个证据，即金融学顶级期刊《金融学期刊》随着时间的推移不断加大引用经济学 5 本顶尖期刊以及另一本老牌经济学期刊《经济期刊》的内容。 金融学家和经济学家之间日益亲密的关系还与商学院有关。商学院作为量产职业经理人 (MBA) 的场所一直想要取得在学术上的合法性，从而摆脱其本身带给人的“土豪”印象。在这种情况下，经济学家自然而然地就跟随着自己的金融学好朋友们进入了商学院这片富饶的土地。根据一项 2004 年的调查，在美国排名前 20 的经济学系教师中，有 637 人拥有经济学博士学位，而在美国排名前 20 的商学院教师中，这个数字是 549 人，几乎与经济学系不相上下。 商学院大量吸收经济学博士任教，使得这批人在经济学内部也形成了一股可怕的力量。以经济学“芝加哥学派”的发源地、赫赫有名的芝加哥大学为例，其 Booth 商学院的经济学实力几乎已经超过了大学本身的经济学系。自上世纪 90 年代以来，一大批“诺贝尔”经济学奖得主都来自商学院，如 Eugene Fama, Oliver Williamson, Robert Engle, Michael Spence, Robert Merton, Myron Scholes, Merton Miller, John Harsanyi, Robert Fogel 等等，不胜枚举。 经济学向商学院方向的转移给经济学家带来了更高的收入、更多与商界高层的接触机会、以及更多的“外快”。然而，经济学家在实现自我价值的同时，与其他社会科学、政府以及普通民众之间的隔阂也日益增大。许多被经济学家认为是极好的想法，如器官自由买卖、排污产权交易等，往往不能被公众所接受。在公共议题方面，有超过三分二的社会学家认为企业利润过高，而只有三分之一的经济学家同意这一看法；几乎所有社会学家都认为，即使政府债务累累也应该竭尽所能帮助穷人，而只有不到一半的经济学家同意这一看法。 文章认为，经济学家与社会学家所持的不同观点与他们自身所处的社会和经济地位有关。下图展示了过去十多年来若干学科从业人员工资收入的前 10% 及中位数。可以看到，在相同的分位数上，经济学家的收入遥遥领先于社会学家。而经济学家中“最赚得动”的那群人的工资收入甚至超过了工程师中“最赚得动”的那群人。 但是，经济学家的优越感并不代表他们不关心普通人的福利。事实上，经济学家手上拥有的大量商业、政府和非政府组织资源使他们培养出了一种“修理”文化。由于对自己的理论充满信心，并且自身处在社会的精英阶层，当代的经济学家时刻想要“修好”一些东西。比如他们在宏观经济学中创造的那个神秘的“社会计划者”，又如他们在计量经济学中开发的那些政策评估模型，再如他们在发展经济学中进行的那些田野实验——这些都说明了社会政策和发展扶助是经济学研究的一个重要部分。 文章指出，与社会学或政治学不同，经济学本身就是一种强大的改造力量。经济学家不仅仅描绘现实，更通过其建议和工具来创造现实。经济学的理论和技巧已经植根于真实经济之中，在日常经济活动中越来越多地被普通人所使用。例如，金融学理论创造出了巨大的衍生品市场，而各类资产定价公式则直接改变了市场参与者的行为，使得模型与现实更加匹配。在经济学家的影响之下，世界已经发生了重要的改变。 然而，正是因为经济学具有改造的力量，经济学家作为改造活动的实施者也更容易受到非议。例如，2008 年的金融危机超出了几乎所有经济学家的预期，这也让一些经济学家，如 Paul Krugman，开始“良心发现”。他们反思自己过度的自信，甚至是引以为傲的专业知识的可靠性。也有经济学家，如 Thomas Piketty，开始关注分配问题。经济学的研究风向或许正在发生着改变。 文章在结尾处引用了凯恩斯 (John Maynard Keynes) 在 1932 年时所说的一句话： 如果经济学家能像牙医一样谦逊又能干，那真是太好不过了！ 当代的经济学家无疑是能干的。他们希望建立起一种由专家指导的民主，并在其中发挥自己的才能。遗憾的是，民主社会往往是高度怀疑专家的。因此，与帮人看牙不同，经济建议永远不可能是谦逊的。 从这种意义上说，经济学家的优越感似乎也是他们的一种无奈。 蕉庵秋霁：说两句稍微补充一下背景吧。我觉得可能对于知识社会学或者更广义的科学技术社会学研究、知识史来说，经济学这个知识体系是其中一个 case 吧。比如经济学学科和其他学科之间的关系、其知识的组织形式，以及经济学和政治的关系。知识社会学和文化社会学有一些其他的关切点，而我感觉这个研究是植根于知识文化社会学的关切点当中的一小部分经验内容。Marion Fourcade 写过许多和经济学家和政治，人的道德判断和经济生活的历史社会学以及文化社会学理论文章。lz提到的这个文章主要是经验性地呈现了某些基本的和经济学学科有关的材料，基本上并没有涉及太多知识社会学或者文化史等关心的文化理论和历史过程。Fourcade自己以及其他学者的其他研究在相关议题上可能讨论的内容包括：经济学学科在二十世纪的发展路径和社会历史过程对学科的形塑有着很大的影响，而不同国家的政治-制度关系又影响了经济学家怎么样和政策和社会发生关系以及怎么理解自我、科学和世界。还有，比如，会涉及到的知识问题是，哪一类型的知识（比如说数量化的、模型化的而非语言性的、反思性的、批判性的）是更加集中化的、可扩张的，有利于政策专家制定而非公共参与和政治过程、赋权抑或情感过程和思维体验的。而后者又怎么样在一个社会中可能会形成有关于经济知识应当怎么样组织的道德判断和可欲的政治目标。等等等等。 从这点上来说 superiority 可能是一个 catchy 的文章标题题目，但我以为并不能完全传达 Fourcade 所有研究作品的整体思路。superiority 传达的分析对象是一个群体的情感，但我觉得她文章主体部分进行想要解释和分析的对象，更适合用来描述的词汇是（用社会学学科内部的语言）：经济学知识生产场域的统一性和相对闭合性；经济学在公共政策制订中的相对中心位置；我们叫做经济学的知识体系/文化体系的组织体系。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[欧洲史·三：浪漫主义与古希腊哲学 z]]></title>
    	<url>/arts/2015/01/22/history-of-europe-03/</url>
		<content type="text"><![CDATA[[原文地址：https://liam0205.me/2014/12/30/history-of-europe-01/ 前两篇文章（第一篇、第二篇）讨论了欧洲文明发源的三股力量，以及欧洲思想发源的几次思想运动。今次，我们将暂别对理性的讨论，转而讨论与理性相对的浪漫主义；同时我们也会一睹古希腊三大哲人的风采。 浪漫主义 浪漫主义运动从德国发端 我们前面说到，欧洲这个混合体由希腊罗马、教会和日耳曼民族组成。前面说到的浪漫主义运动、宗教改革、科学革命还有启蒙运动主要都是前两个部分相互作用的结果。今次要谈的浪漫主义运动则发短语继承自日耳曼民族的德意志民族。（公元 919 年之后建国的国家中的人民称德意志，之前称日耳曼） 18 世纪末，法国爆发了大革命，传统的君主制阶层观念、贵族以及天主教会统治制度在三年内被推翻；新的自由、平等、博爱等原则被建立起来。尽管人们对理性的期望很高，但是法国大革命废除了君主和教会之后，并没有迎来民智开启的时代，反而因为激进的思想，迎来了流血、暴政和独裁。 针对法国启蒙运动的观点，发端于德意志的浪漫主义运动崇尚感受、情绪和强烈的情感。德国人认为理性但抽象的词语不能用来描绘人类和社会，这完全是空谈；他们相信自己的历史和文化根治于自己体内，因此与法国人是不同的，所以法国人在沙龙里谈论的普世理性是不存在的；他们希望知道早期日耳曼民族在进入结合体之前是什么状态，喜欢他们的活力和补拙。 浪漫主义认为文明是人为的，因此文明会束缚和局限我们。因此，超脱这些束缚，活在传统文化当中，才算是活得完整。至此，知识分子才开始尊重并对文化开始有兴趣，开始收集民俗文化和乡村歌谣。并且从此，这些观念深植于欧洲人的思想之中。 民主主义源自浪漫主义 浪漫主义对自己民族历史和文化的尊重被扩大强化之后，就形成了民族主义。民族主义者认为拥有相同历史、文化和语言的人们组成的民族应当生活在一起，并组建自己的政府。民族主义者认为，多民族的国家里，任何一个民族都无法充分表达自己的观点，无法充分伸张自己的文化，因此任何民族的智慧都无法开花结果。 教会的结局 中世纪结束之后，文艺复兴、宗教改革、科学革命、启蒙运动以及浪漫主义运动分别在不同的角度削弱了教会的权威。事实上，现代理性、科学和进步的思想从文艺复兴、科学革命和启蒙运动一脉相承自古希腊；而现在对文化、解放思想的崇尚则从浪漫主义运动继承自日耳曼民族。反观教会，留下的则是一片空白。 理性和浪漫的撕扯 理性和浪漫是两股根植在欧洲人心中的截然相反的力量。这两股力量往往对立，互相撕扯。 我们先来看一个例子。如果在学校里给学生讲述下面这段故事，并将之奉为真理，你会怎么看待？这个故事来自旧约圣经的第一篇：创世纪。 耶和华上帝用地上的尘土造人，将生气吹在他鼻孔里，他就成了有灵的活人，名叫亚当。 如果你的孩子是这个学校里的一个学生，你会怎么想呢？作为一个现代社会的文明人，你一定会反对学校这样的做法，你会认为学校在胡说八道误人子弟。 那么反过来，再看另外一个故事。这个故事是澳大利亚土著人的一个传说。 很久很久以前，有个老人非常疼爱他的侄儿。年轻的侄儿远赴异国，爱上了一个年轻女孩儿。这对情侣后来不顾长辈的反对，私奔出走。但女孩儿已被许配给另外一个人，所以女孩儿的部落派人追到他们，杀死了他的侄儿。老人听到噩耗非常伤心，不顾年老体弱，跋涉到这个国家，打算将侄儿的尸体带回故土。 侄儿已经成年，然而老人年事已高。将侄儿的尸体从异国运回故土，对于老人来说是个沉重的负担。但是老人办到了，它将尸体带回，妥善安葬。至今你仍然能够看到老人跋涉的足迹，仍然能看到他中途休息的地方，仍然能看到他的泪水汇成的水潭。 同样的假设，你会怎么想呢？尽管你知道这个故事浪漫主义色彩浓厚，肯定不是真实的，但是你可能仍然会希望学校将这个「美好」的故事讲给学生们听。 两个故事都有明显不真实的地方，但是我们对它们的态度却截然不同。面对第一个故事的时候，理性占了上风；而面对第二个故事的时候，浪漫主义的情怀却占了上风。 若然生活在启蒙时代，可能会有人对你说：「如果你的孩子想知道水潭的来源，那你可以送他去学地质学」。可能还有人会对你说：「这些土著人的思想已经被黑暗和魔法带来的恐惧束缚住了，你不要相信这些」。这感觉很别扭不是吗？对于我们来说，我们往往会迷失在浪漫主义的情怀之中，潜意识地认为土著人的生活方式更加纯粹、健全和自然，不由自主地希望这些「信仰」传承下去。 这样的撕扯就是理性和浪漫对立的一面。这样的撕扯源自于启蒙运动和浪漫主义运动的冲突，而追根究底的原因是欧洲文明那个奇怪的混合体的矛盾。这样的撕扯不断困扰和瓦解着欧洲的道德观念和智识生活，并成为欧洲人的「宿命」之一。 古希腊三大哲人的风采 让我们「重归」理性。 前文已经提到，文艺复兴时期的人们认为「古典就是最好的」。这样的观点在 17 世纪才因古希腊人的宇宙观（地心说）被打破而逐渐缓解。自那时起，人们慢慢将目光从古希腊已有的成就上转移开来，开始关注现代人可能达成的成就。 不过，尽管如此，在一些领域里我们的成就依然坐落在希腊人打下的基石上。而对后世影响甚深的人就是古希腊的三大哲人：苏格拉底、柏拉图和亚里士多德。 苏格拉底 苏格拉底是一个，呃……，对思辨的向往近乎疯魔的人。他作为古希腊最伟大的哲人之一，并没有给出真理，但是去做了更伟大的事：给出了迈向真理的方法。 苏格拉底认为真理切实存在，但平常人的心智并不足以理解真理。因此，渴望得到真理的人必须耕耘其心智。而这种方法其实就是「有理有据地怀疑一切」。 苏格拉底问答法是类似于这样的对话： 老师：什么是革命？学生：用武力推翻政府就是革命。老师：如果国王的弟弟弑君篡位，那么这样以武力推翻的行为是革命吗？学生：不是。老师：那除了武力推翻之外，还需要怎样的条件才能算是革命呢？ 苏格拉底问答法实际是在不断地举出反例的过程中，修正已有的观点。这样的方法能不断剔除谬误的部分，使得结论逐渐趋向真理。 柏拉图 柏拉图是苏格拉底的弟子。如今有关苏格拉底的实际，一部分就是出自柏拉图之手。苏格拉底「怀疑一切」的主张影响了雅典的统治者，因此被最终处死。在柏拉图的描述中，苏格拉底平静地面对死亡，对无法思辨的苟且无法容忍，从容地结束了自己的生命。 然而，如若换过一个角度，人们对苏格拉底的崇敬可能就会大打折扣。比如，如果公诉苏格拉底的检察官之子因为听信苏格拉底的言论，怀疑一切，最终成为一个不务正业的酒鬼；然后检察官认为苏格拉底是妖言惑众，应当处死。 这样的思辨，其实正是苏格拉底崇尚的「怀疑一切」。而对一切事物都充满怀疑，会让我们迷失方向；我们不可能靠纯粹的理性生活、工作并组成社会，我们在理性的同时，也需要有风俗、习惯甚至是宗教来指引方向。 柏拉图认为，我们在世间的所思所想，其实只是另一个崇高世界里的影子。因此，单纯地依靠思辨，单纯地怀疑一切，并不能求得真理。柏拉图为了说明他的观点，举了一个例子。 假设有一群人，被关在暗无天日的深邃山洞。山洞只有一个洞口，被铁门锁死。洞口的外面是长长的走廊，走廊的尽头有一只火把。 火把摇曳的火光将经过的人和动物投影在山洞里的墙壁上，于是这些人在山洞里为看到的影子评头论足，进行推理辩论，相信这就是这个世界上真实的存在。 这其实就是中国「坐进观天」故事的翻版，很可笑不是吗？当你身锁牢笼却不自知，目光局限在其中，光靠思辨是不可能获得真理的。 亚里士多德 亚里士多德是柏拉图的学生，没错他是苏格拉底的徒孙。亚里士多德在许多方面都有重大贡献，他总结整理了很多已有的知识，并根据自己的观察得出了很多精彩的结论。诚然，受于条件所限（身处某个牢笼），他的结论在如今看来并不完全正确（比如地心说）；但是，亚里士多德对与清晰思考的理论却依然闪耀。这就是「亚里士多德三段论」。 亚里士多德的三段论将论述分为三个部分，前两个部分是前提，第三个部分是结论。第一个部分是抽象的概述；第二个部分是对具体实例的描述；第三个部分则针对实例做出推论。如果前提正确，并且推论过程符合逻辑，那么结论就必然是正确的。 比如： 每一只正常的猫都有四条腿。Kitty 是一只正常的猫。Kitty 有四条腿。 这就是一个成立的结论。首先，前两个前提是正确的；其次，两个前提之间有「连续的主宾判断逻辑」；因此，结论成立。 再比如： 每一只正常的猫都有四条腿。Kitty 有四条腿。Kitty 是一只正常的猫。 这是一个不成立的结论。尽管两个前提是正确的，但是两个前提之间并没有连续的逻辑，因此结论是站不住脚的。比如，Kitty 可能是一只兔子，或是一只狗。 再比如： 每一只猫都是黑色的。Kitty 是一只猫。Kitty 是黑色的。 这也是一个不成立的结论。尽管两个前提之间有连续的逻辑，但是第一个前提是错误的，因此结论也是站不住脚的。 尽管举出的例子很简单，但是却反映了亚里士多德三段论的威力。三段论的意义在于 它给出了推理的一般规则：严谨遵循这个规则就能求得真理； 它给出了检验推论的标准：从前提的正确性和推理的逻辑性两方面检查推论，若然有一方面错误，则结论不成立。 小结 欧洲的文明发端于古代希腊和罗马与教会和日耳曼民族混合成为的一个不稳定的混合体。这个混合体随着文艺复兴运动而逐渐瓦解，经历宗教改革、科学革命、启蒙运动和浪漫主义运动，宗教的力量被削弱，逐渐形成我们如今看到的欧洲。 理性和浪漫的力量彼此对立，互相撕扯。这份根植在欧洲人文化血脉里的矛盾，煎熬着欧洲人的同时，也成为欧洲不断进步的动力。 尽管在科学革命之后，人们已不再将古希腊的学术奉为圭臬，但实际上是人们重新走在了由希腊人奠基的理性思辨道路上。苏格拉底为人们展现了通往真理的道路；他的弟子柏拉图用经典的比喻警醒人们不能坐进观天；柏拉图的弟子亚里士多德则在前辈的基础上整理了很多知识，并总结出了威力强大的「三段论」。 真想去欧洲看看呀~ : )]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[目前社会科学经验研究中「因果识别」都有哪些方法和新发展？z]]></title>
    	<url>/prof/2015/01/16/causal-inference/</url>
		<content type="text"><![CDATA[[1 冉筱韬 (随机实验，自然实验) 1.1 RCT - Random Control Trial，随机试验 1.2 Natural Experiment - Instrumental Variables，自然实验—工具变量 2 Manolo (充分统计量) 3 知乎用户 (流 4 慧航 (同群效应有一个问题) 5 匿名用户 (回归，机器学习，Rubin，结构模型，充分统计量) 6 王同益 (实验与非实验) 7 brillbird (实验，DID，IV，RD，PSM) 8 舒小曼 (实验法，RD, IV, 交叉滞后) 8.1 实验法（experimental method） 8.2 交叉滞后相关（cross-lagged-panel correlation） 8.3 断点回归（Regression Discontinuity,RD） 8.4 工具变量（instrumental variable） 9 知乎用户 10 知乎用户 (PSM) 11 匿名用户 (因果贝叶斯网络) 12 全球主义者 (质性研究方法) 13 fsaom (医学) 14 知乎用户 (合成控制、同群效应、异质处理效应) 14.1 Synthetic control 14.2 peer effects 14.3 heterogeneous treatment effects 15 知乎用户 (书) 16 WENN (两本书，概念与实例) 17 知也 (严辰松-定量型社会科学研究方法) 18 HiI’mPsycho (心理学中的三个条件) 19 张序 (流行病学专业) 20 因缘际会 (方法论文章推荐) 21 知乎用户 (空话) 原文地址：https://www.zhihu.com/question/27516929 1 冉筱韬 (随机实验，自然实验) 「因果识别」可以算得上是社会科学实证研究里的核心问题了。首先来定义一下什么叫做「因果关系」。 举一个例子来说明：读大学并拿到本科学位对收入有什么影响？（这里要注意的一点是，类似于「影响」，「效果」，或者英文里的 impact, effect, lead to, result in 一类的词都代表着你在描述的是因果关系。使用的时候要非常小心。） 对于某个特定的个人，我们需要知道两件事：(1) 这个人读大学并拿到本科学位之后的收入；(2) 让这个人穿越回要上大学之前的那个节点，不上大学，直接去工作的收入。用(1)减去(2)就是这个人读大学并拿到本科学位对他(她)本人收入的影响。 对于某群人，我们同样需要知道两件事：(1) 这群人读大学并拿到本科学位之后收入的均值；(2) 让这群人都穿越回各自上大学之前的节点，不上大学，直接去工作的收入的均值。同样用(1)减去(2)。 所以想要得到因果识别的结果，最关键的一点就是得到对照组（counter-factual），也就是上面列出的(2)代表的结果。 自然科学可以通过实验环境创造出比较可信的对照组，而社会科学的困境就在这里。我们研究的主体是人和人的行为，很难找到两个各方面指标都一致的人互为对照组，而实验也非常难以严密控制。如果有时光机，可以穿越，搞社会科学实证研究的大概都会喜极而泣吧。 所以总结起来，各种「因果识别」的方法本质上都是在寻找可信的对照组。 至于现在主流的常用的方法，@张译文的答案已经写的蛮全了。我这里做一些补充说明，顺便也是给自己读过的文献做个梳理。举的例子主要是劳动—教育经济方面的，角度会比较偏向政策分析。有重复的地方还请见谅哈。 1.1 RCT - Random Control Trial，随机试验 关于这个方法在发展经济学里的应用，MIT的两位大神Duflo和Kremer有一篇很好的工作论文做总结：Duflo, E. and M. Kremer (2005). “Use of randomization in the evaluation of development effectiveness.” Evaluating Development Effectiveness 7: 205-231. http://economics.mit.edu/files/765 RCT现在这么火跟Duflo在MIT的JPAL的大力推广关系很大。 这个方法从理论上来讲非常简单。还是举前面那个例子，因为我们没办法同时观测到一群人上大学拿学位之后的收入均值以及同样这群人不上大学的收入均值，我们可以将这群人随机分为两组，让group I的人去上大学拿学位，并让group II的人不上大学直接工作，用前者的均值减去后者的均值。 由于我们是将两组人随机分配的，理论上如果可以将实验重复很多次，那么多次实验得到结果的均值是趋近于真实的因果效应的。 如果我们能确保分组的随机性，并严格控制实验的进程，这个方法确实能帮我们生成一个可信的对照组，所以RCT也被很多人认为是「因果识别」的黄金准则。也就是说，很多研究者认为，同一个问题用这种研究方法得到的结果更可信，在有条件的情况下，应该尽量去进行随机试验。 但这个方法还是有一些问题的： 第一，结果不可扩展或者推广（external validity）。 比如在墨西哥有个实验PROGESA，政府随机选择一些社区，给这些社区的家庭提供现金支付，看这些社区的学校出勤率，健康诊所的访问率，还有儿童的营养状况是否得到了提升。假使研究者发现这个项目有正效用，是不是说中国政府或者印度政府也可以用同样的政策来提升教育与健康水平呢？显然不能，因为墨西哥的经济，环境，宗教，文化等方方面面都和中国印度不同，在墨西哥成功实行的政策在中国或者印度不见得管用。 很多RCT的支持者认为解决这个问题的办法是，在很多不同的context下重复同一个随机试验，观测结果是否稳定。但是，随机试验是非常昂贵的！为什么现在多数的RCT都是在非洲和东南亚的发展中国家做，这正是因为发达国家人力资本非常昂贵，你要在美国做个试验，光雇人录入数据的钱就可以在肯尼亚把整个试验弄好几年了。一句话，做不起啊！ 第二，以人为对象的实验非常难以操控。 美国田纳西有个实验STAR，目的是研究class size对学习结果的影响。实验把老师和学生随机分成三组：(1) 小班（13到17人）；(2) 普通班（22到25人）搭配带薪的助教；(3) 普通班无助教。 这里就有个问题了——如果你是家长，你孩子被分到了普通无助教的班里，你怎么想？你要是李刚你还不想尽办法把孩子弄去小班里啊！你要是土豪还不快点把孩子转去私立学校啊！就算你不是李刚也不是土豪也可以告诫孩子你被分到了最不好的班里一定要好好努力学习不然没救了啊！当然也有大批对此无动于衷的家长，可是这样的家长与会关心孩子分班结果的家长必然有本质的区别。总的来说，你无法预测被分到不同组别的被试者是否会有不同的反应和行为。 所有这一切实验设计者无法观测到的内部现象都会对实验结果造成影响。 第三，只能观测短期效应（起码目前来说是这样）。 这和前两点是相通的。因为实验的昂贵，和各种不可测的因素，我们很难用RCT来识别某项政策的长期效应。政策制定者当然想知道小学的class size是否会对成年后的收入产生影响，可是跟踪被试者20-30年基本上没有可操作性。 与此相关的另一个问题，被试者的流失。理论上只要流失的被试者是随机的就不会对结果造成影响（统计结果依然无偏）。可是我们可以看class size的例子，因为李刚和土豪的存在，显然普通班的被试者流失会更为严重，而且不是随机的。 而对于政策制定，我们想知道的往往是长期效应。这就是，你给了我一个很正确的结果，但回答不了我想要问的问题。 第四，这个不完全算是问题，但是是实验设计时非常值得注意的一点——选择哪个单位（level）进行随机分组。 上面墨西哥PROGESA的例子，研究者实际要研究的单位是家庭，可是随机分组的单位却是社区。为什么？假设重新设计实验，在一个社区内随机选择一些家庭，给他们现金支付。这时候有了现金的家庭更有能力去看病，感染上某些传染病的几率减少，不仅仅是这些家庭的成员会更健康，那些同一社区没被随机选中的家庭也会受到正的外部性的影响——他们得传染病的几率也减少了。为了消除这些spillover effects，设计实验时选择了更高一个单位的社区进行随机分组。 第五，出于人道主义的立场，有些实验不能做。 抽烟对健康有什么影响？我们能随机抽一些人强迫他们抽烟吗？ 1.2 Natural Experiment - Instrumental Variables，自然实验—工具变量 把这两个放在一起写是因为工具变量实际不是一种研究设计，往往是自然实验里会用到的统计方法。 同样是回答类似上面那个大学教育对收入影响的问题，David Card有一篇文章：Card, D. (1993). Using geographic variation in college proximity to estimate the return to schooling (No. w4483). National Bureau of Economic Research. http://econweb.tamu.edu/gan/econometrics1/w4483.pdf 上了大学和没上大学的人不是很好的对照组，因为他们在家庭背景、智力、偏好等各个方面都有显著的差距，我们不能直接把上了大学和没上大学的人收入均值相减。Card这篇文章是用college proximity(家附近是否有大学)作为工具变量：两个其他条件都一样的人，一个家附近有大学就去上了，另一个家附近没有大学就直接工作了。由于college proximity不同而造成教育结果不同的两类人，可以作为一对比较可信的对照组。 简单来说,样本里可能存在四种人： always-taker: 不管家附近有没有,都要去上大学的人 never-taker: 不管家附近有没有,都不会去上大学的人 complier: 家附近有就去上大学,家附近没有就不去上大学的人 defier: 非要家附近没有才去上大学,家附近有就不去上大学的人 (==!) 工具变量的：(1)第一条假设就是：defier是不存在的。这个没法从数据里验证，但还算合情合理吧。(2) 第二条假设是说always-taker/never-taker在工具变量赋值不同的情况下有同样的结果。这条也叫做exclusion restriction，对于一个always-taker来说，他最后的收入不会因为家附近有没有大学而变化，因为他总是会去上大学，用另一个角度解释，就是college proximity不能通过除了「影响一个人去不去上大学」这条路径以外的其它方式来影响一个人的收入。 所以实际上，工具变量能影响的只有complier，它也可以被表述成“简单OLS回归结果 除以 样本中complier所占的比例”。这里引出第三条假设：(3) college proximity对于是否上大学的决定不能为0，同时因为complier比例是被除数，如果它很小的话，会导致工具变量得到的结果非常大，这就是weak instrument的问题。 另外一条假设是工具变量本身是随机，也就是说你家附近是否有大学不可以是你选择的结果，对于那些为了生活环境特地搬去大学附近居住的家庭来说，这个工具变量就不适用了。这也是为什么工具变量往往是地理、天气、突然的政策变化等等不可人为控制的东西，正是因为它们的不可操控性，它们才更有可能是随机的，可以帮我们得到好的对照组。 另外，关于工具变量必须要强调的一点是，它得到的结果是LATE - local average treatment effect。也就是说它只适用于complier这种去不去上大学的决定(treatment status)会被家和学校距离远近(IV) 影响的人。在这个例子里，这是些什么人呢？因为学校远就不去上的人更有可能家庭条件不好，学习的兴趣更弱等等。这里得到的结果，是对这些人来说，上了大学对收入的影响。而对于前面RCT的例子，那个结果是对于各方面都比较接近社会均值的人，上大学对收入的影响，这两个结果是两个完完全全不同的东西，如果不相等是非常正常的事，那说明我们想要研究的政策/变量对不同的人有不同的因果效应(heterogeneity)。 每每在别人的文章里看到诸如「我们的IV结果与OLS不同，说明OLS的估计是有偏的」的句子，我都非常之烦躁，这本来应该是每个上过最基本统计课的人就了解的道理，可是不乏有顶级期刊里的文章还有类似的表述，实在是太不严谨了！ 本来以为很快就能写完的，结果发现是个大坑。 2 Manolo (充分统计量) @冉筱韬、@慧航等大神的答案已经十分详尽了。这里补充一个近年来在公共经济学，尤其是税收、社保领域用得比较多的方法：充分统计量（Sufficient Statistics）。以下内容全部来自Chetty在2008年完成的关于充分统计量方法的综述。 上图描述了充分统计量的工作机理。我们知道，识别方法有structural和reduced两个门派，前者要建模型，然后去估计其中的所有参数；后者则是通过做实验或者设法构造类似实验得到的数据，通过种种回归得到结果。充分统计量实际上是两者的折衷。以税收政策为例，虽然我们可能也很关心税收变动对商品产量、顾客需求等变量的影响，但说到底，我们最关心的还是社会的整体福利。如果我们能够把社会福利变动表达成其它一些容易估计的量的函数，事情就会好办很多。这种方法既有reduced form容易估计、假设相对少的优点，也有一部分structural form的优点：可以做政策模拟和预测。 Chetty在论文开头使用了Harberger在1964年AER发表的经典论文的简化版作为例子来说明这种方法，我也录在这里。这个方法必须要有实例才能明白其强大。如果为公式所困扰，可能没有什么好办法，因为充分统计量的实质就是找到巧妙的方法去简化模型，从模型中显式地解出我们需要的那些变量。假设经济体中有一种计价物\(y\)和\(J\)种消费品，生产商品需要投入\(y\)，成本函数为\(c(x_{i})\)。记消费者的消费向量是\(\pmb{x}=(x_{1},..., x_{J})\)，经济体中的价格向量是\(\pmb{p}=(p_{1},..., p_{J})\)，假设消费者效用函数取拟线性形式，并面对总数为\(Z\)的收入约束。当政府对商品\(x_{1}\)征收税率为\(t\)的比例税时，我们得到如下优化问题，其中待优化变量都是\(\pmb{x}\)。 消费者： \[ \begin{aligned} \max u(x_{1},..., x_{J})&#43;y\\ s.t. p\pmb{x}&#43;tx_{1}&#43;y=Z \end{aligned} \] 生产者： \[ \max p\pmb{x}-c(\pmb{x}) \] 利用生产=消费这一约束条件可以闭合以上优化问题。如果我们直接求解然后再去估计税收的影响，比如像Deaton和Muellbauer他们做的那样，这就是个structural的问题。但是，如果只是为了得到税收对福利的影响，这里完全没有必要去求解模型。直接写出社会福利的表达式，代入约束，我们得到如下结果： \[ \begin{aligned} W(t) &amp;amp;= \left\{ \max u(x_{1},..., x_{J})&#43;Z-p\pmb{x}-tx_{1} \right\} &#43;\left\{\max p\pmb{x}-c(\pmb{x}) \right\} &#43;tx_{1}\\ &amp;amp; = \left\{ \max u(x_{1},..., x_{J})&#43;Z-tx_{1}-c(x) \right\} &#43;tx_{1} \end{aligned} \] 两边对\(t\)微分，再利用包络定理，我们得到： \[ \frac{dW(t)}{dt} =-x_{1}&#43; x_{1}&#43;t\frac{dx_{1}}{dt} =t\frac{dx_{1}}{dt} \] 从这里可以看到，我们可以把福利的变动写成\(\frac{dx_{1}}{dt}\)和\(t\)相乘的简单形式。假设原有税率为\(t\)，新的税率是\(t&amp;#39;\)，在得到\(\frac{dx_{1} }{dt}\)的估计值，亦即需求的变动之后，我们就可以通过计算\(\int_{t}^{t&amp;#39;} t\frac{dx_{1} }{dt} dt\)来得到税收对福利的影响，以上就是运用充分统计量方法的一般流程。尽管在实际操作中问题通常没有这么简单，但方法是相通的。以下是Chetty整理的近年来利用充分统计量方法的文献，可以选择阅读。其它一些更高级的应用同样请参见这篇文献。 最后还需要强调一点：充分统计量在折衷两派，汲取各自优点的时候，也同时保留了两边的一些缺点。 首先，因为统计量的推导基于结构模型，如果模型设定有误，统计量的正确性也得不到保证。 其次，这样估计得到的结果在外推时不一定有效。最后，我们得到的估计实际上是个黑箱，比如说上面的例子，我们只能看到福利的变动，但影响福利变动的那些参数，我们一个都看不到。由于有充分统计量就有模型，Chetty对此给出的建议是在研究时既做structural，也做充分统计量，这样可以最大程度发挥二者的长处。 参考文献：Chetty R, Fort M, IZA C. Sufficient Statistics for Welfare Analysis A Bridge Between Structural and Reduced-Form Methods[J]. Annual Review of Economics, 2009, 1: 451-488. 3 知乎用户 (流行病学，详细) 居然没有人关注流行病学在causal inference当中的贡献……流行病学才是完完全全聚焦于因果推断的学科啊！我个人博士一年级在本系接受的都是计量经济学和社科那一路的训练，但后来在上过两年完整的流行病学方法后，现已经完全被流病洗脑。 改一篇旧文讲讲这件事~ 因果关系本身，理论上本来就不容易搞清楚，在研究中如何呈现也都很难。所以因果推断在流行病学，以及各类依靠因果关系吃饭的社科里，比如计量经济学，都很令人头疼。当然我曾经看到有人吐槽说：现在社会学里为什么还有人搞因果关系，搞因果关系的人都该剖腹自杀。我的确不明白此人为何这样说，如果那样的话，首先剖腹自杀的是我们整个公卫界，尤其是全体流行病学家，顺带自杀的应该还有计量经济学家等。另外，因果推断这套东西，归根结底是empiricism这一路线之下的困惑，毕竟相关问题的提出最早可以追溯到Hume。所以非这一路线的人自然很难认可因果这种说法，及其衍生讨论。这种路线之争我也就不自己挖坑跳了。（譬如曾经有人类学现象学一脉思路的朋友跟我说，譬如我自己心中种种微妙感受，和社会大环境又有什么关系，又如何梳理得出因果关系？） 目前流行病学界比较接受的因果关系的定义是和counterfactual effect密切相关的。这个坑爹词汇到底啥意思呢？曾经有个蛋疼的流行病学家写了一篇论文讨论希腊诸神参与的临床试验。在论文中，希腊诸神参与了一次标准的随机对照试验，一半的神被分到试验组，一半的神被分到对照组。之后我们可以得到两组众神的死亡率。这些神和我们广大凡人一样，在接受治疗时可能有四种情况：不治不能好但治了能好，不治不能好治了也不能好，不治能好治了也能好，不治能好治了反而不能好。神毕竟是神，所以我们可以知道他们治了以及没治两种情况的后果，从而得出诸神治了之后的死亡率，以及诸神没治的死亡率。这两个死亡率的差，才是治疗的效果（effect）。 但是凡人毕竟不是神，我们要么治了，要么没治，有的路走了就不能回头，我们用法无法获知没走这条路时我们会是什么模样。所以我们永远只能对比治了和没治的人，而无法对比所有人治了之后的结局和所有人没治的结局。对于个体来说，你获得治疗之后的后果，就是事实（factual）的情况，而你没有获得治疗之后的后果，就是反事实（counterfactual）的情况。因果关系的标准定义就是事实后果和反事实后果的差异。 当然，根据这个定义，如果没有更多的假设，我们将永远无法获知因果关系。因为，我们永远无法获知反事实的后果究竟会怎样。而我们在现实中所得到的将永远只是关联（association）而已。 但如果我们加上某些假设，我们在现实研究中所获得的关联，能够迫近因果的效应么？ Yes, we can. 但我们的代价是两个无法验证的基本假设： 你所观察的，就是实际上发生的。如果发生白大褂效应这种悲剧，那么你将永远无法迫近真相。(流病老师的原话是：BAD LUCK!) 你已经控制了所有confounding（混杂）。但是你永远无法确认这一点，因为没有任何统计检验以及数据的模样可以告诉你这一点。 虽然这两点我们无法验证，但没有这两点，我们都可以回家了。（流病老师原话） 在流行病学家喜欢的Pearl路线中，用所得到的关联，去迫近因果的过程叫做：identification。但这也是有条件的，不是所有情况都可以找到因果关系，某些时候，你必须要承认，即便你做到完美，你还是不行。而这个过程不外乎两条路，走前门儿（front door criteria），或者走后门儿（back door criteria）。若想搞清楚走前门儿和走后门儿的准确涵义，需要顺便搞清楚DAG（directed acyclic graph）以及对应的概率表达和结构模型，在此省略一万字。 一般来说，大家常用的路线都是争取符合back-door criteria，那就是控制confunding。当然控制confounding的方式有很多，譬如以上许多答案提到的RCT，再譬如现在社科很喜欢的Natural experiment（可参考《Natural Experiments in the Social Sciences: A Design-Based Approach》）。从DAG的视角来看，随机化本质上也是一个工具变量。之前发在NEJM上关于Oregon医保试验（Medicaid抽签）的文章既是一个RCT的设计，同时分析中也将随机化的结果用作工具变量。而Regression Discontinuity之类方法本质上也是natural experiment的路线，因为目标是要追求暴露分布（或者说干预实施）as if random的效果。当然，如果大家只有观察性数据，不能在研究设计上动脑筋的话，就要在别处动脑筋了。譬如，Pearl想出了front-door criteria。再譬如，社科中常用的propensity score，还有inverse probability weighting，doubly/multiple robust estimation。实在实在不行，就要走上simulation和bias analysis的道路了。 其实以上这两点假设，还有一个隐藏假设：我们没有测量偏倚以及选择偏倚，只有混杂。如果有测量偏倚和选择偏倚，我们的处理方法会更加复杂（流行病学家把所有的偏倚分为三类：混杂、选择偏倚、测量偏倚）。 总体来说说，流行病学家对于计算机、统计、计量经济学领域的因果推断进展吸纳得非常充分，也做出了自己的贡献。但是流行病学家整体来说不太使用structural equation modeling (SEM)，除了学科训练的壁垒，还有一些对SEM方法上的顾虑（参考Vanderweele那篇：structural equation models and epidemiologic analysis）。 知乎用户：一切皆因果，因子太多，结果不明确，导致了整个关系显的复杂。 大象：因为sem是用来做成熟理论框架下的estimation，流行病学或者说医学缺少理论框架，dag还只停留在representation的层面上，如果在卫生经济学里面可能会有所使用吧。就像hastie那本书里说，数据出发能得到无穷多种假设，你要么预设理论，让data来选择模型，要么预设模型，让data来选择理论，sem是前者，我们流行病往往是后者。 4 慧航 (同群效应有一个问题) 谢邀。之前各位已经写了很多很有启发的答案了，最近比较忙，积攒了很多问题没有回答，先行道歉。 在这里我总结一下计量经济学的门派吧，然后再说经济学中的因果识别方法，希望这个答案能在一个稍微大一点的视角做一点总结。 首先大家应该一下子就能想到，计量经济学首先有两个大的门派，微观计量和宏观计量。前者一般从微观个体出发，后者更多应用时间序列数据。当然，宏观经济学用到的方法，除了计量经济学的估计和识别之外，还有校准，这个我不是很熟，就不多说了。 而无论是哪个门派，下面又有很多宗派。比如在宏观计量里面，传统的matching moment、MLE和现在非常流行的Bayesian显然是两个不同的宗派。而在微观计量里面，则区分了structural-form和reduced-form两个宗派。 题主问的是因果识别，那我们就先来说一下不同门派、宗派之间因果识别的差别。 首先说我最不擅长的宏观计量。宏观方面，最popular的概念应该是格兰杰因果了。但是，相信大家都听说过，格兰杰因果不是真的因果。格兰杰意义上的因果仅仅是看滞后的变量能不能预测当期的变量，这里面问题就很多了。在这一领域，传统的方法是VAR，以及相应的VECM等，格兰杰因果也是在这个框架里面的东西。但是如果真的想要识别因果，特别是有当期影响的时候，就需要用SVAR了。 微观计量呢？微观计量的因果识别好玩的多。结构和简约两个门派总在相互竞争中不断发展新的想法。两者的区别在于，结构模型有有力的理论模型作为支撑，目的是估计模型的结构参数。而简约派则是避开复杂的经济理论建模和结构参数的估计，通过使用自然实验、工具变量等方法直接找到想要的参数（这个有点像唯识宗给你丝丝分析宇宙人生，而禅宗则是明心见性，直指人心）。 在举例子之前，我先总结一下以上答案提到的方法： OLS、实验：最基本的办法，除非有理论支撑，或者数据来自与实验数据，一般会失败。 IV：当存在内生性的时候普遍的解决办法，很多方法，比如RD、LATE都可以看作是IV DID：自然实验，实验组和控制组有共同趋势 RD：自然实验，外生的断点 以上的这几个方法都是reduced-form最经常用的。其中OLS和IV因为太general，所以structural model也会大量使用（在structural模型里面，IV经常是系统内部就可以找到的）。当然，对于structural model中足够复杂的模型，MLE、GMM以及许许多多其他估计方法都是非常多的。 为了更直观的给大家说清楚两个宗派的差别，我举个例子，如何识别peer effects。这套文献说白了就是想要看人和人之间的交互影响，比如你努力学习会不会影响到你的朋友也努力学习。首先我们来看structural-form怎么做的（Peer Effects and Social Networks in Education, Calvo-Armengol, Patacchini and Zenou, The Review of Economics Studies.）： 第一步，理论建模： 第二步：讨论均衡 第三步：讨论识别条件 最后，得到估计。 这里我就截屏略过具体的细节了，只想给大家看一下structural model是怎么做的，具体感兴趣可以看原文。 那么同样一个问题，reduced-form是怎么做的呢？Peer Effects in Program Participation, Gordon B. Dahl, Katrine V. Loken and Magne Mogstad, The American Economic Review给出了一个用RD来识别peer effects的例子(感谢 @zc deng 的slides)。 Peer effects的识别有个很严重的问题是reflection problem，也就是相互影响。而这篇文章使用了休产假的改革作为自然实验，巧妙的避开了这个问题。这篇文章没有像前一篇文章那样复杂的理论建模，自然实验的背景一下子把问题简化了许多。而在第一篇文章里面，这个问题则是在给出理论假设的条件下给出识别的。 现在，问题来了：拳有南北，国有南北么？ 哈哈，开个玩笑。有段时间，看文献的时候也会问，究竟哪个宗派更好。直到后来，我才发现我多虑了。在没有达到一定的水平之前，讨论自己归属什么宗派，总是有点自不量力的感觉。就好像有人问我是禅宗还是净土宗，我说按照我的修为，还到不了谈宗派的程度。 宗派归属可以不谈，但是宗派之间的差别却是可以谈的。前面说了宗派之间的差别，那么，两个宗派之间有没有联系呢？下面是私货时间，我自己的想法，不喜欢请轻喷。。。 我觉着，是有的。之前说的什么RD DID都是招式，但是仔细考虑下来，经济学的识别方法无非就两个：找实验、加假设。 这两个东西，一个是数据的问题，另外一个，则是理论的问题。 最理想的情况，是你有实验数据。这个时候，啥都不用多想，OLS就可以解决问题。但是多数情况下，你没有那么好的数据，这个时候，可以退而求其次，找一些自然实验。比如RD，虽然你的数据不是实验数据，但是由于有外生的断点，你只需增加一个假设，那就是其他变量在断点处没有断点，那么，你还是可以在局部识别出你想要的东西。但是，如果你连自然实验都没有，那麻烦就大了。你不得不增加更多的假设，然后你会发现，你的模型慢慢的变成了structural model。 再举一个例子，我想做教育的回报，比如上大学对未来收入的影响。最好的情况，是我做一个实验，找一批孩子，随机让他们上大学或者不上大学，几年之后看他们收入的差距，这是最理想的情况。如果不能做实验，那么退而求其次，我们看看有没有自然实验。比如某次考试高于某个分数的可以上大学，低于这个分数不能上大学，那么好了，我们可以做一个RD，看在这个分数线周围的学生有没有明显的工资差异。如果连这个都没有，那么我可能需要找IV，这个时候你就需要假设你的IV是外生的，而找IV也是个非常艰难的过程。最后，如果你IV也找不到，还有方法是你把上不上大学这个行为也进行建模，然后在这个模型里面加入一些很强的假设（比如方程之间误差项的分布、相关性等），通过这些假设，也可以作出结果。以上的这个过程，随着你的数据越来越差，假设也越来越强，模型也越来越结构化（你需要添加更多的结构）。 其实在宏观计量里面也是这样。比如前面提到的SVAR的识别，一般来说模型很难识别，为了识别这个模型，许许多多可能的假设被加到这个模型里面，使得最后这个模型可以被识别。比如一开始的时候，大家在误差项里面做假设，使得参数可以被识别。后来大家又直接根据宏观理论，假设某些系数的符号，给出识别，或者假设某些参数的值为0给出识别。这个过程跟上面的想法是一样的，如果不能识别，做假设也得把模型识别出来。至于怎么做假设，就看你的理论模型了。 所以，很多时候不是我们去选择用哪个宗派，而是手中的数据决定的。或者说，我们总是在假设和数据之间做trade-off。当然假设是不是合理，那是另外一回事，文章的好坏也跟假设是否合理有直接关系。 扯了这么多，无非是想说，其实因果识别的方法很多，大家列举出来的和大家没有列举出来的，总会有一些共性。答案有点杂，排版有点乱，时间关系，就先写这么多吧，这里面有太多东西可以写，这么一个答案显然是不能给出因果识别的全貌的。此外，如果有错误，请提出更正，欢迎讨论。 另外，大家还忽略了一个问题，就是关于因果，何谓因果，在学术界还是有一些争议的，这点过几天可能会在另外一个答案里面详细描述，暂时不做讨论。 最后，以上给了这么多废话，最后给一点干货吧，除了Angrist的《人畜无害的计量经济学》，有本书特别推荐阅读：Micro-Econometrics for Policy, Program, and Treatment Effects, MYOUNG-JAE LEE作为一本专门讨论treatment effects的书，很值得阅读。 5 匿名用户 (回归，机器学习，Rubin，结构模型，充分统计量) 经济学中主要有五种 approach： 多元线性回归（multiple linear regression） 机器学习（machine learning） Rubin 因果模型（Rubin’s casual models） 结构模型（structural models） 充分统计量（sufficient statistics） 下面转载 Kevin Bryan 的一篇文章，对于这些方法以及各自存在的问题介绍的比较清楚： “Does Regression Produce Representative Estimates of Causal Effects?,” P. Aronow &amp;amp; C. Samii (2016) by Kevin Bryan on February 26, 2016 A “causal empiricist” turn has swept through economics over the past couple decades. As a result, many economists are primarily interested in internally valid treatment effects according to the causal models of Rubin, meaning they are interested in credible statements of how some outcome Y is affected if you manipulate some treatment T given some covariates X. That is, to the extent that full functional form Y=f(X,T) is impossible to estimate because of unobserved confounding variables or similar, it turns out to still be possible to estimate some feature of that functional form, such as the average treatment effect E(f(X,1))-E(f(X,0)). At some point, people like Angrist and Imbens will win a Nobel prize not only for their applied work, but also for clarifying precisely what various techniques are estimating in a causal sense. For instance, an instrumental variable regression under a certain exclusion restriction (let’s call this an “auxiliary assumption”) estimates the average treatment effect along the local margin of people induced into treatment. If you try to estimate the same empirical feature using a different IV, and get a different treatment effect, we all know now that there wasn’t a “mistake” in either paper, but rather than the margins upon which the two different IVs operate may not be identical. Great stuff. This causal model emphasis has been controversial, however. Social scientists have quibbled because causal estimates generally require the use of small, not-necessarily-general samples, such as those from a particular subset of the population or a particular set of countries, rather than national data or the universe of countries. Many statisticians have gone even further, suggestion that multiple regression with its linear parametric form does not take advantage of enough data in the joint distribution of (Y,X), and hence better predictions can be made with so-called machine learning algorithms. And the structural economists argue that the parameters we actually care about are much broader than regression coefficients or average treatment effects, and hence a full structural model of the data generating process is necessary. We have, then, four different techniques to analyze a dataset: multiple regression with control variables, causal empiricist methods like IV and regression discontinuity, machine learning, and structural models. What exactly do each of these estimate, and how do they relate? Peter Aronow and Cyrus Samii, two hotshot young political economists, take a look at old fashioned multiple regression. Imagine you want to estimate y=a&#43;bX&#43;cT, where T is a possibly-binary treatment variable. Assume away any omitted variable bias, and more generally assume that all of the assumptions of the OLS model (linearity in covariates, etc.) hold. What does that coefficient c on the treatment indicator represent? This coefficient is a weighted combination of the individual estimated treatment effects, where more weight is given to units whose treatment status is not well explained by covariates. Intuitively, if you are regressing, say, the probability of civil war on participation in international institutions, then if a bunch of countries with very similar covariates all participate, the “treatment” of participation will be swept up by the covariates, whereas if a second group of countries with similar covariates all have different participation status, the regression will put a lot of weight toward those countries since differences in outcomes can be related to participation status. This turns out to be quite consequential: Aronow and Samii look at one paper on FDI and find that even though the paper used a broadly representative sample of countries around the world, about 10% of the countries weighed more than 50% in the treatment effect estimate, with very little weight on a number of important regions, including all of the Asian tigers. In essence, the sample was general, but the effective sample once you account for weighting was just as limited as some of “nonrepresentative samples” people complain about when researchers have to resort to natural or quasinatural experiments! It turns out that similar effective vs. nominal representativeness results hold even with nonlinear models estimated via maximum likelihood, so this is not a result unique to OLS. Aronow and Samii’s result matters for interpreting bodies of knowledge as well. If you replicate a paper adding in an additional covariate, and get a different treatment effect, it may not reflect omitted variable bias! The difference may simply result from the additional covariate changing the effective weighting on the treatment effect. So the “externally valid treatment effects” we have been estimating with multiple regression aren’t so representative at all. So when, then, is old fashioned multiple regression controlling for observable covariates a “good” way to learn about the world, compared to other techniques. I’ve tried to think through this is a uniform way; let’s see if it works. First consider machine learning, where we want to estimate y=f(X,T). Assume that there are no unobservables relevant to the estimation. The goal is to estimate the functional form f nonparametrically but to avoid overfitting, and statisticians have devised a number of very clever ways to do this. The proof that they work is in the pudding: cars drive themselves now. It is hard to see any reason why, if there are no unobservables, we wouldn’t want to use these machine learning/nonparametric techniques. However, at present the machine learning algorithms people use literally depend only on data in the joint distribution (X,Y), and not on any auxiliary assumptions. To interpret the marginal effect of a change in T as some sort of “treatment effect” that can be manipulated with policy, if estimated without auxiliary assumptions, requires some pretty heroic assumptions about the lack of omitted variable bias which essentially will never hold in most of the economic contexts we care about. Now consider the causal model, where y=f(X,U,T) and you interested in what would happen with covariates X and unobservables U if treatment T was changed to a counterfactual. All of these techniques require a particular set of auxiliary assumptions: randomization requires the SUTVA assumption that treatment of one unit does not effect the independent variable of another unit, IV requires the exclusion restriction, diff-in-diff requires the parallel trends assumption, and so on. In general, auxiliary assumptions will only hold in certain specific contexts, and hence by construction the result will not be representative. Further, these assumptions are very limited in that they can’t recover every conditional aspect of y, but rather recover only summary statistics like the average treatment effect. Techniques like multiple regression with covariate controls, or machine learning nonparametric estimates, can draw on a more general dataset, but as Aronow and Samii pointed out, the marginal effect on treatment status they identify is not necessarily effectively drawing on a more general sample. Structural folks are interested in estimating y=f(X,U,V(t),T), where U and V are unobserved, and the nature of unobserved variables V are affected by t. For example, V may be inflation expectations, T may be the interest rate, y may be inflation today, and X and U are observable and unobservable country characteristics. Put another way, the functional form of f may depend on how exactly T is modified, through V(t). This Lucas Critique problem is assumed away by the auxiliary assumptions in causal models. In order to identify a treatment effect, then, additional auxiliary assumptions generally derived from economic theory are needed in order to understand how V will change in response to a particular treatment type. Even more common is to use a set of auxiliary assumptions to find a sufficient statistic for the particular parameter desired, which may not even be a treatment effect. In this sense, structural estimation is similar to causal models in one way and different in two. It is similar in that it relies on auxiliary assumptions to help extract particular parameters of interest when there are unobservables that matter. It is different in that it permits unobservables to be functions of policy, and that it uses auxiliary assumptions whose credibility leans more heavily on non-obvious economic theory. In practice, structural models often also require auxiliary assumptions which do not come directly from economic theory, such as assumptions about the distribution of error terms which are motivated on the basis of statistical arguments, but in principle this distinction is not a first order difference. We then have a nice typology. Even if you have a completely universal and representative dataset, multiple regression controlling for covariates does not generally give you a “generalizable” treatment effect. Machine learning can try to extract treatment effects when the data generating process is wildly nonlinear, but has the same nonrepresentativeness problem and the same “what about omitted variables” problem. Causal models can extract some parameters of interest from nonrepresentative datasets where it is reasonable to assume certain auxiliary assumptions hold. Structural models can extract more parameters of interest, sometimes from more broadly representative datasets, and even when there are unobservables that depend on the nature of the policy, but these models require auxiliary assumptions that can be harder to defend. The so-called sufficient statistics approach tries to retain the former advantages of structural models while reducing the heroics that auxiliary assumptions need to perform. Aronow and Samii is forthcoming in the American Journal of Political Science; the final working paper is at the link. Related to this discussion, Ricardo Hausmann caused a bit of a stir online this week with his “constant adaptation rather than RCT” article. His essential idea was that, unlike with a new medical drug, social science interventions vary drastically depending on the exact place or context; that is, external validity matters so severely that slowly moving through “RCT: Try idea 1”, then “RCT: Try idea 2”, is less successful than smaller, less precise explorations of the “idea space”. He received a lot of pushback from the RCT crowd, but I think for the wrong reason: the constant iteration is less likely to discover underlying mechanisms than even an RCT, as it is still far too atheoretical. The link Hausmann makes to “lean manufacturing” is telling: GM famously (Henderson and Helper 2014) took photos of every square inch of their joint venture plant with NUMMI, and tried to replicate this plant in their other plants. But the underlying reason NUMMI and Toyota worked has to do with the credibility of various relational contracts, rather than the (constantly iterated) features of the shop floor. Iterating without attempting to glean the underlying mechanisms at play is not a rapid route to good policy. （以下是私货）几个相关问题中我的回答： 为什么这么多人觉得社会科学中的经验研究只能建立相关性，不能建立因果性？ DID, PSM 及 DID&#43;PSM 有何差异？DID 要假定不可观测效应随时间变化趋势相同？ 6 王同益 (实验与非实验) 社会科学研究中的“因果识别”手段可以分为两大类：实验方法和非实验方法。 对于实验方法，大家很容易想到学习生物课程时接触到的实验：一个对照组，一个实验组，两组除了我们所感兴趣的因素（记为A）不同之外，其他条件完全相同，然后我们观察到两组结果的不同就可以归结为A这个因素的影响，即A的不同就是导致结果不同的因。 社会科学研究探究“因果关系”本质上与自然科学实验所要证明的“因果关系”是一模一样的。但社会科学中涉及到的人、社会环境等等差异很大，为了使得研究结果能够推广应用到更大的群体、社会中，实验的样本就需要代表性，数量上也要求更大，所以进行社会实验的成本是非常高昂的。而且人具有能动性，对实验设计的能力也提出了非常高的要求。这也是为什么社会科学中的实验方法在近七八年来才有了显著的进展。 因此，长期起来，社会科学研究者使用的数据大多是调查收集的数据。由于调查对象的异质性、自选择行为等的存在，简单的按照某一个条件（记为B）将对象分成两组进行对比，是无法有效得出“因果关系”的，因为很难说两组对象的差异只有B这个条件的差异。类比于实验方法，其实就是没能为实验组找到一个非常好的对照组。 非实验方法的发展，一直是沿着“找/构造一个更好的对照组”这个方向发展的。从最简单的回归，到加更多的控制变量，到工具变量法IV/倾向性得分法PS/不连续回归法RD……然后越来越接近实验方法。 下面这张图归纳了实验方法-非实验方法。每一种方法的具体介绍，大家都维基百科一下，后者下载相应的介绍论文观摩一下吧。 7 brillbird (实验，DID，IV，RD，PSM) 这绝对是一个好问题啊，社会科学研究的最大问题，不就是难以识别因果关系么？前面四个答主的回答都很好，我也来凑个热闹，纯属抛砖引玉了。 Junyi Hou说的方法应该属于“新进展”，但肯定不是属于主流方法。如果题主是要做社会科学的实证研究，而不是做理论研究，那了解下当前主流的方法可能更实际，更有用。题主如果翻看主流的经济学、社会学期刊，里面做实证研究，特别是政策评估研究的，基本用的都是下面几个方法。 最简单的因果识别方法，当属普通最小二乘OLS。通过多元回归，控制其他变量，了解某两个变量的关系。国内经济学、社会学的实证研究，基本都是用OLS。要添花样的话，可以用GLS，非线性OLS。对于做微观应用计量的，离散选择模型也用的很多，logit, probit是各类期刊的常客。总之，OLS是最基础的，后面的其他方法，很多都是在它上面做改进。如果要刻意针对“因果识别”采取些纠正措施，那么下面几个方法是最常用的。 实验。natural experiment, field experiement, lab experiment 比如，field experiment就是随机招募被试人群进入控制组和其他任务组，比较组别的实验结果。lab experiment顾名思义，就是在实验室里做的，商学院里研究组织行为的经常用这个方法。实验法的好处就是刻意控制干扰变量，但很多时候不具备可操作性，社会科学很多问题没法做实验啊。 Difference in Difference, DID，差分再差分 这算是因果识别里最常用的方法了吧，panel data, time series里都会用到，翻看社会科学里政策评估主题的文章，十篇是可能有一半是panel data&#43;DID的。 工具变量IV和2SLS 这也是解决内生性问题（典型的是回归方程中遗漏变量的问题，以及反向因果的问题）最常用的方法之一。 举个例子，大家都知道制度可能影响经济发展，但是如何用实证方法证明呢？有学者研究了曾经是殖民地的地区，用“殖民者的死亡率”做工具变量，研究制度和经济发展的关系。他的逻辑是：欧洲殖民者来到一个地方，如果这个地方环境好，那么殖民者就会从长计议，把本国的“先进”制度引进过来，搞好“制度建设”；但是如果殖民者来了之后，发现环境恶劣，自己先死了一大半，肯定就不想长远待着，那么他们就倾向于快速“掠夺”地区资源，而忽视“制度建设”。百年之后，“制度建设”型地区和“短期掠夺”型地区经济发展程度明显不同。那个学者把“殖民者的死亡率”和“人均GDP”一回归，发现果然殖民者死的多的地区，人均GDP就越低！为什么呢？只能是因为殖民者死亡率和制度建设有关系，而制度建设又影响了后来的经济发展。 工具变量的最大问题就是，找到一个好的工具变量太难了。工具变量需要若干假定，而这些假定是很难都成立的。比如上面的例子，有人质疑，殖民者死亡率之所以影响经济发展，是因为地理和气候原因！殖民者死的多，是因为地方不适宜人居住啊，鸟都不拉屎的地儿肯定经济发展不好啊，和制度没关系啊。于是就争起来了。。。（那篇文章还是发表在经济学TOP期刊《American Economic Review》上的，引用率超高，不过不少都是去反对他的） regression discountinuity，翻译成中文叫断点回归，有模糊断点和清晰断点回归。 我看有知友解释了，很全面，我就不重复了。断点回归在经济学和社会学里面用的也很多，不过也有其局限性。比如，如果有多个混淆变量都有“中断”，那么就不容易知道，到底是什么造成了因变量取值的不同。 倾向值匹配 这也是2000年之后用的很多的方法。它的思路很简单，就是找到“相似”的对照组和控制组成员，然后再进行比较。用这个方法，先预测倾向值（列出所有可能的混淆变量，用logit/probit预测混淆变量对因变量的影响概率），再用倾向值进行匹配，最后基于匹配样本进行因果系数估计。但是倾向值方法的劣势也很明显，比如我们不可能找到所有的混淆变量，比如它不能很好地解决交互作用问题等等。 最后，不同学科对于这些因果识别方法的运用频率，似乎也有不同。比如，工具变量在经济学里面用的很多，但是似乎社会学就用的少一些，其原因不仅来自研究问题的差异，也和学科间基本思维方式的差异有关。无论如何，方法是为问题服务的，方法是基于理论框架的。对于做实证研究的，自己研究领域内top期刊的论文，对自己肯定是最有借鉴意义的。 8 舒小曼 (实验法，RD, IV, 交叉滞后) 对这个问题非常感兴趣，借此机会，整理了下之前零散的笔记并搜集了一些新的资料，当做是一次复习和沉淀。 社会科学因果推断中面临的一个重要挑战就是：相关不等于因果（也被称为内生性问题）。也就是说，A与B相关，并不能说A导致B。也有可能是B导致A，或者A与B之外的第三个变量C既影响A也影响B，从而导致A与B的共同变化。有一个有趣的例子，即雪糕的销量和淹死的人呈显著正相关，即雪糕销量越高，淹死的人就越多。但我们不能断言说雪糕销量导致人们被淹死。事实上，这是由于二者都发生在夏天造成的。解决这一问题的方法有很多，大致可以分为两种类型，一种是从数据收集出发，通过一定的研究程序来进行因果推断，如实验法，但社会科学中的很多主题和领域没法使用实验法，这就引出了第二种类型：从数据分析出发，通过一定的统计方法来推断因果（主要针对调查数据）。 8.1 实验法（experimental method） 实验法是社会科学尤其是心理学研究中进行因果推断的重要方法。大部分心理学研究都会选择实验法，因为它能帮助研究得出因果结论。在心理学研究中，实验法甚至被称作解释因果关系的唯一方法。在这种方法中，研究者会将实验参与者随机分配到不同情境中（通常是一个实验组和一个控制组），并确保这些情境除了自变量（研究者认为会对人们的行为产生影响的变量）之外，其他的条件完全一致。这样，我们就有理由相信，不同情境下因变量的差异是由自变量造成的。 为了更为直观地理解，这里介绍一个社会心理学当中的经典实验：最简群体范式(tajfel et al., 1971; billing &amp;amp; tajfel, 1973)。在tajfel的实验里，实验参与者是被随机分配到两个不同的群体中（随机分配是控制混淆变量的重要手段，即保证不同情境下除了自变量之外其他条件完全一致）。在一个实验中划分的依据是他们声称的艺术偏好：Klee的画和Kandinsky的画，更喜欢哪个？在另一个实验中，通过投硬币来把实验参与者划分到两个群体中。每组成员实际上从没有见过彼此，也没有见过对方组的成员，所以两组里的人都不认识，没有任何形成内群体或外群体刻板印象的基础，这也是“最简群体”这一名称的由来。最后，tajfel和他的同事让群体成员在内群体和外群体间分配奖励（最多15分）。结果发现，分给自己组的明显多于对方组的。这被称为“最简群体范式”。 8.2 交叉滞后相关（cross-lagged-panel correlation） 先上图。 埃龙及其同事(Eron, et al．, 1972)在一项对同一组儿童进行了为期十年的追踪研究中使用了该方法。上图简要地说明了他们的研究结果。在十三年级的学生中，对暴力性电视节目的爱好与攻击性之间的相关系数基本接近于零（r=-0,05）。同样，他们也发现，三年级与十三年级对暴力性电视节目的爱好之间的相关（r=&#43;0.05）可忽略不计。但他们在两个年级的攻击性上却获得了中等程度的相关（r=&#43;0.38）,这说明攻击性是一种相对稳定的特质。在评估因果关系的方向时，最有趣的发现就是交叉—滞后相关（即图中沿对角线所表示的两个变量间的相关）。如果我们要问，到底是有攻击性特质的人喜欢观看暴力性电视节目还是观看暴力性电视节目导致了攻击性呢？在这种方法中，只要通过检查对角线相关，就可以确定哪一种假设更适宜。三年级的攻击性与十三年级对暴力性电视节目的爱好之间基本上没有关系（r=&#43;0.01）。然而，三年级对暴力性电视节目爱好与十三年级的攻击性之间却存在着相当显著的相关（r=&#43;0.31）。事实上，与早期被试在三年级时对同样两个变量所进行的研究相比，这一相关系数要大得多。因此，因果关系的方向看起来似乎是，三年级时喜欢看暴力性电视节目导致了后来的攻击行为。 8.3 断点回归（Regression Discontinuity,RD） 基本的断点回归设计是一种前测后测的两组设计。前测后测是指在处理前后施以同样的测量（实际上RD设计并不要求前后测的测量一致）。然后，我们将根据前测的断点值分配不同的人或与处理相关的其他分析单位（如家庭、学校、医院、国家）到不同的组。两组是指处理组和控制组或两个不同处理组。为便于直观理解，图示如下： C代表根据前测断点分数分配的不同组；O代表前测；X表示实验处理或干预；上行代表实验组，下行是控制组。 慢！听起来怎么有点像实验法。断点回归与实验法的区别在于分配被试的方式不同：断点回归是根据前测的断点值（cutoff value）来分配被试的，而实验法是随机分配的。 接下来进入具体实例。假设有一个研究想要检验一种新的治疗方案对住院病人的有效性。假设有一种健康诊断，从1到100赋值，分数越高，健康程度越高。然后，我们以50分为分配标准，小于50分的人施以新的治疗方案，大于等于50分的人施以常规治疗。 下图描述了假设所有人都没有接受新的治疗方案的前后测的双变量分布。蓝X表示断点左侧的个案，他们在前后测中都是病的最严重的人。绿O代表相对来说更为健康的比较组，他们在前后测的表现都比较好。穿过双变量分布的实线是回归线，显示出前后测有很高的线性相关。 现在让我们想象实验组（断点值以下的个案）施以新治疗方案且存在积极影响的结果。为简单起见，我们假设新治疗方案对所有人的效果是一样的，都将提高10分的健康分数。如下图： 移去所有数据点，仅留下回归线，得到下图： 观察图3，我们很容易想到断点回归这一名字的由来。如果有治疗效果，我们会在断点附近观察到回归线的“跳跃”或“不连续”： 8.4 工具变量（instrumental variable） 工具变量是社会科学中基于调查数据进行因果推断的一种前沿方法。社会科学因果推断中面临的一个挑战就是：内生性问题（endogeneity）。也就是说，某个潜在的、无法观测的干扰项既影响因又影响果，导致无法做出因果推断（相关不等于因果）。例如，我们想研究家庭中的孩子数是否会影响母亲的就业，但由于生育孩子数量是可以选择的，因此解释变量存在内生性问题。工具变量就是解决上述内生性问题的重要手段。工具变量的原理最早是由Philip G. Wright在上世纪20年代末提出的。首先，我们给出一个典型的线性回归模型： \[ y=\beta_0&#43;\beta_1x_1&#43;\pmb{\beta}X&#43;\varepsilon\tag{1} \] \(x_1\)是自变量，或者解释变量，即因，\(y\)是因变量，即果。大写的\(X\)是外生控制变量。\(\varepsilon\)是误差项。如果有一个重要变量\(x_2\)被模型(1)忽略了，且\(x_1\)和\(x_2\)相关，那么对\(\beta_1\)的估计就是有偏估计。此时，被称作内生的解释变量，也即前面所说的内生性问题。要解决内生性问题，我们需要引入更多信息。工具变量的方法引入了一个外生变量\(Z\)，且\(Z\)必须满足以下两个条件：与\(\varepsilon\)不相关，但与\(x_1\)相关。或者说，\(Z\)仅仅通过影响\(x_1\)来影响\(y\)。即： \[\text{Cov}(Z,x_1)\neq 0;\quad =text{Cov}(Z,\varepsilon)=0\tag{2}\] 由方程(1)可以推导出： \[\text{Cov}(Z,y)=\beta_1\text{Cov}(Z,x_1)&#43;\beta\text{Cov}(Z,X)&#43;\text{Cov}(Z,\varepsilon)\] 在根据方程(2)和X是外生控制变量的假设，可以得到： \[\text{Cov}(Z,y)=\beta_1\text{Cov}(Z,x_1),\quad \beta_1=\text{Cov}(Z,y)/\text{Cov}(Z,x_1)\] 进而对进行无偏估计： \[\hat{\beta}_1=\frac{\sum^n_{i=1}(Z_i-\bar{Z}(y_{i}-y)}{\sum^n_{i=1}(Z_i-\bar{Z}(x_{1i}-x_1)}\tag{3}\] 方程(3)里的\(B_1\)就是引入工具变量后的无偏估计量。 谈完数学模型，再来谈工具变量的基本思想。工具变量\(Z\)在模型外，是完全外生的，其只能通过影响自变量\(x_1\)而间接影响因变量\(y\)。如果\(Z\)和自变量\(x_1\)密切相关，那么，只要\(Z\)有了变化，就必然会对自变量\(x_1\)产生来自模型外的影响。如果自变量\(x_1\)和因变量\(y\)之间真的存在因果关系，那么\(Z\)对\(x_1\)的影响也必然会传递到因变量\(y\)。最终，如果\(Z\)对\(y\)的间接影响能够被统计证明是显著的，那么我们就可以推断出自变量对因变量\(y\)存在因果关系。附上陈云松老师在其论文中的图示： 回到孩子数量影响母亲就业的例子。为了解决内生性问题，研究者巧妙地挖掘了人类生育行为中偏好有儿有女的特征，将子女老大和老二的性别组合作为工具变量。理由是：头两胎如果是双子或双女，那么生育第三胎的可能性大大增加，进而增加子女数。而子女性别是完全随机的，与母亲就业没有任何关系。 为了加深理解，这里再谈谈之前关注到的《Science》上的一篇心理学论文。弗吉尼亚大学的Thahelm及其合作者假设，中国南北方不同农作物的种植（水稻和小麦）会影响区域间的文化差异。简单来讲，水稻种植在灌溉和劳动力方面需要更多的协调和合作，因此相比种植小麦的北方，人们会表现出更多的集体主义倾向。通过调查统计发现，“水稻假说”能够很好地拟合研究中的数据。但我们不能说水稻假说就是导致南北方文化差异的原因，因为南北方除了种植作物不同外，还存在许多其他差异。为了进行因果推断，研究者使用了工具变量法。也就是说，研究者要找到一个变量，这一变量只能够通过影响自变量“种植水稻/小麦”的概率来影响南北方文化差异，而与其他不可观测的影响文化的因素无关（与误差项无关）。研究者找到的工具变量是“当地环境是否适宜种植水稻”，理由是：这一变量是自然环境决定的，不可以认为选择，是随机的、外生的。当然，这一变量是否只能通过影响农作物种植来影响文化还存在一定的疑问，但不在这里的讨论范围内。 参考文献： The Regression-Discontinuity Design 逻辑、想象和诠释: 工具变量在社会科学因果推断中的应用 政见：“南稻北麦”真的导致了文化差异吗？ Talhelm, T., Zhang, X., Oishi, S., Shimin, C., Duan, D., Lan, X.,; Kitayama, S. (2014). Large-Scale Psychological Differences Within China Explained by Rice Versus Wheat Agriculture. Science, 344(6184), 603-608. 9 知乎用户 大概看了下，大多数回答不是太过繁琐就是华而不实没有直指核心，其实我觉得这类问题我看过最好的回答是 Charles F.Manski 的那篇identification problem in social sciences and everyday life，文章很短才12页，静下心来看一看会比看知乎的回答有用的多，当然我想我的这个回答应该也不会有人注意到。 10 知乎用户 (PSM) 在社会学领域，目前实证研究比较火的主要因果识别理论就是倾向值匹配（PSM，propensity score matching）方法吧，按照Morgan &amp;amp; Winship（2007）书中归纳，“倾向值匹配”方法广义来看包括IV啊，DID，断点回归什么的～ 据说这个在国外已经火了十几年，搜causal effect能搜到好多论文把，统计系啊，经济系啊，社会学系什么的都有人在做。。。最近才在国内开始火起来……胡安宁（2012）写过一篇倾向值匹配方法的研究综述；他本人2014年发表的一篇文章也应用了该方法（广义倾向值匹配）去修正教育的回报，参见胡安宁（2014）。陈云松（2012）发表的《农民工收入与村庄网络：基于多重模型识别策略的因果效应分析》讨论了社会网络对农民工工资是否产生影响的问题；张春泥和谢宇（2013）发表的《同乡的力量》也适用了多种倾向值匹配的方法去估计network对于老乡找工作的净影响；郑冰岛和吴晓刚（2013）关于“农转非”问题的研究也使用这一方法…… 一般来说，社会学在因果识别的方法主要包括实验方法和准实验方法，“random experiment”是研究的黄金准则嘛（Fisher语）～可是一般很难实现纯随机的干预，因而社会学者更多使用准实验方法去估计某类干预的净影响，PSM就是其中重要一类方法。而且我本人不懂实验方法，不能妄加评论。 按照我的理解呢，倾向值理论有两派人在搞，一类是统计学家，主要从SUTVA（Stable Unit Treatment Value Assumption）假设和Random Assignment（随机分配）假设出发，认为干预不随着个体、时间、干预的前后而发生变化，这类假设太强以至于很难在社会学中直接应用；另一类是计量经济学家，他们的IV啊，DID啊，Heckman 2SLS啊什么的，这类方法实际使用的就比较多了。具体的操作主要包括两个步骤，一计算倾向值（匹配），二是计算平均干预效应（ATE，ATT之类的）。就是在匹配之前是跑一个logistic回归计算倾向值p呢，还是算一些很有趣的距离（如Mahalanobis distance）去衡量个案之间的相似程度呢。。。具体而言不同的学者根据优化／“缺失数据”填补的逻辑去做，创造了一大堆方法，如optimal matching什么的，不过本人学的很渣，没办法系统地介绍更多了。 如果题主想简单了解一下反事实框架和因果推断不妨参考谢宇（2010）《回归分析》一书p162-169；或者谢宇（2012）《社会学方法与定量研究》（第二版）中因果推论的相关章节。系统性的读物包括当今计量真神Angrist和Pischke（2008），Morgan &amp;amp; Winship（2007），郭申阳和弗雷泽（2012）关于倾向值匹配的书咯～其中第一本和第三本有中译本，第二本目前还没有中译本。 不过老板说得对，统计其实不能帮助我们真正意义上解决因果关系的方向问题，很多时候都没法回避来自reverse causality的批评（类似收入决定健康还是健康决定收入这样的问题），而这因果关系的识别仍然要依赖“理论”的发展。 参考文献 Angrist J D, Pischke J S. Mostly harmless econometrics: An empiricist’s companion[M]. Princeton university press, 2008. Morgan S L, Winship C. Counterfactuals and Causal Inference: Methods and Principles for Social Research[M]. Cambridge University Press, 2007. 陈云松. 农民工收入与村庄网络: 基于多重模型识别策略的因果效应分析[J]. 社会, 2012, 32(4): 68-92. 胡安宁. 倾向值匹配与因果推论: 方法论述评[J]. 社会学研究, 2012, 1: 221-242. 胡安宁. 教育能否让我们更健康——基于 2010 年中国综合社会调查的城乡比较分析[J]. 中国社会科学, 2014 (5): 116-130. 郭申阳, 弗雷泽. 倾向值分析: 统计方法与应用[M]. 郭志刚, 巫锡炜译. 重庆大学出版社. 2012. 谢宇. 回归分析[M]. 社会科学文献出版社. 2010. 谢宇. 社会学方法与定量研究(第二版) [M].北京: 社会科学文献出版社. 2012. 张春泥, 谢宇. 同乡的力量: 同乡聚集对农民工工资收入的影响[J]. 社会, 2013, 33(1): 113-135. 郑冰岛, 吴晓刚. 户口,“农转非” 与中国城市居民中的收入不平等[J]. 社会学研究, 2013 (1): 160-181. 11 匿名用户 (因果贝叶斯网络) 因果推断有两大框架，以 Rubin 等人发展出的 Potential Outcome 模型和 Pearl 等人发展出来的以因果贝叶斯网络（一种图模型）来给因果关系建模的方法。 似乎没人提到因果贝叶斯网络方法的发展，而我最近看到了一些相关的文章，稍微介绍一下好了。 Judea Pearl 的方法基本上在 2000 年出的 Causality 一书里面已经成型了，2009 年出了第二版。当然，相关的方法还在不断发展中。最近我看到的发展有： 做出了将以反事实方式定义的因果在图模型上表示的方法 将缺失值问题作为因果问题进行建模 我就只知道这么多了。正在学习中。 12 全球主义者 (质性研究方法) 在因果关系研究中，除了上述定量方法之外，质性研究方法也能提供有益的工具。“过程追踪” （process tracing）法正逐渐发展为质性研究方法中用来检验一个或者一些变量对某一社会结果的影响的最有力方法。 诚然，过程追踪不可能如定量方法那样对大量案例进行检验进而得出一般性结论，但过程追踪非常有助于达成两个目标：1. 不但验证变量的因果影响，更验证该因果关系的发生机制（mechanisms/causal processes）；2. 发现新变量，改进既有理论，或创造新理论。 不知大家对过程追踪的具体应用有无兴趣？先占坑。 13 fsaom (医学) 这个问题比较有意思，我从医学的角度讨论一下。 因果关系的确定并不容易，我们一般先是从具有统计学意义的相关关系入手。两个变量具有相关关系，例如和俗套的：小朋友和小树在某一年同时测定高度，以后每日测一次高度。我们第一个观察到的现象是人长高，树也长高。这时候我们的第一个问题是：这种现象是随机的，还是非随机的？为了解答这个问题，我们首先可以观察更多的数据，通过统计学方法，明确到这种现象是有统计学意义的，即“非随机现象”。 随后就要讨论，这个现象是否存在有因果关系，这时候我们可以从：时间关系、关联强度、可重复性、分布一致性、合理性、终止效应、特异性等方面来讨论。 时间关系：这个很好理解，就是“因”必然要在“果”之前。 关联强度：是指两个现象相关，那么它的相关程度有多大，描述关联强度的有：决定系数R²， OR，RR值等 可重复性：就是说你认为这两者有因果关系，那么是不是每次因出现都能有果，或者每次果之前都有因，是否可以重复。 分布一致性：是指“因”的分布和“果”的分布是否一致，例如说A地吸烟率高，那么A地的肺癌发生率是否也高，两者分布是否一致。 合理性：是指这种因果在科学上能否得到合理的解释，例如抽烟导致肺癌发生的机制已经很明确，那这就是抽烟导致肺癌的合理性。 终止效应：到“因”被终止后，果是不是会消失或减少。例如戒烟能降低肺癌发病率。 特异性：这个不太好说，现在好像也都不怎么提，我要不太懂，看有没有大神能帮我解惑一下。 因果关系的确定的确不容易，如果大家都能认识到“相关关系”不等以“因果关系”我敢保证微信微博上的那些什么“7岁男孩每天和饮料致白血病”这类的新闻会少很多。 14 知乎用户 (合成控制、同群效应、异质处理效应) 补充几个还没有提到过的进展。回头能想起来的话再补充一些简介。 14.1 Synthetic control 主要讲怎么把多个备选的对照组成员加权平均成一个对照单元，然后去和实验单元比较。文献去Abadie和Hainmueller主页找找就行。主要文献是Abadie, Diamond, and Hainmueller (2010, 2014)。 Doudchenko and Imbens (2016) 有扩展。UCSD的徐轶青写过一个R package，并有扩展。 14.2 peer effects 前面有些答案提到了许多方法都假设SUTVA。peer effects / network effects一脉文献研究这个假设不成立时的情况。文献不太熟，可以关注一下Charles Manski和Matthew Jackson。记得Acemoglu和Jackson在某年NBER夏令营讲过network effects，有视频。 14.3 heterogeneous treatment effects 现在有一批人致力于将machine learning的方法引入causal inference。我知道的比较活跃的有Susan Athey和Victor Chernozhukov. Susan Athey有几篇讲heterogeneous treatment effects的。前两天正好有两个牛逼同学合发了一篇AER，就是对Susan Athey的causal forest方法的应用。 2017-05-31 update: Athey and Imbens刚出了一篇JEP里面有一节就是讲目前heterogeneous treatment effects和机器学习的结合。芝大经济系去年毕业去了北大商院的徐阳和John List去年有一篇工作论文也跟这个主题有关，主要是在实验的条件下怎么测试heterogeneous treatment effects才能更好的处理multiple testing的问题。另外最近听过Amanda Kowalski的一个talk，讲的是怎么利用LATE框架下的always taker和never taker的信息来理解treatment effects heterogeneity. Keven Howe：Xu对SCM做了扩展吧，用bai的interactive fixed effects model。 知乎用户：感谢纠正。 Keven Howe：heterogeneous treatment effects现在都有哪些发展呀？好感兴趣。 知乎用户：Athey and Imbens刚出了一篇JEP里面有一节就是讲目前heterogeneous treatment effects和机器学习的结合。芝大去年毕业去了北大商院的徐阳和John List去年有一篇工作论文也跟这个主题有关，主要是在实验的条件下怎么测试heterogeneous treatment effects才能更好的处理multiple testing的问题。另外最近听过Amanda Kowalski的一个talk，讲的是怎么利用LATE框架下的always taker和never taker的信息来理解treatment effects heterogeneity. 有兴趣可以找来看看。。 15 知乎用户 (书) 推荐一本书：Morgan, S.L., Handbook of Causal Analysis for Social Research. 2013: Springer Netherlands. 16 WENN (两本书，概念与实例) 推荐两本书，基本上梳理了因果推断在社会科学的发展脉络。前一本重在概念，后一本重在实例。 Morgan, Stephen L., and Christopher Winship. 2015. Counterfactuals and Causal Inference. Methods and Principles for Social Research. 2nd ed. Cambridge: Cambridge University Press. Dunning, Thad. 2012. Natural Experiments in the Social Sciences. A Design-Based Approach. Cambridge: Cambridge University Press. 关键在处理混杂因素（Confounder），粗略理了一下方法，再附上一些实例： 实验 Experiment 投资对地区经济发展的影响。随机选择一些地区投资，剩下地区不投资，观察各自经济增长。 不过社会科学做实验的机会比较少，大多数还是通过观察研究。 配对 Matching 研究联合国的介入对和平的影响。19组两两配对的国家，除了一个联合国介入，一个没有，其他情况综合起来（倾向得分）都差不多，比如上次战争持续时间，人口，民族结构，军队大小，民主情况，地理。 回归 Regression 这个很多了，配合其他方法，但是要警惕混杂因素和内生性问题。 面板 Panel 这里也包括横截面数据和时间序列数据的研究，三者的关键都在处理内生性（误差项和自变量相关）问题，涉及到的方法有Difference-in-difference model, First difference model, Fixed effects model, 基本思想是通过差分消除部分误差项的干扰。 一个例子：婚姻对男性收入的影响。一些因素会同时影响婚姻和收入，有些和时间有关，比如年龄，社会经济形势，有些和空间有关，比如地区、家庭，当然还有其他个人因素。通过差分尽量减少与时间和空间有关的误差项的干扰（如，同年龄的人比，结果就不受年龄的干扰）。 断点回归 Regression discontinuity design, RDD 研究奖学金对毕业后收入的影响。假设入学考试成绩过了50分就拿奖学金，学生毕业工作后，比较那些当年拿奖学金和没拿的人的收入差距。问题是，当年考1分和考100分的人，其收入差距可能还来自本身的进取心、智力、家庭等，所以不能直接比较。所以只考虑48到52分的学生，他们其他情况差不多（没有了混杂因素），拿不拿奖学金可能是出于运气，可以看做随机的。 RDD又有精确（Sharp RDD）和模糊(Fuzzy RDD)，上面的例子是Sharp RDD。如果是Fuzzy RDD，则考试成绩越高，越有可能拿奖学金，之间是一个概率关系。 工具变量 Instrument Variable 一些特殊的“随机的”变量：天气、抽签、生日 当年英国管理印度各省有两种方式，一是直接管理，建立殖民政府，收税，二是间接管理，王国自治，代其收税。我们想知道，哪种管理更有利于经济发展。初期研究表明直接管理效果更好，后来有人反对，因为直接管理的地区本来资源更丰富，农业更发达。1848至1856年一条新法令实施，只有地区国王死亡且无继承人时，英国才可直接管理该地区。这样，国王死亡作为工具变量，就可以研究管理方式对经济发展的影响。 但是，不服从指派（Non-compliance）者会出现一些问题。一个例子：服兵役对之后收入的影响。越战时期美国用抽签的方式确定服兵役资格，那么，我们可以直接比较当年服役和没有服役的人现在的收入、健康、政治观点吗？不行。虽然“抽签”作为工具变量确实随机化了，但是抽到“服兵役资格”的人最后不一定真的去服役了，可能体检不合格，可能为了推迟服役去上了大学，也有一些没有抽到服役资格的人自愿服役，这些对收入都会有影响。所以只研究有资格且服役和没资格且不服役的人（服从者Compliers）。 因果图和因果机理 另外，因果图（Causal Graph）可以帮助描述变量间的关系，找到混杂因子（Confounder）、中间变量（Mediator）、冲撞点（Collider），无法观测的变量等，再考虑用上述方法处理混杂因素，方便又直观。推荐工具：DAGitty 其次，一些研究重新回到了探索因果机理（Causal mechanism），而不只局限于研究因果效应（Causal effect）的存在和大小。举个例子，假设，恐袭新闻导致当地民众反对外来移民，我们想要知道这个假设是不是对的，程度大不大，同时，我们也想知道这中间的原理是什么，新闻导致恐慌进而导致反对情绪，还是新闻导致反恐开支增加进而导致反对情绪。重点在分解直接因果和间接因果，间接因果即假设的机理，然而传统方法中介研究Causal mediation analysis(Barron, Kenny 1986)有可能因为受到无法观测变量的干扰，得出错误的结论。近年序列可忽略假设（Sequential ignorability assumption）提出后，再加上Pearl的图，因果分解变得容易不少。 17 知也 (严辰松-定量型社会科学研究方法) 因果关系成立的三条件：怎样判断事物间有因果关系？一般认为，两事物间因果关系成立的条件是：（1）从发生顺序上，因在前，果在后（temporal order）；（2）它们之间有关联（association）或者说共变（co-variation）的关系；（3）必须排除其他可能用于解释结果的因素（elimination of spuriousness)。现分别来说明。 先说时间顺序。假如我们认为两事物之间存在因果关系，而现在需要确定孰是因，孰是果。直觉告诉我们，只有发生在先的事物才可能是因，时光倒逆只是幻想。时间先后顺序说不清而无法断定孰是因孰是果的，两者有可能互为因果。如贫困与多生多育的关系。多生多育也许是贫困的原因，然而后者未尝不是前者的原因。因为穷，所以想多生一些孩子以增加劳力、摆脱困境。 时间顺序是因果关系的必要条件，但并非充分条件。把凡是发生在先的就作为因，显然大谬不然。如我在屋子里打了个喷嚏，外面紧跟着就响了个雷。能说我的喷嚏引起了打雷吗？不能。有个老师骑车摔骨折了，一查原来是家里前一天未给供奉的菩萨烧香。这未免荒谬。尽管如此，仍然有不少人错把时间顺序当作因果关系唯一的条件。 再说关联。关联就是通常所说的“相关性”（co-relation）。当自变量引起因变量的变化时，两个变量之间有一种恒定的联系，也就是说，自变量方面的每一个变化都引起因变量相应的、可以预见的变化。如果研究表明，每当我们改变事物的一个方面，事物的另一个方面就出现可以预见的变化时，我们就会考虑前者是否导致了后者。假设我们在不同的情景和条件下，重复同一个实验，这种共变关系总是保持不变，我们对当初的判断就更有信心了。 两事物之间的共变关系有方向和强度的问题。当自变量的值上升、因变量的值也相应上升时，两者呈正向的联系；而当前者上升、后者下降时，两者呈负向的联系。联系的强度说明共变的显著性。方向和强度都可用统计学中的相关系数来表示，可用统计软件进行计算。相关系数数值的范围是-1至&#43;1之间，越向两端，强度越大；正号表示正向的联系，负号表明是负向的联系，零表示没有联系。 必须注意，两事物之间的共变关系并非一定是因果关系。许多共变的事物之间并无因果关系。有人说，他家不能喝椰子汁，一喝椰子汁就出事，如丢东西，孩子生病等，因此家中现在决不喝椰子汁。这未免可笑。在很多时候，有高度相关的两件事情其实风马牛不相及。比如，美国婴儿腹泻发病率与南部各州柏油路路面的粘滞度呈高度相关。再比如，在上世纪60和70年代，印度儿童的出生率和美国人使用美国造汽车的比率呈高度相关。 像时间顺序一样，共变关系是因果关系的必要条件，但却也不是充分条件。必要条件，顾名思义，指的是事件发生必须具备的条件。如，必须是成熟的女性才能怀孕。然而仅仅是成熟的女性并不就能怀孕。怀孕还需要一个充分条件。充分条件是可以有的条件，而且，一旦具备这个条件，事件就必然会发生。显然，怀孕的充分条件是交媾和受精，这是产生胚胎的充分条件。再比如，给孩子喂奶是孩子生长的充分条件，因为只要有奶喝，孩子必然能发育长大。但是喂奶并不是生长的必要条件，因为喂别的食品，孩子也能获得生长需要的营养。因此，假如我们寻找促使孩子生长的根本原因或唯一的原因，喂奶肯定不是，因为它不具备双重身份，即既是必要条件又是充分条件。当且仅当一个事物既是另一事物发生的必要条件又是充分条件的时候，前者才是后者的真正的原因，即唯一的原因。 确立因果关系的第三个要求是尽可能排除其他可用于解释结果的因素，也就是排除干扰变量（confounding variables或confounds）。歇洛克·福尔摩斯破案是根据证据进行推理。是男管家干的吗？不像。不可能是男管家干的，因为床上的头发是金色的，而管家的头发是黑色的。然而管家会不会有意将金色头发放在那儿迷惑大家呢？科学工作者就像侦探，要确定导致研究（如实验）结果的是否还有其他原因。面对已有的信息，研究者不断思考这样的问题：我们能做出什么样的推论？推论合理吗？逻辑是回答这些问题的有力武器。研究者必须通过精心的设计，审慎的操作，竭力排除对研究结果其他可能的解释，使人信服拟议中的因果关系不是假象（non-spurious)。 值得注意的是，不管怎样努力，在社会科学研究中，我们所说的因果关系中的因，一般都不是那种唯一的原因，即既是后面结果的必要条件又是充分条件。我们只是说，两个事物之间在很多情况下，具有系统的联系。 有些哲学家干脆认为，因果关系是研究者赋予情景的，只表示研究者对世界的认识，并不一定反映现实本身。我们做出的只是对世界的推论，不能说我们已经发现了真实的世界。尽管如此，社会科学家和自然科学家一样，他们孜孜以求的目标仍然是寻找或确立事物间的因果联系。 与其他任何研究方法相比，实验研究最适宜用来验证事物间的因果关系。因果关系的三项条件，时间顺序，关联和排除其他可能的解释，都能在实验研究中得到清楚的论证。 （摘自严辰松《定量型社会科学研究方法》第八章） 18 HiI’mPsycho (心理学中的三个条件) 心理学里面要make a causal claim必须要满足三个条件: temporal precedence(一个变量先改变); covariance(两个变量相关); internal validity(没有其他第三变量或者替代解释)。最简单的方法就是做实验，因为你可以控制变量。也可以做quasi-experiment(准实验)，但会牺牲一些internal validity;也可以做case study，但是generalizability不够。(自己的取舍就比较重要了) 还可以做很多longitudinal和相关，然后找最简的解释。当然心理学属不属于社会科学要另外考虑了(物理学、数学:“必须是！” 19 张序 (流行病学专业) 因果识别方法不请流行病学专业，答主很生气。 在流行病学专业当中，因果识别是非常困难的，所以当你选择流行病学课的时候，因果判断可能被当做非常重要的一个章节去讲。 因果关系的判定极为困难 如今，因果判断有了很多的发展，上学的时候学了很多的准则，mill方法，hill策略。。但是最终发现，无论怎么证明，因果总是有点证不明。 所以我们很多人开始使用循证医学的方法，综合样本，然后用不同的证据去证明。 而你的理论假设证明了，那现实当中如此众多的混杂依旧没有排除。所以，当我们要想找到因果关系的时候，总是那么困难。 因果关系在不断被证明-推翻-再证明的过程 在流行病学研究中，其实有很多研究基于社会因素，而也正是这些因素，是最难以研究的，经常看到，不断的有新的同行进入，在原有的假说上继续证明，很可能就是因为别人认为，可能某个因子可能是一个混杂因素。要知道，当有可能有混杂因素存在的时候，因果可能偏差的非常大。比如说，我们开脑洞的研究穿短裤与雪糕销售的关系，然后发现有统计学关联，但是这样的关联是因果吗？当然不是的，这个关联仅仅是一个统计学关联，也就是数学上看起来有意义，实际上不是真正的因果联系，而真正的因果，大家都知道，其实是因为夏天温度高，所以冰棍卖的快，而这样的假关联在统计上我们只要一排除，马上可能发现没有关系。 当然也可能甚至变高了，因为在调查过程中，还有各种各样的问题，可能由于你的设问方式不对，又出了新的结果。 所以这么诸多的问题，在证明一个假设的时候不同的学者都要争论很多年，就一个吸烟和肺癌，从一开始的没有关系，到确定是独立危险因素，用了好几十年，在不断推翻重建当中，找到的证据质量都在升级。 常用方法 当然啦，横断面，病例对照。也都可以证明因果关系当中的部分。 但是如果说，真的要想证明因果关系，无论什么学科，都只有两种方法，就是队列研究，还有实验。因为，其他的一切的找到关联的研究模式，都不能解释关联的时序性。我们看看统计就知道，既是是纵断面的时间序列，也不能够一边解决关联强度，还有关联时序性的问题。 统计只是告诉我们关联。 我们只有跟踪一群人，无论干预与否，然后采集结果，这样研究才有时序性可言。但是，当然这个开销巨大，花费的资金可能近乎天量资金，然后不一定能够得到我们想要的数据。 20 因缘际会 (方法论文章推荐) 若对最近十年内关于因果关系提取的进展感兴趣，或许可以看下面这篇文章的第二部分：Causal discovery and inference: concepts and recent methodological advances。最近十年内这个领域有很大的发展。具体来说，两个IID的变量之间的瞬时因果关系一般也都能从被动观测数据中提取出来。而且因果关系在machine learning里面有直接用途。 21 知乎用户 (空话) 好有意思，因果识别竟然什么学科都抛出来，因果识别就是建立一个逻辑关系。无非就是在所有事物中找出与目标相关联的因素，因素中存在直接发生效用的就是因果。 这东西国外论文不可能说清楚，看汉语或汉字，根据汉语的词性来归类是最简单的方法。如果你想问的是统计概率或是量化中的要素关联权重的识别问题，那个不属于因果，属于归纳汇总的逻辑方向。 因果有几个基本的特征，第一个，单一要素构成，既一个原因匹配一个结果，多个因素构成一个结局是统计，一个事物包含多少个要素是量化，是否发生判断发生等是概率，单纯的因果只有1对1，不能有两个原因导致1个结果。人类社会行为全部是从结果去找原因，而并非是找寻结果答案。 如某日某地发现一尸体，这是最现实的状况，这里只有结果没有原因，结果是人死了，然后叠加要素来判断原因，自杀还是他杀，这个是最简单的因果状态，一个结果由一个原因所致，死是一个状态的表现，所以一般具有状态表述性的词语都具有结果性质。 那么不可能没有原因的死掉一个人，就算是猝死那也得是在患有相关疾病的情况下。所以怎么死的就是死的原因，这个是核心逻辑，如果是被杀，那么被谁杀和动机就属于引发核心的本质逻辑的原因逻辑，也就是一件事物是在核心的本质逻辑之上又叠加了其他的逻辑所组合成的整体而切均为必然性质。 凡是带有主观目地性的词语大都具备原因要素，动词居多，像生活中最长碰到的语言逻辑会出现这种问题，你你又没喝酒为什么去厕所？去厕所是对结果的一种表述，就以小便来说，问题就不是建立在同一逻辑关系上，小便是因为尿急而不是因为喝酒，喝水也会尿急，而尿急的原因也不是喝酒或喝水，酒水只是说明，跟厕所的用意一样，原因是你喝了。所以喝东西到去小便是两层逻辑，而如果是去厕所又包含了对小便的说明，又含有逻辑。以上是比较基础的逻辑因果关系。 难点的如有效关心 1&#43;1=2 这种逻辑就比较难做因果。总的来说现代科学基本是以数学的方式在不动的角度对不同的因果做解读从而能够建立逻辑]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[为什么计量经济学家不看 R-square - 慧航 - 专栏]]></title>
    	<url>/prof/2015/01/08/r2/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19931167 学过线性回归的同学肯定都学过\(R^2\)，而且老师都会 为什么要重新提出这个问题，主要是在线性回归中的 ANOVA 的作用是什么？ - 方差分析和方差分析和回归分析的异同是什么？ - 统计学这两个问题里面跟某些人发生了一些争执。 如果你去看这两个题目，首先有一点需要注意的是，第一个问题里面的 ANOVA 指的是做完线性回归之后汇报出来的那个方差分析表，跟后面的方差分析还不完全是一回事。做完线性回归之后的那个ANOVA表主要是用来计算\(R^2\)的，这一点可以看我的答案方差分析和回归分析的异同是什么？ - 慧航的回答 那么，为什么我说在计量经济学领域，\(R^2\)是个不靠谱的指标呢？因为计量经济学关注的是解释变量究竟是怎样解释被解释变量的，而拟合的好坏，多数情况下我们并不关心。 为了说明这一点，我做了几个数值模拟告诉大家： clear set more off set obs 1000 gen z=rnormal(0,1) gen z2=rnormal(0,1) gen x1=z&#43;z2 gen x2=z2&#43;rnormal(0,1) gen y=-1*x1&#43;2*x2&#43;rnormal(0,1) gen y2=-1*x1&#43;2*x2&#43;3*rnormal(0,1) *********** different error terms********** reg y x1 x2 reg y2 x1 x2 *********** transformation ****** gen yp=y-3*x1 reg y x1 x2 reg yp x1 x2 *********** IV, negative R-square ******* reg y x1 ivregress 2sls y (x1=z) 以上是用 Stata 写的，非常简单。我们先来比较第一组结果，也就是 *********** different error terms********** reg y x1 x2 reg y2 x1 x2 结果如下： 两个回归的差别仅仅在于，第二个方程的扰动项的方差是第一个的 3 倍，导致\(R^2\)从 86.4% 下降到了 40.95%，于是我们可以得到一个结论：\(R^2\)度量的是你未观察到的部分与观察到的部分的方差，而如果我们的兴趣点在与\(x\)对\(y\)的影响，那么再小的\(R^2\)也不代表模型的解释能力弱。 下面我们来比较第二组结果，也就是： *********** transformation ****** gen yp=y-3*x1 reg y x1 x2 reg yp x1 x2 结果如下： 可以看到，我仅仅是在\(y\)上减去了\(3x_1\)，的到的\(R^2\)就从 86.4% 上升到了 96.17%。我们可以证明，第二个方程的估计结果应该是和第一个方程的估计结果一模一样的（\(x_1\)的系数要加上 3 之后一模一样），连 standard error 也一模一样，说白了，这两个是同一个回归，但是，\(R^2\)却差别很大。请问这样的\(R^2\)的上升有意义么？ 最后一组，也是最 amazing 的： *********** IV, negative R-square ******* reg y x1 ivregress 2sls y (x1=z) 回归结果： 如果仔细看我的数据生成过程，我做回归忽略了\(x_2\)，自然导致了内生性的问题，所以 OLS 的回归结果是有误导性的，不对的。解决办法是用 IV 的方法，也就是下面的回归结果，回归系数与真实值（-1）差别不大。但是你仔细看一下，IV 的回归结果里面没有报告\(R^2\)，知道为什么么？因为经过我精巧的设计，你会发现，在这个例子里面，IV 估计的\(R^2&amp;lt;0\)。但是从计量经济学的观点哪个估计好呢？IV 的估计好，因为 IV 的估计准确的告诉了你\(x_1\)对\(y\)的影响。 综上，在我们做完回归的时候，\(R^2\)高并不代表我们的回归方程解释能力强，\(R^2\)低也不代表我们的回归解释能力就差。用\(R^2\)来评判回归，至少在计量经济学里面，是比较业余的。 p.s. 最后补充一条，我说计量经济学里面\(R^2\)不重要，不是说这个东西完全没用。比如当我们做收入不平等问题的时候，多少不平等来自于观察到的差距、多少来自观察不到的等等，\(R^2\)和 ANOVA 表格还是非常有用的。但是多数情况下，拿\(R^2\)评判别人的模型是非常业余的行为。 下面这段是给 @weixin shi 科普的，证明在此： 命题：有截距项的 OLS，其\(R^2\geq 0\)。 证明： 炒鸡简单的一个证明。看不懂不要问我了。我不用问我计量老师，我本身就是半个计量老师。如果你计量老师告诉你这条定理不对，我真担心你们学校的老师质量差的可以。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[中国劳动市场的匹配与失业 - 慧航 - 专栏]]></title>
    	<url>/prof/2015/01/06/labour-shortage/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19851711 原文：VoX，Labor market matching and unemployment in urban China In the traditional labor suppl approach, unemployment usually results from a lack of labor demand or excess of labor supply. However, in urban China, unemployment coexists with a conflicting phenomenon, shortage of workers in firms. In this study, we employ a novel approach to tackle this issue, search and matching theory, the empirical study of which has not drawn much attention in China. Our multiple model consisted of job-worker matching, job creation and destruction, rural–urban immigration and on-the-job search, and unemployment changes in China. We used non-linear estimation and the three-stage least squares analysis in this study. We found that matching efficiency declined greatly during the 1996–2008 period. The econometric model and simulation results indicated four key factors that led to changes in China’s unemployment level: matching efficiency, job destruction, productivity growth, and job-search services. Finally, by using our econometric model, we identified the reasons for the shifts in the Beveridge curve. 为什么在中国有大量的失业，但是同时存在着劳动力短缺？这篇文章从匹配的角度解释这一问题。文章估计了中国的匹配函数（matching function），发现从 1996–2008 年，匹配效率一直在降低。 匹配和搜寻问题也算是 labor 和 macro 一直以来的热点，最近这段时间在中国问题上使用这一套方法的越来越多，而且我个人认为这个问题也很重要。之前想过一个关于这个东西的 idea，但是卡在一个地方一直没能继续。感兴趣的可以看一下这篇文章，看看是不是可以继续走下去。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[欧洲史·二：欧洲的思潮 z]]></title>
    	<url>/arts/2015/01/06/history-of-europe-02/</url>
		<content type="text"><![CDATA[[原文地址：https://liam0205.me/2015/01/06/history-of-europe-02/ 上一次讲到欧洲文明发端的三股力量和古典、中世纪和近代在事件和时间上的划分。今次写一写和思想有关的东西。 上述三股力量组成的混合体经历了长达一千年的中世纪时期。然而，尽管持续时间很长，这个混合体内部却并不稳定。伴随着文艺复兴的开始，从公元 1400 年左右开始，这个混合体逐渐崩毁，并开始建立起新的秩序。 文艺复兴 前一篇文章里提到，中世纪时期，基督教会保留了许许多多的古希腊科学文化知识。然而，基督教会却没有将这些科学文化知识公开；而是藏起来，选择其中对维护自己教义有利的知识，甚至是曲解其中的含义来维护自己的教义。这实际上是对人的思想的一种控制。基督教会利用这些知识，将人们日常生活中能够观察到的现象和基督教的神学联系起来，控制人们的思想，巩固自己的地位。 文艺复兴运动的意义在于，当时有许多游离于教会之外的学者，将这些知识从神学当中剥离开来。它将人们的生活从神圣的教会的光辉中解脱出来，认为宗教只是私人事情的一部分，不应该左右社会，也不应该控制人们的思想。总而言之，它将社会逐渐世俗化。 将知识从教会的手中解放出来本来应当是一件好事。可是，当时的学者认可古希腊的科技知识，向往其时的社会状态，并对此达到了一种狂热的地步。他们认为古希腊的科技文化知识是经典的、最好的、无与伦比无法超越的。也就是说，他们继承了古希腊的知识，却未能继承古希腊人们辩证的思想。 在文艺复兴运动中，思想得以从教会的控制中解放，然而将古希腊当做是「上限」却又阻碍了欧洲「再进一步」。 事实上，「古典」、「中世纪」和「近代」这样的划分就是文艺复兴时期的观点。那时候人们认为古典世界已然臻至完美，却在中世纪被基督教会掌控逐渐偏离正道，而文艺复兴又重新回到了正轨。古希腊人曾经认为人体是完美的，在艺术上的裸体展现得是人体的力量和美。而中世纪的绘画雕塑则对裸体遮遮掩掩，反映的是基督教的原罪，认为裸体是邪恶的。到了文艺复兴，米开朗琪罗的雕塑作品，当然是裸体塑像，则被认为是尊贵、高尚和美丽的化身。 而反过来，欧洲人至今以基督诞生的那年作为纪年起点。前一种对年代的划分明显排斥基督教，实际的纪元却又与基督教密不可分。 此间的矛盾一方面反映了欧洲继承自「混合体」，另一方面反映出文艺复兴虽然是思想的一次革命，但却不够彻底。 宗教改革 前文提到，在进入罗马帝国的几百年间，基督教发展出了庞大而完善的体制；在罗马帝国灭亡之后，教皇与君主平起平坐，管理文武百官。最终的结果就是前文提到的「基督教变成了罗马人的宗教」。 实际上，教会发展到最后，变成了这样一种怪物：教义由教会制定、整个社会也受教会管理、对犯错的人的审判也由教会进行。你没看错，立法权、行政权、司法权都被教会一手掌握。这种畸形的怪物发展到什么地步呢？如果你是一个富商，在你将死的时候，会有神父告诉你，你必须把钱捐出来给教会，不然你就进不了天堂。 马丁·路德是一名虔诚的基督教徒，他对自己的原罪感到无可奈何，对如何救赎毫无头绪，终日煎熬。他不知道自己一个浑身罪恶的人，如何才能得到救赎。有一天，马丁·路德读到《圣经》中保罗写给罗马教会的书信，信中说：「只要你相信耶稣基督，就能得到救赎」。也就是说，你其实什么也不用做，不用遵循教会制定的法度，不用对神父言听计从，只要相信上帝，坚守自己的信仰，就能得到救赎。 马丁·路德认为，《圣经》是唯一的权威：凡是圣经上没有写的，教会就没有理由去制定或者执行哪个「训令」。马丁·路德意识到，人们的思想为教会所控制，根本原因在于《圣经》是由拉丁文所写，并不是人人都能阅读的。于是马丁·路德将《圣经》翻译成德文，使得人人都能从《圣经》中得到自我救赎的力量。 马丁·路德的基督教后来从罗马教会的基督教中分离出来。遵循「因信称义」的观点的马丁·路德教派，后来发展为「基督新教」，而罗马教会的基督教，则被称为「天主教」。 宗教改革的核心观点在于，《圣经》是唯一的权威，基督教并不是罗马人的，要让基督教回到中世纪之前的样子。 有意思的是，马丁·路德从天主教那里拿来《圣经》把它翻译出来，变得人人都可以读，人人都可以从中得到自己的见解。于是，原本唯一的、权威的解读不复存在了，各种解读并立而起，马丁·路德自己的解释也被攻击。经过百余年的争吵，谁也无法说服谁，天主教和基督新教慢慢变得和平共处。 文艺复兴和宗教改革在一定意义上都是「回到过去」。文艺复兴从罗马教会那里拿来古希腊和古罗马的文化知识，来对抗教会对知识的钳制；宗教改革则从罗马教会那里拿来圣经，颠覆天主教的神学和权威。接下来 17 世纪的科学革命则整个颠覆了过去人们的观念。 科学革命 由于观测手段的限制，希腊人认为地球是宇宙的中心——有谁能推动地球呢？17 世纪，这个观点终于被推翻。地球环绕着太阳运行，太阳才是这个天体系统的中心。 首先是观测。随着观测手段的不断进步，人们对星体运行的轨迹描绘得越来越精确。与此同时，这些轨迹与地心说的轨迹误差越来越大，地心说的维护者不得不去不断进行修正，使得地心说的模型变得越来越复杂。其次时理论。牛顿的万有引力定律和相应的数学手段为日心说提供了有力的理论武器。人们精确地计算出了地球绕太阳运转的轨道，与观测事实符合得很好。 这样一来，地心说终于招架不住，教会不得不低头，承认日心说。在科学发展的过程中，人们产生了两种情绪： 希腊人错了，古典并不是完美的 自以为特别的我们在宇宙中其实很普通 好在，那个时代的人并没有因为日心说、进化论之类的发现而妄自菲薄，贬低自己的重要性。相反，他们认为，如果人们能够借由理性，探索出整个自然体系的运作规律、用数学模型精确表达，再将这些成果反馈到人类生活上，人类就会发生巨大的改变。这种渴望，成为了启蒙运动的原动力。 启蒙运动 启蒙运动于 18 世纪从法国开始，其目的是弘扬理性，用理性改造神学、政府和社会观念。 启蒙运动时期的人们认为，当时的法国有两大非理性势力。其一是教会。教会到处宣传神迹故事，控制人们的思想，威胁他们说如果不听话就要去地狱受苦。其二是法国国王。国王说自己是受上帝神谕管理这个国家的，质疑王权就是违反教义。（奉天承运什么的……） 启蒙运动过程中，有一部百科全书被人们汇总出来。它是第一部现代意义上的百科全书。其将理性运用于一切事物，将神、宗教和其他概念并列于同一个层级。这传达出一个信息：宗教是迷信。 从文艺复兴到宗教改革，从科学革命到启蒙运动，曾经主导欧洲社会的基督教会地位被不断削弱，欧洲走上了理性、进步的道路。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Lorem Ipsum]]></title>
    	<url>/en/2015/01/01/lorem-ipsum/</url>
		<content type="text"><![CDATA[[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[欧洲史·一：欧洲文明开端的三股力量 z]]></title>
    	<url>/arts/2014/12/30/history-of-europe-01/</url>
		<content type="text"><![CDATA[[原文地址：https://liam0205.me/2014/12/30/history-of-europe-01/ 欧洲文明的发端，有三股力量： 古希腊和古罗马的科学文化； 作为犹太民族宗教的分支的基督教； 征服了罗马帝国的日耳曼战士文化。 希腊和罗马 一句话概括的话，希腊人在科学文化上极其聪明；而罗马人则相对更加骁勇善战，治国、治军、工程建筑比希腊人更优秀。 希腊人的聪明不需多说：哲学、艺术、文学、数学、科学、医学、政治，这些学科的源头都可以追溯到希腊。现代数学中，随处可见的「希腊字母」，实际上也是对此的一种「纪念」。 罗马人的骁勇也不需多说：古罗马的版图疆域横跨亚欧非，环绕整个地中海。 这样一来的结果就是，罗马人统治了欧洲，但是罗马人却以懂的希腊的语言和文化为荣。 基督教的力量 基督教是犹太教的一个分支。 在犹太人的眼里，「上帝」是唯一的神，而自己的族群（犹太人）则是上帝的选民。此外，犹太人相信遵守上帝的条律，也就是上帝借由「摩西」的口说出来的「摩西十诫」，就能得到上帝的优待。摩西十诫的要求实际上是一种道德的约束，这些约束在犹太人的发展中逐渐成为律法。 犹太教的一些特点和古希腊和古罗马人的信仰是有很多不同的。首先，犹太教只承认耶和华为唯一的真神；其次，犹太教中，道德和律法是密不可分的，古希腊和古罗马却非是如此（希腊神话里的神各种乱伦什么的）。 耶稣是一个犹太人。在耶稣传道的时候，巴勒斯坦已经被罗马帝国纳入版图。耶稣在传道过程中，修改、发扬了犹太教的教义；特别是，耶稣把犹太人严苛的道德训诫转化成了「宇宙大爱」。 大家都说：「你应当爱你的亲人，应当恨你的仇人」。我却要说：「你们应当爱你的仇人」。&amp;mdash;《马太福音书》 由于耶稣作为讲道人传播的教义与犹太教的教义有相悖的地方，所以犹太教的领导人和罗马帝国练手杀死了耶稣——将他钉在十字架上。 耶稣的死留下了一个分歧，即：只有先变成犹太人，才可以信奉基督教；还是承认耶稣关于「爱」的教会凌驾与一切之上，人人都可以信奉基督教？如果前一个派系获得胜利，那么如今的基督教将只是犹太教的一个小小分支；而显然，后一个派系获得了胜利。 日耳曼民族 日耳曼民族最开始居于罗马帝国的北方。日耳曼人骁勇善战，甚至可以说是「为了打仗而生」：日耳曼人认为「可以用流血去换得的东西，用流汗这样的方式去换得，是没骨气的表现，是下等人才会做的事情」。 从公元 400 年之后开始，日耳曼人开始入侵罗马帝国；而到了公元 476 年前后，这样的蛮族取代了整个罗马帝国。 三者的联结 对于犹太人，罗马人只是把他们当做是一群稀奇古怪的人，所以对信奉犹太教的人，作为统治者的罗马人并不怎么关心。但是在罗马人眼里，信奉基督教的人是一群反动分子——因为他们认为上帝是唯一的神，而不愿意对君主行礼——欲要除之而后快。 就这样，罗马人对基督教打压了三百多年。 不过，在公元 313 年，君士坦丁大帝公开表示支持基督教会。于是基督教成为了罗马帝国正式而唯一的宗教，基督教的发展走上正轨。于是，罗马帝国变成了基督教的天下。 这时候的基督教，在「地下工作」的三四百年间，发展出了自己的法度并设有法庭和监狱；教会也同时掌管婚姻、继承和税收。基督教会俨然是「第二个政府」。在罗马帝国灭亡之后，基督教会保留了下来。教皇和君主平起平坐，管理文武百官。二者结合，基督教会变成了罗马人的教会。 与此同时，基督教会在罗马帝国的灭亡过程中，保留了相当的古希腊和古罗马的科学文化技术。教会的人，利用这些先进的科技文化，对自己的教义进行解读，以维护自己的权利。诚然，基督教曲解了相当的科学成就，但是客观上，基督教将古希腊和古罗马的文明保存了下来。 日耳曼蛮族在侵略罗马帝国的过程中逐渐发现一个棘手的问题：他们目不识丁，不懂得如何统治自己的国家。日耳曼民族的首领在打仗的过程中，化身为国王，将征服的土地分给手下的士兵；这些士兵化身为贵族，条件是在过往需要用兵的时候要出兵。 基督教的教主对国王表示自己并不需要土地，避免了与国王和贵族的冲突；同时表示愿意帮助管理国家，赢得了国王的支持。同时，主教们对士兵说，「如果你认可基督教的上帝，那么就能变得更勇武」。最终，日耳曼蛮族支持基督教。 小结 从古希腊和古罗马开始，到罗马帝国覆灭，这段时间称为古典时期；从这三者的联结开始，到 14 世纪三者联结的崩毁，称为中世纪；中世纪之后则称为近代。 番外 教会虽然最终称为了日耳曼蛮族的搭档，但是基督教并不是一个好战的宗教。虽然在先后和罗马、日耳曼民族的合作中变得渐渐支持「正义的战争」，但是却依旧不能接受日耳曼民族的好战的价值观。 不过日耳曼蛮族毕竟目不识丁，妥妥地被继承了希腊罗马科技文化的教会坑了。 教会鼓励他们对战斗的热爱，却改造他们的思想，让他们只参与「正义的战斗」。而何为正义，则全凭教会说了算。比如教会鼓励十字军东征，去夺回陷入伊斯兰教手中的东方的圣地。 同时，教会也要求骑士尊重出身贵族的女士，保护她们、敬重他们。 在骑士绝迹之后，这些被教会改造的中世纪遗风转化成为了所谓的「绅士风度」。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[2015 Kuznets Prize获奖论文：生育行为的质量-数量权衡 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/12/23/child-quantity-quality/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19912835，数据地址：http://www.cpc.unc.edu 前几天，邮箱里突然多出了这么一封邮件： From: Date: December 9, 2014 at 3:37:31 AM EST To: Cc: Journal of Population Economics Popecon@iza.org Subject: Could you please share: 2015 Kuznets Prize Dear Colleague, We are very pleased to inform you that Dr. Haoming Liu (National University of Singapore) was recently selected as the 2015 Kuznets Prize winner. His paper entitled “The quality–quantity trade-off: evidence from the relaxation of China’s one-child policy” (free download) was chosen as the best article published in the Journal of Population Economics in 2014. With its 2013 Impact Factor reaching 1.470, the Journal of Population Economics is a leading journal in the field, publishing original theoretical and applied &amp;gt; research in all areas of the economics of population, household and human resources. The paper uses the exogenous variation in fertility introduced by China’s family planning policies to identify the impact of child quantity on child quality. The study finds that the number of children has a significant negative effect on child height, which provides support for the quality–quantity trade-off theory. The instrumental quantile regression approach shows that the impact varies considerably across the height distribution, particularly for boys. However, the trade-off is much weaker if quality is measured by &amp;gt; educational attainments, suggesting that the measurement of child quality is also crucial in testing the quality–quantity trade-off theory. Originally awarded every three years, the Kuznets Prize became an annual award in 2014. Papers are judged by the Editors of the Journal of Population Economics. The award ceremony &amp;gt; will be held during the ASSA Annual Meeting in Boston, January 4, 2015. We hope that you will join us in sharing this exciting news with your colleagues. Best Regards, Klaus F. Zimmermann Junsen Zhang Editor-in-Chief Editor Journal of Population Economics Follow us: IZA Newsroom | IZA World of Labor 这个题目其实也是比较老的题目了。所谓的 quality-quantity trade-off，也就是在孩子的数量和质量之间作出取舍。因为现实中家庭的经济能力都是有限的，那么多生孩子意味着每个小孩得到的照顾、资源就会少，可能未来发展也不会太好。比如如果我有一百万给小孩的培养费用和一个孩子，我直接送他出国读最好的大学；但是如果我有两个孩子，我就不得不考虑我是不是要留着一点给小儿子花。所以我到底是生一个孩子送他去哥伦比亚还是生两个孩子送他们去香港？这就是所谓的 trade-off。 已经有很多的文献在 Becker 的理论的基础上做了实证贡献。那么这篇文章有什么贡献呢？或者说，获奖论文总有其长处，为啥这篇获奖了？ 在这个问题的实证上，作者指出了两个严重的问题。首先是，家庭对于高质量孩子的偏好是观察不到的，而这种偏好自然会带来更少的孩子和对孩子人力资本的更多投资。所以观察到的孩子数量与质量的负向关系很可能是由于这种偏好引起的。 另外一点就是，在发达国家，父母的收入水平只对儿童教育等有有限的影响。 所以这篇文章在中国这样一个发展中国家，利用计划生育政策和之后读生子女政策的放松作为自然实验，就很好的解决了以上两个问题。 中国的计划生育开始于 1971 年，提出了所谓“晚婚晚育，少生优生”的口号。而到了 1980 年代，国家开始强制实施计划生育政策。 然而计划生育政策到了 1984 年，在许多地区，特别是农村地区有所放松。最常见的形式是，如果第一胎是女孩，那么允许生第二胎。 说到这里，想想有点后怕，如果没有这个政策，我可能不会来到这个世界。我有个姐姐，82 年的。后来我妈妈又怀了一个，有一天我妈妈突然想起来，万一生了二胎，人民教师的工作都可能保不住了！所以果断去打了胎。直到后来，有了这个允许生二胎的政策，我才出生下来。万一那个时候我妈妈脑子没转过来，或者没有这个政策。 扯远了。此外还有很多少数民族地区允许生二胎，但是如果只生一胎的话会被奖励。 文章的数据来自 CHNS1993 年的调查。这个数据也是公开数据，我们可以好好学习一下数据的整理与 clean。文章使用了身高以及受教育情况度量孩子的质量。 下面这一段内容我觉着是这篇文章最出彩的地方。但是由于篇幅，我就不详细叙述了。本文使用了工具变量的估计，即用计划生育政策及政策的放松作为孩子数量的工具变量。但是政策这个工具变量是不是一个 valid IV? 对于这方面的论述，这篇文章几乎做到了极致。 关于工具变量的 validity，这里有很多的问题。比如，计划生育政策会不会影响孩子的性别选择？是不是对孩子数量偏好更高的社区更容易成为改革放松的对象？以及，Imbens and Angrist 等人论述了 IV 估计只估计了群体的一部分人的效应，那么文章使用的工具变量究竟是怎样影响孩子数目的？比如说，如果允许生第二个孩子的政策只影响第二个孩子出生的概率，那么 IV 估计实际上应该是一个 LATE 的估计。 关于这些问题，文章做了非常详细的论述。这里限于篇幅，只贴出 2SLS 第一阶段的回归结果： 可以看到，放松的独生子女政策显著的增加了孩子的数量。下面是样本中男孩的身高的密度函数： 可以发现，一个孩子的家庭的孩子显著高于两个或者多个孩子家庭的孩子身高。 接下来，作者使用工具变量的方法估计了感兴趣的系数。有意思的是，因为作者估计的方程： \[Y_i=\beta_0&#43;\alpha D_i&#43;X^\prime_i\beta_1&#43;\epsilon_i\] 其中的 D 是孩子的数目，因此潜在的假设是每多出生一个孩子，对质量的影响都是一样的。然而如果边际影响不一样的时候，根据 Mogstad and Wiswall(2010)，估计出来的系数就是一个加权平均，作者同时讨论了这个权重。 以上表格是 OLS 和工具变量的回归结果，不过多解释了，的确存在着数量-质量的 tradeoff。 后面作者还做了分位数回归工具变量的估计，结果不再赘述。 我第一眼看到题目的时候，觉着这个题目的研究都已经做了好多了，为什么还能获奖？看完文章之后，的确从数据到理论到实证策略到敏感性分析，堪称典范，非常值得认真学习！ Ruby Z：精彩！不过有一个问题，孩子的数量是不是应该做泊松回归或者负二项回归？ 慧航：那就很难做工具变量了吧。 Ruby Z：可是这样简单的用线性回归解释生育数量真的严谨嘛？总感觉很奇怪。 慧航：仔细看他的论文，第一阶段回归只要有线性关系就好，第二阶段，他论文里面讨论了这个问题。 李楠：请问，为什么要用身高作为”孩子质量“的变量呢？是因为身高一定程度上反映了身体素质吗？可是身高不是遗传决定的么？那么在 社科研究领域，是否普遍性的拿“身高”作为 quality 的 variable？ 慧航：所以文章里面也控制了父母身高。以身高作为 quality 的指标比较普遍，此外还有教育的投入。身高的决定除了遗传因素还有后天环境的因素，比如营养和运动，而营养是父母可以控制的。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[多个 \expandafter 的展开过程是怎样的？]]></title>
    	<url>/tech/2014/12/05/expandafter/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/26916597 虽然宏展开这方面有更好的解决方案，如 etoolbox 和 LaTeX3 CTAN 上的 ctex1.02d 里的一个替换宏名的例子： \def\CTEX@replacecommand#1#2#3{% \expandafter\expandafter\expandafter\let\expandafter \csname #1#3\expandafter\endcsname \csname #2#3\endcsname \expandafter\expandafter\expandafter\def\expandafter \csname #2#3\expandafter\endcsname {\csname #1#3\endcsname} } 刘海洋 其实本来没那么复杂，那个宏写得不好，只不过后来没有人较真，一直那么用而已。正确的写法是： \def\replacecommand#1#2#3{% \expandafter\let \csname #1#3\expandafter\endcsname \csname #2#3\endcsname \expandafter\def \csname #2#3\endcsname {\csname #1#3\endcsname}% } 并不需要多重 \expandafter。上述定义可以分成形式类似的独立的两组，相互展开互不干涉。第一组中： \expandafter\let \csname #1#3\expandafter\endcsname \csname #2#3\endcsname 第一个 \expandafter 保证先展开 \csname 后进行 \let；第二个 \expandafter 保证先展开第二组 \csname ... \endcsname，再完成第一组 \csname ... \endcsname。于是两组 \csname ... \endcsname 都完成了才进行 \let 的赋值，效果就是（etoolbox 中的） \csletcs{#1#3}{#2#3} 后一组代码类似，效果是 \csdef{#2#3}{\csname#1#3\endcsname} 原来的宏写得更复杂了，多写了几个\expandafter，就多了几个展开步骤。 多重\expandafter的用处还是改变展开次序，不过就是人肉解释起来更累一点而已。 以前多余的写法可能是来自一个展开定式： \expandafter\expandafter\expandafter\A\expandafter\B\C 它的效果是先展开\C，然后是\B，最后是\A。我们首先来分析一下这个定式： 展开第一个\expandafter，于是按定义，我们知道效果是第二个\expandafter后面跟上「第三个\expandafter的展开」。即 ②① 其中用数字①②标出展开顺序。 下面先展开前面①标出的\expandafter，按定义，知道效果是\A后面跟上「最后一个\expandafter的展开」。即变成： ②① 然后\expandafter\B\C展开一步你已经懂了，相当于先展开\C然后前面放上\B。所以得到： ②① 即 \expandafter\A\B「\C的展开」 然后又展开\expandafter\A\B，就是先展开\B，前面再放上\A。最后总结一下，我们就知道，这个展开定式： \expandafter\expandafter\expandafter\A\expandafter\B\C 的效果就是先展开\C，然后\B，最后\A。展开次序与排列次序相反。回到原来ctex宏包的例子，它的形式其实是： \expandafter\expandafter\expandafter\let\expandafter \csname foo\expandafter\endcsname \csname bar\endcsname 最前面部分不就是我们讲的定式么？效果是先展开foo中的f，然后展开\csname，最后展开\let。展开f并没有什么可展的，展开后还是f；而到\csname的部分，按\csname ... \endcsname的语义，则需要向后展开到\endcsname为止——此时\endcsname之前的\expandafter起效果，就进入下一个部分了。后面的分析和最前面的简化代码其实一样。分析ctex原来的例子就可以看到，旧的代码会最先展开\csname后面的内容（上面的分析是f，原例子是#1传参的结果中的第一个 token），这当然是多余的做法。实际上，只需要保证\let在两个\csname之后被展开生效就足够了——这也是一开始简化代码做的事。关于\csname的语义，读TeXbook第7章（或TeX by Topic相关章节）有解释：对\csname &amp;lt;tokens&amp;gt; \endcsname的展开，是完全展开&amp;lt;tokens&amp;gt;到底，留下里面的字符部分，然后把这些字符生成一个宏。这对于理解上面的分析是有益的。这就是整个过程的详细分析，希望你没有被吓到。 为了检验你已经理解了\expandafter的语义和上面说的逆转3个 token 展开次序的定式，你可以再试着理解一下这段代码： \let\ep\expandafter % 简化下面的记号 \ep\ep\ep\ep\ep\ep\ep\A \ep\ep\ep\B \ep\C \D 第一行就是7个\expandafter，有点吓人是么？注意\expandafter是跳着生效的，所以上面的代码的一轮展开之后，就变成了 \ep\ep\ep\A \ep\B \C 「\D 的展开」 是不是有点眼熟？所以其实就是把\A\B\C\D这4个记号的展开顺序逆转一遍。现在你可以考虑：如果要逆转5个记号\A\B\C\D\E的展开顺序，一共需要用几个\expandafter？很有规律性不是么？最后推介 TUGboat 1988 年的一篇很早的文章，叫《A Tutorial on \expandafter》，希望你会喜欢： https://www.tug.org/TUGboat/tb09-1/tb20bechtolsheim.pdf 李清 多个\expandafter也是按照顺序展开的。@李阿玲 已经推荐了很好的资料，作为例子，我们来看看 \CTEX@replacecommand{CTEX}{CJK}{underlinesep} 的展开过程。代入参数后，就展开成 \expandafter\expandafter \expandafter\let \expandafter\csname CTEXunderlinesep\expandafter\endcsname \csname CJKunderlinesep\endcsname \expandafter\expandafter \expandafter\def \expandafter\csname CJKunderlinesep\expandafter\endcsname{% \csname CTEXunderlinesep\endcsname} 只看前面四行，后面的类似。首先被执行的是左边一列的\expandafter，但其实没有什么意义，因为最后的是 \expandafter\csname C 字母C不可展开。然后执行 \expandafter\let \csname CTEXunderlinesep\expandafter\endcsname \csname CJKunderlinesep\endcsname \expandafter将展开\let后的\csname。\csname将展开随后的记号，直到遇到匹配的\endcsname为止。因而\endcsname前面的\expandafter将把 \csname CJKunderlinesep\endcsname 展开成 \CJKunderlinesep。最后就得到了结果 \let\CTEXunderlinesep\CJKunderlinesep 按照顺序慢慢看就可以。动手写代码，还是使用封装好的工具吧，不然有时候写起来是很费劲的。你可以感受一下下面的例子（http://latex-project.org/papers/expl3-boolexpr-example.pdf）： 孟晨 答案分成两个部分。第一个部分讲怎么看：怎样判断一堆\expandafter修饰的代码的展开顺序；第二个部分讲怎么写：怎么根据展开顺序的需要来写\expandafter。以下讨论用\ep代表\expandafter，即 \let\ep\expandafter 有时为了方便，用\ep1代表代码串中第一个\expandafter。 1 判断的步骤如下： 划掉\ep； 跳过一个记号； 如果该记号是\ep，回到1；如果该记号不是\ep，展开它，然后找到代码片段里第一个没有被划掉的\ep，回到1。 如此往复，直到所有的\ep都被划掉，再依次展开剩下尚未展开的宏。 Ex.1 \ep1\ep2\ep3\A \ep4\B \C 步骤： 划掉\ep1，跳到\ep3； 划掉\ep3，跳到\ep4； 划掉\ep4，跳到\C，展开\C，跳到\ep2； 划掉\ep2，跳到\B，展开\B； 没有剩余的\ep，展开剩下的\A。 得到展开顺序是C - B - A。这正是题主问题里的内容。 Ex.2 \ep1\ep2\ep3\ep4\ep5\ep6\ep7\A \ep8\ep9\ep10\B \ep11\C \D 步骤： 划掉\ep1，跳到\ep3； 划掉\ep3，跳到\ep5； 划掉\ep5，跳到\ep7； 划掉\ep7，跳到\ep8； 划掉\ep8，跳到\ep10； 划掉\ep10，跳到\ep11； 划掉\ep11，跳到\D，展开\D，跳到\ep2； % 整理一下，此时剩下的代码是 \ep2\ep4\ep6\A \ep9\B \C 根据Ex.1得到C - B - A的展开顺序。因此展开顺序是D - C - B - A。这正是[@刘海洋](https://www.zhihu.com/people/dae56e83a09288121be52a7cb6a6f8b6)前辈在答案中举出的例子。 Ex.3 \ep1\ep2\ep3\A \ep4\ep5\ep6\B \ep7\C \D 步骤： 划掉\ep1，跳到\ep3； 划掉\ep3，跳到\ep4； 划掉\ep4，跳到\ep6； 划掉\ep6，跳到\ep7； 划掉\ep7，跳到\D，展开\D，跳到\ep2； % 整理一下，此时剩下的代码是 \ep2\A \ep5\B \C 划掉\ep2，跳到\ep5； 划掉\ep5，跳到\C，展开\C； 展开剩下的\A和\B。因此展开顺序是D - C - A - B。 李清：一般是2^n-1个\expandafter，所以分析]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[双城记：房产税降低了房价吗？ - 慧航 - 专栏]]></title>
    	<url>/prof/2014/11/18/property-taxes/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19896045 过去十年中国房地产经历了快速的发展与扩张，同时房价在部分 房产税是一种财产税，因而如果假设房产只有投资品的功能，那么房产税的征收对房价会有一个明显的负向作用。当然在中国的环境下，房产税的征收增加了持有多套房业主的成本，进而对抑制投机性需求也有所帮助。 那么，房产税的征收究竟是不是降低了房价呢？清华大学的白重恩、欧阳敏以及德州 A&amp;amp;M 大学的李奇在 JoE 上的文章：Property taxes and home prices: A tale of two cities 回答了这一问题。 作者使用了 HCW(2012) 的方法，估计了房产税的两个试点：上海和重庆在房产税之后的 conterfactural 的房价水平，从而估计出了房产税的 treatment effects。令人惊讶的是，在两个城市实行房产税之后，政策对房价的影响方向是截然相反的。 文章中所使用的计量方法，在本专栏之前的文章“四万亿”财政刺激的政策评价 - EconPaper - 知乎专栏已经有所介绍，我们不再赘述。我们来看一下他们的结果： 图中左边是上海，右边是重庆，实线为实际的房价，虚线为估计的 conterfactural 的房价，上图为房价的 level，下图为增长率。 可以发现，对于上海来说，房产税的施行显著的降低了房价的增长率，而对于重庆来说，房产税的施行却增加了房价的增长率。这是为什么呢？ 首先是，两个城市征税的对象是不一样的。对于上海： 征收对象是指本暂行办法施行之日起本市居民家庭在本市新购且属于该居民家庭第二套及以上的住房（包括新购的二手存量住房和新建商品住房，下同）和非本市居民家庭在本市新购的住房（以下统称“应税住房”）。 而对于重庆： （一）试点采取分步实施的方式。首批纳入征收对象的住房为： 个人拥有的独栋商品住宅。 个人新购的高档住房。高档住房是指建筑面积交易单价达到上两年主城九区新建商品住房成交建筑面积均价2倍（含2倍）以上的住房。 在重庆市同时无户籍、无企业、无工作的个人新购的第二套（含第二套）以上的普通住房。 新购住房是指《暂行办法》施行之日起购买的住房（包括新建商品住房和存量住房）。新建商品住房购买时间以签订购房合同并提交房屋所在地房地产交易与权属登记中心的时间为准，存量住房购买时间以办理房屋权属转移、变更登记手续时间为准。 （二）未列入征税范围的个人高档住房、多套普通住房，将适时纳入征税范围。 也就是说，在上海，征收的对象是新购的第二套房，而对于重庆，高端住房、独栋别墅都在征收范围之列，不管是不是第二套房。 此外，并不是达到条件的住房整体都要交税，两个城市都有税收减免的措施，对于上海： 本市居民家庭在本市新购且属于该居民家庭第二套及以上住房的，合并计算的家庭全部住房面积（指住房建筑面积，下同）人均不超过 60 平方米（即免税住房面积，含 60 平方米）的，其新购的住房暂免征收房产税；人均超过 60 平方米的，对属新购住房超出部分的面积，按本暂行办法规定计算征收房产税。 而在重庆： 纳税人在本办法施行前拥有的独栋商品住宅，免税面积为 180 平方米；新购的独栋商品住宅、高档住房，免税面积为 100 平方米。纳税人家庭拥有多套新购应税住房的，按时间顺序对先购的应税住房计算扣除免税面积。 也就是说，在重庆，本来想买高端住房的人，因为有了征税的政策，会转而购买普通住房，这就增加了普通住房的需求，进而推高了房价。而在重庆，高端住房只有 6.8% 的份额，从而总体的房价水平可能是上涨的。而在上海，由于只对新购买的住房征税，那么想买房的人就会趁还没征收房产税赶快买房，市场提早消化了这部分需求，房价自然会往下掉。 个人感觉，这个故事比“四万亿”那个更好玩一点。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Regression and causation: a critical examination of six econometrics textbooks - 慧航 - 专栏]]></title>
    	<url>/prof/2014/11/07/econometrica-textbook/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19735322 Bryant Chen and Judea Pearl (UCLA) 对市面上的六本比较流行的计量经济学（中级）教 文章发现 Wooldridge 的书（导论）、Stock and Watson 的书在处理因果方面还是比较靠谱的。似乎这也符合现在教材的流行趋势。 原文可能需要翻墙，我放在我的 Dropbox 里共大家下载：Dropbox - ChenPearl65.pdf 或 http://ftp.cs.ucla.edu/pub/stat_ser/r395.pdf。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[重返北上广：中国人真的喜欢大城市么？ - 慧航 - 专栏]]></title>
    	<url>/prof/2014/11/05/rural-urban-migrants/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19885852 之前一段时间，“逃离北上广”的呼声一直在网上流传。而知乎 在经济学里面，这个问题自然的跟城市的大小这个问题密切相关。城市规模的差别为何如此之大？一个城市最优的大小是多大？这些是城市经济学比较关注的问题。 同时，政策的因素对于城市大小这个问题也是至关重要的。比如相对独裁的政府可能倾向于把资源分配给首都，而且政府可以通过投资、劳动力迁移政策（户口）等工具对城市的规模加以控制。 改革开放以来的中国经历了快速的城市化进程：1978年只有18%的居民生活在城市，而现在已经有超过半数的城市居民。这些变化是与中国的快速工业化以及移动人口管制的放松密不可分的。那么，中国人真的喜欢大城市么？ 为了回答这个问题，北京师范大学的 Chunbing Xing 和 Clark 大学的 Junfu Zhang 的文章：《The preference for larger cities in China: Evidence from rural-urban migrants》通过估计迁移人员对城市规模的支付意愿（willingness to pay），即为了去大城市而放弃的工资收入，而回答了这一问题。 为了估计willingness to pay， 首先作者建立了一个个体的选择模型。假设效用函数为： \[ \left\{ \begin{aligned} &amp;amp; \max\, U_{ij}=C_{ij}^{\alpha_C}H_{ij}^{\alpha_H}\exp\left[\beta_S\ln S_j&#43;\sum^K_{k=1}\beta_k\ln X_{jk}&#43;M_{ij}&#43;\xi_j&#43;\eta_{ij}\right]\\ &amp;amp; \begin{aligned} s.t.\;\; &amp;amp; C_{ij}&#43;p_jH_{ij}=I_{ij} \end{aligned} \end{aligned}\right. \] 其中\(c\)为消费，\(H\)为不可交易的消费，比如房产等。\(S\)为城市规模，\(X\)为城市的特征，比如公共品的提供等等，\(M\)为城市与家乡的距离。\(I\)为\(i\)在城市\(j\)的收入。 解这个效用最大化问题，可以得到间接效用函数： \[V_{ij}=-\alpha_H\ln p_j&#43;\alpha I_{ij}&#43;\beta_S\ln S_j&#43;\sum^K_{k=1}\beta_k \ln X_{jk}&#43;M_{ij}&#43;\xi_{j}&#43;\eta_{ij}\] 那么 willingness to pay(WTP) 就可以定义为： \[\mathrm{WTP}_i=\frac{\partial V_{ij}}{\partial S_j}\bigg /\frac{\partial V_{ij}}{\partial I_{ij}}=\frac{\beta^\ast_S}{\alpha}\frac{I_{ij}}{S_j}\] 或者其弹性形式： \[\frac{\Delta_{ij}}{I_{ij}}\bigg/\frac{\Delta S_j}{S_j}\approx\frac{\partial \ln I_{ij}}{\partial \ln S_j}=\frac{\beta^\ast_S}{\alpha}\] 这个模型的估计可以分为两步。首先，通过估计个体的城市选择(离散选择)，得到对与城市的偏好系数\(\theta\): \[\mathrm{Pr}(\ln V_{ij}&amp;gt;\ln V_{ik},\forall k\neq j)=\frac{\exp(\alpha\ln \hat{I}_{ij}&#43;\pi_D\ln D_{ij}&#43;\pi_1 d^1_{ij}&#43;\pi_2 d^2_{ij}&#43;\theta_j)}{\sum^J_{s=1}\exp(\alpha\ln \hat{I}_{is}&#43;\pi_D\ln D_{is}&#43;\pi_1 d^1_{is}&#43;\pi_2 d^2_{is}&#43;\theta_s)}\] 这一步的困难在于，个体在其他城市的收入是看不到的，我们只能看到个体在最终选择的城市的收入。通过使用 Dahl 的半参数方法，作者得到了这些 conterfactural 的收入水平。 第二步，得到了这些城市的偏好系数之后，看一下是不是城市的 size 越大，偏好系数越大： \[\theta_j=\beta^\ast_S\ln S_j&#43;\sum^K_{k=1}\beta^\ast_k\ln X_{jk}&#43;\xi^\ast_j\] 使用全国人口普查的 1% 抽样数据，作者对以上模型进行了估计。首先是城市固定效应\(\theta\)的估计结果（前 20 的城市，北京=0）： 数值越大，说明人们对城市的偏好程度越强（北上广深的排名原来应该是上深北广，上海优势明显，南京败给莆田？）。 那么，是不是城市越大，人们对这个城市的偏好也越强呢？一个简单的相关图如下： 当然，简单的线性关系图不能替代严谨的回归分析。但是在回归中，一些不可观测的因素总是会被遗漏。为了解决这一问题，作者首先控制了城市所在地区的固定效应，其次使用 1953 年时的城市人口作为城市规模的工具变量，得到如下的回归结果： 可见，个人在选择城市时，的确会更偏好大城市。此外，回归结果还显示，个人做选择时对污染情况也十分敏感。 那么，个人为什么会更喜欢大城市呢？作者认为有这么几种可能的解释。 首先，大城市可以给个人提供更好的学习机会，从而个人可以更快的积累人力资本。 其次，大城市为个人及其子女提供了未来更好的生活。 再次，模型假设消费品同质，但实际上，大城市的消费品更加丰富。 最后，也是之前知友们经常提到的，大城市对外来人口更友好，更少的依赖关系。 所以，你会选择去哪里呢？ zc deng：no structure, no good. just joking. 慧航：感觉这篇文章最关键的地方都用 reduced-form 去避开了，而且我觉着他的 IV 本身也有问题。 zc deng：看 argue 了，中国的城市挺路径依赖的吧。当然 IV 难找… 慧航：感觉可以从政策变量里面去找，外生性更好 argue 吧。 Andrew Lau：看不懂那个图，哪位麻烦解释一下为啥成都孤零零地掉在右下角。 慧航：说明成都虽然不小，但是大家不喜欢跑到成都去工作。 歌未歌：有些地方没有看懂或者搞清楚，但感觉这个思路挺不错的，似乎可以推广到为何人们选择 startup 而不是大公司（或反之），为什么不愿意离开故乡远游等问。这样的研究是归纳与解释现象的，个体使用它检验或者验证结论的合理性没有什么意义。 大锴：我怎么感觉一堆数学之后推论了几个无关痛痒人尽皆知的结论来呢。 慧航：说明你还没懂什么是经济学，你来告诉我到底大家更喜欢去北京还是上海，有那么显然麽？文章的结论是排除了公共品等其他条件的净效应，跟你想的喜欢大城市完全不是一码事。 Fire Ling：文献里说的是 migrant 吧，这意味着被调查的群体本身就有要更换城市/地区的意愿啊，于是人往高处走，水往低处流，去好的地方（这不废话么），您这样一写“中国人”，似乎全国人民都有要向大城市进军的意愿一样啊。 慧航：good point，你可以认为我是标题党， 实际上我也在想作者为什么不把非迁移的也计算一下。实际上很多人并非没有去大城市的意愿，而是没有这个客观条件（能力以及政策）。 Fire Ling：“实际上很多人并非没有去大城市的意愿，而是没有这个客观条件（能力以及政策）”这个仅能当猜测了，因为也可以说，去大城市本身就是为了更好的政策或让自己变得更有能力。 慧航：exactly, 后面的解释有说。我没有必要去帮原作者为了模型的设定去 fight，你说的“中国人”的问题的确有标题党嫌疑。anyway，标题党无处不在。 SS元元：看了下没有\(D\)，只有\(M\)，\(M\)是\(i\)到\(j\)城市的费用与距离相关联。另外\(\xi\)和\(\eta\)两个指的东西也略微社会学了点。 慧航：后面实证模型还有加入是否与家乡相邻等。\(\xi\)和\(\eta\)是非常标准的 fixed effects 和误差项的假定，什么叫社会学了点？ SS元元：没看下去，因为\(M\)在这模型里代表了从家乡出来到城市的成本，上面有人问过你这一块。嗯，果然要看下去。最后将不能量化的城市特性——友好度和城市人口规模大小正相关了。这样，城市越大人越多果然更友好，楼上有些话没有说错。社会学的参数都是人为设定的。只是我个人观感。 慧航：模型都是人为设定的。 SS元元：至少有些是量化的不是么。好感度什么的…… 慧航：frankly, i don’t know what you are saying。 SS元元：就是我上面说的城市特性友好度。社会学会给个统计，但这个统计纳入到模型里去就有些过于主观了。所以我的意思是，尽管我对科学地研究这个问题也很感兴趣，但这个模型这个结论还就是越大的城市给的收入越高正相关越高啊，楼下有说越大的越有人要去他也知道。就是这个意思。 慧航：首先，这个偏好不是通过调查得到的，而是基于个人的选择估计出来的。社会学怎么做我是不知道了，但是这个设定在计量经济学是没有问题的。 其次，收入的问题已经在第一步排除了，而且还排除了大城市的公共品比小城市更好的因素等等，这篇文章的结论不是说大家都喜欢大城市，而是在排除了其他因素之后，大家有某种很难解释的对大城市的偏好。 如果感兴趣，请仔细读原文。虽然我觉着这篇文章做的问题不少，但是上面批评的没几个在点子上的。 SS元元：是么，我觉得作为一个和城市大小正相关的函数来设定，不就是加大了城市规模大就更有好的权重么？而你说的有某种很难解释的对大城市的偏好，我的理解，影响就在这个参数。而这个参数 As mentioned above, observed city size \(S_j\) and unobserved city characteristics \(\Xi*j\) are likely to be correlated. As a solution, we will instrument for city size. 慧航：我觉着，首先，你先好好学习一下计量经济学。 SS元元：是高校教师么？我是挺想学学的，我是搞商业地产的，也经常有这种调研和预测的研究，由于设计到的都是实际的投资，因此有时候也满无奈的需要说服投资人。总是上过学不是，微观宏观还是学过的。一些工具也会用，数学不是太好，因此不会太深入。但总觉得有各种问题。比如类似这样的问题，这是讨论城镇化移民对大城市的偏好研究，最后如果归结到大城市因为人多而导致政策透明度高、某些城市政府行政较为规范，这的确是因素，但这说服不了投资人。专栏查看评论很麻烦，如果还有指导请私信。 慧航：指导不敢，不过对于业界的话，倒是不用这么复杂的工具。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[AMD 激发了 Intel 的创新么？ - 慧航 - 专栏]]></title>
    	<url>/prof/2014/10/25/amd-intel-innovation/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19875276 经济学家一直在寻求理解市场结构与创新之间的关系，因为这影 熊比特认为市场集中度（垄断）会促进创新，因为垄断企业会通过不断的创新来增强自己的垄断势力；阿罗则认为这两者存在负向的关系，即小企业更有激励进行创新；Scherer 则提出了两者之间倒 U 型的关系。由于很难控制不同产业的特征，因此现有的证据对这一问题仍然没有达成一致的结论。 来自芝加哥大学的 Goettler 和哥伦比亚大学的 Gordon 在 2011 年的 JPE 上的论文《Does AMD Spur Intel to Innovate More?》，从一个特定的产业（CPU）出发，构造了一个动态双寡头理论模型，并对此模型做结构化的估计，讨论了竞争对创新、利润以及消费者剩余的影响。 为什么研究 CPU 市场呢？一个好处是，这个行业的创新是完全可度量的（计算速度）。此外，这个市场基本上是一个双寡头市场，Intel 和 AMD 两家占了 PC 机 CPU 95% 的市场份额。 我们先来看一下这个产业的背景数据。 （a）图描述了从 93 年到 05 年，两个厂商 CPU 的运算速度的变化（取log之后）。可见“摩尔定律”是几乎成立的。一开始，Intel 一直处于领先定位，AMD 后来居上，甚至一度赶超 Intel（我记得那个时候在家用 CPU 领域，AMD 有很多新技术，比如 64 位和双核的概念一度是领先与 Intel 的，现在好像 AMD 被打的找不到牙）。于此同时，家庭的 CPU 大约落后 5 年。 图（c）给出了两家厂商最先进的 CPU 的价格走势图。一开始可能因为性能的差异，AMD 便宜很多，但是进入 2000 年以后，两家厂商最顶尖的 CPU 的价格难分伯仲。 但是绝大多数家庭不会购买最顶尖的 CPU，所以如果计算一下平均的 CPU 的价格，AMD 比 Intel 便宜近 100 美元，图（d）描述了两个厂商平均价格的走势图。Intel 一直处于下降的趋势，而 AMD 比较平稳，但是即使这样，两家的价格差仍然十分明显。 从市场份额来看，Intel 一直是老大，地位很难撼动啊。 在作者的样本中，Intel 的平均价格与质量之间的相关系数为 0.66，而 AMD 的平均价格与 Intel 的质量之间的相关系数为 -0.34，Intel 的市场份额与质量之间的相关系数为 0.39。 最后，从两家公司的 R&amp;amp;D 支出来看，AMD 的支出水平大约是 Intel 的 1/4，尽管 AMD 的 R&amp;amp;D 支出与公司收益之比是 Intel 的两倍。然而尽管如此，AMD 却总能提供与 Intel 相似的高质量的 CPU，为了解释这种不对称的现象，文章的理论模型里面将会允许创新的溢出效应。 现在来看看文章的理论模型。时间是连续的，每个厂商售卖一种产品并且对产品质量进行投资。如果研发成功，质量在下一期会有所改进，否则质量保持不变。 由于 CPU 是一种 durable goods（耐用品），因此用户的购买行为取决于之前的购买历史。每一期，消费者选择买新的产品，还是继续使用已经购买的老产品。消费者的这个特点引出了厂商在定价时的一种权衡，即如果这一期卖的多，那么以后就卖的少。 下面有数学出没，看不懂请略过 消费者\(i\)从厂商\(j\)购买 cpu 的效用函数为： \[u_{ijt}=\gamma q_{jt}-\alpha p_{jt}&#43;\xi_j&#43;\varepsilon_{ijt}\] 其中\(q\)为质量，\(p\)为价格，\(\xi_j\)为对厂商的偏好。不购买 CPU 的效用假设为 \[u_{i0t}=\gamma\tilde{q}_{it}&#43;\varepsilon_{i0t}\] 每个消费者最大化他们的期望效用，也就是一个动态规划问题： \[ \begin{aligned} V(q,\Delta,\tilde{q},\varepsilon) = &amp;amp;\max_{y\in\{0,\dotsc,J\}} u_y&#43;\beta\sum_{q^\prime,\Delta^\prime}\int V(q^\prime,\Delta^\prime,\tilde{q}^\prime_y,\varepsilon^\prime) f_\varepsilon(\varepsilon^\prime)\,\mathrm{d}\varepsilon^\prime\\ &amp;amp; \times h_c(q^\prime|q,\Delta,\varepsilon)g_c(\Delta^\prime|\Delta, q, q^\prime, \varepsilon) \end{aligned} \] 其中\(h\)为消费者对未来产品质量的预期，\(g\)为状态的转移概率。 每一期企业作出定价与投资的决策。假设 innovation 的成功概率为： \[\chi_j(\tau=\delta|x,q)=\frac{a^j(q)x}{1&#43;a^j(q)x}\] 其中\(\chi\)为 R&amp;amp;D 的投资，投资效率 \[a^j(q)=a_{0,j}\max\left[1, a_1\left(\frac{\bar{q}-q_j}{\delta}\right)^{1/2}\right]\] 假定一个正的\(a_1\)，这个假定意味着，离技术前沿更近，那么创新的难度就越大。厂商的利润函数为： \[\pi_j(p,q,\delta) = Ms_j(p,q,\Delta)[p_j-mc_j(q)]\] 其中\(M\)为固定的市场容量，\(s\)为市场份额，\(mc\)为边际成本。厂商的动态最优问题为 \[ \begin{aligned} W_j(q_j,q_{-j},\Delta) = &amp;amp; \max_{p_j,x_j} \pi_j(p,q,\Delta)-x_j&#43;\beta\sum_{\tau_j,\,q^\prime_{-j},\,\Delta^\prime} W_j(q_j&#43;\tau_j,q^\prime{-j},\Delta^\prime)\\ &amp;amp; \times\chi_j(\tau_j|x_j,q)h_{f_j}(q^\prime_{-j}|q,\Delta)g_{f_j}(\Delta^\prime|\Delta,q,p) \end{aligned} \] 这里，作者引入了“纯策略马尔可夫完美纳什均衡（MPNE）”的概念，即在均衡状态下，厂商和消费者的均衡策略都只跟这一起的状态相关，消费者和厂商具有理性预期。 数学过去了，大家过来看结果吧 定义了均衡之后，作者使用了 GMM 的方法（实际上是 simulated moments 或者 simulated minimum distance, SMD 方法）估计了这个模型中的参数（奇怪的是 J-statistics 拒绝了原假设，作者一句现实太复杂就带过去了。 首先\(\alpha\)和\(\gamma\)的估计结果表示，消费者愿意为一单位的质量增加多付费 21 美元。此外，通过两个品牌固定效应的估计值可以看到，消费者平均愿意为 Intel 的 CPU 多付 194 美元（AMD 哭晕在厕所），实际上这个模型必须有这么一个差价才能解释现实当中 Intel 的市场占有率为何如此之高。而\(a\)的估计值可以看到，AMD 的创新效率比 Intel 高很多，这与观察到的 AMD 时而技术领先 Intel，尽管 R&amp;amp;D 投资比 Intel 少很多是相吻合的。 有了这些参数之后，很多问题就可以好好讨论了。作者进而使用这些参数分析了如果市场是 双寡头（现在的情况） 对称的双寡头 垄断 对称的双寡头且没有溢出效应 短视的双寡头（即厂商不解动态问题而是每期解静态最优） 短视的垄断 social planner 这些情况下的市场结果： 可以看到，首先，在垄断条件下，产品质量的创新要比双寡头条件下高 2.4%；其次，在垄断和双寡头的条件下，均衡的投资都比社会最优的投资要低。为什么呢？因为如果市场结构是垄断的，那么厂商就会不断创新来引导消费者不断的升级，从而获得更高的利润。 从社会福利的角度来看，首先，双寡头的市场结构比垄断产生了更高的消费者剩余，但是社会总剩余却要少 1%. 文章中还有很丰富的结果，这里不一一介绍了，稍微总结一下，这篇文章有这么一些发现： 首先，在双寡头的条件下，消费者剩余要比垄断的条件下高。同时，就像熊皮特预言的那样，产业的创新在垄断条件下达到了更高的水平。 创新的来源有两个：一是企业之间的竞争，二是促进消费者产品的更新换代。对于双寡头来说，两种激励都有，但是对于垄断来说，只面临后一种激励。此外市场的增长对于创新的激励非常重要：一个快速增长的市场减少了垄断厂商促进消费者更新换代的激励，所以这种情况下，双寡头可能比垄断厂商创新更多。 WarmForest：不过事实上，近年来 AMD 的 CPU 技术提高太慢，Intel 也随之放慢了脚步。看上去，还是竞争才更有效率。 慧航：这篇文章数据比较早，但是方法很值得学习。 d efer：个人观点是，这个推论是建立在 PC 市场规模稳定的状态下得出的。而现实中产生了问题，产业资源离开了 PC 市场。Intel 在以前所未有的投入补贴移动芯片 Atom，AMD 则靠定制芯片维生。 慧航：数据到05年。 d efer：我注意到了，上面的说法是为了回应 WarmForest 为什么近年来双方的创新变的更加缓慢。WarmForest 认为是 AMD 变弱导致的，而我更倾向于是因为市场变小了。而数据所取的 93 到 05 正是 PC 大发展的时期，反而回避了移动设备崛起这个意外。 马志航：实际上两家公司的技术不对等，那么按照商业惯性，减缓开发或转移开发目的，是一件正常的事情，而激发…Intel 至少在地球上不存在对手，只能看以后科技需求，如需求不能被满足或即将不能满足，就会激发研发热情。 现在 cpu 性能过剩，所以开发进度越来越慢。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[八卦之王：社交网络中谁最八卦？ - 慧航 - 专栏]]></title>
    	<url>/prof/2014/10/05/gossip-social-network/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19862463 随着对社交网络（social network）研究的不断深 这里的困难在于，无论对于政策制定者和社交网络中的个人，都不可能有完整的社交网络的信息（即谁是谁的朋友）。即使是身处社交网络中的个人，也只可能知道自己认识的人的信息，而对于自己的朋友之外的信息仍然很难获取。 之前的研究定义了社交网络中每个个体的“重要性”，或者说“集中度（centrality）”，比如特征向量集中度（ eigenvector centrality ）以及 Katz-Bonacich 集中度。但是这些定义前提是我们必须知道整个社交网络的信息。 Banerjee, Chandrasekhar, Duflo 以及 Jackson 等人在 NBER 上的一篇 working paper：GOSSIP: IDENTIFYING CENTRAL INDIVIDUALS IN A SOCIAL NETWORK从理论上解决了这一问题。这篇文章从理论上证明了，社交网络中的每个个体，通过简单的数一下每条消息传过来的源头的次数，个体可以知道社交网络中每个人的重要性（集中度）的排序。 故事是怎样的呢？假设一个社交网络中有\(n\)个个体，\(n\)个个体之间的关系可以通过一个矩阵\(g\)来表示（\(g_{i,j}=1\)表示\(i\)认识\(j\)），假设每个个体在得知一个消息(gossip)之后以\(p\)的概率告知其朋友，那么可以定义矩阵 \[\boldsymbol{\mathrm{H}}(\boldsymbol{g};p,T):=\sum^T_{t=1}(p\boldsymbol{g})^t\] 其中\(\boldsymbol{H}\)的第\(i,j\)个元素代表在消息传递了\(T\)次之后，第\(j\)个人从\(i\)得到消息的次数的期望。这样就可以定义一个传播的集中度（diffusion centrality）： \[\mathrm{DC}(\boldsymbol{g};p,T):=\boldsymbol{\mathrm{H}}(\boldsymbol{g};p,T)\cdot\boldsymbol{1}=\left(\sum^T_{t=1}(p\boldsymbol{g})^t\right)\cdot\boldsymbol{1}\] 这个集中度（以下简称 DC）代表了在经过了\(T\)次传播之后，第\(i\)个个体所传播的人数的期望值，这个个体能直接或者间接传播的人数越多，那么显然这个个体在社交网络中就越重要。其他的集中度定义（特征向量集中度、KB 集中度）在这里不赘述。 作者考虑了这么一个消息传播的过程(Gossip Process)：有一个新消息（无论是消息是事实、猜测或者甚至是谣言、观点）从\(i\)传出，\(i\)可以告诉\(k\)和\(q\)，\(k\)可能告诉\(j\)，\(j\)可能告诉\(q\)和\(r\)，但是在每一次传递中，每个人都告诉传递的下家，这条消息是从\(i\)这里传出来的。比如在这里，\(k\)从\(i\)这里听到了一次，而\(q\)则直接从\(i\)听到了一次，间接从\(j\)这里听到了一次，所以共两次。这里的关键点在于，这个过程并不需要每个人知道消息传播的路径，而仅仅需要知道消息是从谁传出来的，以及自己听到了多少次这个消息。这个计数过程可以如下描述： \[\mathrm{NG}(\boldsymbol{g};p,T)_j:=\boldsymbol{\mathrm{H}}(\boldsymbol{g};p,T)_{\cdot j}\] 也就是\(\boldsymbol{\mathrm{H}}\)矩阵的第\(j\)列，代表了经过\(T\)次消息传递之后，\(j\)从每个个体听到消息的期望次数。 作者证明了，每个人的排序与定义的\(\mathrm{DC}\)是正相关的，而且随着\(T\)趋向于无穷，每个个体都可以完美的知道每个人的集中度的排序。 好了，证明了上面的结论，你能拿出点证据来说明你的理论是对的么？ 作者于是需要证明，每个个体的确有识别出“八卦之王”的能力。 于是，作者调查了 35 个村庄，首先通过调查的方式（比如问你你曾拜访过谁，谁曾拜访过你等问题）描绘出了每个村庄的社交网络结构（\(g\)）。然后问了两个问题来统计村民心目中谁是八卦之王：1. 如果你有一个贷款产品你想告诉村子里每一个人，你会告诉谁？2. 如果有演出消息你想告诉村子里每个人，你会告诉谁？这样，作者就获得了每个村庄的“八卦之王”的提名以及排序。 此外，作者还把商店老板、教师等与其他人接触较多的个体单独列出来作为“leaders”，因为这些人是天然的“八卦之王”候选人（仿照 Bharatha Swamukti Samsthe 的做法）。 首先，比较一下被提名的人以及 leaders 的分布： 可以发现，(A) 中的 leaders 更容易包括很多不重要的人物，而 (B) 中的排名看起来更靠谱一点。 既然有了网络的信息，就可以计算一下特征值集中度，然后看看村民报告的“八卦之王”跟理论计算出来的特征值集中度是不是一致的： 可以发现，通过特征向量集中度计算的“八卦之王”更容易被村民提名（上图），而且被提名的也更多的出现于特征向量集中度更高的人群（下图）。 当然，这种比较太过于粗略。村民可能仅仅报告那些有更多朋友的人，或者地理位置上更方便传播消息的人，所以作者还做了回归分析，通过控制其他的变量，看前面的DC集中度是不是能更好的预测被提名的概率（次数）： 至于回归结果吗，大家看看就好，作者的DC集中度一开始很显著吗，但是随着控制其他变量，显著性水平越来越差，虽然系数值越来越大。但是作者指出，如果做三个集中度变量的联合检验，联合起来却是显著的。问题可能出现在三个变量的共线性上（这也就是我一直强调的，共线性怎么办？没办法，看大神写文章是顺着写的，一开始变量少，慢慢增加变量，而不是相反，怀疑有共线性了再删变量）。 所以呢，你看，村民都是有这种能力的～ 慧航：我对这个领域不是特别熟，我是追着 duflo 看的这篇文章，这个数据我也是第一次看到。 杨张博：请问 duflo 是哪个领域的啊？各种中心度的共线性应该是天然的，都是从 degree 这个基本概念推导出来的。另外，我看这篇论文用的是 possion 回归，但是网络数据回归是否有效一直存在争论。Jackson 在他的课上也讲了基于随机图的方法，但奇怪的是在这篇文章中没有用。 慧航：经济学领域的大神，这篇文章主要是理论贡献吧，后面的 poission 回归只是说明理论的预测是合乎现实的，可能不同学科做 social network 关注的点不一样吧。 养猪就用金坷垃：我其实不太明白他后面做回归加变量干什么，因为如果你加的是无关或者弱相关的变量，很明显会干扰原来的变量，造成显著水平变差啊？ 慧航：如果加的是无关的变量，是不会干扰原来的变量的，不信你推一下 养猪就用金坷垃：恩，应该是有相关性。但是对这篇论文来说，相关性完全不是问题啊。因为本来就是创造出一个指标来统计八卦指数，那和其他因素相关也无所谓啊。反倒正好说明这个指数和其他包含了各种因素。 慧航：我觉着这篇文章的意义不是创造一个指数，而是证明了社交网络中的个体都有能力知道谁是这个社交网络中的关键先生。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[制度、人力资本及经济发展：从微观到宏观的视角 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/09/30/institution-labor-captial-eco-devlop/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19860158 阿德莱德大学的何晓波最近针对 Heckman and Mosso (2014) 以及 Acemoglu et al. (2014) 两篇文章 这篇文章不算是严谨的学术论文，仅仅是一个介绍性的短文，而且是用中文写的，所以我觉着在这里没有必要详细介绍，感兴趣的请戳以下链接（可能需翻墙）：制度、人力资本及经济发展]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[“四万亿”财政刺激的政策评价 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/09/23/treatment-effects-estimation-economic-stimulus-plan-of-china/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19855025 2008年金融危机之后，中国政府在年底推出了“四万亿”财 清华大学 Min Ouyang 与阿肯色大学的 Yulei Peng 在 Journal of Econometrics 发表的文章：The Treatment Effects Estimation: A Case Study of the 2008 Economic Stimulus Plan of China 通过计算“反事实 (counterfactuals)”的经济指标，对这一政策的实际效果作出了评价： In 2008, the Chinese government put forth an economic stimulus package of 586 billion USD to minimize the impact of the global financial crisis. This is considered as one of the most important macroeconomic policy interventions in the past decade. This paper employs this policy intervention as a case study to address the challenge of evaluating the treatment effects under time-varying latent factors. In particular, we return to the framework of Hsiao, Ching, and Wan (2012), relax their linear conditional mean assumption, and extend it to a semi-parametric setting. The asymptotic distribution properties of the average treatment effect estimator is derived and studied. Both the linear model and the semi-parametric model are applied to study the treatment effect of the 2008 stimulus package on China’s macroeconomy. The estimation results show the stimulus package had raised the annual real GDP growth in China by about 3.2%, but only temporarily. These results are robust to linear setting, semiparametric setting, and various control group selections. The temporary boost in macroeconomic outcome is also evident in the estimation of other economic indicators such as real investment, real consumption, real export, and real import. 政策评价的一个难点在于，在没有这个政策的情况下结果是观察不到的。比如在这里，我们只能看到在有了四万亿刺激之后的经济增长情况（反事实，counterfactuals），但是我们永远看不到如果没有这个刺激计划，经济增长情况是怎样的。 对于这一问题，计量经济学已经发展出很多方法来解决这一问题。常见的有自然实验的方法，比如 DID、RD 等方法，以及一些结构模型的方法。然而这里还有一个难点在于，这些方法大都需要控制组与处理组都有大量的观测，而在这个应用情景中，只有中国推出了四万亿刺激，因而这些方法很难适用。 这篇文章扩展了Hsiao, Ching and Wan (2012)的方法，通过估计控制组与处理组的相关性来预测处理组的反事实的结果，从而计算政策的处理效应。这篇文章通过放松 HCW 方法的线性设定，使用半参数的方法估计 HCW 的反事实模型，从而对 2008 年的四万亿政策作出评价。 HCW 的具体方法这里不再赘述，通过假设因子模型的形式，HCW 得到了如下式： \[y_{1t}=\delta_1&#43;\delta&amp;#39;Y_t&#43;\epsilon_{1t}\] 其中\(y_{1t}\)为收到处理的个体（中国）的 outcome（比如 GDP），\(Y_t\)为其他国家的 outcome。这样就可以通过估计政策发生之前的这个式子得到\(\delta\)的系数，然后预测政策执行之后的\(y_{1t}\)，即反事实的 outcome，从而得到 treatment effects。 在实际计算的过程中，\(Y_t\)中的国家选择必须要满足两个条件，即这些国家的 outcome 必须和中国的 outcome 相关，而且这些国家的结果必须对想对于处理组的处理（四万亿计划）是外生的。 作者通过计算与中国的贸易量占这个国家 GDP 的比重来挑选进入\(Y_t\)的国家，最终选出斯洛文尼亚、法国和爱沙尼亚三个国家作为控制组。其估计结果如下： 可以看到，财政刺激政策在一开始的确刺激了经济，特别是在 2009 年第三季度左右，提高了 5.4% 左右的 GDP，但是这一政策的效果在随后迅速降低，在 2010 年第四季度之后甚至变成了负值。从这一点看，四万亿的财政刺激政策只是临时对经济有刺激作用，并无长期影响。 在 JoE 上发中国问题的文章的确罕见，所以我想这篇文章的理论意义大于实际意义。关键在与对控制组的选择是一个非常 tricky 的工作，既要求控制组对处理组有预测能力，还要求处理组的政策变动对控制组的结果几乎没有影响，能满足这两个要求的国家应该很少。但是不管怎样，这样做至少得到了一个看上去非常符合直觉的答案。 罗金峰：这里有一个对四万亿较肯定的观点：http://research.stlouisfed.org/wp/2014/2014-007.pdf。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[写给非专业人士看的 Shadowsocks 简介]]></title>
    	<url>/tech/2014/09/22/ss/</url>
		<content type="text"><![CDATA[[原文地址：http://vc2tea.com/whats-shadowsocks/ 这个文章来源于一个朋友在科学上网的过程中，搞不清楚 Shadowsocks 的配置 Shadowsocks 的理解简单梳理一下，以便一些非专业人士也能了解。 long long ago… 在很久很久以前，我们访问各种网站都是简单而直接的，用户的请求通过互联网发送到服务提供方，服务提供方直接将信息反馈给用户 when evil comes 然后有一天，GFW 就出现了，他像一个收过路费的强盗一样夹在了在用户和服务之间，每当用户需要获取信息，都经过了 GFW，GFW将它不喜欢的内容统统过滤掉，于是客户当触发 GFW 的过滤规则的时候，就会收到Connection Reset这样的响应内容，而无法接收到正常的内容 ssh tunnel 聪明的人们想到了利用境外服务器代理的方法来绕过 GFW 的过滤，其中包含了各种HTTP代理服务、Socks服务、VPN服务… 其中以 ssh tunnel 的方法比较有代表性 1) 首先用户和境外服务器基于 ssh 建立起一条加密的通道； 2-3) 用户通过建立起的隧道进行代理，通过 ssh server 向真实的服务发起请求； 4-5) 服务通过 ssh server，再通过创建好的隧道返回给用户。 由于 ssh 本身就是基于 RSA 加密技术，所以 GFW 无法从数据传输的过程中的加密数据内容进行关键词分析，避免了被重置链接的问题，但由于创建隧道和数据传输的过程中，ssh 本身的特征是明显的，所以 GFW 一度通过分析连接的特征进行干扰，导致 ssh 存在被定向进行干扰的问题 shadowsocks 于是 clowwindy 同学分享并开源了他的解决方案。 简单理解的话，shadowsocks 是将原来 ssh 创建的 Socks5 协议拆开成 server 端和 client 端，所以下面这个原理图基本上和利用 ssh tunnel 大致类似。 6) 客户端发出的请求基于 Socks5 协议跟 ss-local 端进行通讯，由于这个 ss-local 一般是本机或路由器或局域网的其他机器，不经过 GFW，所以解决了上面被 GFW 通过特征分析进行干扰的问题； 5) ss-local 和 ss-server 两端通过多种可选的加密方法进行通讯，经过 GFW 的时候是常规的TCP包，没有明显的特征码而且 GFW 也无法对通讯数据进行解密； 3、4) ss-server 将收到的加密数据进行解密，还原原来的请求，再发送到用户需要访问的服务，获取响应原路返回。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[贝克尔的时间分配理论 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/09/11/becker-allocation-of-time/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19847231 原文：Gary Beckerís “A Theory of the Allocation of Time” B (1965) paper, “A Theory of the Allocation of Time” revolutionized the modeling of household behavior, by unifying Marshallian demand functions for goods with labor supply and related time use decisions within the household. In this paper we Örst summarize Beckerís time allocation model and associated key contributions, then we show how his original framework extends to modern collective household models. 刚刚去世的经济学巨擘 Becker 因为在家庭、生育、犯罪等领域的开拓性研究而获得诺贝尔奖，并且其影响至今仍然深刻的改变着经济学的研究领域。让我们先对这位前辈致以崇高的敬意和哀悼。 Pierre-André Chiappori 和 Arthur Lewbel 回顾了贝克尔的家庭时间分配的理论。贝克尔通过将家庭生产函数引入到家庭的决策中，提出了一个比较一般性的时间分配的理论框架。作者进而对这一模型的识别问题做了一些综述和评价。 由于贝克尔的这一理论框架是建立在家庭有一个共同的效用函数上的，而最近的理论在 intra-household 的层面上，即家庭内部的层面上，给每个家庭成员设定效用函数来研究家庭的决策问题。其中一个方法是 collective 的方法，即消费品在家庭内部可能是私人消费也可能是公共消费，家庭最终选择一个帕累托最优的组合（其实实证中有的文献做出来发现家庭内部的资源分配不是帕累托最优的）。作者对这一类模型也做了一些简短的综述和评价。 如果对这一块文献感兴趣，可以读一下。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[中国的预防性储蓄估计：以国企改革打破“铁饭碗”作为自然实验 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/09/04/iron-breaking-the-rice-bowl/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19841390 我校 Hui He, Feng Huang, Dongming Zhu 与旧金山联储的 Zheng Liu 合作的文章：《Brea the “Iron Rice Bowl” and Precautionary Savings: Evidence from Chinese State-Owned Enterprises Reform》使用上世纪末的国企改革作为自然实验，估算了中国家庭预防性储蓄的规模，摘要如下： We use China’s large-scale reform of state-owned enterprises (SOE) in the late 1990s as a natural experiment to identify and quantify the importance of precautionary savings for wealth accumulation. Before the reform, SOE workers enjoyed the same job security as government employees. After the reform, a cumulative of over 35 million SOE workers have been laid off, although government employees kept their “iron rice bowl.” The change in unemployment risks for SOE workers relative to that of government employees before and after the reform provides a clean identification of income uncertainty that helps us estimate the importance of precautionary savings. In our estimation, we correct a self-selection bias in occupational choices and we disentangle the effects of uncertainty from pessimistic outlook. We obtain evidence that precautionary savings contribute to about one-third of the wealth accumulations for SOE workers between 1995 and 2002. Precautionary savings motive is thus an important factor that drives the observed rising Chinese saving rate. 文献中识别预防性储蓄是非常困难的，因为识别家庭收入风险的原因本身就很困难。文章使用上世纪 90 年代的国企改革作为外生的冲击，比较了政府工作人员与国有企业工人的储蓄行为。因为政府工作人员几乎不受到这次改革的影响，而国有企业工人的铁饭碗被打破，因而收入风险大大增加。利用这一外生的政策变化，使用（类似于）DID 的方法，识别除了预防性储蓄的规模。 此外，此类文献一般受到 self-selection biases 的影响。为了去除自选择效应，作者将样本限制在政府分配的工人中，这样就不会存在风险偏好不同的工人选择不同种类工作的问题。 根据作者估计的结果，预防性储蓄大约构成了30%的家庭金融资产积累。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Acemoglu 对《Capital in the 21st Century》的批判 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/08/31/21th-century-captial-acemoglu/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19838920 原文：http://economics.mit.edu/ 毛咕噜还是耐不住寂寞，终于发声了： Thomas Piketty’s recent book, Capital in the Twenty First Century, follows in the tradition of the great classical economists, Malthus, Ricardo and Marx, in formulating “general laws” to diagnose and predict the dynamics of inequality. We argue that all of these general laws are unhelpful as a guide to understand the past or predict the future, because they ignore the central role of political and economic institutions in shaping the evolution of technology and the distribution of resources in a society. Using the economic and political histories of South Africa and Sweden, we illustrate not only that the focus on the share of top incomes gives a misleading characterization of the key determinants of societal inequality, but also that inequality dynamics are closely linked to institutional factors and their endogenous evolution, much more than the forces emphasized in Piketty‘s book, such as the gap between the interest rate and the growth rate. 毛咕噜开骂了，还是从马尔萨斯到马克思一直骂到 Piketty。文章还列举了马克思的所谓“资本积累的一般法则”，承认了这些法则符合当时以及一段时间的观察，但是资本主义从其诞生就是一系列制度的产物，而马克思所谓的“一般法则”忽略了制度变迁的因素。 而对与《21世纪》这本书，毛咕噜用模型和数据展示了r&amp;gt;g，即资本回报率大于经济增长率，跟不平等的关系并非 Piketty 所描述的那样。同时毛咕噜还举例说明，不平等更多的是个制度问题，而不是像马克思和 Piketty 那样是资本主义必然会导致的。 大神毛咕噜的文章，就算你没读过《21世纪》原作，读读这篇文章也会很有启发。 badou：不过为什么大家都讨论 Daron，而不是他的评论里说的有没有道理？我觉得挺有道理的：马尔萨斯，马克思和 Piketty 都是看到了他们所处时代的某一个暂时性的 trend，就以为是 general law。Malthus 和 Marx 已经被证明错了，Piketty 应该也会。第一，如果r&amp;gt;g导致资本积累越来越多，那么r会逐渐下降的；第二，r&amp;gt;g不代表 capitalists 的资本增长就会超过g，如果他们不消费，那是当然的，不过他们也要消费啊。Acemoglu也搜集了数据，发现最重要的还是 institution。不管怎么说，他还是在用逻辑和数据说话，这个是最重要的。 慧航：嗯，其实他的论点论据很多，都很有道理，所以推这篇文章我真的不知道该怎么写了哈哈。 牛尔：当年虐那本《introduction to MEG》的时候感觉 DA 很神奇，对各种模型的评论和衍生都很精当，课后习题尤其棒，DA 的很多 PAPER 点子是不错的，但缺点在于无法顺着思路挖下去，follow 很难，难在数据可得性和思路。 badou：感觉他有一套整体思路：找到 institution 在 development 里的作用，特别是通过 technology 的影响。他 2000 年以后的 paper 都是沿着这个再走。不过好像确实是，每个 paper 之后比较难找到直接可以挖下去的东西。可能是因为他的文章都是走比较浅显易懂 intuitive 的路线。 badou：在美国还好？胡扯。只是还好会邀请他做 econometrica 主编？只是还好会让他主持 NEBR Summer Institute 的 Growth 的 Session？他在宏观发展里面的声望还是很好的。 笑死我了，你懂一点 dynamic programming 就一定要人家用？人家做 empirical 的 paper 也要没事弄个 dynamic programming？人家做 political economy 的 paper 就一定得篇篇用 dynamic programming？你怎么不去看看他做 directed technology 的 paper，篇篇都是 dynamic programming。只有初学者才老惦记着一点简单的工具，还以为多了不起。大牛们更在意的已经是 idea，甚至是研究方向了。 我明白了，你就是自己做 applied fields 的人，妒忌 Daron，就说“美国”觉得他“还好”。你胡扯的是“美国”觉得他还好。你应该说，“我们某些做 applied field 的人觉得他‘还好’”。你承认吗？ 还有，D.A. 的那本书很浅显的好吗？Dynamic Programming 的技术就是第六第七章，而且都是基础。不知道你为什么觉得这本书的技术很高，他的 Paper 跟那本书相去甚远。难道是你因为看不懂这本书就以为大家都觉得这本书技术很高？ 牛尔：看看 IDEAS RANKING，搜搜他的文章，你就知道 DARON 有多牛叉。他对制度经济学的贡献很大，尤其是殖民地制度演变有非常精彩的研究，他觉得有资格去评论皮克提的观点。 师兄说过：和 MIT 的教授交流，不要谈技术，讲思路就行，因为你懂的技术他都懂，你不懂的他也懂。Blanchard、nobuhiro（尤其敬仰，上了年纪还雄辩滔滔，又不失温文尔雅）这些人的技术都很好，只是慢慢都不愿意搞太复杂的模型，点子和思路才是最重要的，大概记得 Blanchard 说过这么一句：学习宏观经济学的技术花了 2 年时间，理解宏观经济学花了 10 多年，且历久更新。 Luo Patrick：我还没看过 21 世纪资本论。我的心得是，如果一部专业作品引起了很大社会反响的话，这部作品应该主要在讲价值判断，价值判断看不看也不打紧。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[为什么断点回归不能用高阶多项式逼近 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/08/27/caution-rdd/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19835380 Gelman and Imbense 在NBER上的最新工作论文《Why High-order Polynomials Should not be Used in Regression Discontinuity Designs》讨论了断点回归中使用高阶多项式的问题： It is common in regression discontinuity analysis to control for high order (third, fourth, or higher) polynomials of the forcing variable. We argue that estimators for causal effects based on such methods can be misleading, and we recommend researchers do not use them, and instead use estimators based on local linear or quadratic polynomials or other smooth functions. 其理由是： 断点回归中多项式回归的结果可以看成是对 outcome 的一个加权平均，而这种权重仅仅取决与阈值与关键变量的值，因此这些权重可能有问题。 使用多项式回归对多项式阶数选择十分敏感。 使用多项式回归的统计推断表现很差。 既然两位大神发话了，以后做 RDD 就要小心了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[人是怎样学习以及形成信念（belief）的？ - 慧航 - 专栏]]></title>
    	<url>/prof/2014/08/27/belief/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19874368 好久不见，这段时间快被数据整死了！恰巧 Yingyao Hu 在我们学校上课 文章的题目是《Nonparametric Learning Rules from Bandit Experiments: The Eyes Have It!》，由 Yingyao Hu, Yutaka Kayaba, and Matthew Shum 大神合写，发在 Games and Economic Behavior 上。 看到题目也许大家会楞一下，这是经济学研究的范畴吗？当然是。经济学慢慢慢慢地已经把领域扩展到一些本来属于理科的领域了，比如现在有一些人在做神经经济学，就是从生物、神经的角度来解释人类的行为，各种生物实验搞的眼花缭乱，高端大气上档次！ 实际上这篇文章是在讲，人是怎样形成 belief 的。熟悉博弈论的同学应该对 belief 这个词不陌生。说个形象点的，空城计里面，我的祖先司马懿面对诸葛亮，心里究竟怎么想的呢？攻还是受？奥不，撤退？他自己心里也会嘀咕，也会想一下这个空城是真的空城还是陷阱的概率。这个就是司马懿的 belief。也许正是诸葛亮之前的神算以及高超的演技误导了司马懿的 belief。 那么，如果诸葛多搞几次空城计，司马懿的 belief 会不会改变呢？这个 belief 改变的过程就是 learning 的过程。 当然，历史不能假设，所以三位作者用实验的数据和一些计量的处理方法，向我们揭示了这一问题。 这个设计是这样的，每次实验，被实验者面前都有两台老虎机，一台绿色一台蓝色。每台老虎机都有一个状态，good和bad。如果老虎机状态是good，那么就会以0.7的概率获得0.5美元，以0.3的概率失去0.5美元；如果老虎机状态是bad，那么就会以0.4的概率获得0.5美元，以0.6的概率失去0.5美元。 两台老虎机的状态是随机的，而且以一定的概率转化。如果老虎机的状态是good，那么下一次这台老虎机还是good的概率是0.85，有0.15的概率，这台老虎机的状态变成bad，另一台变成good。 每次选择结束之后，屏幕会提示这次是赢了还是输了，以及提示一下以上的状态转化的概率。 这个实验共有 21 个被实验者。高端的是，每个被实验者前面有台 eye-tracker，也就是这个实验可以记录被实验者眼睛盯着哪个颜色共多长时间（需要借助这个数据才能使用计量方法把后面的数据估计出来，可以把它想像成为一个工具变量）。 实验的过程如下图： 第一张图就是一个白点，提示开始。第二张图让被实验者选择蓝色还是绿色的老虎机。第三张图提示这次的结果，第四张图提示被实验者转化概率。 下面，作者设定了被实验者的 learning rule，也就是给了一定的结构，描述人是怎么更新自己的 belief 的。假设如下： 其中X*是信念（belief），R是这一期的收益，Y是这一期选择。也就是说，作者假设了下一期的信念是完全取决与上一期的信念、上一期的收益以及上一期的选择。那么实际上这三者就形成了一个离散的马尔可夫链。 下面忽略掉复杂的计量理论细节，我们来看看作者发现了什么。 首先，第一张表格说明，如果被实验者相信绿色的是good，那么他有98.66%的概率选择绿色；如果他不确定，那么有44.21%的概率他选择绿色，大约一半一半。至于为什么概率不是100%和50%，也许有一些心理学的理论（e-greedy）可以解释。但是基本上，有什么样的信念就有什么样的行为。 其次，第二张表格表明，如果被实验者相信绿色的是good，那么他看绿色的时间也会长一点。这个也比较 make sense。 关键是下面几张表格： 第一张表格可以看到，如果某一次选择了绿色，但是输了，下一次被实验者还有57.24%的概率会认为绿色的是good。看来“固执”也是人的本性，也就是说人们不太愿意改变自己的 belief。不知道金牛座的同学做的话会不会概率更大。 第二张表格可以看到，如果某一次选择了绿色，然后赢了，下一次被实验者会有88.89的概率相信下一期绿色是good，11.11%的会相信下一期蓝色是good。这个概率跟85% 15%的概率相近。 到此为止，数据都是按照作者的假定来看这些转移概率的。那么相对于其他的设定，这个设定表现怎么样呢？ 目前比较流行的还有另外两个设定，一个是使用贝叶斯法则的模型（类似与贝叶斯纳什均衡里面更新 belief 的方式），另外一种是 reinforment learning。 作者同样使用这两个设定进行了比较，结果如下： 其中B*是使用贝叶斯学习的结果，V*是使用 reinforment，X*是使用这篇文章的方法。第二列可以看出，使用这篇文章的学习方法预测成功的概率最高，所以看起来这个 belief 更新的模型设定应该是最靠谱的。 嗯，上篇文章有人说我不做 conclusions，这篇我也不做。这篇文章其实理论贡献和实际贡献都有，但是理论部分我没有介绍，所以也就不总结了。那么实际人们形成 belief 的过程和 learning 的过程上面都介绍清楚了，有什么好总结的呢，看官还是慢慢看正文吧，不看正文，总结了你也看不懂。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[线性面板数据模型的各种关系整理 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/08/18/linear-panel/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19828028 很多人问我关于线性面板数据里面各种估计量的关系，稍微整理 上面每个箭头都是可以证明的，感兴趣的可以自己证明，不会证的。也没必要证这么无聊的东西。 Dessy：fd 的 gls 形式是 fixed effect？ 慧航：yes。 loveapple：右边那个一阶差分之后不就可以用 ols 了吗？ 慧航：嗯，就是这么做的啊，只不过再 gls 之后就是 fixed effects。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[深刻理解 Python 中的元类 (metaclass) z]]></title>
    	<url>/tech/2014/08/10/python-class-object/</url>
		<content type="text"><![CDATA[[原文地址： 中文：http://blog.jobbole.com/21351/ 英文：https://stackoverflow.com/que 这里的语法基于 2.X，3.X 的一些细节，特别是class className(..., metaclass=)的语法参考廖的教程。 译注：这是一篇在 Stack overflow 上很热的帖子。提问者自称已经掌握了有关 Python OOP 编程中的各种概念，但始终觉得元类(metaclass)难以理解。他知道这肯定和自省有关，但仍然觉得不太明白，希望大家可以给出一些实际的例子和代码片段以帮助理解，以及在什么情况下需要进行元编程。于是e-satis同学给出了神一般的回复，该回复获得了 4928 点的赞同点数，更有人评论说这段回复应该加入到 Python 的官方文档中去。而 e-satis 同学本人在 Stack Overflow 中的声望积分也高达 64271 分，以下就是这篇精彩的回复。 MyClass = MetaClass() MyObject = MyClass() 类也是对象 在理解元类之前，你需要先掌握 Python 中的类。Python 中类的概念借鉴于 Smalltalk 语言，这显得有些奇特。在大多数编程语言中，类就是一组用来描述如何生成一个对象的代码段。在 Python 中这一点仍然成立： &amp;gt;&amp;gt;&amp;gt; class ObjectCreator(object): … pass … &amp;gt;&amp;gt;&amp;gt; my_object = ObjectCreator() &amp;gt;&amp;gt;&amp;gt; print my_object &amp;lt;__main__.ObjectCreator object at 0x8974f2c&amp;gt; 但是，Python 中的类还远不止如此。类同样也是一种对象。是的，没错，就是对象。只要你使用关键字class，Python 解释器在执行的时候就会创建一个对象。下面的代码段： &amp;gt;&amp;gt;&amp;gt; class ObjectCreator(object): … pass … 将在内存中创建一个对象，名字就是ObjectCreator。这个对象（类）自身拥有创建对象（类实例）的能力，而这就是为什么它是一个类的原因。但是，它的本质仍然是一个对象，于是乎你可以对它做如下的操作： 你可以将它赋值给一个变量； 你可以拷贝它； 你可以为它增加属性； 你可以将它作为函数参数进行传递； 下面是示例： &amp;gt;&amp;gt;&amp;gt; print ObjectCreator # 你可以打印一个类，因为它其实也是一个对象 &amp;lt;class &#39;__main__.ObjectCreator&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; def echo(o): … print o … &amp;gt;&amp;gt;&amp;gt; echo(ObjectCreator) # 你可以将类做为参数传给函数 &amp;lt;class &#39;__main__.ObjectCreator&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print hasattr(ObjectCreator, &#39;new_attribute&#39;) Fasle &amp;gt;&amp;gt;&amp;gt; ObjectCreator.new_attribute = &#39;foo&#39; # 你可以为类增加属性 &amp;gt;&amp;gt;&amp;gt; print hasattr(ObjectCreator, &#39;new_attribute&#39;) True &amp;gt;&amp;gt;&amp;gt; print ObjectCreator.new_attribute foo &amp;gt;&amp;gt;&amp;gt; ObjectCreatorMirror = ObjectCreator # 你可以将类赋值给一个变量 &amp;gt;&amp;gt;&amp;gt; print ObjectCreatorMirror() &amp;lt;__main__.ObjectCreator object at 0x8997b4c&amp;gt; 动态地创建类 因为类也是对象，你可以在运行时动态的创建它们，就像其他任何对象一样。首先，你可以在函数中创建类，使用class关键字即可。 &amp;gt;&amp;gt;&amp;gt; def choose_class(name): … if name == &#39;foo&#39;: … class Foo(object): … pass … return Foo # 返回的是类，不是类的实例 … else: … class Bar(object): … pass … return Bar … &amp;gt;&amp;gt;&amp;gt; MyClass = choose_class(&#39;foo&#39;) &amp;gt;&amp;gt;&amp;gt; print MyClass # 函数返回的是类，不是类的实例 &amp;lt;class &#39;__main__&#39;.Foo&amp;gt; &amp;gt;&amp;gt;&amp;gt; print MyClass() # 你可以通过这个类创建类实例，也就是对象 &amp;lt;__main__.Foo object at 0x89c6d4c&amp;gt; 但这还不够动态，因为你仍然需要自己编写整个类的代码。由于类也是对象，所以它们必须是通过什么东西来生成的才对。当你使用class关键字时，Python 解释器自动创建这个对象。但就和 Python 中的大多数事情一样，Python 仍然提供给你手动处理的方法。还记得内建函数type()吗？这个古老但强大的函数能够让你知道一个对象的类型是什么，就像这样： &amp;gt;&amp;gt;&amp;gt; print type(1) &amp;lt;type &#39;int&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print type(&amp;quot;1&amp;quot;) &amp;lt;type &#39;str&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print type(ObjectCreator) &amp;lt;type &#39;type&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print type(ObjectCreator()) &amp;lt;class &#39;__main__.ObjectCreator&#39;&amp;gt; 这里，type()有一种完全不同的能力，它也能动态的创建类。type()可以接受一个类的描述作为参数，然后返回一个类。（我知道，根据传入参数的不同，同一个函数拥有两种完全不同的用法是一件很傻的事情，但这在 Python 中是为了保持向后兼容性） type()可以像这样工作： type(类名, 父类构成的 tuple（用于继承，可以为空），包含属性的 dict（名称和值）) 比如下面的代码： &amp;gt;&amp;gt;&amp;gt; class MyShinyClass(object): … pass 可以手动像这样创建： &amp;gt;&amp;gt;&amp;gt; MyShinyClass = type(&#39;MyShinyClass&#39;, (), {}) # 返回一个类对象 &amp;gt;&amp;gt;&amp;gt; print MyShinyClass &amp;lt;class &#39;__main__.MyShinyClass&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print MyShinyClass() # 创建一个该类的实例 &amp;lt;__main__.MyShinyClass object at 0x8997cec&amp;gt; 你会发现我们使用&#39;MyShinyClass&#39;作为类名，并且也可以把它当做一个变量来作为类的引用(MyShinyClass)。类和变量是不同的，这里没有任何理由把事情弄的复杂。 type()接受一个字典来为类定义属性，因此 &amp;gt;&amp;gt;&amp;gt; class Foo(object): … bar = True 等价翻译为： &amp;gt;&amp;gt;&amp;gt; Foo = type(&#39;Foo&#39;, (), {&#39;bar&#39;: True}) 并且可以将Foo当成一个普通的类一样使用： &amp;gt;&amp;gt;&amp;gt; print Foo &amp;lt;class &#39;__main__.Foo&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print Foo.bar True &amp;gt;&amp;gt;&amp;gt; f = Foo() &amp;gt;&amp;gt;&amp;gt; print f &amp;lt;__main__.Foo object at 0x8a9b84c&amp;gt; &amp;gt;&amp;gt;&amp;gt; print f.bar True 当然，你可以向这个类继承，所以，如下的代码： &amp;gt;&amp;gt;&amp;gt; class FooChild(Foo): … pass 就可以写成： &amp;gt;&amp;gt;&amp;gt; FooChild = type(&#39;FooChild&#39;, (Foo,),{}) &amp;gt;&amp;gt;&amp;gt; print FooChild &amp;lt;class &#39;__main__.FooChild&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; print FooChild.bar # bar 属性是由 Foo 继承而来 True 最终你会希望为你的类增加方法。只需要定义一个有着恰当签名的函数并将其作为属性赋值就可以了。 &amp;gt;&amp;gt;&amp;gt; def echo_bar(self): … print self.bar … &amp;gt;&amp;gt;&amp;gt; FooChild = type(&#39;FooChild&#39;, (Foo,), {&#39;echo_bar&#39;: echo_bar}) &amp;gt;&amp;gt;&amp;gt; hasattr(Foo, &#39;echo_bar&#39;) False &amp;gt;&amp;gt;&amp;gt; hasattr(FooChild, &#39;echo_bar&#39;) True &amp;gt;&amp;gt;&amp;gt; my_foo = FooChild() &amp;gt;&amp;gt;&amp;gt; my_foo.echo_bar() True 你可以看到，在 Python 中，类也是对象，你可以动态的创建类。这就是当你使用关键字class时 Python在幕后做的事情，而这就是通过元类来实现的。 到底什么是元类 元类就是用来创建类的“东西”。你创建类就是为了创建类的实例对象，不是吗？但是我们已经学习到了 Python 中的类也是对象。好吧，元类就是用来创建这些类（对象）的，元类就是类的类，你可以这样理解为： MyClass = MetaClass() MyObject = MyClass() 你已经看到了type()可以让你像这样做： MyClass = type(&#39;MyClass&#39;, (), {}) 这是因为函数type()实际上是一个元类。type()就是 Python 在背后用来创建所有类的元类。现在你想知道那为什么type()会全部采用小写形式而不是Type呢？好吧，我猜这是为了和str保持一致性，str是用来创建字符串对象的类，而int是用来创建整数对象的类。type就是创建类对象的类。你可以通过检查__class__属性来看到这一点。Python 中所有的东西，注意，我是指所有的东西——都是对象。这包括整数、字符串、函数以及类。它们全部都是对象，而且它们都是从一个类创建而来。 &amp;gt;&amp;gt;&amp;gt; age = 35 &amp;gt;&amp;gt;&amp;gt; age.__class__ &amp;lt;type &#39;int&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; name = &#39;bob&#39; &amp;gt;&amp;gt;&amp;gt; name.__class__ &amp;lt;type &#39;str&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; def foo(): pass &amp;gt;&amp;gt;&amp;gt;foo.__class__ &amp;lt;type &#39;function&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; class Bar(object): pass &amp;gt;&amp;gt;&amp;gt; b = Bar() &amp;gt;&amp;gt;&amp;gt; b.__class__ &amp;lt;class &#39;__main__.Bar&#39;&amp;gt; 现在，对于任何一个__class__的__class__属性又是什么呢？ &amp;gt;&amp;gt;&amp;gt; a.__class__.__class__ &amp;lt;type &#39;type&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; age.__class__.__class__ &amp;lt;type &#39;type&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; foo.__class__.__class__ &amp;lt;type &#39;type&#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; b.__class__.__class__ &amp;lt;type &#39;type&#39;&amp;gt; 因此，元类就是创建类这种对象的东西。如果你喜欢的话，可以把元类称为类工厂（不要和工厂类搞混了:D） type()就是 Python 的内建元类，当然了，你也可以创建自己的元类。 __metaclass__属性 你可以在写一个类的时候为其添加__metaclass__属性。 class Foo(object): __metaclass__ = something… […] 如果你这么做了，Python 就会用元类来创建类Foo。小心点，这里面有些技巧。你首先写下class Foo(object)，但是类对象Foo还没有在内存中创建。Python 会在类的定义中寻找__metaclass__属性，如果找到了，Python 就会用它来创建类Foo，如果没有找到，就会用内建的type()来创建这个类。把下面这段话反复读几次。当你写如下代码时： class Foo(Bar): pass Python 做了如下的操作： Foo中有__metaclass__这个属性吗？ 如果是，Python 会在内存中通过__metaclass__创建一个名字为Foo的类对象（我说的是类对象，请紧跟我的思路）； 如果 Python 没有找到__metaclass__，它会继续在Bar（父类）中寻找__metaclass__属性，并尝试做和前面同样的操作； 如果 Python 在任何父类中都找不到__metaclass__，它就会在模块层次中去寻找__metaclass__，并尝试做同样的操作； 如果还是找不到__metaclass__，Python 就会用内置的type()来创建这个类对象。 现在的问题就是，你可以在__metaclass__中放置些什么代码呢？答案就是：可以创建一个类的东西。那么什么可以用来创建一个类呢？type()，或者任何使用到type()或者子类化type()的东东都可以。 自定义元类 元类的主要目的就是为了当创建类时能够自动地改变类。通常，你会为 API 做这样的事情，你希望可以创建符合当前上下文的类。假想一个很傻的例子，你决定在你的模块里所有的类的属性都应该是大写形式。有好几种方法可以办到，但其中一种就是通过在模块级别设定__metaclass__。采用这种方法，这个模块中的所有类都会通过这个元类来创建，我们只需要告诉元类把所有的属性都改成大写形式就万事大吉了。 幸运的是，__metaclass__实际上可以被任意调用，它并不需要是一个正式的类（我知道，某些名字里带有class的东西并不需要是一个class，画画图理解下，这很有帮助）。所以，我们这里就先以一个简单的函数作为例子开始。 # 元类会自动将你通常传给 type 的参数作为自己的参数传入 def upper_attr(future_class_name, future_class_parents, future_class_attr): &amp;quot;&amp;quot;&amp;quot; 返回一个类对象，将属性都转为大写形式 &amp;quot;&amp;quot;&amp;quot; # 选择所有不以&#39;__&#39;开头的属性，将它们转为大写形式 uppercase_attr = {} for name, val in future_class_attr.items(): if not name.startswith(&#39;__&#39;): uppercase_attr[name.upper()] = val else: uppercase_attr[name] = val # # 通过 type() 来做类对象的创建 return type(future_class_name, future_class_parents, uppercase_attr) __metaclass__ = upper_attr # 这会作用到这个模块中的所有类 class Foo(): # global __metaclass__ won&#39;t work with &amp;quot;object&amp;quot; though # 我们也可以只在这里定义__metaclass__，这样就只会作用于这个类中 # and this will work with &amp;quot;object&amp;quot; children bar = &#39;bip&#39; print(hasattr(Foo, &#39;bar&#39;)) # Out: False print(hasattr(Foo, &#39;BAR&#39;)) # Out: True f = Foo() print(f.BAR) # Out: &#39;bip&#39; 现在让我们再做一次，这一次用一个真正的class来当做元类。 # 请记住，&#39;type&#39;实际上是一个类，就像&#39;str&#39;和&#39;int&#39;一样 # 所以，你可以从 type 继承 class UpperAttrMetaclass(type): # __new__ 是在__init__之前被调用的特殊方法 # __new__是用来创建对象并返回之的方法 # 而__init__只是用来将传入的参数初始化给对象 # 你很少用到__new__，除非你希望能够控制对象的创建 # 这里，创建的对象是类，我们希望能够自定义它，所以我们这里改写__new__ # 如果你希望的话，你也可以在__init__中做些事情 # 还有一些高级的用法会涉及到改写__call__特殊方法，但是我们这里不用 def __new__(upperattr_metaclass, future_class_name, future_class_parents, future_class_attr): uppercase_attr = {} for name, val in future_class_attr.items(): if not name.startswith(&#39;__&#39;): uppercase_attr[name.upper()] = val else: uppercase_attr[name] = val return type(future_class_name, future_class_parents, uppercase_attr) 但是，这种方式其实不是 OOP。我们直接调用了type，而且我们没有改写父类的__new__方法。现在让我们这样去处理： class UpperAttrMetaclass(type): def __new__(upperattr_metaclass, future_class_name, future_class_parents, future_class_attr): uppercase_attr = {} for name, val in future_class_attr.items(): if not name.startswith(&#39;__&#39;): uppercase_attr[name.upper()] = val else: uppercase_attr[name] = val # 复用type.__new__方法 # 这就是基本的 OOP 编程，没什么魔法 return type.__new__(upperattr_metaclass, future_class_name, future_class_parents, uppercase_attr) 你可能已经注意到了有个额外的参数upperattr_metaclass，这并没有什么特别的。类方法的第一个参数总是表示当前的实例，就像在普通的类方法中的self参数一样。当然了，为了清晰起见，这里的名字我起的比较长。但是就像self一样，所有的参数都有它们的传统名称。因此，在真实的产品代码中一个元类应该是像这样的： class UpperAttrMetaclass(type): def __new__(cls, clsname, bases, dct): uppercase_attr = {} for name, val in dct.items(): if not name.startswith(&#39;__&#39;): uppercase_attr[name.upper()] = val else: uppercase_attr[name] = val return type.__new__(cls, clsname, bases, uppercase_attr) 如果使用super()方法的话，我们还可以使它变得更清晰一些，这会缓解继承1（是的，你可以拥有元类，从元类继承，从type继承） class UpperAttrMetaclass(type): def __new__(cls, clsname, bases, dct): uppercase_attr = {} for name, val in dct.items(): if not name.startswith(&#39;__&#39;): uppercase_attr[name.upper()] = val else: uppercase_attr[name] = val return super(UpperAttrMetaclass, cls).__new__(cls, clsname, bases, uppercase_attr) 就是这样，除此之外，关于元类真的没有别的可说的了。使用到元类的代码比较复杂，这背后的原因倒并不是因为元类本身，而是因为你通常会使用元类去做一些晦涩的事情，依赖于自省，控制继承等等。确实，用元类来搞些“黑暗魔法”是特别有用的，因而会搞出些复杂的东西来。但就元类本身而言，它们其实是很简单的： 拦截类的创建； 修改类； 返回修改之后的类。 为什么要用 metaclass 类而不是函数？ 由于__metaclass__可以接受任何可调用的对象，那为何还要使用类呢，因为很显然使用类会更加复杂啊？这里有好几个原因： 意图会更加清晰。当你读到UpperAttrMetaclass(type)时，你知道接下来要发生什么； 你可以使用 OOP 编程。元类可以从元类中继承而来，改写父类的方法。元类甚至还可以使用元类； 你可以把代码组织的更好。当你使用元类的时候肯定不会是像我上面举的这种简单场景，通常都是针对比较复杂的问题。将多个方法归总到一个类中会很有帮助，也会使得代码更容易阅读； 你可以使用__new__(), __init__()以及__call__()这样的特殊方法。它们能帮你处理不同的任务。就算通常你可以把所有的东西都在__new__()里处理掉，有些人还是觉得用__init__()更舒服些； 哇哦，这东西的名字是metaclass，肯定非善类，我要小心！ 究竟为什么要使用元类？ 现在回到我们的大主题上来，究竟是为什么你会去使用这样一种容易出错且晦涩的特性？好吧，一般来说，你根本就用不上它： “元类就是深度的魔法，99% 的用户应该根本不必为此操心。如果你想搞清楚究竟是否需要用到元类，那么你就不需要它。那些实际用到元类的人都非常清楚地知道他们需要做什么，而且根本不需要解释为什么要用元类。” —— Python 界的领袖 Tim Peters 元类的主要用途是创建 API。一个典型的例子是 Django ORM2。它允许你像这样定义： class Person(models.Model): name = models.CharField(max_length=30) age = models.IntegerField() 但是接下来如果你像这样做的话： guy = Person(name=&#39;bob&#39;, age=&#39;35&#39;) print guy.age 返回的并不是一个IntegerField对象，而是会返回一个int，甚至可以直接从数据库中取出数据。这是有可能的，因为models.Model定义了__metaclass__， 并且使用了一些魔法能够将你刚刚定义的简单的Person类转变成对数据库的一个复杂hook。Django 框架将这些看起来很复杂的东西通过暴露出一个简单的使用元类的 API 将其化简，通过这个 API 重新创建代码，在背后完成真正的工作。 结语 首先，你知道了类其实是能够创建出类实例的对象。好吧，事实上，类本身也是实例，当然，它们是元类的实例。 &amp;gt;&amp;gt;&amp;gt;class Foo(object): pass &amp;gt;&amp;gt;&amp;gt; id(Foo) 142630324 Python 中的一切都是对象，它们要么是类的实例，要么是元类的实例，除了type。type实际上是它自己的元类，在纯 Python 环境中这可不是你能够做到的，这是通过在实现层面耍一些小手段做到的。其次，元类是很复杂的。对于非常简单的类，你可能不希望通过使用元类来对类做修改。你可以通过其他两种技术来修改类： Monkey patching； 类装饰器； 当你需要动态修改类时，99%的时间里你最好使用上面这两种技术。当然了，其实在99%的时间里你根本就不需要动态修改类 which will ease inheritance (because yes, you can have metaclasses, inheriting from metaclasses, inheriting from type. ↩ 可参考廖雪峰教程中的相关内容。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[横截条件]]></title>
    	<url>/prof/2014/08/03/pongzi-condition/</url>
		<content type="text"><![CDATA[[横截条件(Transversality condition)也称边界条件，实际上是库恩—塔克条件(the Kuhn-Tucker conditions)的极限形式。让 假设一个只能再活两期(1和2期)的消费者，每一期都有禀赋\(\omega_t\)，在0期时遗留下来债券为\(b_0\)，可以借贷，没有劳动。该消费者的最优化问题为(贴现到第0期)： \[\begin{equation} \left\{ \begin{aligned} &amp;amp; \max\;\beta u(c_1)&#43;\beta^2u(c_2)\\ &amp;amp; \begin{aligned} s.t.\;\; &amp;amp; (1)\; b_1&#43;c_1\leqslant \omega_1&#43;(1&#43;r_0)b_0\\ &amp;amp; (2)\; b_2&#43;c_2\leqslant \omega_2&#43;(1&#43;r_1)b_1 \end{aligned} \end{aligned}\right. \end{equation}\] 可是这些条件是否足够我们求得所需的最优解呢？答案是否定的，原因在于如果对于第2 期，也就是最后一期的借贷没有限制的话，则该消费者一定会设法借入无穷的债务，即令\(b_2=-\infty\)，由此来达到更高的效用。因此，我们必须对\(b_2\)做出一个“合理”的限制，来确保可以得到符合现实的解。一个很自然的限制1是\(b_2\geqslant 0\)，加上这个条件以后，我们再来求解这个最优化问题。 写出拉格朗日函数： \[ \begin{aligned} \mathcal{L}=\beta u(c_1)&#43;\beta^2u(c_2)&amp;amp; &#43;\beta\lambda_1[\omega_1&#43;(1&#43;r_0)b_0-b_1-c_1]\\ &amp;amp; &#43;\beta^2\lambda_2[\omega_2&#43;(1&#43;r_1)b_1-b_2-c_2] \end{aligned} \] 对于\(b_2\geqslant 0\)的Kuhn-Tucker条件为： \[ \frac{\partial \mathcal{L}}{\partial b_2}\leqslant 0; \quad b_2\cdot\frac{\partial \mathcal{L}}{\partial b_2}=0 \] 上式中的第二个条件即为\(\beta^2\lambda_2b_2=0\)，如果是三期的消费者，可以写出该条件为\(\beta^3\lambda_3b_3=0\)，继续推广下去，到无穷期时，就成为横截条件： \[ \lim_{t\to\infty}\beta^t\lambda_tb_t=0 \] 即非 Ponzi 博弈条件，含义是在最后一期时该消费者不能有负债。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[什么是双重差分模型（difference-in-differences model）？ - 慧航 - 专栏]]></title>
    	<url>/prof/2014/07/04/did/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19793523 双重差分吗，就是差分两次。我们先来举个栗子。 现在要修一条 现在我们比较好奇铁路修好以后，被铁路穿过的城市是不是经济增长更快了？我们该怎么做呢？ 一开始的想法是，我们把\(D_i=1\)的城市的 GDP 加总，减去\(D_i=0\)的城市的 GDP 加总，然后两者一减，即\(\mathrm{E}(Y_i|D_i==1)-\mathrm{E}(Y_i|D_i==0)\)，这样我们就算出了两类城市的 GDP 的平均之差。 这样做不用说肯定有问题。万一被铁路穿过的城市在建铁路之前 GDP 就高呢？为了解决这个问题，我们需要观察到至少两期，第一期是建铁路之前，第二期是建铁路之后。我们先把两类城市的 GDP 做两期之差，即： \[\Delta Y_i=\frac{1}{N}\sum(Y_{i1}-Y_{i0})\] 这是第一次差分，经过这一步，我们实际上算出了每个城市 GDP 的增长（率，如果取 log 之后），也就是 GDP 的趋势。 完了之后，计算： \[\mathrm{Treatment Effect}=\mathrm{E}(\Delta Y_i|D_i=1)-\mathrm{E}(\Delta Y_i|D_i=0)\] 这是第二次差分。这一步就把两类城市在修建铁路之前和之后的 GDP 增长率的差异给算出来了，这就是我们要的处理效应，即修建铁路之后对城市经济的促进作用。 这个东西你还可以换一个写法。记\(T=1\)如果时间为建铁路之后，\(T=0\)如果时间为建铁路之前，那么我们可以得到一个表： Treated \(D_i=0\) \(D_i=1\) \(T=0\) 0 0 \(T=1\) 0 1 \(Treated\)代表在某一期，某一类城市是不是建了铁路。第零期肯定没有建铁路，第一期只有\(D_i=1\)的城市建了铁路。所以\(Treated=D_i*T\)。因此我们把方程写成： \[Y_{it}=\alpha D_i&#43;\beta T&#43;\gamma(D_i\times T)&#43;u_{it}\] 对时间差分，得到： \[\Delta Y_i=\beta&#43;\gamma D_i&#43;\Delta u_{it}\] 再次差分，取期望： \[\mathrm{E}(\Delta Y_1-\Delta Y_0)=\gamma\] 可见，\(\gamma\)就是我们想要估计的处理效应。 所以实际做的时候，可以直接跑 \[Y_{it}=\alpha D_i&#43;\beta T&#43;\gamma(D_i\times T)&#43;u_{it}\] 这个式子的回归，得到的交叉项的系数就是所要估计的处理效应。 用一个图表示就是： 所以看清楚了，这里 DID 最关键的假设是 common trend，也就是两个组别在不处理的情况下，\(y\)的趋势是一样的。 那么你会说了，铁路穿过的城市可能本身 GDP 也高，而 GDP 高的城市按照理论 GDP 增长率可能更高可能更低，所以 common trend 的假设可能是不对的，那怎么办？ 如果这个问题存在，我们可以进一步假设在控制了某些外生变量之后，common trend 是对的，比如上个问题，我们可以控制城市在\(T=0\)期的 GDP level。当我们控制其他变量之后，自然不能直接减两次了，我们需要用上面说的回归式子，即 run the following OLS: \[Y_{it}=\alpha D_i&#43;\beta T&#43;\gamma(D_i\times T)&#43;\boldsymbol{X}^\prime\delta&#43;u_{it}\] 好了，我就说到这里了。当然也有可能你控制了\(X\)之后 common trend 还是不成立，那么你要想其他办法了。有些人会去找工具变量，这个方法我不是多么认可，所以我也就不介绍了。需要的时候再说。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[R-squared&lt;0 in 2SLS-IV estimation? - 慧航 - 专栏]]></title>
    	<url>/prof/2014/05/20/2sls-r2/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19756603 今天一个漂亮的小妹妹来问我问题，Stata 里面做工具变量 R2 的值。我一听也十分诧异，从来没遇到这种情况。当然本着学术的纯洁目的仔细研究了一下这个问题（真的不是为了学妹！），发现 Stata 官网上有对这个问题的描述：Stata | FAQ: Negative and missing R-squared for 2SLS/IV 说的抽象一点，因为我们之前做 OLS 的时候，是一个正交投影，所以不会出现这个问题。但是当我们做 2SLS 的时候，是一个非正交的投影。既然是非正交的投影，投影的时候就会有角度。当这个角度比较小的时候，就会出现预测的y_hat跟实际的y夹角大于90度，就会出现这种情况。 具体从计算的角度来看，我们做 2SLS 实际上做了两步。假设内生变量是x1,x2,外生变量是z1,z2,z3,z4。我们要估计 y on x1 x2 z3 z4 但是我们实际上先回归了 reg x1 on z1 z2 z3 z4 =&amp;gt; x1_hat reg x2 on z1 z2 z3 z4 =&amp;gt; x2_hat 然后回归： reg y on x1_hat x2_hat z3 z4 =&amp;gt; y_hat 如果计算残差的时候是用这个y_hat的，那么不会出现R2&amp;lt;0的问题。但是我们计算残差的时候实际上是用 y-b1*x1-b2*x2-b3*z3-b4*z4 而非 y-b1_hat*x1-b2_hat*x2-b3*z3-b4*z4 所以计算的u的 variance 肯定更大，就会出现R2&amp;lt;0的问题。 那么这个问题怎么解决呢？实际上这根本不是个问题。因为本来 IV 就要求不是个最优的 predictor，所以忽略这个问题吧！]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[计量经济学中那些从统计学、初级计量里面带来的恶习 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/05/05/econometrica-wrong-misunderstanding/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19743214 回答线性回归中的ANOVA的作用是什么？这个问题的时候顺 为什么要写这个。因为你经常在知乎看到这样的问题： 或者这种问题： 甚至这个问题还会出现这样的回答： 对于这些问题和回答，实在无力吐槽。 随意删变量 什么？某个变量不显著？删掉！呵呵，这个变量如果理论上对你的y有影响，但是做不出显著，一可能是你的模型错了，二可能是数据没有足够的 variation 做出显著。如果删掉，你其他的估计都会受到“遗漏变量”的影响，估计的系数理论上都不对的。 多重共线性 这个多少跟第一条有关系。什么？你的模型有多重共线性？好严重啊！删变量吧！ 为什么不能删变量第一条已经说了。你删掉变量所导致的问题比多重共线性来的还要严重你造吗？ 解决多重共线性最好的办法是增加样本，别的好像没办法了。 至于有人用“主成份分析法”，呵呵，你还知道你估计的东西是啥不？ 变量筛选 也跟第一条有关系，做很多很多回归，把显著的变量留下来，不显著的删掉。不解释了，参见第一条。统计学有很多变量筛选的方法，但是，经济学统统不适用。经济学模型里面如果理论上支持有，就不能删。 异方差 都21世纪了，你还在线性模型里面检验异方差？没听说过 white heteroskedasticity robust 的统计量吗？这个还需要检验？还需要加权最小二乘？ 只有非线性模型中异方差是致命的！线性模型中异方差（自相关）可以很方便的用 white 或者 Newey-west 来解决。 当然如果是面板数据，组内的异方差自相关可以直接cluster来做。 R2 这个多少跟主题有关。实际情况是，时间序列你做出低于90%的 R2 都不正常，但是微观数据你做出50%的 R2 都很困难。 OLS 是在给定的数据和变量条件下 R2 最高的，因为他是个线性投影。工具变量估计是一个非正交投影，所以 R2 肯定比 OLS 的要低。但是我们还是要发展 IV 之类的方法，这也从侧面反映了 R2 不重要。 R2 实际上是在比较误差项u的方差和y_hat的方差。但是经济学中，我们不太关心u的方差有多大，而更关心u里面有什么。 所以你如果用 R2 去比较模型，意义不大。 Box-Jenkins 不是专业做时间序列的，不做过多评价。但是基于 ACF、PACF 图的什么“截尾”、“拖尾”是很不靠谱的方法，已经是共识了。 王率宾：计量经济学和统计学什么关系？ 慧航：http://www.zhihu.com/question/22935472。 江灏：一个模型里的变量不显著的话，不就是说明这个变量无关紧要吗，为什么不能删啊？ 慧航：好问题。有可能仅仅是这个变量没有足够的 variation 做出显著而已。也就是说如果你扩大样本，可能就显著了。如果这个变量的确应该存在于这个模型里面，而你忽略了，那么其他的参数估计都会受到这个变量的干扰而导致不一致。 江灏：这就是隔行如隔山吧，我完全想象不出来什么情况扩大样本会让变量变得显著，赶明儿找本计量经济学的书看看。 慧航：这个很正常啊，小样本属性比较差呗。你想想，b的显著性取决于其方差，而其方差取决于x的方差的倒数。所以如果x的方差比较小，那么小样本情况下，很有可能b是不显著的。 刘美丽：学统计的表示不在一个世界里生活，增大样本个数怎么能解决多重共线性的问题呢？ 慧航：多重共线性导致的是估计量的方差变大，增大样本量可以使这种影响降到最小。注意不是完全共线性，完全共线性只能删掉一个变量。 王鹤喧：我们教金融工程的老师一个劲跟我们说，这些异方差、自相关什么的，都不重要，经济原理才是重要的，现在重点是让方程符合理论，拟和优度、显著性什么的都是凑合着看看的。 慧航：对也不对，比如非线性模型异方差就是非常严重的问题，而且很难解决。 brillbird：哈哈，基本赞同作者的观点。出来读研究生，才发现国内本科学的啥 R2，共线性，删变量是多么不靠谱了。不过作者的有些观点，也极端了点，就我所见，北美这边 GLS，ACF, PACF 之类还是要教的。只不过实际经验研究里面用的少罢了。计量里头，理论是一回事，应用是另一回事的例子太多了。 慧航：教是要教的，比如 GLS 在 NLS 里面就挺好用，而且非线性模型里面有些东西还可以直接看成是某种形式的 GLS。至于 ACF、PACF，也要讲，但是讲的时候应该告诉学生，理论上这两个东西不能用来判断阶数的，以及为什么。的确，理论和应用有的时候差别很大，某些做应用的最喜欢的就是 rule of thumb 以及 acf pacf 这种主观的东西，也没办法。 洁白似血：关于异方差：有些人认为如果模型设定无误的话，那么残差应该是不包含任何信息的白噪声，因此异方差检验实际上是一个模型设定偏误的检验，异方差是模型设定偏误的信号。而模型设定一旦存在较大偏误，就很有可能不光是方差不对了，光修正个方差然后假装自己没事了，是不可取的。如果遇到异方差，需要想一想自己的模型是不是有问题，而不是二话不说直接用花式 robust standard error，这个 robust 只能修正特定的方差问题，而不能管到模型的全部问题。分别从朱家祥，Achen和我现在的计量老师那里各自听到了这种看法，我觉得还是有一定道理的，值得思考。当然 WLS 是不是修正模型设定偏误的，那是另一个具体问题了。 关于 R2：一个月前上计量课的时候听到老师的一个关于 R2 的观点，他说虽然大家都一致认为 R2 不重要，但是他不禁会想有一些实证结果 R2 只有零点零几，虽然关键解释变量高度显著，但是这种结果下y的变动大多是由误差项所贡献的，那么他很难相信这个时候误差项是纯粹的 noise，或者遗漏了和x无关的变量，模型没有遗漏变量偏误的问题。所以他认为 R2 实际上可能包含关于模型质量的信息的，过低的 R2 可能包含了遗漏变量偏误的信号。我觉得还是有些道理的。尤其是众所周知 IV 大部分情况下还是挺不靠谱的，R2 低担心有遗漏变量搞 IV 这条路，也让人感觉不太踏实。 另外一点是把模型和理论摆在比数据之前的哲学信条。虽然我也这么受的教育，但是做多了总是会有疑问：“我（这个作者）何德何能，搞出来的理论也好模型也罢，非得塞一些我（他）觉得有道理的东西，搞的还跟真的似的”？后来接触了一点点另一个山头的 agnostic, 那种让 data 自己选择的哲学信条还是有些令人无法抗拒的。当然会有“你还知道你估计的东西是啥不？”这种问题的出现，但是转过来想，尤其是对一个还搞一点理论的人来说，“你知道你的理论是啥不？”这种问题，同样令人无法回答。 扯了这么多，我想说的是国内非顶尖的计量教育不知道是啥时候哪个流派的哲学信条下的技术，可惜的是他们从不讲技术背后的哲学信仰，还挺遗憾的。 慧航：对于第一条，的确，异方差非常重要，但是并不是在线性情况下重要，非线性条件下这个东西非常重要。关于模型设定，建议阅读 Wooldidge 的 what are we weighting for，会有多种情况导致异方差，也有多种情况导致异方差被消除，除非你能 fully understand the data generating process，线性模型对异方差的讨论多数是没什么额外收益的。 关于 R2，是统计显著和经济显著的差别。这取决于外部干扰有多大，有的时候研究者并非为了解释这个特有的问题，而是为了验证经济理论，这个时候 R2 并不重要。 洁白似血：R2 我就再说验证理论的事。过低的 R2 可能有遗漏变量的信号在里头，这个时候系数就不一致了，因此 R2 也不是完全一点都不重要。至于线性模型对于异方差的讨论没什么额外收益，不理解。我讲的东西和模型线性不线性似乎没啥关系。 慧航：变量总会被遗漏的，all models are wrong, but some are useful, 不然还引入误差项做什么。你讲的是对的，无论任何时候，都要考虑误差项里面究竟有什么，会不会导致不一致。只是单纯拿出异方差来说，在非线性模型里面单异方差就可以导致不一致，而线性模型里面不会。基本上，这是两个问题，就算没有异方差，你也要考虑模型里面究竟遗漏了什么，是不是跟x相关。 洁白似血：我觉得 all models are wrong 这句话本身就有问题，既然 all models are wrong 那么可不可以 “all models are wrong” can be wrong？ 另外我的观点是“异方差很可能不单纯”。当然真正“单纯”的异方差不会影响一致性，但是你怎么知道线性模型里的异方差就是“单纯”的？难道不应该单纯的误差项就是 IID 的么，纯粹的白噪音？单纯的异方差真要单纯的起来，需要满足： 缺失一个变量z在误差项u里头，z的条件方差与x有相关性。 z本身与x不相关。 反正一般的实证情况下，我是不太可能相信z条件方差与x相关，本身却与x无关的，除非那种特殊构造出来的。 所以第一我的哲学观点是有些模型 can be right。第二异方差是 wrong model 的 signal。当然没有异方差不代表模型一定对，但是有异方差，我很难相信模型是对的。 慧航：所以微观实证你就不要做了，几乎没有什么东西是没异方差的。我的意思是说，最关键的问题是你想好误差项里面有什么是跟你的x相关的，单独的异方差不能给你任何这方面的信息。此外，很多模型，特别是加总数据，得到异方差的结构很简单，还是建议你读上面那篇论文。最后，即便不是加总数据，单独异方差的情况也很多，比如薪酬问题，高管的薪酬比一般小职员的方差大，你可以 argue 说忽略了很多因素，但是恐怕控制了能力之类，这个东西也难说同方差。我不用知道异方差存在也知道模型忽略了很多东西，但是忽略了什么你能告诉我麽？如果照你这么说，我可以拒绝任何计量的文章，特别是那些用非线性模型的，没有没问题的。 更直白的例子是化疗。化疗究竟有没有延长寿命？没有化疗的可能平均活一年，方差很小，有化疗的可能没几天就挂了，有的治好了活了十年。考虑到病人会根据自己的身体情况选择是否化疗，你想一下如果是随机分组实验的话才有异方差，自选择反而异方差更小了，这你怎么解释？ Eric Huang：我写的毕业论文涉及ARMA，然而我用的是相关图定阶，有更好的定阶方法吗？ 慧航：AIC、BIC 之类。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[经济学家数据可视化指南 - 慧航 - 专栏]]></title>
    	<url>/prof/2014/04/24/eco-visual/</url>
		<content type="text"><![CDATA[[原文地址：https://zhuanlan.zhihu.com/p/19735479 可视化的数据展示方案比用文字来描述数据效率更高。但是如何 中 Jonathan A. Schwabish 给了我们一些指导。 Dropbox - An Economist’s Guide to Visualizing Data.pdf]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[各位涉及过的最复杂的计量经济学模型的例子是什么？ - 慧航 - 问答]]></title>
    	<url>/prof/2014/02/11/econometrica-model/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/22718515 首先谢邀。但是这个问题问的好像很模糊。“大规模”是指 structural form，跟 reduced from 相对。一般 structural model 在 IO（产业组织理论）和 trade（贸易理论）里面用的较多，但是在 labor（劳动经济学）等领域用的很少。在宏观里面也有 Structural VAR 的识别和估计问题。如果是按照计算难度的话，一般来说 structural model 多数用 MLE 和 GMM 来进行估计，涉及到非线性的最优化，所以计算难度一般较高，耗费时间非常的长。当然现在发展了很多方法解决计算的问题。 如果是按照规模（scale）的话，我听说过（没做过）美联储用的宏观模型有几百个变量，不过我没有仔细追究他们具体用的什么模型。当然可以肯定不是SVAR，不然就算解决了识别问题，计算难度也已经无法想象了。希望提问者可以补充一下问题，我们可以针对具体问题具体讨论。 非线性最优化意味着不能简单的通过线性代数的运算得到估计值。实际上除了一般的最小二乘、线性面板、线性联立方程、SUR、VAR 等模型，几乎都要牵涉到求非线性函数的极值。打个比方，求解线性问题类似于x&#43;y=5; x-y=3的问题，但是非线性的问题类似于求解x^4&#43;e^y=5; ln(x)&#43;sin(y)=0这样的问题。一般解决这类问题不会用到插值法，根据不同的问题，有不同的求最优化的方法。这里面有很多问题，首先是识别问题，也就是说，你要保证你最优化的目标函数有且仅有一个。其次是局部最优的问题，计算最优化的算法有可能会落入到局部最优，即在这个值的邻域内是极值，但是并不是全局的最大（小）值。最后才是效率的问题，一些问题的算法程序运行动辄几天，几月，很夸张的。如果说例子的话，比如 IO 里面的 BLP 模型。最简单的 BLP 模型几分钟就可以出结果，复杂的计算时间无上限。 徐昇：能举个你涉及到过的需要考虑效率问题的计量经济模型的例子吗？ Huang Zibin：你随便找一个现在宏观的 DSGE 模型，都是不可能找到解析解的。宏观这里有时候和慧航提到的方法不一定一样。一般都先选几个一阶矩条件用 GMM 校准参数，然后用数值分析的办法找方程组的解，一般是 stationary equilibrium，不是定值。比如这篇文章：Agency Costs, Net Worth, and Business Fluctuations: A Computable General Equilibrium Analysis. by Charles T. Carlstrom and Timothy S. Fuerst, The American Economic Review. Vol. 87, No. 5 (Dec., 1997), pp. 893-910]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[语法高亮：如何在文本复制时自动忽略行号 z]]></title>
    	<url>/tech/2014/02/04/lstlistings-numbering/</url>
		<content type="text"><![CDATA[[原文地址：http://www.latexstudio.net/archives/765 用 listings 包排版源代码时通常为了便于说明，会显示代码的行号 我们想实现如下的复制，更便于读者提取文档中的源代码进行相关测试。 我们可以使用 accsupp 包来实现，具体代码如下： \documentclass{article} \usepackage{xcolor}% http://ctan.org/pkg/xcolor \usepackage{listings}% http://ctan.org/pkg/listings \usepackage{accsupp}% http://ctan.org/pkg/accsupp \renewcommand{\thelstnumber}{% Line number printing mechanism \protect\BeginAccSupp{ActualText={}}\arabic{lstnumber}\protect\EndAccSupp{}% } \lstset { language={[LaTeX]TeX}, numbers=left, numbersep=1em, numberstyle=\tiny, frame=single, framesep=\fboxsep, framerule=\fboxrule, rulecolor=\color{red}, xleftmargin=\dimexpr\fboxsep&#43;\fboxrule\relax, xrightmargin=\dimexpr\fboxsep&#43;\fboxrule\relax, breaklines=true, basicstyle=\small\tt, keywordstyle=\color{blue}, commentstyle=\color[rgb]{0.13,0.54,0.13}, backgroundcolor=\color{yellow!10}, tabsize=2, columns=flexible, morekeywords={maketitle}, } \begin{document} \begin{lstlisting} \documentclass{article} \usepackage{listings} \title{Sample Document} \author{John Smith} \date{\today} \begin{document} \maketitle Hello World! % This is a comment. \end{document} \end{lstlisting} \end{document} 需要说明的是，这一效果适用于 Adobe Reader PDF 阅读器，sumatraPDF 不适用，其他阅读器还未测试。 http://tex.stackexchange.com/questions/30783/how-to-make-text-copy-in-pdf-previewers-ignore-lineno-line-numbers http://tex.stackexchange.com/questions/57141/is-there-a-latex-trick-to-prevent-a-pdf-viewer-from-copying-the-line-number]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[初级、中级、高级计量经济学内容都有哪些区分与衔接？ - 慧航 - 问答]]></title>
    	<url>/prof/2014/02/03/econometrica/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/21871314/answer/22137633 慧航 初级：regre regression, regression 中级：estimation, estimation, estimation 高级：identification, identification,identification 其实不必要分什么初中高级了。计量经济学给人的第一印象是统计，跑跑 ols，搞搞检验，什么异方差啊、自相关啊、多重共线性啊之类的。其实这些东西现在来说并不重要。也已经有很多技术工具完美解决了。这个时候还在 regression 阶段。当你继续学，你会发现除了线性模型，除了 ols，还有好多其他模型，比如 binary choice 模型，面板数据模型，固定效应随机效应。一般到了这个层次会误以为这些模型的估计是最重要的，处在 estimation 阶段。 当你真正学通了计量，你会发现，不管做应用还是做理论，不管微观计量还是宏观计量，identification 才是最重要的。牛逼的 paper 之所以牛逼是因为 identification 比较清楚。做应用的可以找到很好的例子，用相对简单的模型，甚至 ols 就可以说清楚因果关系，把因果关系识别出来。做理论计量的，你会发现很多模型 identification 证明清楚了，下面的工作也就简单了。这个过程也就是从学招式，到应用招式，到无招胜有招的转变吧。 Haitian Wei：模型 identification 这个是说怎么定模型么？之前我都是把一堆可疑的模型都试一遍然后用效果最好的。这个本来的话应该是怎么搞啊。 慧航：简单的说就是你的模型有且仅有一个解。这个问题解释起来比较困难，主要是因果的识别和参数的识别。比如你做读硕士能不能增加工资，但是呢你做线性回归发现读硕士的工资高，可能是读硕士的人普遍能力比较强，但是能力又是你观察不到的。这里你就很难识别清楚究竟是能力提高了工资还是读硕士提高了。 再比如当你做 SVAR 的时候，比如实际利率和汇率吧，这两个在同一期可能是交互影响的，那么究竟实际利率能多大程度上影响汇率，汇率能多大程度上影响利率，也是很难识别清楚的，因为你用 VAR 之后返回计算 SVAR 的系数会发现解不唯一，那么如何得到那个正确的唯一解？这就是所谓的识别。因为如果你想知道因果的话，考虑同期的效应才是真正的因果关系。 大体明白了吗？不过你做 quant 的话不用管什么识别的，我们这些蛋疼的经济学家要严格识别因果关系才会搞这些东西。 Haitian Wei：学位和能力这个是不是应该找个能力的工具变量，然后看看系数是不是显著什么的？大概晓得 identification 是啥了。因为我研究生学计量的，然后就停留在 estimation 上所以比较好奇 identification 是啥。 慧航：应该找教育的工具变量，跟教育相关但是跟能力无关。还有别的办法了，比如双胞胎。我最近想到了一个工具变量，不过话题太老旧了，不是很有写的动力。 李锜：正因为要探讨因果关系，所以经济才比金融有趣。 LCHEN 从估计方法上来讲： 初级：OLS, MLE 中级：IV, GMM, RDD, DID, quantile regression... 高级：Bayesian, MCMC, Empirical Likelihood, Simulation based estimators, semiparametric and nonparametric, semi-nonparametric... 从检验方法来讲： 初级：t value, p value, F statistics, R^2... 中级: Hausman test, over-identification test, structural break... 高级：各种 optimal test, when a parameter presents only under the alternative, bootstrap, subsampling, exact test, 从数据类型来讲： 初级：cross section 中级：time series, panel data, VAR, multiple equations, nonlinear models 高级：unit root, co-integration (ECM), high dimensional panel data, high frequency, continuous time, spatial... 从识别(identification)问题来讲 初级：基本忽略 中级：怎么找 instrument 高级：semiparametric and nonparametric identification, partial identification, weak identification.. 从模型思想方法来讲： 初级：reduced-form 中级：reduced-form 高级：structural (e.g., dynamic games, dynamic discrete choice, DSGE)... 从所需主要数学工具来讲： 初级：basic matrix algebra, mathematical statistics 中级：Law of large numbers, central limit theorem, Slutsky&#39;s theorem... 高级：Empirical processes, functional analysis...]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[面板 logit 的固定效应怎么做？ - 慧航 - 问答]]></title>
    	<url>/prof/2014/01/30/panel-logit/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/22481031 对于面板 logit，如果每一个 obs 在所有的时间都是 1 或 0，那么固定效应是不是无效的？ 是的！几乎任何时候做固定效应都不允许出现跨时间不变的变量。要么会被减掉（比如 fixed effect Logit model），要么会导致识别问题。 关键点在于，跨时间不变的变量只能使用组间（between group）信息去估计，而加入了 fixed effect 之后，只有组内（within group）信息可以用。所以 random effect 是可以的，因为既使用了组内信息，又使用了组间信息。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[断点回归设计（RD Design）与添加虚拟变量有什么区别？ - 慧航 - 问答]]></title>
    	<url>/prof/2014/01/29/rdd-dummy/</url>
		<content type="text"><![CDATA[[原文地址：https://www.zhihu.com/question/22612514/answer/22021130 慧航 如果问这个问题， RD。建议先读一下关于 treatment effect 的基础知识。（sharp）RD 的意思是，个体接受某种 treatment 是根据某个连续的变量（z）来外生决定的。比如某次考试大于 90 分的可以参加某项竞赛辅导，小于 90 分的不可以。这里的 treatment 是参加竞赛辅导，1(z&amp;gt;90)是个虚拟变量，指示的是个体有没有参加竞赛辅导，而参不参加竞赛辅导是由某次考试分数决定的。所以关键的区别是，这里的虚拟变量是指示的 treatment，而非一般的男女、种族等的 dummy variables，性别、种族不是 treatment。 诸小默：那么接受 treatment 的依据是否可以是时间序列呢？时间可以看做连续变量，在某一时刻上做了 treatment，由此对这一时刻前后进行 RD Design 处理？ 慧航：有拿跟时间有关的比如年龄作为z的，但是一般很少有人拿时间直接做为z。这里面问题很多，首先是有些问题可能同一个人只有一个outcome1。其次，RD 设计最关键的是要保证未观察到的变量不存在断点，但是从时间层面上来看，太多的东西容易改变，需要 argue 的东西太多。具体问题具体分析吧。 曾阿屎：running variable 不一定是连续的，也可以有离散的，像年龄，很多时候我们拿到的数据是以年或者月为单位的，严格说，应该是离散的。不过 RD 设计依然有效，只是要比连续的复杂一点。 慧航：对，的确有人做，台湾有几个人做了离散的，特别是时间上离散的时候的 RD 该怎么处理，抱歉论文名字忘掉了，这种的话不同应用情景可能差别很大，有的可以，有的不可以，具体看应用场景。 其实你仔细想一想，如果是时间上离散的 sharp rd，如果你用 nonparametric 的办法做，等同于相邻两个月份之间y的平均值的差。所以这种东西拿出来的时候，别人会有很多质疑的，你为啥不直接减一减呢2？建议最好不要直接这样做。以日期作为 running variable 的发的最好的我印象中的是这篇：Peer Effect in Program Participation，你可以去看一下，他的数据非常好，有每个孩子的出生日期。如果只有月份的话，我猜再做 RD 就要被质疑了 JoeyM 还是用录取学校这个例子来说假设要研究录取到一本学校对学生未来工资的影响。 首先，没法做随机试验是肯定的现在如果我们用 OLS 的方法，为了排除“录取到一本学校”这个 treatment 的内生性我们就要往加入足够多的解释变量，比如说家庭教育，个人能力，经济能力等等等，变量越多估计结果越准确。以及和这个 treatment 的虚拟变量。 但如果我们只加入一个虚拟变量，说明我们默认了是否被录取一本学校对不同能力、不同家庭背景的学生的影响都是一样的，这显然让人难以信服。 为了更好地识别录取到一本学校的影响，我们再往方程里加入它与各个变量的交互项，这个回归跑出来，理论上我们就可以得出，对于任意一类学生，是否录取到一本学校对他的影响有多大。想想都知道一旦变量多起来这个回归有多不靠谱。 但是如果用断点回归的方法，我们起码可以在有限的数据集中估计出一个相对准确的结果。在“连续性”的假设下，我们并不需要控制住那么多变量，因为我们认为以 running variable 为参考的某个点附近，比如高考分 500 分附近的人，他们的学习成绩，乃至智商、情商、abcdQ 等其他因素都是差不多的（这个说法可能要斟酌一下），而在这个 499 分和 500 分这群人差不多的人以后工资的差异，就可以被看做是录取到一本学校的纯的影响。 但显然，这个影响显然是针对那群差不多的人而言的，对另一群学生来说，这个影响就毫无说服力了，因为你认为录取到一本学校的影响对于不同群体而言是不一样的。这句话是不是和上面下划线那句话很像（其实就是一个意思嘛）。所以我认为两者的关系是，RD（上面其实是个 Sharp RD 的例子）估计出来的影响，相当于在回归方程中加入足够多的虚拟变量交互项，再估计出 treatment 在某个点的偏效应。多元回归做不好这件事，而 RD 能做好。 Chinhogo：要斟酌的那段应该这么说：因为 RD 假定个体在断点周围接受 treatment 是一个随机事件，所以你所说的那些不可观测的变量与 treatment 这个 dummy 的相关性为 0，所以不会产生内生性问题。 Chinhogo 主要差别有以下几种： RD 运用于 quasi-experiment 实验，有别于自然随机实验下直接添加 dummy 采用 OLS 估计的模型。如果你能明白为什么经济学家偏好自然随机实验，你就能明白为什么 RD 在很多情况下估计的准确度要优于 OLS。自然随机事件下，不可观测变量（即性别、IQ 等等等）与个体接收 treatment 的相关性为 0，故我们可以用添加 dummy 的方法来估计 treatment effect，但是一旦这个随机事件并不完全随机（用一个计量史学上蛮经典的例子讲解，隋炀帝挖运河对于经济是否有增益作用，你使用 dummy 就不再合适了，因为地理和经济环境的因素会影响隋政府对于运河挖掘的选择，导致你对挖河这一政策对经济作用估计的偏差）。如果要准确估计这种 treatment effect，你必须准确地将所有可能导致你估计偏误的 variable 加入模型，控制起来，但那样做不经济有时也不可能（有时遗漏变量不可观测）。所以我们退而求其次，使用 RD 进行估计，假定个体在断点周围接受 treatment 是一个随机事件，所以你所说的那些不可观测的变量与 treatment 这个 dummy 的相关性为 0，所以不会产生内生性问题。 估计方法的不同。RD 通常采用局部线性回归的方法（即不选用全样本，而选用一定带宽内的样本，），本质上是对断点周围局部效应（LATE，Local Average Treatment Effect)的一个估计。最优带宽的估计由 Imbens and Kalyanaraman(2009) 提供，并且一般要提供不同带宽的结果以显示结果的 robustness。有时，RD 还采用核回归的非参方法。 RD需要检验内生分组（endogenous sorting)的问题，即要假设如果个体事先知道分组规则，并可通过自身努力而完全控制分组变量，引起断点回归的失效。 如果在RD中加入协变量，还需检验协变量对于的条件密度是否在断点处连续，即断点处的jump不是由协变量的jump产生。 注：以上内容主要面向 Sharp RD. Fuzzy RD 使用时分组变量是否大于断点的 dummy（称为Z）作为处理变量（称为D，即我们主要的估计量）的工具变量。Z显然与D相关，而Z在断点附近相当于局部随机实验，故只通过D影响变量y，与扰动项不相关，故满足外生性。可以使用Z作为D的工具变量，使用 2SLS 进行估计。 王小毛 基本同意之前各位的回答，RD 其实就是一种计算 treatment effect 的方法，但是你在 estimate 的时候只加一个 treated/controlled 的 dummy variable 还有些交互项（interaction terms）就说你做了 RD 是有问题的。 RD 的使用很麻烦，更 restrict，一个很大的 assumption 就是你起码要确保你的 running variable 在 cutoff 的两侧是 as good as a randomized experiment，意思就是这个 treated/controlled 的 dummy variable 不能轻易就被人操控 （当然这也需要跑大量的测试）。 建议看这篇文章: Regression Discontinuity Designs in Economics，文章第 55 页有个 checklist 可以最后检查一下结果，这些都是必须要考虑的 robustness check。 FlyRideR 确定形的断点回归和添加虚拟变量有点像，不过还有一种模糊型的断点回归（Fuzzy Regression Discontinuity），即个体接受处置效应的概率均大于 0 小于 1，个体在临界值一边接受处置的概率大于在临界值另一边接受处置的概率。 曾阿屎 RD和添加虚拟变量关系不是太大呢，问 RD 和 IV（工具变量）的关系更确切。 如楼上所言，RD 分为 Sharp 型（running variable 超过门槛值个体就一定会 get treated，低于这个值就不会。）和 Fuzzy 型（get treated 只是部分地与 running variable 超过门槛值相关，也就是说，存在某些超过门槛值的个体没有 treated，也有些低于门槛值的个体 get treated）。 在 Fuzzy RD 中，我们就把这个 running variable 作为 treatment 的 IV 啦，然后就可以用 2SLS 求解，流程和 IV 的 2SLS 估计差不多。 Lullaby 今天刚看到一篇文章，刚好可以回答这个问题 Calonico, Sebastian, et al. Regression Discontinuity Designs Using Covariates. working paper, University of Michigan, 2016. 他们主要观点就是，以前大家得过且过都这么用的，RD 里面加 covariates, 不过从来没有人知道为什么。他们这篇文章证明在简单的 regularity condition 下，实际上加 covariates 会更好一些。 以下取自原文： &amp;quot;Applied researchers often include additional pre-intervention&amp;quot; covariates in their speci cations to increase e ciency. However, no results justifying covariate adjustment have been formally derived in the RD literature, leaving applied researchers with little practical guidance and leading to a proliferation of ad-hoc methods that may result in invalid estimation and inference. We examine the properties of a local polynomial estimator that incorporates discrete and continuous covariates in an additive separable, linear-in-parameters way and imposes a common (likely misspeci ed) covariate e ffect on both sides of the cuto . Under intuitive, minimal assumptions, we show that this covariate-adjusted RD estimator remains consistent for the standard RD treatment effect, while also providing point estimation and inference improvements.&amp;quot; 这句没有理解。↩ 这句没有理解。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[网络：从外网访问局域网内主机(端口映射) z]]></title>
    	<url>/tech/2013/08/11/port-mapping/</url>
		<content type="text"><![CDATA[[首先要做的事情如下： 确认你内网的路由器是否支持端口映射功能 如果你的路由器支持端口映射功能，在你本机安装远程控制软件 远程控制软件安装后，设置路 在你本机安装花生壳之类的动态域名软件 测试成功后，你在外网打开远程软件控制端，输入动态域名就可以访问你自己电脑了 端口映射（Port Mapping） 如果你是 ADSL、MODEM 或光纤等宽带接入用户，想在公司或单位内部建一个服务器或 WEB 站点，并且想让互联网上的用户访问你的服务器，那么你就会遇到端口映射问题。通常情况下，路由器都有防火墙功能，互联网用户只能访问到你的路由器WAN口(接 ADSL 的电话线口或路由宽带外网口)，而访问不到内部服务器。要想让互联网用户访问到你建的服务器，就要在路由器上做一个转发设置，也就是端口映射设置，让互联网用户发送的请求到达路由器后,再转发到你建立的服务器或 WEB 站点。这就是端口映射。由于各个路由器厂商所取功能名称不一样，有的叫虚拟服务器，有的叫NAT设置(BitComet 中常见问题)端口映射。其实做端口映射设置很简单，例如要映射一台内网 IP 地址为 192.168.0.66 的WEB服务器，只需把WEB服务器的 IP 地址 192.168.0.66 和 TCP 端口 80 填入到路由器的端口映射表中就OK了。 关于打开端口映射后的安全问题：设置了端口映射后，互联网用户能够通过设置好映射的端口，跳过路由器防火墙访问到你的服务器，在通过攻击你服务器上的漏洞控制你的主机，所以打开端口映射后有必要在你的服务器上再挂一个防火墙也确保安全性。 花生壳是一套完全免费的动态域名解析服务客户端软件。当您安装并注册该项服务，无论您在任何地点、任何时间、使用任何线路，均可利用这一服务建立拥有固定域名和最大自主权的互联网主机。“花生壳”支持的线路包括普通电话线、ISDN、ADSL、有线电视网络、双绞线到户的宽带网和其它任何能够提供互联网真实IP的接入服务线路，而无论连接获得的 IP 属于动态还是静态。 动态域名 用户每次上网得到新的动态分配的 IP 地址之后，安装在用户计算机里的动态域名软件就会把这个IP地址发送到动态域名解析服务器，更新域名解析数据库。Internet 上的其他人要访问这个域名的时候，动态域名解析服务器会返回正确的IP地址给他。这叫动态域名。 因为绝大部分 Internet 用户上网的时候分配到的 IP 地址都是动态的，用传统的静态域名解析方法，用户想把自己上网的计算机做成一个有固定域名的网站，是不可能的。而有了动态域名，这个美梦就可以成真。用户可以申请一个域名，利用动态域名解析服务，把域名与自己上网的计算机绑定在一起，这样就可以在家里或公司里搭建自己的网站，非常方便。 端口映射（Port Mapping/Port Forwarding）有点类似服务重定向, 所以有些路由器(Router)中也称为虚拟服务器(Virtual Server)。为了描述方便，下面的叙述中统一称为[端口映射]。 采用端口映射的方法，可以实现从 Internet 到局域网内部机器的特定端口服务的访问。 端口映射的实现方式可以分为纯软件和软硬结合方式。以纯软件方式实现端口映射功能软件有很多, 比如, MS Windows9x/200/XP 下的 PortTunnel 专门针对 HTTP、FTP、SMTP 服务的端口映射，提供了较多的参数设置，在相应的标签菜单下调整。又如各种版本的Linux 操作系统本身就支持端口映射，只需要网络管理员做相应的设置和调整即可实现。而以软硬结合方式实现端口映射功能的，主要常见于各种路由器(提供网关路由功能) 。 各种路由器(Router)中如何实现端口映射 一般路由器中有个端口映射（Port Mapping）或者虚拟服务器(Virtual Server)的设置。用户需要在路由器(Router)的“管理界面”中相应的端口映射界面中, 设置好相应的需要映射的端口, 协议,内网地址等, 才能生效。设置的方法可能会因为路由器(Router)不同的品牌和型号，在设置的方法上也会有所不同。端口映射支持的网络协议有TCP/UDP/两者, 所以进行端口映射设置时, 如果不熟悉, 可以选择两者都支持。 举例说明 以某路由器(Router)为例，在启用其路由功能之后，网络拓扑图如下： 这里假定路由器(Router)默认 IP 内网地址为 192.168.1.1，内网中电脑一般可以设置成为 192.168.1.X（X=2~254），在内网中某一台电脑上打开 IE，在地址栏输入 http://192.168.1.1，输入初始用户名、密码，之后就可以看到设置界面了。 针对邮件服务器要做如下设置：进入“端口映射”，在端口填入 25, 协议中选择： TCP, IP 地址： 192.168.1.x (x 为安装邮件服务器电脑的局域网IP 地址），同样方法设置 110(pop3),6080(webmail) 端口等。 以上假定用户内网段地址为: 192.168.1.0。设置好后, 就实现了端口映射功能了, 发往路由器的邮件就会自动转发到指定的内网主机上 (192.168.1.x)。 同样, 如果想设置特殊端口, 比如: 6000。 在端口填入6000, 协议中选择： ALL(或根据具体情况选择), IP 地址：192.168.1.x(x 为内网段地址1~254)。 设置好后, 发往路由器 6000 端口的任何数据就会自动转发到主机 192.168.1.x 的端口 6000 上了。 以上说明没有针对具体路由器。 具体情况, 请查阅您的路由器说明书，看看如何作端口映射。 参考：1. http://www.hrtl.com.cn/News_398.aspx2. https://my.oschina.net/eicyan/blog/209440]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[调和曲线图和轮廓图的比较 z]]></title>
    	<url>/prof/2013/07/15/parallel-coordinate-plot-andrews-curves/</url>
		<content type="text"><![CDATA[[原文地址：http://cos.name/2009/03/parallel-coordinates-and-andrews-curve/#m 作者：魏太云 多元数据的可视化方法很多，譬如散点图、星图、雷达图、脸谱图、协同图等，大致可分为以下几类： 基于点（如二维、三维散点图）； 基于线（如轮廓图、调和曲线图）； 基于平面图形（如星图、雷达图、蛛网图）； 基于三维曲面（如三维曲面图）。 其思想是将高维数据映射到低维空间（三维以下）内，尽量使信息损失最少，同时又能利于肉眼辨识。调和曲线图和轮廓图(即平行坐标图)都是多元数据的可视化方法，它们基于“线”的形式，将多元数据表示出来，对于聚类分析有很好的帮助。 轮廓图 轮廓图的思想非常简单、直观，它是在横坐标上取\(p\)个点，依次表示各个指标(即变量)；横坐标上则对应各个指标的值(或者经过标准化变换后的值)，然后将每一组数据对应的点依次连接即可。 lattice包中的parallel()函数可以轻松绘出轮廓图。利用iris数据，以下代码可以画出其轮廓图。 library(lattice) data(iris) parallel(~iris[1:4], iris, groups = Species, horizontal.axis = FALSE, scales = list(x = list(rot = 90))) 图1 Iris 数据的轮廓图(Parallel Coordinate Plots) 观察上图，可以发现同一品种的鸢尾花的轮廓图粗略地聚集在一起。 调和曲线图 调和曲线图的思想和傅立叶变换十分相似，是根据三角变换方法将\(p\)维空间的点映射到二维平面上的曲线上。假设\(X_r\)是\(p\)维数据的第\(r\)个观测值，即 \[X_r^T=(x_{r1},\dotsc,x_{rp})\] 则对应的调和曲线是 \[f_r(t)=\frac{x_{r1}}{\sqrt{2}} &#43;x_{r2}\sin t&#43;x_{r3}\cos t&#43;x_{r4}\sin 2 t&#43;x_{r5} \cos 2 t&#43;\cdots\] 其中\(-\pi\leqslant t\leqslant \pi\). 同样利用iris数据，下面代码(主要取自《统计建模与R软件》，尚未优化)可以画出其调和曲线图 x = as.matrix(iris[1:4]) t = seq(-pi, pi, pi/30) m = nrow(x) n = ncol(x) f = matrix(0, m, length(t)) for (i in 1:m) { f[i, ] = x[i, 1]/sqrt(2) for (j in 2:n) { if (j%%2 == 0) f[i, ] = f[i, ] &#43; x[i, j] * sin(j/2 * t) else f[i, ] = f[i, ] &#43; x[i, j] * cos(j%/%2 * t) } } plot(c(-pi, pi), c(min(f), max(f)), type = &amp;quot;n&amp;quot;, main = &amp;quot;The Unison graph of Iris&amp;quot;, xlab = &amp;quot;t&amp;quot;, ylab = &amp;quot;f(t)&amp;quot;) for (i in 1:m) lines(t, f[i, ], col = c(&amp;quot;red&amp;quot;, &amp;quot;green3&amp;quot;, &amp;quot;blue&amp;quot;)[unclass(iris$Species[i])]) legend(x = -3, y = 15, c(&amp;quot;setosa&amp;quot;, &amp;quot;versicolor&amp;quot;, &amp;quot;virginica&amp;quot;), lty = 1, col = c(&amp;quot;red&amp;quot;, &amp;quot;green3&amp;quot;, &amp;quot;blue&amp;quot;)) Iris 数据的调和曲线图 图2 Iris 数据的调和曲线图 观察上图，同样可以发现同一品种鸢尾花数据的调和曲线图基本上扭在一起。同第一幅图比较后，发现第二幅图更加清楚明白，事实上Andrews证明了调和曲线图有许多良好性质。 讨论 轮廓图和调和曲线图有着相近的功能，而技巧大有不同。轮廓图简单却现得粗糙，调和曲线图公式复杂却十分精细。从这一个侧面可以发现直观的统计思想固然重要，但存在很多种不可能通过直观思想得到的、而又非常精细、美妙的方法，此时倍受众多统计学家责难的数学显得优雅而又强大。 谢益辉：正好我前一段时间也写过一个调和曲线图的R函数，发出来分享一下： andrews.curve = function(x, n = 101, type = &amp;quot;l&amp;quot;, lty = 1, lwd = 1, pch = NA, xlab = &amp;quot;t&amp;quot;, ylab = &amp;quot;f(t)&amp;quot;, ...) { p = ncol(x) if (is.null(p)) stop(&amp;quot;&amp;#39;x&amp;#39; must be a matrix or data.frame!&amp;quot;) if (p &amp;lt; 1) stop(&amp;quot;&amp;#39;x&amp;#39; must have at least one column!&amp;quot;) theta = matrix(seq(-pi, pi, length.out = n), nrow = n, ncol = 1) if (p == 1) { a = matrix(x/sqrt(2), nrow = n, ncol = nrow(x), byrow = TRUE) } if (p &amp;gt; 1) { b = matrix(rep(1:(p/2), each = 2, length.out = p - 1), nrow = 1, ncol = p - 1) a = cbind(1/sqrt(2), sin(theta %*% b &#43; matrix(rep(c(0, pi/2), length.out = p - 1), nrow = n, ncol = p - 1, byrow = TRUE))) %*% t(x) } matplot(theta, a, type = type, lty = lty, lwd = lwd, pch = pch, xlab = xlab, ylab = ylab, ...) } 里面都是以矩阵的形式做的运算，不过我没有测试，不知道速度会不会快一些。对于调和曲线图，观测\(X_i\)和\(X_j\)的欧式距离正好是曲线垂直距离的积分，这是数学性质和图形性质能够结合的关键。 魏太云：快很多呢，没有一个显式循环，十分方便:)。调和曲线图的确很好地将数学融进了图形，可谓鬼斧神工啊。 fasterr：andrews.curve(x, col=c(‘red’, ‘green3′, ‘blue’))可是画出来的不是很一样：《统计建模与R软件》版的调和曲线的颜色比较友好，相近族类的颜色一样、不同类的颜色不同。 andrews.curve()绘制的曲线颜色是是交替的（一次red，一次green3, 一次blue，依次循环使用），而不是颜色按类聚集，这样画出来的色彩整体很凌乱（颜色重叠后就更变味了）。 传入什么控制参数andrews.curve()也能得到一样的效果么？]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[滤波的基本概念]]></title>
    	<url>/tech/2013/07/10/filter/</url>
		<content type="text"><![CDATA[[参考资料： 滤波(filter)：http://baike.baidu.com/view/162707.htm 增益(gain)：http:// 低通滤波(Low-pass filter)：http://baike.baidu.com/view/1669798.htm 高通滤波(high-pass filter)：http://baike.baidu.com/view/1877222.htm 滤波 滤波是将信号中特定波段频率滤除的操作，是抑制和防止干扰的一项重要措施。分经典滤波和现代滤波。 滤波是将信号中特定波段频率滤除的操作，是抑制和防止干扰的一项重要措施。是根据观察某一随机过程的结果，对另一与之有关的随机过程进行估计的概率理论与方法。 起源 滤波一词起源于通信理论，它是从含有干扰的接收信号中提取有用信号的一种技术。“接收信号”相当于被观测的随机过程，“有用信号”相当于被估计的随机过程。例如用雷达跟踪飞机，测得的飞机位置的数据中，含有测量误差及其他随机干扰，如何利用这些数据尽可能准确地估计出飞机在每一时刻的位置、速度、加速度等，并预测飞机未来的位置，就是一个滤波与预测问题。这类问题在电子技术、航天科学、控制工程及其他科学技术部门中都是大量存在的。历史上最早考虑的是维纳滤波，后来R.E.卡尔曼和R.S.布西于20世纪60年代提出了卡尔曼滤波。现对一般的非线性滤波问题的研究相当活跃。 经典滤波 经典滤波的概念，是根据傅立叶分析和变换提出的一个工程概念。根据高等数学理论，任何一个满足一定条件的信号，都可以被看成是由无限个正弦波叠加而成。换句话说，就是工程信号是不同频率的正弦波线性叠加而成的，组成信号的不同频率的正弦波叫做信号的频率成分或叫做谐波成分。 滤波器 只允许一定频率范围内的信号成分正常通过，而阻止另一部分频率成分通过的电路，叫做经典滤波器或滤波电路。实际上，任何一个电子系统都具有自己的频带宽度(对信号最高频率的限制)，频率特性反映出了电子系统的这个基本特点。而滤波器，则是根据电路参数对电路频带宽度的影响而设计出来的工程应用电路。 现代滤波 用模拟电子电路对模拟信号进行滤波，其基本原理就是利用电路的频率特性实现对信号中频率成分的选择。根据频率滤波时，是把信号看成是由不同频率正弦波叠加而成的模拟信号，通过选择不同的频率成分来实现信号滤波。 当允许信号中较高频率的成分通过滤波器时，这种滤波器叫做高通滤波器。 当允许信号中较低频率的成分通过滤波器时，这种滤波器叫做低通滤波器。 设低频段的截止频率为fp1，高频段的截止频率为fp2: 频率在fp1与fp2之间的信号能通过其它频率的信号被衰减的滤波器叫做带通滤波器。 反之，频率在fp1到fp2的范围之间的被衰减，之外能通过的滤波器叫做带阻滤波器。 理想滤波器的行为特性通常用幅度-频率特性图描述，也叫做滤波器电路的幅频特性。 滤波问题及分类 对于滤波器，增益幅度不为零的频率范围叫做通频带，简称通带，增益幅度为零的频率范围叫做阻带。例如对于LP，从-w1到w1之间，叫做LP的通带，其他频率部分叫做阻带。通带所表示的是能够通过滤波器而不会产生衰减的信号频率成分，阻带所表示的是被滤波器衰减掉的信号频率成分。通带内信号所获得的增益，叫做通带增益，阻带中信号所得到的衰减，叫做阻带衰减。在工程实际中，一般使用dB作为滤波器的幅度增益单位。 按照滤波是在一整段时间上进行或只是在某些采样点上进行，可分为连续时间滤波与离散时间滤波。前者的时间参数集\(T\)可取为实半轴\([0,\infty)\)或实轴\((-\infty,\infty)\);后者的\(T\)可取为非负整数集\(\{0,1,2,\dotsc\}\)或整数集\(\{\dotsc,-2,-1,0,1,2,\dotsc\}\)。设$X=\{X_t\in T=\{Y,t\in T)\}\}$有穷，即其中\(X\)为被估计过程，它不能被直接观测；\(Y\)为被观测过程，它包含了\(X\)的某些信息。用表示到时刻\(t\)为止的观测数据全体，如果能找到中诸元的一个函数?(),使其均方误差达到极小，就称为\(X_t\)的最优滤波；如果取极小值的范围限于线性函数，就称为\(X_t\)的线性最优滤波。可以证明，最优滤波与线性最优滤波都以概率1惟一存在。对于前者，悯t就是\(X_t\)关于\(\sigma()\)(生成的\(\sigma\)域)的条件期望，记作对于后者，若进一步设均值\(E(X_t)\)呏\((EY_t)\)呏0,则悯t就是\(X_t\)在所张成的希尔伯特空间上的投影，记作如果\((X,Y)\)是二维正态过程，则最优滤波与线性最优滤波是一致的。这部分有错 为了应用和叙述的方便，有时还把上面的定义更细致地加以分类。设\(\tau\)为一确定的实数或整数，且考虑被估计过程。按照\(\tau=0,\tau&amp;gt;0,\tau&amp;lt;0\)，分别称为最优滤波、(\(\tau\)步)预测或外推、(\(\tau\)步)平滑或内插，分别为对应的误差与均方误差，而统称这类问题为滤波问题。滤波问题的主要课题是研究对哪些类型的随机过程\(X\)和\(Y\)，可以并且如何用观测结果的某种解析表示式，或微分方程，或递推公式等形式,表达出并进而研究它们的种种性质。此外，上面所指的一维随机过程\(X,Y\)，都可以推广为多维随机过程。 维纳滤波 历史上最先考虑的是宽平稳过程(见平稳过程)的线性预测和滤波问题，它的一般模型是\(Y_t=X_t&#43;N_t\)，其中\((X，N)\)为二维宽平稳过程或序列，其谱分布函数已知，其均值为零。设从\(-\infty\)到时刻\(t\)为止的全部\(Y\)的值都已被观测到，求\(X\)的\(\tau\)步线性预测及其均方误差。如果限于考虑\(N=0,\tau&amp;gt;0\)的情形，则变成在无误差观测条件下\(X\)本身的线性预测问题；如果\(N\neq 0,\tau\leqslant 0\)，则变成从受到噪声\(N\)干扰的接收信号\(Y\)中提取有用信号\(X\)的滤波问题。1939~1941 年，Α.Η.柯尔莫哥洛夫利用平稳序列的沃尔德分解(见平稳过程)，给出了线性预测的一般理论与处理办法，随即被推广到连续时间的平稳过程。N.维纳则在1942年对于平稳序列与过程的谱密度存在且满足某种正则条件的情形，利用谱分解导出了线性最优预测和滤波的明显表达式，即维纳滤波公式，并在防空火力控制、电子工程等部门获得了应用。上述模型在50年代被推广到仅在有限时间区间内进行观测的平稳过程以及某些特殊的非平稳过程，其应用范围也扩充到更多的领域。至今它仍是处理各种动态数据（如气象、水文、地震勘探等）及预测未来的有力工具之一。 维纳滤波公式是通过平稳过程的谱分解导出的，难以推广到较一般的非平稳过程和多维情形，因而应用范围受到限制。另一方面，在不断增加观测结果时，不易从已算出的滤波值及新的观测值较简单地求出新的滤波值，特别是不能满足在电子计算机上快速处理大量数据的需要。 卡尔曼滤波 由于高速电子计算机的发展以及测定人造卫星轨道和导航等技术问题的需要，R.E.卡尔曼与 R.S.布西于 20 世纪 60 年代初期提出了一类新的线性滤波的模型与方法，通称为卡尔曼滤波。其基本假设是，被估计过程\(X\)为随机噪声影响下的有限阶多维线性动态系统的输出，而被观测的\(Y_t\)则是\(X_t\)的部分分量或其线性函数与量测噪声的叠加，这里并不要求平稳性，但要求不同时刻的噪声值是不相关的。此外，观测只需从某一确定时刻开始，而不必是无穷长的观测区间。更重要的是，适应电子计算机的特点，卡尔曼滤波公式不是将估计值表成观测值的明显的函数形式，而是给出它的一种递推算法(即实时算法)。具体地说，对于离散时间滤波，只要适当增大\(X\)的维数，就可以将\(t\)时刻的滤波值表成为前一时刻的滤波值与本时刻的观测值\(Y_t\)的某种线性组合。对于连续时间滤波,则可以给出与\(Y_t\)所应满足的线性随机微分方程。在需要不断增加观测结果和输出滤波值的情形，这样的算法加快了处理数据的速度，而且减少了数据存贮量。卡尔曼还证明，如果所考虑的线性系统满足某种“可控性”和“可观测性”(这是现代控制理论中由卡尔曼提出的两个重要概念)，那么最优滤波一定是“渐近稳定”的。大致说来，就是由初始误差、舍入误差及其他的不准确性所引起的效应，将随着滤波时间的延长而逐渐消失或趋于稳定， 不致形成误差的积累。这在实际应用上是很重要的。 卡尔曼滤波也有多种形式的推广，例如放宽对噪声不相关性的限制，用线性系统逼近非线性系统，以及所谓“自适应滤波”，等等，并获得了日益广泛的应用。 非线性滤波 前已说明，一般的非线性最优滤波可归结为求条件期望的问题。对于有限多个观测值的情形，条件期望原则上可以用贝叶斯公式来计算。但即使在比较简单的场合，这样得出的结果也是相当繁杂的，无论对实际应用或理论研究都很不方便。与卡尔曼滤波类似，人们也希望能给出非线性滤波的某种递推算法或它所满足的随机微分方程。但一般它们并不存在，因此必须对所讨论的过程X与Y加以适当的限制。非线性滤波的研究工作相当活跃，它涉及随机过程论的许多近代成果，如随机过程一般理论、鞅、随机微分方程、点过程等。其中一个十分重要的问题，是研究在什么条件下，存在一个鞅 M，使得在任何时刻，M 和 Y 都包含同样的信息；这样的M称为Y的新息过程。目前对于一类所谓“条件正态过程”，已经给出了非线性最优滤波的可严格实现的递推算式。在实际应用上，对非线性滤波问题往往采用各种线性近似的方法。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[散点图与回归拟合线]]></title>
    	<url>/prof/2013/07/05/scatter-fitted-line/</url>
		<content type="text"><![CDATA[[在初级的计量教材中看到一幅用 Eviews 制作的散点图，这幅图的最大特点是在 X-Y 散点图中还加入了回归拟合线。这种类型的图我知道用 Excel 是很容易制作的，在网上查 Stata, SAS, SPSS, R 等软件中来完成。 我曾经一度非常希望能够通过 Eviews 内带的命令来完成这一幅图的制作，但似乎没有特别简洁的做法。现在想起来，其实这种图没什么太大意义，毕竟现实工作中很少会出现一元回归的场合。 不过从另一角度来看，这种图或许在分位数回归中有些许实用性，毕竟即使解释变量只有一个的情形下，这种类型的图也可以让初学者更清楚的看到平均值回归与分位数回归的差异所在。 Eviews 的作图功能太弱1，R、Stata 等都能最方便的得到想要的图形。 但并不难看，特别是用代码做的效果，很漂亮。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[分位点与分位数 z]]></title>
    	<url>/prof/2013/06/25/quantile/</url>
		<content type="text"><![CDATA[[分位数在统计学是中一个比较简单和容易理解的概念，大部分初级的统计学教材中都会有相应的介绍。但实际在理解这个概念的时候，要注意连续情形下与分位 离散情形 举一个例子，将 100 名同学的期末考试总分成绩所组成的向量 \(\mathbf{x}\) 按从小到大的规则进行排序，生成一个新的顺序统计量 \(\mathbf{x}&amp;#39;\)，那么 \(\mathbf{x}\) 的 \(1/4\) 分位数指的是 \(\mathbf{x}^\prime\) 第 25 号位上的那个数字，同理可知中位数以及 \(3/4\) 分位数等。 连续情形 以正态分布为例，\(1/4\) 分位数指的是 \(\mathrm{Pr}(X\leqslant x)=1/4\) 对应的 \(x\) 值，类似的，标准正态分布中中位数对应的 \(x\) 为 0。 从上面的例子可以看出，分位点确定的基础是在离散情形下将样本数量标准化为 1，而在连续情形下则是将 x 坐标轴的可用长度标准化为 1(对于正态分布和 t 分布而言，原始长度为 \(-\infty\) 到 \(&#43;\infty\) 之间的全部范围。 在连续情形下，若考虑的置信系数是左单侧的，那么置信系数 \(\alpha\) 对应的下 \(\alpha\) 分位点的 \(x\) 值与 \(\alpha\) 分位数以及 \(\mathrm{Pr}(X\leqslant x)=\alpha\) 中的 \(x\) 实际上指向同一个 \(x\) 值。 这个值在大部分统计软件中对应的命令以 q 开头，比如 R 中的 qnorm 命令。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[CSS: block、inline 和 inline-block 概念和区别 z]]></title>
    	<url>/tech/2013/06/16/css-block-inline/</url>
		<content type="text"><![CDATA[[原文地址：http://www.cnblogs.com/KeithWang/p/3139517.html 总体概念 block和inline这两 block-level elements (块级元素) 和 inline elements (内联元素)。block元素通常被现实为独立的一块，会单独换一行；inline元素则前后不会产生换行，一系列inline元素都在一行内显示，直到该行排满。 大体来说HTML元素各有其自身的布局级别（block元素还是inline元素）： 常见的块级元素有DIV, FORM, TABLE, P, PRE, H1~H6, DL, OL, UL等。 常见的内联元素有SPAN, A, STRONG, EM, LABEL, INPUT, SELECT, TEXTAREA, IMG, BR等。 block元素可以包含block元素和inline元素；但inline元素只能包含inline元素。要注意的是这个是个大概的说法，每个特定的元素能包含的元素也是特定的，所以具体到个别元素上，这条规律是不适用的。比如P元素，只能包含inline元素，而不能包含block元素。 一般来说，可以通过display:inline和display:block的设置，改变元素的布局级别。 block，inline和inline-block细节对比 display:block block元素会独占一行，多个block元素会各自新起一行。默认情况下，block元素宽度自动填满其父元素宽度。 block元素可以设置width, height属性。块级元素即使设置了宽度,仍然是独占一行。 block元素可以设置margin和padding属性。 display:inline inline元素不会独占一行，多个相邻的行内元素会排列在同一行里，直到一行排列不下，才会新换一行，其宽度随元素的内容而变化。 inline元素设置width,height属性无效。 inline元素的margin和padding属性，水平方向的padding-left, padding-right, margin-left, margin-right都产生边距效果；但竖直方向的padding-top, padding-bottom, margin-top, margin-bottom不会产生边距效果。 display:inline-block（新、重要） 简单来说就是将对象呈现为inline对象，但是对象的内容作为block对象呈现。之后的内联对象会被排列在同一行内。比如我们可以给一个link（a元素）inline-block属性值，使其既具有block的宽度高度特性又具有inline的同行特性。 补充说明 一般我们会用display:block，display:inline或者display:inline-block来调整元素的布局级别，其实isplay的参数远远不止这三种，仅仅是比较常用而已。 IE（低版本IE）本来是不支持inline-block的，所以在IE中对内联元素使用display:inline-block，理论上IE是不识别的，但使用display:inline-block在IE下会触发layout，从而使内联元素拥有了display:inline-block属性的表象。 input元素是inline-block元素，所以设置宽高可用。可以使用window.getComputedStyle()这个方法拿到input元素的display属性试一下。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[R 的应用领域包介绍 z]]></title>
    	<url>/tech/2013/06/09/r-packages-introduction/</url>
		<content type="text"><![CDATA[[Analysis of Pharmacokinetic Data 药物(代谢)动力学数据分析 网址：http://cran.r-project.org/web/views/Pharmacokineti 维护人员：Suzette Blanchard 版本：2008-02-15 翻译：R-fox, 2008-04-12 药物(代谢)动力学数据分析的主要目的是用非线性浓度时间曲线(concentration time curve)或相关的总结(如曲线下面积)确定给药方案(dosing regimen)和身体对药物反应间的关系。R基本包里的nls()函数用非线性最小二乘估计法估计非线性模型的参数，返回nls类的对象，有coef(), formula(), resid(), print(), summary(), AIC(), fitted(), vcov()等方法。 在主要目的实现后，兴趣就转移到研究属性(如：年龄、体重、伴随用药、肾功能)不同的人群是否需要改变药物剂量。在药物(代谢)动力学领域，分析多个个体的组合数据估计人群参数被称作群体药动学(population PK)。非线性混合模型为分析群体药动学数据提供了自然的工具，包括概率或贝叶斯估计方法。 nlme包用Lindstrom和Bates提出的概率方法拟合非线性混合效应模型(1990, Biometrics 46, 673-87)，允许nested随机效应(nested random effects)，组内误差允许相关的或不等的方差。返回一个nlme类的对象表示拟合结果，结果可用print(), plot()和summary()方法输出。nlme对象给出了细节的结果信息和提取方法。 nlmeODE包组合odesolve包和nlme包做混合效应建模，包括多个药动学/药效学(PK/PD)模型。 面版数据(panel data)的贝叶斯估计方法在CRAN的Bayesian Inference任务列表里有所描述(http://cran.r-project.org/web/views/Bayesian.html)。 PKtools包为nlme, NONMEM和WinBUGS包提供单剂量群体药动学数据的接口，分别返回&amp;quot;PKNLME&amp;quot;，&amp;quot;NONMEM&amp;quot;和&amp;quot;WinBUGS&amp;quot;类的对象；促进了混合似然和贝叶斯方法的使用。PKtools包的其它函数有：AICcomp()函数从NONMEM和nlme计算模型的AIC, AICc (small sample AIC)和对数似然值。paramEst()和indEst()分别返回群体和个体参数，对NONMEM类使用最大似然法，对nlme类使用广义最小二乘法，对WinBUGS类使用MCMC贝叶斯估计法。HTMLtools()和tex()函数分别输出群体和个体参数的HTML和LaTeX报道文件，和诊断图(diagnostic plot)便于用户选择估计方法。还能分别产生HTMLtools和tex文件里的诊断图。 其它的分析药物(代谢)动力学数据的包还有： PK, PKfit和drc。VR包束的MASS包包括一些基本的方法，如：计算Logit或Probit模型的半数致死计量LD50。 分析药物(代谢)动力学数据的图形展示也非常重要，lattice包的trellis图用来可视化面板数据。 计算计量经济学(Computational Econometrics) 网址：http://cran.r-project.org/web/views/Econometrics.html 维护人员：Achim Zeileis 版本：2008-04-02 翻译：R-fox, 2008-04-15 R的很多基本函数都可用于计量经济学，尤其是stats包。CRAN的许多包也有可以分析计量经济学，下面做个简要的综述。这里介绍的工具可能与CRAN的计量金融(empirical finance)任务列表(http://cran.r-project.org/web/views/Finance.html)有许多的重合。此外，从邮件列表finance SIG(https://www.stat.math.ethz.ch/mailman/listinfo/R-SIG-Finance/)可获得计量经济和计量金融相关的帮助和讨论问题。CRAN的Social Sciences任务列表(http://cran.r-project.org/web/views/SocialSciences.html)覆盖了许多社会科学的工具，因此也与这里的工具有所重合，如：政治科学。这里综述的包大致可分为如下的几个话题： 线形回归模型(Linear regression models)线形模型可由lm()函数拟合，也有各种检验方法用来比较模型，如：summary() 和anova()。类似的函数也支持。类似的功能也适合于渐近检验(如：z检验而不是t检验，卡方检验而不是F检验)，此外还有lmtest包里的coeftest()和waldtest()函数。car包里的linear.hypothesis()可检验更广义的线形假设。HC和HAC协方差矩阵的这些功能可在sandwich包里实现。car和lmtest包还提供了许多线形回归模型的诊断方法。 微观计量经济学(Microeconometrics)：许多微观计量经济学模型属于广义线形模型，可由stats包的glm()函数拟合。包括用于选择类数据(choice data)的Logit和probit模型，用于计数类数据(count data)的poisson模型。负二项广义线形模型可由MASS包的glm.nb()实现。边缘(zero—inflated)和hurdle计数模型可由pscl包提供，zicounts包里也实现了边缘模型。双变量Poisson回归模型可在bivpois包里实现。基本的删失回归模型(censored regression model)，如：tobit模型，可由survival包里的survreg()函数拟合。micEcon包里提供了微观计量经济学的更好的工具。bayesm包执行微观计量济学和营销学(marketing)中的贝叶斯方法。reldist包提供了相对分布(relative distributions)相关的方法。 其它的回归模型(Further regression models)：R和CRNA包里有各种延伸的线形回归模型和其它模型拟合方法。非线性最小二乘回归建模可用stats包里的nls()实现。相关的包还有：quantreg(分位数回归Quantile Regression)，crq(截取分位点回归censored quantile regression)，plm(面板数据的线形回归)，sem(线性结构方程模型，包括二阶段最小平方)，systemfit(联立方程估计)，np(非参核方法)，betareg(beta回归)，nlme(非线性混合效应模型)，VR(nnet 包的多项Logit模型)，MNP(贝叶斯多项Probit模型)。Design和Hmisc包提供广义线形回归模型的工具。 基本的时间序列架构(Basic time series infrastructure)：stats包的&amp;rdquo;ts&amp;rdquo; 类是R的规则间隔时间序列的标准类。Zoo包提供了规则和不规则间隔时间序列的架构。建立在&amp;rdquo;POSIXt&amp;rdquo;时间-日期类上的its, tseries和fCalendar包也提供不规则间隔时间序列的架构，特别用于金融分析。 时间序列建模(Time series modelling)：stats包里有经典的时间序列建模工具，arima()函数做ARIMA建模和Box-Jenkins-type分析。stats包还提供StructTS()函数拟合结构时间序列，decompose()过滤时间序列，HoltWinters()分解时间序列。forecasting包束提供了一些延伸的方法，尤其是预测和模型选择。多种时间序列的过滤器可在mFilter包里找到。为了估计VAR模型，stats包的ar()拟合简单的模型，vars包、dse包的estVARXls()提供了更精巧的模型，MSBVAR包提供了贝叶斯方法。Dynlm包提供了经由OLS过滤动态回归模型的方便接口；dyn包里则提供了不同的方法。更高级的动态系统方程可由dse包拟合。高斯线形状态空间模型可由dlm包拟合(用最大斯然，kalman滤波/平滑，和贝叶斯方法)。Unit root(单位根)和cointegration technique(协整技术)可在urca，uroot和tseries包里找到。tsfa包可做时间序列因子分析。sde包提供随机微分方程的模拟和推论。 矩阵处理(Matrix manipulations)：作为一个向量和矩阵语言，R有许多基本函数处理矩阵，与Matrix和SparseM包互补。 放回再抽样(Bootstrap)：除了推荐的boot包，bootstrap或simpleboot包里有一些其它的常规bootstrapping技术；还有些函数专门为时间序列数据而设计，如：meboot包里的最大熵bootstrap，tseries包里的tsbootstrap()函数。 不平等(Inequality)：为了测量不平等(inequality)，集中(concentration)和贫穷(poverty)，ineq包提供了一些基本的工具，如：劳伦茨曲线(Lorenz curves)，Pen&amp;rsquo;s parade，基尼系数(Gini coefficient)。 结构变化(Structural change)：R有很强的处理参数模型的结构变化和变化点的能力，可参考strucchange和segmented包。 数据集(Data sets)：这里介绍的许多包里都有来自计量经济学文献里的数据集，Ecdat包包括许多来自计量经济学教科书和杂志(应用计量经济学，商业/经济统计)的数据集。FinTS包针对书&amp;rsquo;Analysis of Financial Time Series&amp;rsquo; (2nd ed., 2005, Wiley)，包括数据集，函数，列子的脚本文件。CDNmoney包提供加拿大货币流通额，pwt包提供佩恩世界表(Penn World Table)。 R空间分析 很高兴看到R在生态学里的众多应用，我是生态学的外行，但也想来凑下热闹。希望越来越多的人喜欢R(http://www.r-project.org/)，喜欢R语言中文论坛(http://rbbs.biosino.org/Rbbs/forums/list.page)。下面根据CRAN的介绍资料综述一下R分析空间数据的功能(http://cran.r-project.org/web/views/Spatial.html；http://r-spatial.sourceforge.net/；http://sal.uiuc.edu/csiss/Rgeo/)，仅仅是翻译总结资料，有不对的地方请批评指正。 R分析空间数据(Spatial Data)的包主要包括两部分： 导入导出空间数据 分析空间数据 功能及函数包： 分类空间数据(Classes for spatial data)：包sp(http://cran.r-project.org/web/packages/sp/index.html)为不同类型的空间数据设计了不同的类，如：点(points)，栅格(grids)，线(lines)，环(rings)，多边形(polygons)。另外sp提供总结数据，获取坐标等功能；提供画图函数，并且允许在图上添加空间元素(spatial elements)和参考元素(reference elements)，如：比例尺(scale bar)，指北针(north arrows)等。现在很多包都利用了sp包中的类，如：rgdal, maptools。 处理空间数据(Handling spatial data)：spsurvey包提供做概率抽样的函数(http://cran.r-project.org/web/packages/spsurvey/index.html)；trip包扩展sp包的类，针对动物跟踪数据(http://cran.r-project.org/web/packages/trip/index.html)；hdeco包用等级分解熵比较类型地图(categorical map)(http://cran.r-project.org/web/packages/hdeco/index.html)；GeoXp包允许交互式的分析空间数据(http://cran.r-project.org/web/packages/GeoXp/index.html)。 读写空间数据(Reading and writing spatial data)：图像有向量式绘图和光栅式两种。Rgdal可以读入和导出GDAL支持的光栅式格式(http://www.gdal.org/)和OGR(http://www.gdal.org/ogr/)支持的向量格式(http://cran.r-project.org/web/packages/rgdal/index.html)。ncdf包用来处理NetCDF文件(http://cran.r-project.org/web/packages/ncdf/index.html)；maps包可连接一些地理学数据库并展示地理图(http://cran.r-project.org/web/packages/maps/index.html)；RArcInfo包可读取ArcInfo v.7二进制文件和*.e00文件()；maptools包管理和读入地理数据，也为PBSmapping包、spatsta包和sp类提供接口函数(http://cran.r-project.org/web/packages/maptools/index.html)，还可以通到GSHHS数据库；classInt包为专题地图制图选择单变量的类间距(http://cran.r-project.org/web/packages/classInt/index.html)；gmt包提供R和GMT绘图软件的接口(http://cran.r-project.org/web/packages/gmt/index.html)。 点格局分析(Point pattern analysis)：spatstat包做空间点分布型态(Spatial Point Patterns)分析，长处在于模型拟合和仿真(http://cran.r-project.org/web/packages/spatstat/index.html)；spatgraphs包提供点格局的可视化图形(http://cran.r-project.org/web/packages/spatgraphs/index.html)；splancs包允许分析多边形区域，包括很多种方法，如：2维核密度(http://cran.r-project.org/web/packages/splancs/index.html)；ecespa包提供书《Introduccion al Analisis Espacial de Datos en Ecologia y Ciencias Ambientales: Metodos y Aplicaciones》里用的点格局分析函数和数据(http://cran.r-project.org/web/packages/ecespa/index.html)；aspace包计算空间中心统计(centrographic satistics)和最小凸多边形(http://cran.r-project.org/web/packages/aspace/index.html)；spatialkernel包做多元数据的非参核密度估计和核回归估计(http://cran.r-project.org/web/packages/spatialkernel/index.html)。 地质统计学(Geostatistics) ：gstat包做单变量和多变量地质统计，适合于大的数据集(http://cran.r-project.org/web/packages/gstat/index.html)；geoR包(用贝叶斯模型，http://cran.r-project.org/web/packages/geoR/index.html)和geoRglm包(用线性模型，http://cran.r-project.org/web/packages/geoRglm/index.html)做基于模型的地质统计；fields包也提供许多类似的函数(http://cran.r-project.org/web/packages/fields/index.html)；spBayes包用蒙特卡洛一马尔科夫链方法(MCMC)做单变量和多变量的高斯模型(http://cran.r-project.org/web/packages/spBayes/index.html)。RandomFields包模拟和分析随机场(http://cran.r-project.org/web/packages/RandomFields/index.html)；tripack包用于不规则数据的三角测量法(http://cran.r-project.org/web/packages/tripack/index.html)；akima包用于不规则数据的线性或三次样条插值(http://cran.r-project.org/web/packages/akima/index.html)；spatialCovariance包计算矩形数据的空间协方差矩阵(http://cran.r-project.org/web/packages/spatialCovariance/index.html)……。 疾病制图和地区数据分析(Disease mapping and areal data analysis)：DCluster包用计数数据探测疾病的空间聚类，计算空间权重，测试空间自相关，建立空间回归模型等(http://cran.r-project.org/web/packages/DCluster/index.html)；spgwr包做地理加权回归模型，检测平稳性(http://cran.r-project.org/web/packages/spgwr/index.html)；spatclus包(http://cran.r-project.org/web/packages/spatclus/index.html)。spatclus包探测2维或3维空间点分布的任意形状的聚类(http://cran.r-project.org/web/packages/spatclus/index.html)。 生态学分析(Ecological analysis)：R有很多分析生态和环境数据的包。如：grasp包用GAM模型(灰色代数曲线型模型)做环境预报(http://cran.r-project.org/web/packages/grasp/index.html)；ade4包用做环境科学里的探索和欧几里德方法(http://cran.r-project.org/web/packages/ade4/index.html)；adehabitat包分析动物的栖息地选择(http://cran.r-project.org/web/packages/adehabitat/index.html)；pastecs包做时空序列的分解和分析(http://cran.r-project.org/web/packages/pastecs/index.html)；vegan包做群落和植被生态学中的排序方法(http://cran.r-project.org/web/packages/vegan/index.html)；WeedMap包做空间预测(http://cran.r-project.org/web/packages/WeedMap/index.html)；clustTool包做聚类分析(http://cran.r-project.org/web/packages/clustTool/index.html)。更多资料见：http://cran.r-project.org/web/views/Environmetrics.html。 Multivariate Statistics (多元统计) 网址：http://cran.r-project.org/web/views/Multivariate.html 维护人员：Paul Hewson 版本：2008-02-08 翻译：R-fox, 2008-04-04 基本的R包已经实现了传统多元统计的很多功能，然而CRNA的许多其它包提供了更深入的多元统计方法，下面做个简要的综述。多元统计的特殊应用在CRNA的其它任务列表(task view)里也会提及，如：排序(ordination)会在Environmetrics(http://cran.r-project.org/web/views/Environmetrics.html)里说到；有监督的分类方法能在Machine Learning(http://cran.r-project.org/web/views/MachineLearning.html)里找到；无监督的分类在Cluster(http://cran.r-project.org/web/views/Cluster.html)里。这里要综述的包主要分为以下几个部分： 多元数据可视化(Visualising multivariate data)： 绘图方法：基本画图函数(如：pairs()、coplot())和lattice包里的画图函数(xyplot(), splom())可以画成对列表的二维散点图，3维密度图。car包里的scatterplot.matrix()函数提供更强大的二维散点图的画法。cwhmisc包集合里的cwhplot包的pltSplomT()函数类似pair()画散点图矩阵，而且可以在对角位置画柱状图或密度估计图。除此之外，scatterplot3d包可画3维的散点图，aplpack包里bagplot()可画二变量的boxplot，spin3R()可画可旋转的三维点图。misc3d包有可视化密度的函数。YaleToolkit包提供许多多元数据可视化技术，agsemisc也是这样。更特殊的多元图包括：aplpack包里的faces()可画Chernoff’s face；MASS包里的parcoord()可画平行坐标图(矩阵的每一行画一条线，横轴表示矩阵的每列)；graphics包里的stars()可画多元数据的星状图(矩阵的每一行用一个星状图表示)。ade4包里的mstree()和vegan包里的spantree()可画最小生成树。calibrate包支持双变量图和散点图，chplot包可画convex hull图。geometry包提供了和qhull库的接口，由convexhulln()可给出相应点的索引。ellipse包可画椭圆，也可以用plotcorr()可视化相关矩阵。denpro包为多元可视化提供水平集树形结构(level set trees)。graphics包里的mosaicplot()和vcd包里的mosaic()函数画马赛克图(mosaic plot)。gclus包提供了针对聚类的散点图和平行坐标图。rggobi包和DescribeDisplay包是GGobi的接口，DescribeDisplay的图可达到出版质量的要求；xgobi包是XGobi和XGvis的接口，可实现动态交互的图。最后，iplots包提供强大的动态交互图，尤其是平行坐标图和马赛克图。seriation包提供seriation方法，能重新排列矩阵和系统树。 数据预处理：AIS包提供多元数据的初步描述函数。Hmisc包里的summarize()和summary.formula()辅助描述数据，varclus()函数可做聚类，而dataRep()和find.matches()找给定数据集的典型数据和匹配数据。KnnFinder包里的nn()函数用kd-tree找相似变量的个数。dprep包为分类提供数据预处理和可视化函数，如：检查变量冗余性、标准化。base包里的dist()和cluster包里的daisy()函数提供距离计算函数；proxy包提供更多的距离测度，包括矩阵间的距离。simba包处理已有数据和缺失数据，包括相似性矩阵和重整形。 假设检验(Hypothesis testing)：ICSNP包提供霍特林(Hotellings)T2检验和许多非参检验方法，包括基于marginal ranks的位置检验(location test)，计算空间中值和符号，形状估计。cramer包做两样本的非参检验，SpatialNP可做空间符号和秩检验。 多元分布(Multivariate distributions)： 描述统计(Descriptive measures)：stats包里的cov()和cor()分别估计协方差和相关系数。ICSNP包提供几种数据描述方法，如：spatial.median()估计空间中值，其它的函数估计scatter。MASS包里的cov.rob()提供更健壮的方差/协方差矩阵估计。covRobust包用最近邻方差估计法估计协方差。robustbase包的covMCD()估计协方差和covOGK()做Orthogonalized Gnanadesikan-Kettenring。rrcov包提供可扩展和稳健的估计函数covMcd(), covMest()。corpcor包可计算大规模的协方差和偏相关矩阵。 密度估计和模拟(Densities (estimation and simulation))：MASS包的mvrnorm()产生多元正态分布的随机数。Mvtnorm包有多元t分布和多元正态分布的概率和分位数函数，还可计算多元正态分布的密度函数。mvtnormpcs包提供基于Dunnett的函数。mnormt包提供元t分布和多元正态分布的密度和分布函数，并可产生随机数。sn包提供多元偏t分布和偏正态分布的密度、分布、随机数函数。delt包提供了许多估计多元密度的函数方法，如：CART和贪婪方法。CRAN的Cluster任务列表(http://cran.r-project.org/web/views/Cluster.html)有更全面的信息，ks包里的rmvnorm.mixt()和dmvnorm.mixt()函数产生随机数和估计密度，bayesm包里有多种拟合方法。很多地方都提供了模拟Wishart分布的函数，如：bayesm包里的rwishart()，MCMCpack包里的rwish()，而且MCMCpack包还有密度函数dwish()。KernSmooth包里的bkde2D()和MASS包的kde2d()做分箱(binned)或不分箱二维核密度估计。ks包也像ash和GenKern包样可做核平滑(kernel smoothing)。prim包用法找高维多元数据的高密度区域，feature包可计算多元数据的显著特征。 正态检验(Assessing normality)：mvnormtest包提供Shapiro-Wilks检验的多元数据延伸方法，mvoutlier包检测多元离群点(outlier)，ICS包可检验多元正态分布。energy包里的mvnorm.etest()基于E统计量做正态检验，k.sample()检验多个数据是否来自同一分布。dprep包里的mardia()用Mardia检验正态性。stats包里的mauchly.test()可检验Wishart分布的协方差矩阵。 连接函数(Copulas)：copula包提供常规的copula函数的程序，包括：normal, t, Clayton, Frank, Gumbel。fgac包提供generalised archimedian copula，mlCopulaSelection包可做二变量的copula。 线形模型(Linear models)：stats包里的lm()可做多元线形模型，anova.mlm()比较多个多元线形模型，manova()做多元方差分析(MANOVA)。sn包的msn.mle()和mst.mle()可拟合多元偏正态和偏t分布模型。pls包提供偏最小二乘回归(PLSR)和主成分回归；ppls包可做惩罚偏最小二乘回归；dr包提供降维回归方法，如片逆回归法(Sliced Inverse Regression)、片平均方差估计(sliced average variance estimation)。plsgenomics包做基于偏最小二乘回归的基因组分析。relaimpo包可评估回归参数的相对重要性。 投影方法(Projection methods)： 主成分(Principal components)：stats包的prcomp()(基于svd())和princomp()(基于eigen())能计算主成分。sca包做单分量分析。nFactors可评价碎石图(Scree plot)，paran包可评估主成分分析得到的主成分和因子分析得到的因子。pcurve包做主曲线(Principal Curve)分析和可视化。gmodels包提供适合大矩阵的fast.prcomp()和fast.svd()。kernlab包里的kpca()用核方法做非线性的主成分分析。pcaPP包用投影寻踪(projection pursuit)法计算稳健/鲁棒(robust)主成分。amap包的acpgen()和acprob()函数分别针对广义(generalized)和稳健(robust)主成分分析。主成分在很多方面也有相应的应用，如：涉及生态的ade4包，感官的SensoMinR包。psy包里有用于心理学的各种程序，与主成分相关的有：sphpca()用球形直观表示相关矩阵，类似于3D的PCA；fpca()图形展示主成分分析的结果，而且允许某些变量间有相关性；scree.plot()图形展示相关或协方差矩阵的特征值。PTAk包做主张量分析(Principal Tensor Analysis)。smatr包提供关于异速生长(allometry)的函数。 典型相关(Canonical Correlation)：stats包里的cancor()是做典型相关的函数。kernlab包提供更稳健的核方法kcca()。concor包提供了许多concordance methods。 冗余度分析(Redundancy Analysis)：calibrate包里的rda()函数可做冗余度分析和典型相关。fso包提供了模糊集排序(Ordination)方法。 独立成分(Independent Components)：fastICA包用fastICA算法做独立成分分析(ICA)和投影寻踪分析(Projection Pursuit)，mlica包提供独立成分分析的最大似然拟合，PearsonICA包用基于互信息的打分函数分离独立信号。ICS包能执行不变坐标系(invariant coordinate system)和独立成分分析(independent components)。JADE包提供就JADE算法的接口，而且可做一些ICA。 普鲁克分析(Procrustes analysis)：vegan包里的procrustes()可做普鲁克分析，也提供排序(ordination)函数。更一般的普鲁克分析可由FactoMineR包里的GPA()实现。 主坐标/尺度方法(Principal coordinates / scaling methods)：stats包的cmdscale()函数执行传统的多维尺度分析(multidimensional scaling，MDS)(主坐标分析Principal Coordinates Analysis)，MASS包的sammon()和isoMDS()函数分别执行Sammon和Kruskal非度量多维尺度分析。vegan包提供非度量多维尺度分析的包装(wrappers)和后处理程序。 无监督分类(Unsupervised classification)： 聚类分析：CRAN的Cluster任务列表全面的综述了R实现的聚类方法。stats里提供等级聚类hclust()和k-均值聚类kmeans()。cluster包里有大量的聚类和可视化技术，clv包里则有一些聚类确认程序，e1071包的classAgreement()可计算Rand index比较两种分类结果。Trimmed k-means聚类分析可由trimcluster包实现，聚类融合方法(Cluster Ensembles)由clue包实现，clusterSim包能帮助选择最佳的聚类，hybridHclust包提供一些混合聚类方法。energy包里有基于E统计量的距离测度函数edist()和等级聚类方法hclust.energy()。LLAhclust包提供基于似然(likelihood linkage)方法的聚类，也有评定聚类结果的指标。fpc包里有基于Mahalanobis距离的聚类。clustvarsel包有多种基于模型的聚类。模糊聚类(fuzzy clustering)可在cluster包和hopach包里实现。Kohonen包提供用于高维谱(spectra)或模式(pattern)的有监督和无监督的SOM算法。clusterGeneration包帮助模拟聚类。CRAN的Environmetrics任务列表里也有相关的聚类算法的综述。mclust包实现了基于模型的聚类，MFDA包实现了功能数据的基于模型的聚类。 树方法：CRAN的MachineLearning任务列表有对树方法的细节描述。分类树也常常是重要的多元方法，rpart包正是这样的包，rpart.permutation包还可以做rpart()模型的置换(permutation)检验。TWIX包的树可以外部剪枝。hier.part包分割多元数据集的方差。mvpart包可做多元回归树，party包实现了递归分割(recursive partitioning)，rrp包实现了随机递归分割。caret包可做分类和回归训练，进而caretLSF包实现了并行处理。kknn包的k-近邻法可用于回归，也可用于分类。 有监督分类和判别分析(Supervised classification and discriminant analysis)：MASS包里的lda()和qda()分别针对线性和二次判别分析。mda包的mda(), fda()允许混合和更灵活的判别分析，mars()做多元自适应样条回归(multivariate adaptive regression splines)，bruto()做自适应样条后退拟合(adaptive spline backfitting)。earth包里也有多元自适应样条回归的函数。rda包可用质心收缩法(shrunken centroids regularized discriminant analysis)实现高维数据的分类。VR的class包的knn()函数执行k-最近邻算法，knncat包里有针对分类变量的k-最近邻算法。SensoMineR包的FDA()用于因子判别分析。许多包结合了降维(dimension reduction)和分类。klaR包可以做变量选择，可处理多重共线性，还有可视化函数。superpc包利用主成分做有监督的分类，classPP包则可为其做投影寻踪(projection pursuit)，gpls包用广义偏最小二乘做分类。hddplot包用交叉验证的线性判别分析决定最优的特征个数。supclust包可以根据芯片数据做基因的监督聚类。ROCR提供许多评估分类执行效果的方法。predbayescor包可做朴素贝叶斯(na&amp;iuml;ve Bayes)分类。关于监督分类的更多信息可以看MachineLearning任务列表。 对应分析(Correspondence analysis)：MASS包的corresp()和mca()可以做简单和多重对应分析。ca包提供单一、多重和联合(joint)对应分析。ade4包的ca()和mca()分别做一般的和多重对应分析。vegan包里也有类似的函数。cocorresp可实现两个矩阵间的co-correspondence分析。FactoMineR包的CA()和MCA()函数也能做类似的简单和多重对应分析，还有画图函数。homals执行同质分析(homogeneity)。 前向查找(Forward search)：Rfwdmv包执行多元数据的前向查找。 缺失数据(Missing data)：mitools包里有缺失数据的多重估算(multiple imputation)的函数, mice包用chained equations实现了多重估算，mvnmle包可以为多元正态数据的缺失值做最大似然估计(ML Estimation)，norm包提供了适合多元正态数据的估计缺失值的期望最大化算法(EM algorithm)，cat包允许分类数据的缺失值的多重估算，mix包适用于分类和连续数据的混合数据。pan包可为面版数据(panel data)的缺失值做多重估算。VIM包做缺失数据的可视化和估算。Hmisc包的aregImpute()和transcan()提供了其它的估算缺失值方法。EMV包提供了knn方法估计缺失数据。monomvn包估计单调多元正态数据的缺失值。 隐变量方法(Latent variable approaches)：stats包的factanal()执行最大似然因子分析，MCMCpack包可做贝叶斯因子分析。GPArotation包提供投影梯度(Gradient Projection)旋转因子法。FAiR包用遗传算法作因子分析。ifa包可用于非正态的变量。sem包拟合线形结构方程模型。ltm包可做隐含式语义分析 (Latent semantic analysis)，eRm包则可拟合Rasch模型(Rasch models)。FactoMineR包里有很多因子分析的方法，包括：MFA()多元因子分析，HMFA()等级多元因子分析，ADFM()定量和定性数据的多元因子分析。tsfa包执行时间序列的因子分析。poLCA包针对多分类变量(polytomous variable)做潜类别分析(Latent Class Analysis)。 非高斯数据建模(Modelling non-Gaussian data)：bivpois包建模Poisson分布的二变量。mprobit包提供了适合二元和顺序响应变量的多元概率模型。MNP包实现了Bayesian多元概率模型。polycor包可计算多组相关(olychoric correlation)和四分相关(tetrachoric correlation)矩阵。bayesm包里有多种模型，如：表面非相关回归(Seemingly unrelated Regression)，多元logit/probit模型, 工具变量法(Instrumental Variables)。VGAM包里有：广义线形和可加模型(Vector Generalised Linear and Additive Models)，减秩回归(Reduced Rank regression)。 矩阵处理(Matrix manipulations)：R作为一种基于向量和矩阵的语言，有许多处理矩阵的强有力的工具，由包Matrix和SparseM实现。matrixcalc包增加了矩阵微积分的功能。spam包提供了更深入的针对稀疏矩阵的方法。 其它(Miscellaneous utitlies)：DEA包执行数据包络分析(data envelopment analysis,DEA)。abind包组合多维array。Hmisc包的mApply()扩充了apply()的功能。除了前面描述的功能，sn包还未偏正态和偏t分布提供边缘化(marginalisation)、仿射变换(affine transformations)等。SharedHT2包执行芯片数据的Hotelling&amp;rsquo;s T2检验。panel包里有面版数据(panel data)的建模方法。mAr包可做向量自回归模型(vector auto-regression)，MSBVAR包里有贝叶斯向量自回归模型。Hmisc包的rm.boot()函数bootstrap重复测量试验(Repeated Measures Models)。compositions包提供复合数据分析(compositional data analysis)。cramer包为两样本数据做多元非参Cramer检验。psy里有许多心理学的常用方法。cwhmisc包集合的cwhmath包里有许多有趣的功能，如各种旋转函数。desirability包提供了基于密度函数的多变量最优化方法。geozoo包可以画geozoo包里定义的几何对象。 Machine Learning &amp;amp; Statistical Learning (机器学习 &amp;amp; 统计学习) 网址：http://cran.r-project.org/web/views/MachineLearning.html 维护人员：Torsten Hothorn 版本：2008-02-18 18:19:21 翻译：R-fox, 2008-03-18 机器学习是计算机科学和统计学的边缘交叉领域，R关于机器学习的包主要包括以下几个方面： 神经网络(Neural Networks)：nnet包执行单隐层前馈神经网络，nnet是VR包的一部分(http://cran.r-project.org/web/packages/VR/index.html)。 递归拆分(Recursive Partitioning)：递归拆分利用树形结构模型，来做回归、分类和生存分析，主要在rpart包(http://cran.r-project.org/web/packages/rpart/index.html)和tree包(http://cran.r-project.org/web/packages/tree/index.html)里执行，尤其推荐rpart包。Weka里也有这样的递归拆分法，如：J4.8, C4.5, M5，包Rweka提供了R与Weka的函数的接口(http://cran.r-project.org/web/packages/RWeka/index.html)。party包提供两类递归拆分算法，能做到无偏的变量选择和停止标准：函数ctree()用非参条件推断法检测自变量和因变量的关系；而函数mob()能用来建立参数模型(http://cran.r-project.org/web/packages/party/index.html)。另外，party包里也提供二分支树和节点分布的可视化展示。mvpart包是rpart的改进包，处理多元因变量的问题(http://cran.r-project.org/web/packages/mvpart/index.html)。rpart.permutation包用置换法(permutation)评估树的有效性(http://cran.r-project.org/web/packages/rpart.permutation/index.html)。knnTree包建立一个分类树，每个叶子节点是一个knn分类器(http://cran.r-project.org/web/packages/knnTree/index.html)。LogicReg包做逻辑回归分析，针对大多数自变量是二元变量的情况(http://cran.r-project.org/web/packages/LogicReg/index.html)。maptree包(http://cran.r-project.org/web/packages/maptree/index.html)和pinktoe包(http://cran.r-project.org/web/packages/pinktoe/index.html)提供树结构的可视化函数。 随机森林(Random Forests)：randomForest包提供了用随机森林做回归和分类的函数(http://cran.r-project.org/web/packages/randomForest/index.html)。ipred包用bagging的思想做回归，分类和生存分析，组合多个模型(http://cran.r-project.org/web/packages/ipred/index.html)。party包也提供了基于条件推断树的随机森林法(http://cran.r-project.org/web/packages/party/index.html)。varSelRF包用随机森林法做变量选择(http://cran.r-project.org/web/packages/varSelRF/index.html)。 Regularized and Shrinkage Methods：lasso2包(http://cran.r-project.org/web/packages/lasso2/index.html)和lars包(http://cran.r-project.org/web/packages/lars/index.html)可以执行参数受到某些限制的回归模型。elasticnet包可计算所有的收缩参数(http://cran.r-project.org/web/packages/elasticnet/index.html)。glmpath包可以得到广义线性模型和COX模型的L1 regularization path(http://cran.r-project.org/web/packages/glmpath/index.html)。penalized包执行lasso (L1)和ridge (L2)惩罚回归模型(penalized regression models)(http://cran.r-project.org/web/packages/penalized/index.html)。pamr包执行缩小重心分类法(shrunken centroids classifier)(http://cran.r-project.org/web/packages/pamr/index.html)。earth包可做多元自适应样条回归(multivariate adaptive regression splines)(http://cran.r-project.org/web/packages/earth/index.html)。 Boosting :gbm包(http://cran.r-project.org/web/packages/gbm/index.html)和boost包(http://cran.r-project.org/web/packages/boost/index.html)执行多种多样的梯度boosting算法，gbm包做基于树的梯度下降boosting，boost包包括LogitBoost和L2Boost。GAMMoost包提供基于boosting的广义相加模型(generalized additive models)的程序(http://cran.r-project.org/web/packages/GAMMoost/index.html)。mboost包做基于模型的boosting(http://cran.r-project.org/web/packages/mboost/index.html)。 支持向量机(Support Vector Machines)：e1071包的svm()函数提供R和LIBSVM的接口 (http://cran.r-project.org/web/packages/e1071/index.html)。kernlab包为基于核函数的学习方法提供了一个灵活的框架，包括SVM、RVM……(http://cran.r-project.org/web/packages/kernlab/index.html) 。klaR包提供了R和SVMlight的接口(http://cran.r-project.org/web/packages/klaR/index.html)。 贝叶斯方法(Bayesian Methods)：BayesTree包执行Bayesian Additive Regression Trees (BART)算法(http://cran.r-project.org/web/packages/BayesTree/index.html，http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/BART%206--06.pdf)。tgp包做Bayesian半参数非线性回归(Bayesian nonstationary, semiparametric nonlinear regression)(http://cran.r-project.org/web/packages/tgp/index.html)。 基于遗传算法的最优化(Optimization using Genetic Algorithms)：gafit包(http://cran.r-project.org/web/packages/gafit/index.html)和rgenoud包(http://cran.r-project.org/web/packages/rgenoud/index.html)提供基于遗传算法的最优化程序。 关联规则(Association Rules)：arules包提供了有效处理稀疏二元数据的数据结构，而且提供函数执Apriori和Eclat算法挖掘频繁项集、最大频繁项集、闭频繁项集和关联规则(http://cran.r-project.org/web/packages/arules/index.html)。 模型选择和确认(Model selection and validation)：e1071包的tune()函数在指定的范围内选取合适的参数(http://cran.r-project.org/web/packages/e1071/index.html)。ipred包的errorest()函数用重抽样的方法(交叉验证，bootstrap)估计分类错误率(http://cran.r-project.org/web/packages/ipred/index.html)。svmpath包里的函数可用来选取支持向量机的cost参数C(http://cran.r-project.org/web/packages/svmpath/index.html)。ROCR包提供了可视化分类器执行效果的函数，如画ROC曲线(http://cran.r-project.org/web/packages/ROCR/index.html)。caret包供了各种建立预测模型的函数，包括参数选择和重要性量度(http://cran.r-project.org/web/packages/caret/index.html)。caretLSF包(http://cran.r-project.org/web/packages/caretLSF/index.html)和caretNWS(http://cran.r-project.org/web/packages/caretNWS/index.html)包提供了与caret包类似的功能。 统计学习基础(Elements of Statistical Learning)：书《The Elements of Statistical Learning: Data Mining, Inference, and Prediction》(http://www-stat.stanford.edu/~tibs/ElemStatLearn/)里的数据集、函数、例子都被打包放在ElemStatLearn包里(http://cran.r-project.org/web/packages/ElemStatLearn/index.html)。 背景介绍： Weka：Weka有两种意思：一种不会飞的鸟的名字，一个机器学习开源项目的简称(Waikato Environment for Knowledge Analysis，http://www.cs.waikato.ac.nz/~ml/weka/)。我们这里当然要介绍的是第二种意思啦，Weka项目从1992年开始，由新西兰政府支持，现在已在机器学习领域大名鼎鼎。Weka里有非常全面的机器学习算法，包括数据预处理、分类、回归、聚类、关联规则等。Weka的图形界面对不会写程序的人来说非常方便，而且提供“KnowledgeFlow”功能，允许将多个步骤组成一个工作流。另外，Weka也允许在命令行执行命令。 R：R就不用我废话了吧，呵呵，越来越受欢迎的统计软件(http://www.r-project.org/)。 R与Weka：R里有很多机器学习的函数和包，不过Weka里提供的函数更全面更集中，所以我有时候需要用到Weka。以前我是这样用R和Weka的： 在R中准备好训练的数据(如：提取数据特征……)； 整理成Weka需要的格式(*.arff)； 在Weka里做机器学习(如：特征选择、分类……)； 从Weka的预测结果计算需要的统计量(如：sensitivity, specificity, MCC……)。 来回捣腾两个软件还是挺麻烦的；为了偷懒，我没学Weka的命令行，只会用图形界面的，在数据量大的时候非常受罪，有时候还会内存不够。现在发现R竟然提供了和Weka的接口函数包RWeka，以后方便多了哦，下面介绍一下RWeka的功能： RWeka (http://cran.r-project.org/web/packages/RWeka/index.html)： 数据输入和输出 WOW()：查看Weka函数的参数。 Weka_control()：设置Weka函数的参数。 read.arff()：读Weka Attribute-Relation File Format (ARFF)格式的数据。 write.arff：将数据写入Weka Attribute-Relation File Format (ARFF)格式的文件。 数据预处理 Normalize()：无监督的标准化连续性数据。 Discretize()：用MDL(Minimum Description Length)方法，有监督的离散化连续性数值数据。 分类和回归 IBk()：k最近邻分类 LBR()：naive Bayes法分类 J48()：C4.5决策树算法(决策树在分析各个属性时，是完全独立的)。 LMT()：组合树结构和Logistic回归模型，每个叶子节点是一个Logistic回归模型，准确性比单独的决策树和Logistic回归方法要好。 M5P()：M5 模型数算法，组合了树结构和线性回归模型，每个叶子节点是一个线性回归模型，因而可用于连续数据的回归。 DecisionStump()：单层决策树算法，常被作为boosting的基本学习器。 SMO()：支持向量机分类 AdaBoostM1()：Adaboost M1方法。-W参数指定弱学习器的算法。 Bagging()：通过从原始数据取样(用替换方法)，创建多个模型。 LogitBoost()：弱学习器采用了对数回归方法,学习到的是实数值 MultiBoostAB()：AdaBoost 方法的改进，可看作AdaBoost和“wagging”的组合。 Stacking()：用于不同的基本分类器集成的算法。 LinearRegression()：建立合适的线性回归模型。 Logistic()：建立logistic回归模型。 JRip()：一种规则学习方法。 M5Rules()：用M5方法产生回归问题的决策规则。 OneR()：简单的1-R分类法。 PART()：产生PART决策规则。 聚类 Cobweb()：这是种基于模型方法，它假设每个聚类的模型并发现适合相应模型的数据。不适合对大数据库进行聚类处理。 FarthestFirst()：快速的近似的k均值聚类算法 SimpleKMeans()：k均值聚类算法 XMeans()：改进的k均值法，能自动决定类别数 DBScan()：基于密度的聚类方法，它根据对象周围的密度不断增长聚类。它能从含有噪声的空间数据库中发现任意形状的聚类。此方法将一个聚类定义为一组“密度连接”的点集。 关联规则 Apriori()：Apriori是关联规则领域里最具影响力的基础算法，是一种广度优先算法，通过多次扫描数据库来获取支持度大于最小支持度的频繁项集。它的理论基础是频繁项集的两个单调性原则：频繁项集的任一子集一定是频繁的；非频繁项集的任一超集一定是非频繁的。在海量数据的情况下，Apriori 算法的时间和空间成本非常高。 Tertius()：Tertius算法。 预测和评估 predict()：根据分类或聚类结果预测新数据的类别 table()：比较两个因子对象 evaluate_Weka_classifier()：评估模型的执行，如：TP Rate，FP Rate，Precision，Recall，F-Measure]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Disqus 与 Mathjax 可能有冲突]]></title>
    	<url>/tech/2013/06/03/conflict-between-disqus-and-mathjax/</url>
		<content type="text"><![CDATA[[在系统中添加 Mathjax 支持的时候，发现如果 LaTeX 公式与 Disqus 评论系统同时存在，会导致如下的错误1： This page is forcing your browser to use legacy mode, which is not compatible with Disqus. Please see our troubleshooting guide to get more information about this error. 这 IE 8.0 中出现，但在 Firefox 21 中却不存在，初步怀疑是 Mathjax 调用 Mootools 1.4.5 而导致，具体见 http://wordpress.org/support/topic/disqus-browser-legacy-error。 到 2017 年时，这个问题已经不再是问题，一切都因为有了 Pandoc。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Math version `bold&#39; is not defined]]></title>
    	<url>/tech/2013/04/26/math-version-bold-not-defined/</url>
		<content type="text"><![CDATA[[使用 Y &amp;amp; Y Mathtime 字体，用 XeLaTeX 编译，提示： “Error: Math version `bold&#39; is not defined.” 解决方法是在导言区添加： \DeclareMathVersion{bold}]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[MRO：多继承属性查找机制 z]]></title>
    	<url>/tech/2013/03/29/mro/</url>
		<content type="text"><![CDATA[[原文地址：http://blog.csdn.net/imzoer/article/details/8737642 还记得什么是新式类和旧式类吗 中，一个class继承于object，或其 bases class 里面任意一个继承于object，这个class都是 new-style class。 在 Python 中，类是可以多重继承的。Python 类中的所有成员变量都是类似 Java 语言中的public的。 在 Python 中，类中定义的函数也是对象。也可以修改赋值。 # -*- coding:utf-8 -*- class A(object): def f(self): print &amp;quot;f&amp;quot; def ff(): print &amp;quot;ff&amp;quot; a=A() a.f() xf=a.f xf() a.f=ff a.f() 这个例子的输出结果如下： f f ff 通过上面的结果，可看出来，a.f=ff的时候，对象a中的函数已经被修改成ff函数了。另外，xf也是一个函数对象。 对于继承，Python 提供了两个函数：issubclass()和isinstance() 看例子： &amp;gt;&amp;gt;&amp;gt; issubclass(bool,int) True &amp;gt;&amp;gt;&amp;gt; 结果输出是True，说明bool是int的子类。 # -*- coding:utf-8 -*- class A(object): def a(self): print &amp;quot;a&amp;quot; if __name__ == &amp;quot;__main__&amp;quot;: a=A() print isinstance(a,A) 主要还是学习一下多重继承的概念。Python 中多重继承的语法如下： class Myclass(base1,base2,base3): MRO MRO 即 Method Resolution Order，主要用于在多继承时判断调的属性的路径(来自于哪个类)。 之前查看了很多资料，说 MRO 是基于深度优先搜索算法的。但不完全正确。在 Python2.3 之前是基于此算法，但从 Python2.3 起应用了新算法：C3算法。 为什么采用 C3 算法 C3 算法最早被提出是用于 Lisp 的，应用在 Python 中是为了解决原来基于深度优先搜索算法不满足本地优先级，和单调性的问题。 本地优先级：指声明时父类的顺序，比如C(A, B)，如果访问C类对象属性时，应该根据声明顺序，优先查找A类，然后再查找B类。 单调性：如果在C的解析顺序中，A排在B的前面，那么在C的所有子类里，也必须满足这个顺序。 在新式类中，查找一个要调用的函数或者属性的时候，是广度优先搜搜的。在旧式类当中，是深度优先搜索的。如下图所示： 看下面的例子： # -*- coding:utf-8 -*- # 最好在 2.X 中测试 class D(object): def foo(self): print &amp;quot;class D&amp;quot; class B(D): pass class C(D): def foo(self): print &amp;quot;class C&amp;quot; class A(B, C): pass f = A() f.foo() 例子中定义D类的时候，D是新式类，所以D的所有子类都是新式类。A的实例对象f在调用foo函数的时候，根据广度优先搜索原则，调用的是C类里面的foo函数。上面的代码输出class C；如果定义D类的时候直接class D，而不是class D(object)，那么上述代码就该输出class D了。 需要说明的是，在 3.X 中运行代码时class D:和class D(object)的返回结果都是class C，说明完全不再有深度优先支持；但在 2.X 中，class D:的结果是class D，说明 2.X 还是支持深度优先的。返回的结果是class C 命名空间、作用域 Python 中，不同命名空间中的内容可以重名。比如说在A模块中定义一个max函数，在B模块中也定义一个max函数，那么二者是不冲突的。在调用的时候，只需要在函数名字前面加上模块名字即可。 在 Python 中，一切都是对象。严格的说，在模块中，对名字的引用就是属性引用。在表达式modulename.functionname中，modulename是一个模块对象。function则是该对象的一个属性。 属性分为只读的和可写的。如果是可写的属性，那么就可以使用del来删除了。比如说在一个类中删除一个属性的例子如下： # -*- coding:utf-8 -*- class OOO(object): def __init__(self, value): self.value=value if __name__ == &amp;quot;__main__&amp;quot;: a=OOO(100) print a.value del a.value print a.value 执行代码的结果： Traceback (most recent call last): File &amp;quot;C:\Users\naughty\workspace\ttt\com\d.py&amp;quot;, line 15, in &amp;lt;module&amp;gt; 100 print a.value AttributeError: &#39;OOO&#39; object has no attribute &#39;value&#39; 可以看到，属性value在删除之前是可以输出的。删除之后，再次输出就会抛出异常了。 也可以删除引入的另外一个模块的内容： # -*- coding:utf-8 -*- import data if __name__ == &amp;quot;__main__&amp;quot;: print data.a print data.b del data.a print data.a data模块如下： # -*- coding:utf-8 -*- a = &amp;quot;aa&amp;quot; b = &amp;quot;bb&amp;quot; 既然涉及到了命名空间，那么有必要说一下global的使用。global的使用了是为了在一个代码块中声明一个变量是全局变量。 # -*- coding:utf-8 -*- import data a=&amp;quot;global a!&amp;quot; def modify(): global a a=&amp;quot;inner a!&amp;quot; print a if __name__ == &amp;quot;__main__&amp;quot;: print a modify() print a 在上面这个例子中，modify函数中使用了global，然后修改了a的值，并打印。在代码最后也打印了a的值。执行代码输出如下： global a! inner a! inner a! # 这里的值被修改了 这说明，global确实起到作用了。如果这里不使用global的话，那么根据 Python 对变量赋值的原则，这里会在modify这个函数的局部空间中修改变量a，并不会反映到全局。删除global a之后，再次执行，输出如下： global a! inner a! global a!]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[导致 “Missing number, treated as zero” 的原因]]></title>
    	<url>/tech/2013/02/01/missing-number-treated-as-zero/</url>
		<content type="text"><![CDATA[[从http://web.mit.edu/ghudson/dev/nokrb/third/tetex/texmf/doc/help/faq/u ! Missing number, treated as zero. \relax l.21 \begin{Ventry}{Return values} 很可能添加calc宏包的支持就可能可以解决问题，至少我确实解决了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Linux 中的 grep 命令详解]]></title>
    	<url>/tech/2013/01/13/grep/</url>
		<content type="text"><![CDATA[[1 简介 2 grep 常见用法 3 grep 与正则表达式 3.1 字符类 3.2 行首 ^ 与行尾 $ 3.3 任意一个字节 . 与重复字节 * 3.4 限定连续 RE 字符范围 {} 3.5 扩展 grep (grep -E 或者 egrep) 3.6 不使用正则表 4 选项 5 参考 原文地址：http://www.cnblogs.com/ggjucheng/archive/2013/01/13/2856896.html 1 简介 grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并打印匹配的行）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 Unix 的 grep 家族包括 grep、egrep 和 fgrep。egrep和 fgrep 的命令只跟 grep 有很小不同。 egrep 是 grep 的扩展，支持更多的 re 元字符； fgrep 就是 fixed grep 或 fast grep，它们把所有的字母都看作单词，也就是说，正则表达式中的元字符表示回其自身的字面意义，不再特殊。 Linux 使用 GNU 版本的 grep。它功能更强，可以通过 -G、-E、-F 命令行选项来使用 egrep 和 fgrep 的功能。 2 grep 常见用法 [root@www ~]# grep [-acinv] [--color=auto] &amp;#39;搜寻字符串&amp;#39; filename 选项与参数： -a ：将 binary 文件以 text 文件的方式搜寻数据 -c ：计算找到 &amp;#39;搜寻字符串&amp;#39; 的次数 -i ：忽略大小写的不同，所以大小写视为相同 -n ：顺便输出行号 -v ：反向选择，亦即显示出没有 &amp;#39;搜寻字符串&amp;#39; 内容的那一行！ --color=auto ：可以将找到的关键词部分加上颜色的显示喔！ 将 /etc/passwd 文件中有 root 的行取出来 # grep root /etc/passwd root:x:0:0:root:/root:/bin/bash operator:x:11:0:operator:/root:/sbin/nologin 或 # cat /etc/passwd | grep root root:x:0:0:root:/root:/bin/bash operator:x:11:0:operator:/root:/sbin/nologin 将 /etc/passwd 文件中有 root 的行取出来，同时显示这些行在 /etc/passwd 的行号 # grep -n root /etc/passwd 1:root:x:0:0:root:/root:/bin/bash 30:operator:x:11:0:operator:/root:/sbin/nologin 将 /etc/passwd 文件中没有 root 的行取出来 # grep -v root /etc/passwd root:x:0:0:root:/root:/bin/bash operator:x:11:0:operator:/root:/sbin/nologin 将 /etc/passwd 中没有出现 root 和 nologin 的行取出来 # grep -v root /etc/passwd | grep -v nologin root:x:0:0:root:/root:/bin/bash operator:x:11:0:operator:/root:/sbin/nologin 用 dmesg 列出核心信息，再以 grep 找出内含 eth 的那行，将匹配到的关键词用彩色标识，且加上行号来表示 [root@www ~]# dmesg | grep -n --color=auto &amp;#39;eth&amp;#39; 247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10 248:eth0: Identified 8139 chip type &amp;#39;RTL-8139C&amp;#39; 294:eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1 305:eth0: no IPv6 routers present # 你会发现除了 eth 会有特殊颜色来表示之外，最前面还有行号喔！ 在关键字的显示方面，grep 可以使用 --color=auto 来将关键字部分使用颜色显示。这可是个很不错的功能啊！但是如果每次使用 grep 都得要自行加上 --color=auto 又显的很麻烦，此时那个好用的 alias 就得来处理一下啦！你可以在 ~/.bashrc 内加上这行：alias grep=&#39;grep --color=auto&#39;，再以 source ~/.bashrc 来立即生效即可喔！这样每次运行 grep 他都会自动帮你加上颜色显示啦。 用 dmesg 列出核心信息，再以 grep 找出内含 eth 的行，关键词所在行的前两行与后三行也一起提取出来显示 [root@www ~]# dmesg | grep -n -A3 -B2 --color=auto &amp;#39;eth&amp;#39; 245-PCI: setting IRQ 10 as level-triggered 246-ACPI: PCI Interrupt 0000:00:0e.0[A] -&amp;gt; Link [LNKB] ... 247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10 248:eth0: Identified 8139 chip type &amp;#39;RTL-8139C&amp;#39; 249-input: PC Speaker as /class/input/input2 250-ACPI: PCI Interrupt 0000:00:01.4[B] -&amp;gt; Link [LNKB] ... 251-hdb: ATAPI 48X DVD-ROM DVD-R-RAM CD-R/RW drive, 2048kB Cache, UDMA(66) # 如上所示，你会发现关键字 247 所在的前两行及 248 后三行也都被显示出来！ # 这样可以让你将关键字前后数据捉出来进行分析啦！ 根据文件内容递归查找目录 # grep &amp;#39;energywise&amp;#39; * # 在当前目录搜索带&amp;#39;energywise&amp;#39;行的文件 # grep -r &amp;#39;energywise&amp;#39; * # 在当前目录及其子目录下搜索&amp;#39;energywise&amp;#39;行的文件 # grep -l -r &amp;#39;energywise&amp;#39; * # 在当前目录及其子目录下搜索&amp;#39;energywise&amp;#39;行的文件，但是只显示匹配的文件 这几个命令很实用，是查找文件的利器。 3 grep 与正则表达式 3.1 字符类 字符类的搜索：想要搜寻 test 或 taste 这两个单词时，可以发现到，其实它们有共通的 &#39;t?st&#39; 存在，这个时候可以这样来搜寻 [root@www ~]# grep -n &amp;#39;t[ae]st&amp;#39; regular_express.txt 8:I can&amp;#39;t finish the test. 9:Oh! The soup taste good. 其实 [] 里面不论有几个字节，他都谨代表某 一个 字节， 所以，上面的例子说明了，我需要的字符串是 tast 或 test 两个字串而已！ 字符类的反向选择 [^]：如果想要搜索到有 oo 的行，但不想要 oo 前面有 g，如下 [root@www ~]# grep -n &amp;#39;[^g]oo&amp;#39; regular_express.txt 2:apple is my favorite food. 3:Football game is not use feet only. 18:google is the best tools for search keyword. 19:goooooogle yes! 第 2,3 行没有疑问，因为 foo 与 Foo 均可被接受！但是第 18 行明明有 google 的 goo 啊，别忘记了，因为该行后面出现了 tool 的 too 啊！所以该行也被列出来，也就是说， 18 行里面虽然出现了我们所不要的项目(goo) 但是由於有需要的项目(too) ， 因此符合字串搜寻规则！至于第 19 行，同样的，因为 goooooogle 里面的 oo 前面可能是 o ，例如： go(ooo)oogle ，所以，这一行也是符合需求的！ 字符类的连续：再来，假设我 oo 前面不想要有小写字节，所以，我可以这样写 [^abcd....z]oo， 但是这样似乎不怎么方便，由於小写字节的 ASCII 上编码的顺序是连续的，因此，我们可以将之简化为底下这样 [root@www ~]# grep -n &amp;#39;[^a-z]oo&amp;#39; regular_express.txt 3:Football game is not use feet only. 也就是说，当我们在一组集合字节中，如果该字节组是连续的，例如大写英文、小写英文、数字等等，就可以使用 [a-z], [A-Z], [0-9] 等方式来书写，那么如果我们的要求字串是数字与英文呢？ 呵呵！就将他全部写在一起，变成：[a-zA-Z0-9]。我们要取得有数字的那一行，就这样 [root@www ~]# grep -n &amp;#39;[0-9]&amp;#39; regular_express.txt 5:However, this dress is about $ 3183 dollars. 15:You are the best is mean you are the no. 1. 3.2 行首 ^ 与行尾 $ 行首字符：如果我想要让 the 只在行首列出呢？这个时候就得要使用定位字节了！我们可以这样做 [root@www ~]# grep -n &amp;#39;^the&amp;#39; regular_express.txt 12:the symbol &amp;#39;*&amp;#39; is represented as start. 此时，就只剩下第 12 行，因为只有第 12 行的行首是 the 开头。此外， 如果我想要开头是小写字节的那一行就列出呢？可以这样 [root@www ~]# grep -n &amp;#39;^[a-z]&amp;#39; regular_express.txt 2:apple is my favorite food. 4:this dress doesn&amp;#39;t fit me. 10:motorcycle is cheap than car. 12:the symbol &amp;#39;*&amp;#39; is represented as start. 18:google is the best tools for search keyword. 19:goooooogle yes! 20:go! go! Let&amp;#39;s go. 如果我不想要开头是英文字母，则可以这样 [root@www ~]# grep -n &amp;#39;^[^a-zA-Z]&amp;#39; regular_express.txt 1:&amp;quot;Open Source&amp;quot; is a good mechanism to develop programs. 21:# I am VBird 提示：^符号，在 [] 之内与之外是不同的！ 在 [] 内代表反向选择，在 [] 之外则代表定位在行首的意义！ 如果想要找出行尾结束为小数点 . 的那一行 [root@www ~]# grep -n &amp;#39;\.$&amp;#39; regular_express.txt 1:&amp;quot;Open Source&amp;quot; is a good mechanism to develop programs. 2:apple is my favorite food. 3:Football game is not use feet only. 4:this dress doesn&amp;#39;t fit me. 10:motorcycle is cheap than car. 11:This window is clear. 12:the symbol &amp;#39;*&amp;#39; is represented as start. 15:You are the best is mean you are the no. 1. 16:The world &amp;lt;Happy&amp;gt; is the same with &amp;quot;glad&amp;quot;. 17:I like dog. 18:google is the best tools for search keyword. 20:go! go! Let&amp;#39;s go. 特别注意到，因为小数点具有其他意义(底下会介绍)，所以必须要使用转义字符 \ 来加以解除其特殊意义！ 找出空白行： [root@www ~]# grep -n &amp;#39;^$&amp;#39; regular_express.txt 22: 因为只有行首跟行尾 ^$，所以，这样就可以找出空白行啦！ 3.3 任意一个字节 . 与重复字节 * 这两个符号在正则表达式的意义如下 . (小数点)：代表一定有一个任意字节的意思； * (星号)：代表重复前一个字符，0 到无穷多次的意思，为组合形态。 假设需要找出 g??d 的字符串，亦即共有四个字节， 起头是 g 而结束是 d，可以这样做： [root@www ~]# grep -n &amp;#39;g..d&amp;#39; regular_express.txt 1:&amp;quot;Open Source&amp;quot; is a good mechanism to develop programs. 9:Oh! The soup taste good. 16:The world &amp;lt;Happy&amp;gt; is the same with &amp;quot;glad&amp;quot;. 因为强调 g 与 d 之间一定要存在两个字节，因此，第 13 行的 god 与第 14 行的 gd 就不会被列出来啦！ 如果想要列出有 oo, ooo, oooo 等等的数据， 也就是说，至少要有两个(含) o 以上，该如何是好？ 因为 * 代表的是“重复 0 个或多个前面的 RE 字符”的意义， 因此，o*代表的是“拥有空字节或一个 o 以上的字节”，因此，grep -n &#39;o*&#39; regular_express.txt将会把所有的数据都列印出来终端上！ 当我们需要“至少两个 o 以上的字串”时，就需要 ooo*，即 [root@www ~]# grep -n &amp;#39;ooo*&amp;#39; regular_express.txt 1:&amp;quot;Open Source&amp;quot; is a good mechanism to develop programs. 2:apple is my favorite food. 3:Football game is not use feet only. 9:Oh! The soup taste good. 18:google is the best tools for search keyword. 19:goooooogle yes! 如果想要字串开头与结尾都是 g，但是两个 g 之间仅能存在至少一个 o，亦即是 gog, goog, gooog.... 等等，那该如何？ [root@www ~]# grep -n &amp;#39;goo*g&amp;#39; regular_express.txt 18:google is the best tools for search keyword. 19:goooooogle yes! 如果想要找出 g 开头与 g 结尾的行，当中的字符可有可无 [root@www ~]# grep -n &amp;#39;g.*g&amp;#39; regular_express.txt 1:&amp;quot;Open Source&amp;quot; is a good mechanism to develop programs. 14:The gd software is a library for drafting programs. 18:google is the best tools for search keyword. 19:goooooogle yes! 20:go! go! Let&amp;#39;s go. 因为是代表 g 开头与 g 结尾，中间任意字节均可接受，所以，第 1, 14, 20 行是可接受的！ 这个 .* 的 RE 表示任意字符是很常见的。 如果想要找出“任意数字”的行，因为仅有数字，所以就成为 [root@www ~]# grep -n &amp;#39;[0-9][0-9]*&amp;#39; regular_express.txt 5:However, this dress is about $ 3183 dollars. 15:You are the best is mean you are the no. 1. 3.4 限定连续 RE 字符范围 {} 我们可以利用 . 与 RE 字符及 * 来配置 0 个到无限多个重复字节， 那如果我想要限制一个范围区间内的重复字节数呢？ 举例来说，我想要找出两个到五个 o 的连续字串，该如何做？这时候就得要使用到限定范围的字符 {} 了。 但因为 { 与 } 的符号在 shell 是有特殊意义的，因此，我们必须要使用字符 \ 来让他失去特殊意义才行1。 至於 {} 的语法是这样的，假设我要找到两个 o 的字串，可以是 [root@www ~]# grep -n &amp;#39;o\{2\}&amp;#39; regular_express.txt 1:&amp;quot;Open Source&amp;quot; is a good mechanism to develop programs. 2:apple is my favorite food. 3:Football game is not use feet only. 9:Oh! The soup taste good. 18:google is the best tools for search ke 19:goooooogle yes! 假设我们要找出 g 后面接 2 到 5 个 o ，然后再接一个 g 的字串，会是这样 [root@www ~]# grep -n &amp;#39;go\{2,5\}g&amp;#39; regular_express.txt 18:google is the best tools for search keyword. 如果我想要的是 2 个 o 以上的 goooo....g 呢？除了可以是 gooo*g ，也可以是 [root@www ~]# grep -n &amp;#39;go\{2,\}g&amp;#39; regular_express.txt 18:google is the best tools for search keyword. 19:goooooogle yes! 3.5 扩展 grep (grep -E 或者 egrep) 使用扩展 grep 的主要好处是增加了额外的正则表达式元字符集。 打印所有包含 NW 或 EA 的行。如果不是使用 egrep，而是 grep，将不会有结果查出。 # egrep &amp;#39;NW|EA&amp;#39; testfile northwest NW Charles Main 3.0 .98 3 34 eastern EA TB Savage 4.4 .84 5 20 对于标准 grep，如果在扩展元字符前面加 \，grep 会自动启用扩展选项 -E。 #grep &amp;#39;NW\|EA&amp;#39; testfile northwest NW Charles Main 3.0 .98 3 34 eastern EA TB Savage 4.4 .84 5 20 搜索所有包含一个或多个 3 的行 # egrep &amp;#39;3&#43;&amp;#39; testfile # grep -E &amp;#39;3&#43;&amp;#39; testfile # grep &amp;#39;3\&#43;&amp;#39; testfile #这3条命令将会 northwest NW Charles Main 3.0 .98 3 34 western WE Sharon Gray 5.3 .97 5 23 northeast NE AM Main Jr. 5.1 .94 3 13 central CT Ann Stephens 5.7 .94 5 13 搜索所有包含 0 个或 1 个小数点字符的行 # egrep &amp;#39;2\.?[0-9]&amp;#39; testfile # grep -E &amp;#39;2\.?[0-9]&amp;#39; testfile # grep &amp;#39;2\.\?[0-9]&amp;#39; testfile #首先含有2字符，其后紧跟着0个或1个点，后面再是0和9之间的数字。 western WE Sharon Gray 5.3 .97 5 23 southwest SW Lewis Dalsass 2.7 .8 2 18 eastern EA TB Savage 4.4 .84 5 20 搜索一个或者多个连续的 no 的行 # egrep &amp;#39;(no)&#43;&amp;#39; testfile # grep -E &amp;#39;(no)&#43;&amp;#39; testfile # grep &amp;#39;\(no\)\&#43;&amp;#39; testfile #3个命令返回相同结果， northwest NW Charles Main 3.0 .98 3 34 northeast NE AM Main Jr. 5.1 .94 3 13 north NO Margot Weber 4.5 .89 5 9 3.6 不使用正则表达式 fgrep 查询速度比 grep 命令快，但是不够灵活：它只能找固定的文本，而不是规则表达式。如果你想在一个文件或者输出中找到包含星号字符的行 fgrep &amp;#39;*&amp;#39; /etc/profile for i in /etc/profile.d/*.sh ; do # 或 grep -F &amp;#39;*&amp;#39; /etc/profile for i in /etc/profile.d/*.sh ; do 4 选项 -a 不要忽略二进制数据。 -A&amp;lt;显示列数&amp;gt; 除了显示符合范本样式的那一行之外，并显示该行之后的内容。 -b 在显示符合范本样式的那一行之外，并显示该行之前的内容。 -c 计算符合范本样式的列数。 -C&amp;lt;显示列数&amp;gt;或-&amp;lt;显示列数&amp;gt; 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。 -d&amp;lt;进行动作&amp;gt; 当指定要查找的是目录而非文件时，必须使用这项参数，否则 grep 命令将回报信息并停止动作。 -e&amp;lt;范本样式&amp;gt; 指定字符串作为查找文件内容的范本样式。 -E 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。 -f&amp;lt;范本文件&amp;gt; 指定范本文件，内容有一或多个范本样式，grep 查找符合范本条件的文件内容，格式为每一列的范本样式。 -F 将范本样式视为固定字符串的列表。 -G 将范本样式视为普通的表示法来使用。 -h 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。 -H 在显示符合范本样式的那一列之前，标示该列的文件名称。 -i 胡列字符大小写的差别。 -l 列出文件内容符合指定的范本样式的文件名称。 -L 列出文件内容不符合指定的范本样式的文件名称。 -n 在显示符合范本样式的那一列之前，标示出该列的编号。 -q 不显示任何信息。 -R/-r 此参数的效果和指定 “-d recurse” 参数相同。 -s 不显示错误信息。 -v 反转查找。 -w 只显示全字符合的列。 -x 只显示全列符合的列。 -y 此参数效果跟 “-i” 相同。 -o 只输出文件中匹配到的部分。 5 参考 http://vbird.dic.ksu.edu.tw/linux_basic/0330regularex_2.php http://www.cnblogs.com/stephen-liu74/archive/2011/11/14/2243694.html http://man.linuxde.net/grep http://blog.csdn.net/wscdylzjy/article/details/45645045 http://blog.csdn.net/qing619a/article/details/8471850：find 是搜索文件，而 grep 是搜索文件中的内容； http://www.itshouce.com.cn/linux/linux-grep.html 普通的正则表达式中不需要这样转义↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[SAS 定义宏的三种用法 z]]></title>
    	<url>/tech/2013/01/09/2013-01-09-sas-macro-definition/</url>
		<content type="text"><![CDATA[[原文地址：http://www.cda.cn/view/18718.html SAS 中定义宏变量方法主要有三种。 %LET 定义 这是最常用的方法。语法过程为 %let 宏变量名=值; 调用宏变量： &amp;amp;宏变量名 以分号作为宏变量定义的结束。 在定义时候不要加引号，系统会把引号当作宏变量的值之一。 宏变量的作用域分为全局与局部。 一种在程序（这里一般是指一个宏程序）外定义，则在整个 SAS 任何地方都可以被调用。 另外一种则是在程序内部，则作用在该程序出现宏变量后面，在程序外则不可以被调用，否则会出错。（如果一个程序内部定义有宏变量变成全局变量，则需要使用%global） 当宏变量一个表达式，使用(感觉有错)： %str() %let print=%str(proc print; run;); 如果宏变量是数值计算表达式，则需要根据数值类型使用二个不同的宏变量： %eval_r() 用于计算整数例如： %eval_r(1&#43;2) %sysevalf() 用于计算浮点数或者空值，例如：%eval_r(1.0&#43;2.3) data 步中定义 call symput() 这个方法只允许在data步中，这个一定需要注意。 语法过程： call symput(&amp;quot;宏变量名&amp;quot;,值); 在 DATA 过程中，宏变量可以包括常值、变量值。或者函数赋予宏变量。 SQL 过程中定义 在使用 SQL 过程中定义宏变量,语法过程为： SELECT 宏值 INTO :宏变量名 SEPARATED BY &amp;#39;分隔符号&amp;#39; 使用一个案例过程说明，以后我们再详细介绍其中的一个技巧： data test; input var @@; datalines; 1 2 3 4 5 ; run; proc sql noprint ; select var into :macro_var separated by &amp;#39; &amp;#39; from test; quit;]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Matlab 中的 eval 与 feval 函数]]></title>
    	<url>/tech/2012/12/07/matlab-eval-feval/</url>
		<content type="text"><![CDATA[[eval函数用法简介 原文地址：http://www.ilovematlab.cn/thread-53554-1-1.html eval(exp expression。例如，把 August1.mat 到 August3.mat 加载到 MATLAB workspace： for d=1:3 s = [&#39;load August&#39; int2str(d) &#39;.mat&#39;] eval(s) end 上面的部分代码也可以写成 s = [&#39;load August&#39;, int2str(d), &#39;.mat&#39;]，也就是中间用逗号隔开，这样才能把三部分合成一个字符串，以下是被执行的 s 语句： s = load August1.mat s = load August2.mat s = load August3.mat feval函数用法简介 原文地址：http://www.madio.net/thread-170200-1-1.html [y1,..,yn] = FEVAL(F,x1,...,xn)，F 是需要使用函数的函数名，或者句柄(见下面程序注释)；xi 是函数的参数，yi 是函数的返回值。 举例，假设需要调用的函数 foo 定义如下： function x=foo(a,b) x=a*b; 若在 main 函数中用 feval 调用 foo，可以有以下几种方式： result=feval(&#39;foo&#39;,3,15); result=feval(@foo,3,16); % 这里@foo是函数foo的句柄 若调用的函数要作为main的参数，则 function result=main(f) result=feval(f,3,10); 然后调用 main 时将 &#39;foo&#39; 传入即可： &amp;gt;&amp;gt;main(&#39;foo&#39;); feval和eval的区别 原文地址：http://powerelite.blog.163.com/blog/static/42965891201272725641245/ feval 和 eval 运行区别之一：feval 的 FN 不能是表达式，其 FN 只接受函数名。函数 eval 给 MATLAB 提供 宏 的能力，该函数提供了将用户创建的函数名传给其它函数能力，以便求值。函数 feval 与 eval 类似，但在用法上有更多的限制。feval(&#39;fun&#39;,x) 求由字符串 &#39;fun&#39; 给定的函数值，其输入参量是变量 x，即 feval_r(&#39;fun&#39;,x) 等价于求 fun(x) 值，注意下面代码中的运行错误解决方法见上面 feval 函数的三种使用方法。 format short x=pi/4; Ve=eval(&#39;1&#43;sin(x)&#39;) Ve = 1.7071 Vf=feval(&#39;1&#43;sin(x)&#39;,x) ??? Error using ==&amp;gt; feval Invalid function name &#39;1&#43;sin(x)&#39;. 关于带参数的积分问题 原文地址：http://forum.chinavib.com/thread-42369-1-1.html 有不少人常问带参数的积分问题该如何处理,现举一个例子，希望能起到抛砖引玉的作用。 %%%--------------------------------------------%%% 例如以下问题: 函数为 y=sin(k.*x).*x.^2，对x积分， 积分区域为【1，5】，目的是要画 k 和 y 的图形. %%%============================================%%% %%% 作k的一个循环, k作为 inline函数的参数即可. clear all k=linspace(0,5); for i=1:length(k) kk=k(i); fun=strcat(&#39;sin(&#39;,num2str(kk),&#39;*x).*x.^2&#39;); y(i)=quadl(inline(fun),1,5); end plot(k,y) %%%============================================%%% 注意：这个程序的特别意义在于，对于任何复杂的、无显式积分表达式的带参数积分问题具有通用性，我主要是针对此而写的。 Matlab中函数调用及feval函数，带参数积分问题 原文地址：http://www.ilovematlab.cn/thread-36666-1-1.html function InlineSubAnonymousNestedDemo % 带参数的积分问题 % 例如 % 函数为 y=sin(k.*x).*x.^2，对x积分， % 积分区域为【1，5】，目的是要画 k 和 y 的图形. % 例子意义在于,对于一些复杂的、无显式积分表达式的带参数积分问题具有通用性 % 问题来源于振动论坛原xjzuo版主发的一个帖子； % 原帖地址：http://www.chinavib.com/forum/thread-42369-1-2.html%% 用inline解决 tic; k=linspace(0,5); y1 = zeros(size(k)); for i=1:length(k) kk=k(i); fun=inline([&#39;sin(&#39;,num2str(kk),&#39;*x).*x.^2&#39;]); y1(i)=quadl(fun,0,5); end time = toc; disp([&#39;用inline方法的时间是：&#39;,num2str(time),&#39;秒!&#39;])%% 用anonymous function 解决 tic; f=@(k) quadl(@(x) sin(k.*x).*x.^2,0,5); kk=linspace(0,5); y2=zeros(size(kk)); for ii=1:length(kk) y2(ii)=f(kk(ii)); end time = toc; disp([&#39;用anonymous function方法的时间是：&#39;,num2str(time),&#39;秒!&#39;])%% 用nested function解决 function y = ParaInteg(k) y=quadl(@(x) sin(k.*x).*x.^2 ,0,5); end tic; kk=linspace(0,5); y3=zeros(size(kk)); for ii=1:length(kk) y3(ii)=ParaInteg(kk(ii)); end time = toc; disp([&#39;用nested function方法的时间是：&#39;,num2str(time),&#39;秒!&#39;])%% 用 arrayfun &#43; anonymous function 解决 tic;y4 = arrayfun(@(k) quadl(@(x) sin(k.*x).*x.^2,0,5),linspace(0,5));time = toc; disp([&#39;用arrayfun &#43; anonymous function方法的时间是：&#39;,num2str(time),&#39;秒!&#39;]) plot(kk,y2); end]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[R 中字符串处理和 grep 的用法 z]]></title>
    	<url>/tech/2012/12/05/r-string-grep/</url>
		<content type="text"><![CDATA[[原文地址：http://jliblog.com/archives/26 R通常被用来进行数值计算比较多，字符串处理相对较少，而且关于字符串的函 grep的全称是global search regular expression and print out the line，是Unix下一种强大的文本搜索工具，可以通过正则表达式搜索文本，并把匹配的行打印出来，包括grep、egrep和fgrep（egrep是扩展的grep，fgrep是快速的搜寻方式并没有真正利用正则表达式）。Linux下使用GNU版的grep，该套规范也被广泛地使用，R中的grep函数就是其中之一。 grep的核心就是正则表达式（Regular Expressions，通常缩写为regex），所谓正则表达式，就是用某种模式去匹配一类字符串的一个公式，很多文本编辑器或者程序语言都支持该方式进行字符串的操作，最开始是由上文介绍的Unix工具grep之类普及的，后来得到广泛应用。尤其是Perl语言中将正则表达式发挥到了极致。 R中的正则表达式非常专业，从grep系列函数的参数就可以看出，有个参数“extended”，默认为T，表示使用扩展 grep，也就是 egrep，如果选择为F就表示基础的 grep，不过该种方式不被 R 推荐，即使使用了也会出现警告，实际上 grep 能做的egrep也都能做，而且还要简单不少。我刚开始在egrep中使用总是不能通过，后来发现其实egrep中更简单，很多时候直接写在[]内就行。还有一个参数“perl”，默认为F，如果选择T表示使用Perl的正则表达式规则，功能更加强大，不过如果没有专门学过Perl语言的话用egrep也就够了。另一个参数“fixed”虽然描述的不是同一个东西，但是也很相关，选择之后就会进行精确的匹配，不再使用正则表达式的规则，在效率上会快很多，我觉得这个可能就是fgrep。R的帮助文档中也明确说明了这三个参数实际上代表了四种模式，常规grep、扩展grep、Perl正则表达式、精确匹配，使用者可以根据具体的含义选择自己需要的，如果参数设置互有冲突，会自动忽略后面的参数，并会在Warning中明确指出。 grep系列函数其实包括grep、grepl、sub、gsub、regexpr、gregexpr，他们的参数很类似，在R中也是把帮助文档集成在了一起，查找任意一个都会得到一个统一的文档。里面对各个参数也是一起介绍的，除了刚才说的三个以外，第一个参数就是最重要的“pattern”，这是一个字符串，直接表示正则表达式，根据模式的不同注意规则就行，另外有个“x”表示要查找的向量，这也是R中的独特之处，不是查找文件，而是查找向量，该处也可以只输入一个字符串，就成了基础的字符串处理函数。对于grep函数，结果只有匹配或者不匹配，因此匹配时输出向量中该元素的下标，如果是单个字符就输出1，对于grepl，和grep其实一样，不过输出的是逻辑值，匹配就是T，不匹配就是F。参数“value”默认为F，输出的值就是刚才说的元素下标或者逻辑值，如果改成T，就会输出查找的字符串。还有一个参数“ignore.case”，默认是F，表示大小写敏感，可以改为T，表示大小写不敏感。参数“useBytes”默认是F，表示按字符查找，如果是T则表示按字节查找，对于中文字符影响还是很大的。参数“invert ”默认为F，表示正常的查找，如果为T则查找模式的补集。像sub和gsub这样的替换函数，还多一个参数“replacement”，用来表示替换的字符。 这些函数的参数都比较类似，但是输出各不一样，grep输出向量的下标，实际上就是找到与没找到，grepl返回的逻辑值更能说明问题。sub是一个很强大的替换函数，远胜过substr，正则表达式中可以设置非常灵活的规则，然后返回被替换后的字符串，如果正则表达式写得好，基本可以解决所有子字符串的问题。sub函数和gsub函数唯一的差别在于前者匹配第一次符合模式的字符串，后者匹配所有符合模式的字符串，也就是说在替换的时候前者只替换第一次符合的，后者替换所有符合的。regexpr和gregexpr被使用的似乎比较多，因为它们很像其他语言中的instr函数，可以查找到某些字符在字符串中出现的位置，不过我觉得用处并不是很大，因为通常情况下寻找某字符位置的目的就是为了做相关处理，而sub都能搞定。regexpr和gregexpr的关系和sub与gsub差不多，gregexpr操作向量时会返回列表。 以上就是grep系列函数的一些用法，根据例子可以很方便地使用，个人建议使用参数“pattern”和“x”就行（sub和gsub当然还有replacement），其他的都用默认的。在pattern中按照egrep的规则写正则表达式，基本上可以解决所有的字符串处理问题。只需要对正则表达式有简单的了解，就可以得到R中这些强大的功能。关于正则表达式的用法就在后文中分解了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[R 中正则表达式简介 z]]></title>
    	<url>/tech/2012/12/05/r-reg-exp/</url>
		<content type="text"><![CDATA[[原文地址：http://jliblog.com/archives/28 R 中的grep、grepl、sub、gsub、regexpr、greg Perl 语言的规则。在这里，我们以 R 中的sub函数为例（因为该函数可以返回替换字符串后的具体内容）介绍正则表达式的用法。 对该函数的逻辑参数都使用默认值（ignore.case = FALSE，表示大小写敏感；extended = TRUE，表示使用egrep规则；perl = FALSE，表示不使用Perl规则；fixed = FALSE，表示不使用精确匹配；useBytes = FALSE，表示按字符匹配）。另外三个中，pattern为字符串表示正则表达式，replacement也是字符串表示替换的内容，x为字符型向量表示被替换的字符向量。该函数会根据pattern的规则对x中各元素进行搜索，遇到符合条件的第一个子字符串的位置（gsub是替换所有符合条件的），用replacement替换该子字符串，返回替换后的结果，和x的结构相同。为了清晰地介绍例子，我们对replacement统一赋值为“”，相当于去掉搜寻出来的子字符串。例如sub(&amp;quot;a&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;abcd&amp;quot;,&amp;quot;dcba&amp;quot;))，将向量中的两个字符串中的a都去掉了，返回[1] &amp;quot;bcd&amp;quot; &amp;quot;dcb&amp;quot;。该例中的&amp;quot;a&amp;quot;只是一个字符，并不是正则表达式，真正的正则表达式依靠元字符进行灵活的匹配。 “^”匹配一个字符串的开始，比如sub(&amp;quot;^a&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;abcd&amp;quot;,&amp;quot;dcba&amp;quot;))，表示将开头为a的字符串中的a替换成空，在返回值中可以发现后面出现的a并没有被替换。如果要将开头的一个字符串替换，简单地写成“^ab”就行。 “$”匹配一个字符串的结尾，比如sub(&amp;quot;a$&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;abcd&amp;quot;,&amp;quot;dcba&amp;quot;))表示将以a结尾的字符串中的a替换成空。&amp;quot;.&amp;quot;表示除了换行符以外的任一字符，比如sub(&amp;quot;a.c&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;abcd&amp;quot;,&amp;quot;sdacd&amp;quot;))。“*”表示将其前的字符进行0个或多个的匹配，比如sub(&amp;quot;a*b&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;aabcd&amp;quot;,&amp;quot;dcaaaba&amp;quot;))。类似地，“?”匹配0或1个正好在它之前的那个字符，“&#43;”匹配1或多个正好在它之前的那个字符。“.*”可以匹配任意字符，比如sub(&amp;quot;a.*e&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;abcde&amp;quot;,&amp;quot;edcba&amp;quot;))。 “|”表示逻辑的或，比如sub(&amp;quot;ab|ba&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;abcd&amp;quot;,&amp;quot;dcba&amp;quot;))，可以替换ab或者ba。“^”还可以表示逻辑的补集，需要写在“[]”中，比如sub(&amp;quot;[^ab]&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;abcd&amp;quot;,&amp;quot;dcba&amp;quot;))，由于sub只替换搜寻到的第一个，因此这个例子中用gsub效果更好。 “[]”还可以用来匹配多个字符，如果不使用任何分隔符号，则搜寻这个集合，比如在sub(&amp;quot;[ab]&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;abcd&amp;quot;,&amp;quot;dcba&amp;quot;))中，和&amp;quot;a|b&amp;quot;效果一样。“[-]”的形式可以匹配一个范围，比如sub(&amp;quot;[a-c]&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;abcde&amp;quot;,&amp;quot;edcba&amp;quot;))匹配从a到c的字符，sub(&amp;quot;[1-9]&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;ab001&amp;quot;,&amp;quot;001ab&amp;quot;))匹配从1到9的数字。 以上是最基础的正则表达式元字符，在一些正则表达式的书籍和资料中有非常详细的介绍。最后需要提一下的是“贪婪”和“懒惰”的匹配规则。默认情况下是匹配尽可能多的字符，是为贪婪匹配，比如sub(&amp;quot;a.*b&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;aabab&amp;quot;,&amp;quot;eabbe&amp;quot;))，默认匹配最长的a开头b结尾的字串，也就是整个字符串。如果要进行懒惰匹配，也就是匹配最短的字串，只需要在后面加个“?”，比如sub(&amp;quot;a.*?b&amp;quot;,&amp;quot;&amp;quot;,c(&amp;quot;aabab&amp;quot;,&amp;quot;eabbe&amp;quot;))，就会匹配最开始找到的最短的a开头b结尾的字串。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[R 的内存管理和垃圾清理 z]]></title>
    	<url>/tech/2012/12/05/r-mem-gc/</url>
		<content type="text"><![CDATA[[原文地址：http://jliblog.com/archives/276 写 R 程序的人，相信都会遇到过 “cannot allocate vector of size” 或者 “无法 这样的错误。原因很简单，基本都是产生一个大矩阵等对象时发生的，最干脆的解决办法有两种，第一种是加大内存换64位系统，第二种是改变算法避免如此大的对象。第一种办法，是最好的办法，不过大对象的需求是没有止尽的，终究不是长久之道。第二种办法是最好的思路，无论多么大的对象都是可以弄小的，无非就是分而治之、时间换空间等，对算法的研究也是没有止尽的。 升级硬件和改进算法是解决内存问题的永恒的办法，超出了本文想要表述的范围。在这里，只是简单谈谈R语言的内存管理和垃圾清理机制，只有对这些有所了解，才能对任何问题都能找到针对性的解决办法。 相信所有人在遇到无法分配矢量这一问题后，都能很快地找到改变“–max-mem-size”（假设都是在Windows下）或者“memory.limit”的方法，的确，这是最直接的方法。因为出现新对象无法分配内存的直接原因就是内存不够，R获取内存的方式和其他应用程序一样，都是向操作系统要内存，如果无法获取连续的某个大小的内存空间，就会出现无法分配内存的错误。由于大家使用R时通常都是自动安装自动运行，操作系统愿意分配给R多少内存都是采用的默认设置，在R中使用命令memory.size(NA)或者memory.limit()可以看到当前设置下操作系统能分配给R的最大内存是多少。同时可以使用memory.size(F)查看当前R已使用的内存，memory.size(T)查看已分配的内存（注意刚开始时已使用内存和已分配内存是同步增加的，但是随着R中的垃圾被清理，已使用内存会减少，而已分配给R的内存一般不会改变。）。如果memory.limit()得到的数是一个很小的内存，说明操作系统太小气了，留那么多内存给别的程序用不给R。解决办法很简单，就是打开R时不通过双击图标，而是在“运行”中输入“Rgui –max-mem-size 2Gb”（假设要分配2G内存且在环境变量中正确设置了R的安装文件夹），在运行memory.limit()就会发现内存加大了，其实更简单的方法是直接在R中运行memory.limit(2000)，效果一模一样，而且不用重启R。 可惜大多数情况下改变这个值也不会有效果，因为这个值已经足够大，那么无法分配内存的原因不是操作系统小气对R不公，而是它确实拿不出来，谁找它要也拿不出来。这个时候就需要了解R的内存管理到底是怎么回事了。 R的操作基本都是通过变量来实现的，变量可以是各种各样的对象类型，R中的对象（比如矩阵）在内存中存于两种不同的地方，一种是堆内存（heap），其基本单元是“Vcells”，每个大小为8字节，新来一个对象就会申请一块空间，把值全部存在这里，和C里面的堆内存很像。第二种是地址对（cons cells），和LISP里的cons cells道理一样，主要用来存储地址信息，最小单元一般在32位系统中是28字节、64位系统中是56字节。在R中，可以通过ls()来查看当前所有对象名，对于每一个对象，可以通过object.size(x)来查看其占用内存的大小。 如果是因为当前对象占用内存过多，那么可以通过处理对象来获取更大的可用内存。一个很有用的方法是改变对象的存储模式，通过storage.mode(x)可以看到某个对象的存储模式，比如某个矩阵默认就是“double”的，如果这个矩阵的数值都是整数甚至0-1，完全没必要使用double来占用空间，可以使用storage.mode(x） &amp;lt;- &amp;quot;integer&amp;quot;将其改为整数型，可以看到该对象的大小会变为原来的一半。 对于当前对象占用内存过多的情况，一个很主要的原因就是在写程序的过程中造成了太多的中间对象，R是一个很方便的语言，大家使用它一般都是写各种复杂的模型和算法，很多问题构造几个矩阵经过一系列的矩阵运算就可以很快解决，但是这些辅助算法的大矩阵如果不清理，就会留在系统中占内存。因此在写程序中对于中间对象，经常使用rm(x)是一个很好的习惯，如果是非常重要的信息不想删掉，可以存在硬盘里，比如csv文件或者RSqlite等。 rm()用来删除对象时，只会删除变量的引用，并不会立即清除占用的内存空间，失去引用的对象就成了内存中的垃圾，R清理垃圾的机制和JAVA很像，都是在一定时间内自动发现垃圾再集中清理。所以通过rm()删除对象后在Windows的任务管理器可以看到R进程占用的内存并没有被立即释放，而是过一段时间后才会清理。如果想要删除的对象立刻被清理，可以运行垃圾处理函数gc()，将会立刻释放空间。但是通常不是很必要，因为当内存不够时系统会自动清理垃圾的，我们要做的只是将不再使用的对象rm()掉，在写R程序时应该养成习惯。 很多时候，在程序中尤其是循环里，如果内存处理不当，还没来得及垃圾清理，就会把内存撑爆，因此新建对象时一定要考虑到R的内存管理机制。大家都知道R中矩阵的维度并不需要赋一个固定的值（很多语言的数组长度不能为变量），这为写程序带来了极大的方便，因此经常在循环中会出现某个矩阵越来越长的情况，实际上，矩阵每增长一次，即使赋给同名的变量，都需要新开辟一块更大的空间，假设初始矩阵为100K，第二个为101K，一直增到120K，那么，将会分别开辟100K、101K一直到120K的连续堆内存，如果一开始就开一块120K的，使之从101K逐渐增长到120K，将会大大地节约内存。cbind函数也是这个道理，所以在循环中要注意不要滥用。 要处理好内存的问题其实很简单，养成随时关注内存的习惯即可，每新建一个对象或者循环赋值的时候适当估算一下所占内存，大内存的中间变量用完后记得清理。如果实在需要新建一个巨大的对象，那么就该考虑一些专门处理大内存对象以及并行处理的包，比如bigmemory等。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Matlab 统计学工具箱]]></title>
    	<url>/tech/2012/12/04/matlab-statistics-toolbox/</url>
		<content type="text"><![CDATA[[表1 概率密度函数 函数名 对应分布的概率密度函数 betapdf 贝塔分布的概率密度函数 binopdf 二项分布的概率密度函数 chi2pdf 卡方分布的概率密度函数 exppdf 指数分布的概率密度函数 fpdf f分布的概率密度函数 gampdf 伽玛分布的概率密度函数 geopdf 几何分布的概率密度函数 hygepdf 超几何分布的概率密度函数 normpdf 正态（高斯）分布的概率密度函数 lognpdf 对数正态分布的概率密度函数 nbinpdf 负二项分布的概率密度函数 ncfpdf 非中心f分布的概率密度函数 nctpdf 非中心t分布的概率密度函数 ncx2pdf 非中心卡方分布的概率密度函数 poisspdf 泊松分布的概率密度函数 raylpdf 雷利分布的概率密度函数 tpdf 学生氏t分布的概率密度函数 unidpdf 离散均匀分布的概率密度函数 unifpdf 连续均匀分布的概率密度函数 weibpdf 威布尔分布的概率密度函数 表2 累加分布函数 函数名 对应分布的累加函数 betacdf 贝塔分布的累加函数 binocdf 二项分布的累加函数 chi2cdf 卡方分布的累加函数 expcdf 指数分布的累加函数 fcdf f分布的累加函数 gamcdf 伽玛分布的累加函数 geocdf 几何分布的累加函数 hygecdf 超几何分布的累加函数 logncdf 对数正态分布的累加函数 nbincdf 负二项分布的累加函数 ncfcdf 非中心f分布的累加函数 nctcdf 非中心t分布的累加函数 ncx2cdf 非中心卡方分布的累加函数 normcdf 正态（高斯）分布的累加函数 poisscdf 泊松分布的累加函数 raylcdf 雷利分布的累加函数 tcdf 学生氏t分布的累加函数 unidcdf 离散均匀分布的累加函数 unifcdf 连续均匀分布的累加函数 weibcdf 威布尔分布的累加函数 表3 累加分布函数的逆函数 函数名 对应分布的累加分布函数逆函数 betainv 贝塔分布的累加分布函数逆函数 binoinv 二项分布的累加分布函数逆函数 chi2inv 卡方分布的累加分布函数逆函数 expinv 指数分布的累加分布函数逆函数 finv f分布的累加分布函数逆函数 gaminv 伽玛分布的累加分布函数逆函数 geoinv 几何分布的累加分布函数逆函数 hygeinv 超几何分布的累加分布函数逆函数 logninv 对数正态分布的累加分布函数逆函数 nbininv 负二项分布的累加分布函数逆函数 ncfinv 非中心f分布的累加分布函数逆函数 nctinv 非中心t分布的累加分布函数逆函数 ncx2inv 非中心卡方分布的累加分布函数逆函数 icdf norminv 正态（高斯）分布的累加分布函数逆函数 poissinv 泊松分布的累加分布函数逆函数 raylinv 雷利分布的累加分布函数逆函数 tinv 学生氏t分布的累加分布函数逆函数 unidinv 离散均匀分布的累加分布函数逆函数 unifinv 连续均匀分布的累加分布函数逆函数 weibinv 威布尔分布的累加分布函数逆函数 表4 随机数生成器函数 函数名 对应分布的随机数生成器 betarnd 贝塔分布的随机数生成器 binornd 二项分布的随机数生成器 chi2rnd 卡方分布的随机数生成器 exprnd 指数分布的随机数生成器 frnd f分布的随机数生成器 gamrnd 伽玛分布的随机数生成器 geornd 几何分布的随机数生成器 hygernd 超几何分布的随机数生成器 lognrnd 对数正态分布的随机数生成器 nbinrnd 负二项分布的随机数生成器 ncfrnd 非中心f分布的随机数生成器 nctrnd 非中心t分布的随机数生成器 ncx2rnd 非中心卡方分布的随机数生成器 normrnd 正态（高斯）分布的随机数生成器 poissrnd 泊松分布的随机数生成器 raylrnd 瑞利分布的随机数生成器 trnd 学生氏t分布的随机数生成器 unidrnd 离散均匀分布的随机数生成器 unifrnd 连续均匀分布的随机数生成器 weibrnd 威布尔分布的随机数生成器 表5 分布函数的统计量函数 函数名 对应分布的统计量 betastat 贝塔分布函数的统计量 binostat 二项分布函数的统计量 chi2stat 卡方分布函数的统计量 expstat 指数分布函数的统计量 fstat f分布函数的统计量 gamstat 伽玛分布函数的统计量 geostat 几何分布函数的统计量 hygestat 超几何分布函数的统计量 lognstat 对数正态分布函数的统计量 nbinstat 负二项分布函数的统计量 ncfstat 非中心f分布函数的统计量 nctstat 非中心t分布函数的统计量 ncx2stat 非中心卡方分布函数的统计量 normstat 正态（高斯）分布函数的统计量 poisstat 泊松分布函数的统计量 raylstat 瑞利分布函数的统计量 tstat 学生氏t分布函数的统计量 unidstat 离散均匀分布函数的统计量 unifstat 连续均匀分布函数的统计量 weibstat 威布尔分布函数的统计量 表6 参数估计函数 函数名 对应分布的参数估计 betafit 贝塔分布的参数估计 betalike 贝塔对数似然函数的参数估计 binofit 二项分布的参数估计 expfit 指数分布的参数估计 gamfit 伽玛分布的参数估计 gamlike 伽玛似然函数的参数估计 mle 极大似然估计的参数估计 normlike 正态对数似然函数的参数估计 normfit 正态分布的参数估计 poissfit 泊松分布的参数估计 unifit 均匀分布的参数估计 weibfit 威布尔分布的参数估计 weiblike 威布尔对数似然函数的参数估计 表7 统计量描述函数 函数名 描述 bootstrap 任何函数的自助统计量 corrcoef 相关系数 cov 协方差 crosstab 列联表 geomean 几何均值 grpstats 分组统计量 harmmean 调和均值 iqr 内四分极值 kurtosis 峰度 mad 中值绝对差 mean 均值 median 中值 moment 样本模量 nanmax 包含缺失值的样本的最大值 Nanmean 包含缺失值的样本的均值 nanmedian 包含缺失值的样本的中值 nanmin 包含缺失值的样本的最小值 nanstd 包含缺失值的样本的标准差 nansum 包含缺失值的样本的和 prctile 百分位数 range 极值 skewness 偏度 std 标准差 tabulate 频数表 trimmean 截尾均值 var 方差 表8 统计图形函数 函数名 描述 boxplot 箱形图 cdfplot 指数累加分布函数图 errorbar 误差条图 fsurfht 函数的交互等值线图 gline 画线 gname 交互标注图中的点 gplotmatrix 散点图矩阵 gscatter 由第三个变量分组的两个变量的散点图 lsline 在散点图中添加最小二乘拟合线 normplot 正态概率图 pareto 帕累托图 qqplot Q-Q图 rcoplot 残差个案次序图 refcurve 参考多项式曲线 refline 参考线 surfht 数据网格的交互等值线图 weibplot 威布尔图 表9 统计过程控制函数 函数名 描述 capable 性能指标 capaplot 性能图 ewmaplot 指数加权移动平均图 histfit 添加正态曲线的直方图 normspec 在指定的区间上绘正态密度 schart S图 xbarplot x条图 表10 聚类分析函数 函数名 描述 cluster 根据linkage函数的输出创建聚类 clusterdata 根据给定数据创建聚类 cophenet Cophenet相关系数 dendrogram 创建冰柱图 inconsistent 聚类树的不连续值 linkage 系统聚类信息 pdist 观测量之间的配对距离 squareform 距离平方矩阵 zscore Z分数 表11 线性模型函数 函数名 描述 anova1 单因子方差分析 anova2 双因子方差分析 anovan 多因子方差分析 aoctool 协方差分析交互工具 dummyvar 拟变量编码 friedman Friedman检验 glmfit 一般线性模型拟合 kruskalwallis Kruskalwallis检验 leverage 中心化杠杆值 lscov 已知协方差矩阵的最小二乘估计 manova1 单因素多元方差分析 manovacluster 多元聚类并用冰柱图表示 multcompare 多元比较 多项式评价及误差区间估计 polyfit 最小二乘多项式拟合 polyval 多项式函数的预测值 polyconf 残差个案次序图 regress 多元线性回归 regstats 回归统计量诊断 Ridge 岭回归 rstool 多佳节又重阳维响应面可视化 robustfit 稳健回归模型拟合 stepwise 逐步回归 x2fx 用于设计矩阵的因子设置矩阵 表12 非线性回归函数 函数名 描述 nlinfit 非线性最小二乘数据拟合（牛顿法） nlintool 非线性模型拟合的交互式图形工具 nlparci 参数的置信区间 nlpredci 预测值的置信区间 nnls 非负最小二乘 表13 试验设计函数 函数名 描述 cordexch D-优化设计（列交换算法） daugment 递增D-优化设计 dcovary 固定协方差的D-优化设计 ff2n 二水平完全析因设计 fracfact 二水平部分析因设计 fullfact 混合水平的完全析因设计 hadamard Hadamard矩阵（正交数组） rowexch D-优化设计（行交换算法） 表14 主成分分析函数 函数名 描述 barttest Barttest检验 pcacov 源于协方差矩阵的主成分 pcares 源于主成分的方差 princomp 根据原始数据进行主成分分析 表15 多元统计函数 函数名 描述 classify 聚类分析 mahal 马氏距离 manova1 单因素多元方差分析 manovacluster 多元聚类分析 表16 假设检验函数 函数名 描述 ranksum 秩和检验 signrank 符号秩检验 signtest 符号检验 ttest 单样本t检验 ttest2 双样本t检验 ztest z检验 表17 分布检验函数 函数名 描述 jbtest 正态性的Jarque-Bera检验 kstest 单样本Kolmogorov-Smirnov检验 kstest2 双样本Kolmogorov-Smirnov检验 lillietest 正态性的Lilliefors检验 表18 非参数函数 函数名 描述 friedman Friedman检验 kruskalwallis Kruskalwallis检验 ranksum 秩和检验 signrank 符号秩检验 signtest 符号检验 表19 文件输入输出函数 函数名 描述 caseread 读取个案名 casewrite 写个案名到文件 tblread 以表格形式读数据 tblwrite 以表格形式写数据到文件 tdfread 从表格间隔形式的文件中读取文本或数值数据 表20 演示函数 函数名 描述 aoctool 协方差分析的交互式图形工具 disttool 探察概率分布函数的GUI工具 glmdemo 一般线性模型演示 randtool 随机数生成工具 polytool 多项式拟合工具 rsmdemo 响应拟合工具 robustdemo 稳健回归拟合工具 if (document.getElementById(&#34;content&#34;).offsetWidth == 700) { document.getElementById(&#34;leftDivPre&#34;).style.float = &#34;none&#34;; document.getElementById(&#34;leftDivPre&#34;).style.borderRight = &#34;none&#34;; document.getElementById(&#34;leftDivPre&#34;).style.marginRight = &#34;0em&#34;; document.getElementById(&#34;leftDivPre&#34;).style.paddingRight = &#34;0em&#34;; }]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[R 中如何取消科学计数法 z]]></title>
    	<url>/tech/2012/12/04/cancel-scientific-notation-in-r/</url>
		<content type="text"><![CDATA[[原文地址：http://bbs.pinggu.org/thread-1403060-1-1.html R应该会自动的把太大和太小的数用科学计数 &amp;gt; 10^seq(1:5) [1] 1e&#43;01 1e&#43;02 1e&#43;03 1e&#43;04 1e&#43;05 &amp;gt; options(scipen=200) &amp;gt; 10^seq(1:5) [1] 10 100 1000 10000 100000]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Matlab 画二维图心得]]></title>
    	<url>/tech/2012/11/23/matlab-ezplot-latex/</url>
		<content type="text"><![CDATA[[plot()函数主要用于已有坐标对的连接（包括单个坐标，利用这一点再加上修改MarkSize可以画出指定大小的圆点等各种符号）； ezplot 根据ezplot()作隐函数图像的原理，http://www.ilovematlab.cn/thread-21438-1-1.html给出了一种用ezplot()同时画两条曲线的巧妙方法，但要注意该方法无法将两条曲线区分开来，因此可能会导致利用legend()添加图例的意图无法达到。 添加多条曲线时要利用好hold on这一语句； axis的刻度在默认情况下会自动根据给出的最后一次ezplot()进行调整，如果不希望画出的图像为自适应大小，需要在图像绘制完成后，手工指定axis的x轴与y轴最大、最小刻度； 添加箭头可以用annotation完成，要注意其坐标是相对大小，在[0,1]之间，我个人比较喜欢PSTricks的箭头风格，因此也会考虑在PSTricks中对已生成的图像进行深加工； 可以将ezplot()和legend()的结果赋给某个变量，再通过set()函数对图像对象或者legend对象进行字体大小、颜色等的操作； latex()函数可以用于将指定的符号表达式转换成LaTeX语法格式； sym()函数可以用于将某个数值对象转换成符号表达式以供latex()函数使用； title、xlabel、ylabel、zlabel、textbox和text等函数可以利用&#39;Interpreter&#39;,&#39;latex&#39;加载LaTeX格式； 因为Legend没有Interpreter属性，所以如果要在其中使用LaTeX，必须获取对应的文字句柄，并对文字对象设置String和Interpreter属性，关于这一点可以参考：http://www.mathworks.com/matlabcentral/newsreader/view_thread/254118和http://sites.google.com/site/sleepingwalking/matlab/latex-in-legend，从下面的示例可以看到，String不一定必要； 将图像插入LaTeX文档时，建议不要使用pdf格式，而是使用eps格式，否则得到的图像结果锯齿感会比较明显。 在Matlab中使用latex()函数的效果 &amp;gt;&amp;gt; x = [1:5;6:10;11:15]; &amp;gt;&amp;gt; y = sym(x); &amp;gt;&amp;gt; latex(y) ans =\left(\begin{array}{ccccc} 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5\\ 6 &amp;amp; 7 &amp;amp; 8 &amp;amp; 9 &amp;amp; 10\\ 11 &amp;amp; 12 &amp;amp; 13 &amp;amp; 14 &amp;amp; 15 \end{array}\right) &amp;gt;&amp;gt; syms a b; &amp;gt;&amp;gt; x = sin(a) &#43; b/a; &amp;gt;&amp;gt; latex(x) ans =\sin\!\left(a\right) &#43; \frac{b}{a} 下面的例程中用到了大部分上面的心得 % 使用 ezplot 画多条曲线，不需要加 legend 时，可以考虑 f1*f2 型 % 实际上这是一种对 ezplot 隐函数原理画线的深刻理解 syms x y; a = sin(x) &#43; sinh(y); b = sin(x); f_a = ezplot(a); hold on f_b = ezplot(b); set(f_a, &#39;LineWidth&#39;, 3); xx = legend(&#39;$$\alpha^a_b (x)\frac{b}{a}$$&#39;,&#39;\alpha&#39;); % 前一个为 LaTeX 格式，后一个为 LaTeX 语法； set(xx,&#39;Interpreter&#39;,&#39;latex&#39;) % legend 中的 latex 解释器直接使用不起作用；text() 函数等可以直接用。 % legend 可以作为一个对象赋给某个变量以进一步修改设置 xlabel(&#39;$$\sin x$$&#39;,&#39;interpreter&#39;,&#39;latex&#39;) title(&#39;&#39;); text(&#39;Interpreter&#39;,&#39;latex&#39;,... &#39;String&#39;,&#39;$$\int_0^x\!\int_y dF(u,v)$$&#39;,... &#39;Position&#39;,[.5 .5],... &#39;FontSize&#39;,16) % latex(s) 转化符号表达式为 LaTeX 形式 % title、xlabel、ylabel、zlabel、textbox和legend、text 可以利用 &#39;Interpreter&#39;,&#39;latex&#39; 加载 LaTeX 格式 % sym() 函数可以将一个数值矩阵转换成为一个可以被 latex() 处理的符号类结果 一个利用PSTricks修改已有图像的简单例子 % !Mode:: &amp;quot;TeX:UTF-8&amp;quot; \documentclass{ctexart} \usepackage{amsmath,bm,galois} \usepackage{pstricks} \begin{document} \pagestyle{empty} \begin{pspicture}(-1,-1)(5,5) \rput[bl](0,0){\includegraphics{fig1}} %\psgrid[subgriddiv=0] \rput(0,0){价值} \rput(3.15,0){结果} \rput(1.6,2.85){报价}\rput(0.4,1.5){$\bm{\beta}$} \rput(3.5,1.5){$(\bm{\pi},\bm{\mu})$} \rput(1.5,-0.32){$(\bm{\pi},\bm{\mu})\comp\bm{\beta}$} \end{pspicture} \end{document}]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[apsrtable 在 knitr 中生成 LaTeX 表格]]></title>
    	<url>/tech/2012/11/21/apsrtable-knitr-latex-table/</url>
		<content type="text"><![CDATA[[原文地址：http://cos.name/cn/topic/108631 apsrtable() 里有个 Sweave 参数，若设为 TRUE，则生成 tabular 环境，若为默认的 FALSE， table 环境。 \begin{table}[htbp] \caption{some text} \label{tb:ex} \centering &amp;lt;&amp;lt;results=&#39;asis&#39;&amp;gt;&amp;gt;= apsrtable(mod1, mod2, mod3, Sweave = TRUE) @ \end{table} Yihui: 这大概就是最好的办法了吧……这参数名干嘛非得叫 Sweave，明明意思是 tabular.only。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[设置 MATLAB 中 Current Folder 的默认文件夹]]></title>
    	<url>/tech/2012/11/18/matlab-current-default-folder/</url>
		<content type="text"><![CDATA[[原文地址：http://www.yueye.org/2011/set-matlab-current-default-folder.html 在 Folder面板会给我们带来一定的便利性。但遗憾的是，MATLAB自身没有提供友好的设置界面，以供用户自如地设置Current Folder面板上的起始文件夹。这就给我们带来了一定的不便，毕竟每次启动MATLAB后都重新在Current Folder中设置到我们想要的文件夹地址会耗费一定的时间，也会影响我们的心情。 那么，我们应该如何在MATLAB中设置Current Folder面板上的起始文件夹位置呢？ 在以前的MATLAB版本中，我们可以通过右击MATLAB安装目录下MATLAB快捷方式而在其弹出的对话框中通过设置快捷方式的起始位置而对Current Folder面板上的起始文件夹进行设置。但自从MATLAB进入R2010和R2011后，我们已经不能通过这样的设置来实现我们想要的效果。但MATLAB自身提供了另一种方式，虽然不太友好，但毕竟一次设置永久受益，还是让我们来一起设置一下吧。 首先，思考选择一个你要设置的文件夹路径，比如你可以保持一贯的传统，而设置为“MATLAB\R2011a\work\”文件夹； 接着，让我们进入MATLAB的安装目录下的“MATLAB\R2011a\toolbox\local”文件夹下，在local文件夹下新建一个名为startup的.m文件，在其中输入如下内容： cd D:\Program Files\MATLAB\R2011a\work 其中cd后面的内容为你要设置的起始文件夹路径。 需要注意的是，这里的路径不需要用引号引起来，也不需要做任何处理。编写好startup.m文件并保存后，再次启动MATLAB，可以看到，MATLAB的Current Folder面板上的起始文件夹已经变成了我们设置的路径。 这里还有一点需要特别说明的是，如果你要设置的路径包含中文名，比如“D:\我的文档\MATLAB”，则在编写好如下所示的startup.m文件内容： cd D:\我的文档\MATLAB 之后，还需要将该文件的编码修改为Unicode，MATLAB才能正常识别，从而你的设置才能正常发挥作用，MATLAB启动时，Current Folder面板上的起始文件夹初始地址才能变成设置的效果。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[knitr、Markdown、Pandoc 相关的几份资料]]></title>
    	<url>/tech/2012/09/17/knitr/</url>
		<content type="text"><![CDATA[[为什么 Markdown&#43;R 有较大概率成为科技写作主流？http://www.yangzhiping.com/tech/r-markdown-knitr.htm Markdown 语法说明：http://wowubuntu.com/markdown/ Markdown Wiki：http://en.wikipedia.org/wiki/Markdown Pandoc：http://johnmacfarlane.net/pandoc/ 如何高效利用 GitHub：http://www.yangzhiping.com/tech/github.html]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Beginners guide to creating a REST API z]]></title>
    	<url>/tech/2012/09/13/rest/</url>
		<content type="text"><![CDATA[[原文地址：http://www.andrewhavens.com/posts/20/beginners-guide-to-creating- If you&amp;rsquo;re reading this, you&amp;rsquo;ve probably heard the terms API and REST thrown around and you&amp;rsquo;re starting to wonder what the fuss is all about. Maybe you already know a little bit, but don&amp;rsquo;t know how to get started. In this guide, I will explain the basics of REST and how to get started building an API (including authentication) for your application. What is an API? The term API stands for Application Programming Interface. The term can be used to describe the features of a library, or how to interact with it. Your favorite library may have &amp;ldquo;API Documentation&amp;rdquo; which documents which functions are available, how you call them, which arguments are required, etc. However, these days, when people refer to an API they are most likely referring to an HTTP API, which can be a way of sharing application data over the internet. For example, Twitter has an API that allows you to request tweets in a format that makes it easy to import into your own application. This is the true power of HTTP APIs, being able to &amp;ldquo;mashup&amp;rdquo; data from multiple applications into your own hybrid application, or create an application which enhances the experience of using someone else&amp;rsquo;s application. For example, let&amp;rsquo;s say we have an application that allows you to view, create, edit, and delete widgets. We could create an HTTP API that allows you to perform these functions: http://example.com/view_widgets http://example.com/create_new_widget?name=Widgetizer http://example.com/update_widget?id=123&amp;amp;name=Foo http://example.com/delete_widget?id=123 A problem has started to arise when everyone starts implementing their own APIs. Without a standard way of naming URLs, you always have to refer to the documentation to understand how the API works. One API might have a URL like /view_widgets whereas another API might use /widgets/all. Don&amp;rsquo;t worry, REST comes to rescue us from this mess. What is REST? REST stands for Representational State Transfer. This is a term invented by Roy Fielding to describe a standard way of creating HTTP APIs. He noticed that the four common actions (view, create, edit, and delete) map directly to HTTP verbs that are already implemented: GET, POST, PUT, DELETE. If you&amp;rsquo;re new to HTTP, you may not be familiar with some of these verbs. So let me give a brief rundown on HTTP methods. HTTP methods There are technically 8 different HTTP methods: GET POST PUT DELETE OPTIONS HEAD TRACE CONNECT Most of the time, when you&amp;rsquo;re clicking around in your browser, you are only ever using the GET HTTP method. GET is used when you are &amp;ldquo;getting&amp;rdquo; a resource from the internet. When you submit a form, you are usually using the POST method to &amp;ldquo;post data&amp;rdquo; back to the website. As for the other methods, some browsers don&amp;rsquo;t even implement them all. However, for our uses, that doesn&amp;rsquo;t matter. What matters is that we have a bunch of &amp;ldquo;verbs&amp;rdquo; to choose from which help to describe the actions we are taking. We will be using client libraries which already know how to use the different HTTP methods. Examples of REST Let&amp;rsquo;s look at a few examples of what makes an API &amp;ldquo;RESTful&amp;rdquo;. Using our widgets example again&amp;hellip; If we wanted to view all widgets, the URL would look like this: GET http://example.com/widgets Create a new widget by posting the data: POST http://example.com/widgets Data: name = Foobar To view a single widget we &amp;ldquo;get&amp;rdquo; it by specifying that widget&amp;rsquo;s id: GET http://example.com/widgets/123 Update that widget by &amp;ldquo;putting&amp;rdquo; the new data: PUT http://example.com/widgets/123 Data: name = New name color = blue Delete that widget: DELETE http://example.com/widgets/123 Anatomy of a REST URL You might have noticed from the previous examples that REST URLs use a consistent naming scheme. When you are interacting with an API, you are almost always manipulating some sort of object. In our examples, this is a Widget. In REST terminology, this is called a Resource. The first part of the URL is always the plural form of the resource: /widgets This is always used when referring to this collection of resources (&amp;ldquo;list all&amp;rdquo; and &amp;ldquo;add one&amp;rdquo; actions). When you are working with a specific resource, you add the ID to the URL. /widgets/123 This URL is used when you want to &amp;ldquo;view&amp;rdquo;, &amp;ldquo;edit&amp;rdquo;, or &amp;ldquo;delete&amp;rdquo; the particular resource. Nested Resources Let&amp;rsquo;s say our widgets have many users associated with them. What would this URL structure look like? List all: GET /widgets/123/users Add one: POST /widgets/123/users Data: name = Andrew Nested resources are perfectly acceptable in URLs. However, it&amp;rsquo;s not a best practice to go more than two levels deep. It&amp;rsquo;s not necessary because you can simply refer to those nested resources by ID rather than nesting them within their parents. For example: /widgets/123/users/456/sports/789 &amp;hellip;can be referenced as: /users/456/sports/789 &amp;hellip;or even: /sports/789 HTTP Status Codes Another important part of REST is responding with the correct status code for the type of request that was made. If you&amp;rsquo;re new to HTTP status codes, heres a quick summary. When you make an HTTP request, the server will respond with a code which corresponds to whether or not the request was successful and how the client should proceed. There are four different levels of codes: 2xx = Success 3xx = Redirect 4xx = User error 5xx = Server error Here&amp;rsquo;s a list of the most important status codes: Success codes: 200 - OK (the default) 201 - Created 202 - Accepted (often used for delete requests) User error codes: 400 - Bad Request (generic user error/bad data) 401 - Unauthorized (this area requires you to log in) 404 - Not Found (bad URL) 405 - Method Not Allowed (wrong HTTP method) 409 - Conflict (i.e. trying to create the same resource with a PUT request) API response formats When you make an HTTP request, you can request the format that you want to receive. For example, making a request for a webpage, you want the format to be in HTML, or if you are downloading an image, the format returned should be an image. However, it&amp;rsquo;s the server&amp;rsquo;s responsibility to respond in the format that was requested. JSON has quickly become the format of choice for REST APIs. It has a lightweight, readable syntax that can be easily manipulated. So when a user of our API makes a request and specifies JSON as the format they would prefer: GET /widgets Accept: application/json &amp;hellip;our API will return an array of widgets formatted as JSON: [ { id: 123, name: &#39;Simple Widget&#39; }, { id: 456, name: &#39;My other widget&#39; } ] If the user requests a format that we haven&amp;rsquo;t implemented, what do we do? You can throw some type of error, but I would recommend enforcing JSON as your standard response format. It&amp;rsquo;s the format that your developers will want to use. No reason to support other formats unless you already have an API which needs to be supported. Building a REST API Actually building a REST API is mostly outside the scope of this tutorial since it is language specific, but I will give a brief example in Ruby using a library called Sinatra: require &#39;sinatra&#39; require &#39;JSON&#39; require &#39;widget&#39; # our imaginary widget model # list all get &#39;/widgets&#39; do Widget.all.to_json end # view one get &#39;/widgets/:id&#39; do widget = Widget.find(params[:id]) return status 404 if widget.nil? widget.to_json end # create post &#39;/widgets&#39; do widget = Widget.new(params[&#39;widget&#39;]) widget.save status 201 end # update put &#39;/widgets/:id&#39; do widget = Widget.find(params[:id]) return status 404 if widget.nil? widget.update(params[:widget]) widget.save status 202 end delete &#39;/widgets/:id&#39; do widget = Widget.find(params[:id]) return status 404 if widget.nil? widget.delete status 202 end API authentication In normal web applications, handling authentication is usually handled by accepting a username and password, and saving the user ID in the session. The user&amp;rsquo;s browser saves a cookie with ID of the session. When the user visits a page on the site that requires authentication, the browser sends the cookie, the app looks up the session by the ID (if it hasn&amp;rsquo;t expired), and since the user ID was saved in the session, the user is allowed to view the page. With an API, using sessions to keep track of users is not necessarily the best approach. Sometimes, your users may want to access the API directly, other times the user may way to authorize another application to access the API on their behalf. The solution to this is to use token based authentication. The user logs in with their username and password and the application responds with a unique token that the user can use for future requests. This token can be passed onto the application so that the user can revoke that token later if they choose to deny that application further access. There is a standard way of doing this that has become very popular. It&amp;rsquo;s called OAuth. Specifically, version 2 of the OAuth standard. There are a lot of great resources online for implementing OAuth so I would say that is outside the scope of this tutorial. If you are using Ruby, there are some great libraries that handle most of the work for you, like OmniAuth. Hopefully, I&amp;rsquo;ve filled in enough blanks for you to get started. If you still have questions, you may find this tutorial helpful. Feel free to post any questions or criticisms in the comments.]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Office 卸载工具下载]]></title>
    	<url>/tech/2012/02/07/office-uninstall/</url>
		<content type="text"><![CDATA[[Office 2003 卸载工具下载 Office 2007 卸载工具下载 Office 2010 卸载工具下载 参考：http://support.microsoft.com/kb/290301]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[盘点 JangBi 在本次 OSL 曲折的征途，暨 OSL2011 回顾贴 z]]></title>
    	<url>/cn/2012/01/09/jangbi/</url>
		<content type="text"><![CDATA[[原文地址：http://bbs.plu.cn/thread-1718316-1-1.html 盘点 JangBi 在本次 OSL 曲折的征途，暨OSL回顾贴。求斑竹加亮，望各位 PLUer 们喜欢。 希望SC1经典永恒，希望 JangBi 和范太子能够打出精彩的决赛。 还有希望此文能够给冰凌火焰童鞋一点灵感和资料，再写出更好的文章。 背景：本人无聊逛论坛，无意中点进了 2011OSL 综合导航帖，从头到尾的看了下，突然发现原来 JangBi 此次进入决赛的路程如此曲折，甚至远超当年的火星大王 EFFORT 获得不死鸟称号的曲折程度。 网吧赛 本次随着 OSL 赛程的改变，非职业选手有机会能够在 OSL 上露面，于是 OSL 网吧赛就精彩异常，总共 12 个组，每组有 16 人，而每组只能有1人出线，相当于在这一天就要经历一个完整的 16 强的杯赛，而只有冠军才能晋级…… 12 个亚军要争夺只有 1 个因为火鸡退役而空出的名额进军 ODT。于是，有很多很多优秀的选手在本次网吧赛就折戟沉沙，比如 BISU，比如 LETA，比如 STATS，比如 ZERO。还有 REALLY。看了网吧赛之后终于明白我们的星矢童鞋有多郁闷了，OSL 网吧赛第一轮就遭到淘汰，MST 又被寂寞给搞掉，什么叫屋漏偏逢连夜雨啊…… 顺便要提的是，非职业选手只有 1 人过了 1 轮，在小组的 8 强中就全军覆没了。 言归正传，看看我们的飞本同学，作为 L 组的种子选手，第一轮轮空，再干掉 2 个小强，遇到了 soo 本，被 0:2 横扫，屈居小组亚军。如果没有火鸡童鞋的退役，飞本童鞋本次的 OSL 就要画上句号了。可是，就是这小小的希望，让我们的飞本童鞋进军 ODT。12 个亚军，其中有着 ZERO，BRAVE，LETA，MOVIE 这样的 A 甚至 S 级选手！飞本开始显现出其万夫不当之勇，在角斗士上上来就把 MSL 亚军 ZERO 斩于马下，在拉曼查砍翻风骚人皇如花 LETA，再在新血岭上斩翻经典人皇 CLASSIC，最后在 BO3 的小决赛中 2:0 横扫三星小强 sharp，拿到了唯一的一个小组亚军出线的名额！ ODT 比赛来到 ODT，首先映入眼帘的就是李双的对决，如此可见 ODT 比 MST 真的是要精彩太多了，李双携手出线，于是小正太悲情了……下次见到一对基友一定要躲远啊…… 然后就是神龟录，SOO 其实只是路人，但是没想到这次路人成了主角，神在败给了玄武之后又输给了输本，杯具出局…… 之后我们的 JangBi 大帝出场，2 战 2 捷，砍翻没有炮哥光环的傻本和偶尔能搞掉吃肉的宝儿，猛将本色显露无疑，直接晋级！ 而后是本剧另外一个重要人物，大 KILLER 小激动，OZ 二当家，大激动唯一接瑞脑消金兽班人 Baxter（希望大家也能够记住他的新 ID），同样双战双捷。 再后是金鱼 Constantly（同样是新 ID）的双杀，让我以为 OZ 就要从单核变成3核了。 最后是胡克大将军默默的在准备完成他的护国使命。 OSL小组赛 在见证李激动桑苏的杯具同时，我们的猛张飞败给了脑虫和状态正佳的 Baxter (Killer)。但是小激动说了，要帮助我们的进攻之王出线。于是小萌王生擒脑虫，三战三捷，之后就得要看张飞自己的了。于是在这个生死攸关的关键时刻，进攻之王又彰显出其万夫不当之勇，擒杀小 baby，给自己终于争取到了附加赛的机会，而处境相同 的Hiya 和 Stork，却都没有把握住最后的机会。大魔王遭遇大将军的护驾，杯具在所难免。于是 4 个组中，其实都有打附加赛的可能，但只有 C 组张飞组因为飞本的勇猛打了附加赛，并且在附加赛上勇猛无敌，直接双胜出线，小 baby 和脑虫再次成为进攻之王的刀下鬼。张飞成为了 8 强中唯一的神族！ 淘汰赛阶段 好不容易小组赛出线，就遇到了最终兵器，在第一场 FLASH 获胜之后，多半看官都觉得神族要覆灭了……但是就算是最终兵器，也扛不住伤病和各路败 RP，在大师的号召下，教徒纷纷开始为教主的手术攒 RP 而不是比赛，最终兵器就这样告别了 OSL……其实从过程来看，教主应该还是受到了伤病的困扰，没有充分的准备，战术也单一了许多，侦查和骚扰也非常有限。其实张飞能过这关还真的是蛮幸运的……不过换做是别人，恐怕还把握不住这种幸运，早都倒在了教主的气势之下，张飞依然还是那么的勇猛！ 然而帮助张飞出线，于张飞约战 4 强的小萌王童鞋却因为自己冲动送出一局，被输本翻盘……或许是因为小萌王为张飞的气势所倾倒，怕自己不得已将会终结一位猛将的决赛之路，于是自己动摇了一下下……总之飞本最后对上了曾经在网吧赛差点让自己没能出线的输本。然而此时的张飞已经越战越勇，和当时的废本已经判若两人，轻松加愉快的 3:0 拿下，断绝了虫族进军决赛的希望，也断绝了 SKT1 会师决赛的希望！ 决赛，即将上演…… 如果飞本夺得了冠军，那他一定是 OSL 历史上晋级道路最曲折，历经比赛数量最多的冠军！只是，面对太子神一样的雷车，飞本还将面临着严峻的考验。从地图上看，开拓者是太子主场，而拉曼查（优势种族圣图）和角斗士（中央高低不平，三矿易守）在 PVT 中对 P 稍有优势。虽然血岭也被认为是神族优势图，在老血岭上吃肉曾经说过“人族一波冲不下来就可以打 GG 了”的话，但是太子的雷车可以充分利用本图可以不断绕路的特点，或许决战地，就是血染岭！ 而小美和甜甜的加入也增加了决赛的火东篱把酒黄昏后药味，小美终于可以在上次情人节和太子的约会后再次进入太子的“闺房”……而甜甜在主观上支持飞本，更何况两人从长相上说是一个类型。。如果太子真的能够牵手小美的话……（以下省略 5000 YY 字），其实比赛的输赢，有那么重要么？ JangBi 加油！太子也加油！大师和三炮更要加油！！ LMY 原创 韩国三国战队(saM)知名选手名单 http://bbs.plu.cn/thread-731431-3-1.html AnRyang - 颜良 BanJang - 范疆 ZoAng - 曹昂 MunChu - 文丑 WiYeon - 魏延 MaSok - 马谡 Junwi - 典韦 MaDae - 马岱 BangTong - 庞统 Jangbi - 张飞 Hazin - 何进 ZoZo - 曹操 Yubi - 刘备 ZinGun - 陈群 JangHap - 张合 Kwanro - 管辂 Yeomong - 吕蒙 Jangik - 张翼 TaeSaJa - 太史慈 GongMyung - 孔明(诸葛亮) SonKun - 孙乾 SoonWook - 荀彧 Oban - 吴班 KwakKa - 郭嘉 SoonYu - 荀攸 JangGak - 张角 JeKalLyang - 诸葛亮 HwaWoong - 华雄 Yeopo - 吕布 YookSon - 陆逊 JangLyo - 张辽 SonKwon - 孙权 HaHuDon - 夏侯惇 KwanHeng - 关兴 JangLyang - 张粱 KwanSek - 关索 另外解释一下一些韩文ID的意思 HwaSIn - 火神、和珅 MuJuK - 无敌 Maru - 地面 ShinHwA - 神话、新华、神化 DoGGi - 斧子]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[引用最近的一个公式：lasteqn z]]></title>
    	<url>/tech/2011/12/28/lasteqn/</url>
		<content type="text"><![CDATA[[原文地址：http://bbs.ctex.org/viewthread.php?tid=69755 \documentclass{article} \usepackage{amsmath} \providecommand\currentseclabel{} \newcommand\seclabel[1]{ \renewcommand\currentseclabel{#1}% \label{sec:#1}} \newcommand\secref[1]{ \ref{sec:#1}} \newcommand\eqlabel[1]{ \label{eq:\currentseclabel:#1}} \let\saveeqref\eqref \renewcommand\eqref[2][\currentseclabel]{ \saveeqref{eq:#1:#2}} \newcommand\lasteqn{(\theequation)} \begin{document} \section{Rectangle} \seclabel{rect} \begin{equation} \eqlabel{area} A = ab \end{equation}Area formula \eqref{area}.Previous equation \lasteqn.\section{Circle} \seclabel{circ} \begin{equation} \eqlabel{area} A = \pi r^2 \end{equation} Area formula \eqref{area}.\section{Summary}Area of rectangles see \eqref[rect]{area} in section~\secref{rect}; area of circles see \eqref[circ]{area} in section~\secref{circ}. \end{document}]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[底层命令解释]]></title>
    	<url>/tech/2011/12/02/latex-basic-instructions/</url>
		<content type="text"><![CDATA[[判断某个命令是否已经用过 判断是否已经定义用 \ifdefined 或者 \ifcsname，如果没有eTeX支持也可以用 \ifx\foo\undefined 或者 LaTeX 内核的 \@ifundefined。 判 \let和\xdef有什么区别？ \xdef把定义中的内容完全展开，用来定义一个宏；\let让新宏与旧宏意义相同。\xdef就是\global\edef。\let没有\global的意思。 \def\a{foo} \def\b{\a} \edef\c{\b} 得到的是 \c -&amp;gt; foo \let\d\b 得到的是 \d -&amp;gt; \a 一个外链：What is the difference between \let and \edef \renewcommand*的作用 \renewcommand所带参数可以包含用\par或空行表示的新段落；\renewcommand*不行 \string的作用 将一个命令输出为带斜杠的字符串。 \expandafter的作用 让制控命令顺序颠倒过来，后面的命令先起作用，下边是一个证明的例子： \def\test{9999} \makeatletter \def\testt#1#2#3#4{\@alph{#1}\@Alph{#2}\@Roman{#3}\@roman{#4}} \makeatother \expandafter\testt\test 如果不加\expandafter的话： \testt\test 就会出错。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[CTeX 2.9.0.152 下 Beamer 中使用 theorem 环境出错的解决方法 z]]></title>
    	<url>/tech/2011/11/27/ctex-2-9-0-152-beamer-theorem/</url>
		<content type="text"><![CDATA[[原文地址：http://hi.baidu.com/zjunmm/blog/item/0915d010d7ce2d175aaf53d1.htm CTeX 2.9.0.152 下在 Beamer 中使用 theorem, definitioin 环境会出错： ! Undefined control sequence. \trans@languagepath -&amp;gt;\languagename ,English l.226 \end{frame} 解决方法有两种： 在导言区添加 \usepackage[english]{babel}; 在线升级到最新版本。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[footmisc 宏包选项的简短说明]]></title>
    	<url>/tech/2011/11/21/options-of-footmisc/</url>
		<content type="text"><![CDATA[[原文地址：http://hi.baidu.com/albertleemon/blog/item/a7f4d2e715772926b93820 该宏包提供了许多选项，可使脚注命令\footnote{注释}生成多种样式的脚注。其中： perpage：为每页脚注单独编号； stable：可避免章节标题中的脚注随同章节标题出现在目录或页眉之中； side：将脚注改为边注； multiple：给正文中两个以上的并排脚注标号之间加上分隔逗号； para：将本页的所有脚注合为一个段落； symbol：将脚注的数字序号改为*号等不同的符号； ragged：不采用断词等方法使脚注文本右端对齐； marginal：使脚注首行不缩格； flushmargin：类似marginal选项，只是脚注序号更靠近脚注； hang：使脚注文本向右缩进一段距离； norule：取消正文与脚注之间的一条短横线。 PS：在使用 footmisc 宏包后，如果脚注的编号不对或者编号所在的位置有问题时，再编译一次即可解决。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[用 pgfpages 生成 slides 的 4 in 1 版式效果]]></title>
    	<url>/tech/2011/11/12/pgfpages-slides-4-in-1-changing-margins/</url>
		<content type="text"><![CDATA[[Getting a 4 in 1 pdf Slides document and “Changing PDF Margins With The pdfpages Package” \documentclass[a4paper, landscape]{article} \usepackage[final]{pdfpages} \begin{document} \includepdf[nup=2x2,pages=-, delta=15 15, offset=20 0, frame, scale=0.92,% turn=false]{slides.pdf} \end{document} % offset=20 0 为偏移横向的内容，给左边留一定的空白用于装订； % 不使用 offset 的纵向偏移，否则虽然下方有空白，但天头没有； % 解决纵向留空使用了 scale 选项，直接缩小页面的尺寸，产生自动纵向留白。 用 Beamer 做的 Slides，最刚开始用 Acrobat 打印机生成幻灯片的四合一效果，但发现 Acrobat 打印过程中会对图片的质量造成一定的损伤。 后来发现psnup工具可以实现4 in 1的效果，但试验之后感觉速度和方便程度都很欠缺，需要先将 pdf 文件转换 ps 文件，光是这个过程就太漫长，受不了；另外就是该工具提供的选项看起来不是很舒服。 最后回想起其实 LaTeX 自带的pgfpages宏包可以完成这个任务，又回头查了了下 Beam User Guide，发现 Beamer 中也尝试过使用该宏包。但是 Beamer 中的\pgfpagesuselayout{2 on 1}[a4paper,border shrink=5mm]类命令我不能直接在 Beamer 之外用，而pgfpages中的delta与offset组合工作还存在问题(见上面的代码注释)，最后发现\includepdf的scale选项配合delta与offset就可以完美的达到想要的效果，问题搞定！ PS：事实上pgfpages宏包的手册中没有对scale选项进行专门的说明，这个选项其实源于graphicsx宏包的\includegraphic命令，解决上面问题的思路来自下面的资料： http://magic.aladdin.cs.cmu.edu/2007/11/13/changing-pdf-margins-with-the-pdfpages-package/ http://www.tardis.ed.ac.uk/~ajcd/psutils/psnup.html http://apps.hi.baidu.com/share/detail/21414399]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[如何在 Word 中分栏状态下正确的输入脚注？]]></title>
    	<url>/tech/2011/11/10/word-footnote/</url>
		<content type="text"><![CDATA[[在论文排版中，在题目或作者处插入了脚注后，再对正文分栏时；或者先分栏，再在题目或作者处插入脚注，分栏后的正文就跑到下一页上去了。 稿件格式的要 解决方案： word 2003：“工具”→“选项”→“兼容性”→“选项”→“按照 word 6.x/95/97 的方式排放脚注”； word 2007：“左上角圆形图标”→“word选项”→“在左边列中点高级”→在右边的“兼容性选项”下点“版式选项”，展开版式选项，勾选按“word 6.x/95/97的方式排安排脚注”。 经过以上操作后，分栏后，加入脚注或者加入脚注后再分栏，正文跑到第二页的问题就可以解决了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[LaTeX 问题贴收集]]></title>
    	<url>/tech/2011/11/07/latex-solution/</url>
		<content type="text"><![CDATA[[各类问题 TeX Live 2011 中的 ConTeXt MkIV 配置(包括 Linux 和 Win7) 中文书籍模板示例 (《线性代数》的文档类) What does \ifx\#1\ stand for? How to include a picture over two pages, left part on left side, right on right (for books)? How to change title in listings? 代码 Listings 设置示例) 用listings包实现左边代码右边输出 高中数学试卷举例 多种期刊的统一模板 我的beamer幻灯片模板 多子图分页 学习底层命令的参考书目 如何实现重复 抛砖引玉谈“脆弱” LaTeX的循环里面怎样定义命令 这种改中文标点 catcode 的方法靠谱么 生成可展开的改变大小写命令 重要模块 Asymptote 中文资源导引 PGF/TikZ 学习帖汇总]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Markdown 快速入门 z]]></title>
    	<url>/tech/2011/11/06/markdown-basis/</url>
		<content type="text"><![CDATA[[1 Markdown 语法要领 2 段落、标题、区块代码 2.1 修辞和强调 3 列表 4 链接 5 图片 6 代码 声明： 这份文档派生(fork)于繁体中文版，在此基础上进行了繁体转简体 Markdown 语法编写，你可以到这里查看它的源文件。「繁体中文版的原始文件可以查看这里」–By @riku 注： 本项目托管于 GitHub 上，请通过“派生”和“合并请求”来帮忙改进本项目。 1 Markdown 语法要领 此页提供了 Markdown 的简单概念， 语法说明 页提供了完整详细的文档，说明了每项功能。但是 Markdown 其实很简单就可以上手，此页文档提供了一些范例，并且每个范例都会提供输出的 HTML 结果。 其实直接试试看也是一个很不错的方法， Dingus 是一个网页应用程序，你可以把自已编写的 Markdown 文档转成 XHTML。 2 段落、标题、区块代码 一个段落是由一个以上的连接的行句组成，而一个以上的空行则会划分出不同的段落（空行的定义是显示上看起来像是空行，就被视为空行，例如有一行只有空白和 tab，那该行也会被视为空行），一般的段落不需要用空白或换行缩进。 Markdown 支持两种标题的语法，Setext 和 atx 形式。Setext 形式是用底线的形式，利用 = （最高阶标题）和 - （第二阶标题），Atx 形式在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶。 区块引用则使用 email 形式的 ‘&amp;gt;’ 角括号。 Markdown 语法: A First Level Header ==================== A Second Level Header --------------------- Now is the time for all good men to come to the aid of their country. This is just a regular paragraph. The quick brown fox jumped over the lazy dog&amp;#39;s back. ### Header 3 &amp;gt; This is a blockquote. &amp;gt; &amp;gt; This is the second paragraph in the blockquote. &amp;gt; &amp;gt; ## This is an H2 in a blockquote 输出 HTML 为： &amp;lt;h1&amp;gt;A First Level Header&amp;lt;/h1&amp;gt; &amp;lt;h2&amp;gt;A Second Level Header&amp;lt;/h2&amp;gt; &amp;lt;p&amp;gt;Now is the time for all good men to come to the aid of their country. This is just a regular paragraph.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;The quick brown fox jumped over the lazy dog&amp;#39;s back.&amp;lt;/p&amp;gt; &amp;lt;h3&amp;gt;Header 3&amp;lt;/h3&amp;gt; &amp;lt;blockquote&amp;gt; &amp;lt;p&amp;gt;This is a blockquote.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;This is the second paragraph in the blockquote.&amp;lt;/p&amp;gt; &amp;lt;h2&amp;gt;This is an H2 in a blockquote&amp;lt;/h2&amp;gt; &amp;lt;/blockquote&amp;gt; 2.1 修辞和强调 Markdown 使用星号和底线来标记需要强调的区段。 Markdown 语法: Some of these words *are emphasized*. Some of these words _are emphasized also_. Use two asterisks for **strong emphasis**. Or, if you prefer, __use two underscores instead__. 输出 HTML 为: &amp;lt;p&amp;gt;Some of these words &amp;lt;em&amp;gt;are emphasized&amp;lt;/em&amp;gt;. Some of these words &amp;lt;em&amp;gt;are emphasized also&amp;lt;/em&amp;gt;.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Use two asterisks for &amp;lt;strong&amp;gt;strong emphasis&amp;lt;/strong&amp;gt;. Or, if you prefer, &amp;lt;strong&amp;gt;use two underscores instead&amp;lt;/strong&amp;gt;.&amp;lt;/p&amp;gt; 3 列表 无序列表使用星号、加号和减号来做为列表的项目标记，这些符号是都可以使用的，使用星号： * Candy. * Gum. * Booze. 加号： &#43; Candy. &#43; Gum. &#43; Booze. 和减号 - Candy. - Gum. - Booze. 都会输出 HTML 为： &amp;lt;ul&amp;gt; &amp;lt;li&amp;gt;Candy.&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;Gum.&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;Booze.&amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; 有序的列表则是使用一般的数字接着一个英文句点作为项目标记： 1. Red 2. Green 3. Blue 输出 HTML 为： &amp;lt;ol&amp;gt; &amp;lt;li&amp;gt;Red&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;Green&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;Blue&amp;lt;/li&amp;gt; &amp;lt;/ol&amp;gt; 如果你在项目之间插入空行，那项目的内容会用 &amp;lt;p&amp;gt; 包起来，你也可以在一个项目内放上多个段落，只要在它前面缩排 4 个空白或 1 个tab。 * A list item. With multiple paragraphs. * Another item in the list. 输出 HTML 为： &amp;lt;ul&amp;gt; &amp;lt;li&amp;gt;&amp;lt;p&amp;gt;A list item.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;With multiple paragraphs.&amp;lt;/p&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;&amp;lt;p&amp;gt;Another item in the list.&amp;lt;/p&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; 4 链接 Markdown 支援两种形式的链接语法： 行内 和 参考 两种形式，两种都是使用角括号来把文字转成连结。 行内形式是直接在后面用括号直接接上链接： This is an [example link](http://example.com/). 输出 HTML 为： &amp;lt;p&amp;gt;This is an &amp;lt;a href=&amp;quot;http://example.com/&amp;quot;&amp;gt; example link&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt; 你也可以选择性的加上title属性： This is an [example link](http://example.com/ &amp;quot;With a Title&amp;quot;). 输出 HTML 为： &amp;lt;p&amp;gt;This is an &amp;lt;a href=&amp;quot;http://example.com/&amp;quot; title=&amp;quot;With a Title&amp;quot;&amp;gt; example link&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt; 参考形式的链接让你可以为链接定一个名称，之后你可以在文件的其他地方定义该链接的内容： I get 10 times more traffic from [Google][1] than from [Yahoo][2] or [MSN][3]. [1]: http://google.com/ &amp;quot;Google&amp;quot; [2]: http://search.yahoo.com/ &amp;quot;Yahoo Search&amp;quot; [3]: http://search.msn.com/ &amp;quot;MSN Search&amp;quot; 输出 HTML 为： &amp;lt;p&amp;gt;I get 10 times more traffic from &amp;lt;a href=&amp;quot;http://google.com/&amp;quot; title=&amp;quot;Google&amp;quot;&amp;gt;Google&amp;lt;/a&amp;gt; than from &amp;lt;a href=&amp;quot;http://search.yahoo.com/&amp;quot; title=&amp;quot;Yahoo Search&amp;quot;&amp;gt;Yahoo&amp;lt;/a&amp;gt; or &amp;lt;a href=&amp;quot;http://search.msn.com/&amp;quot; title=&amp;quot;MSN Search&amp;quot;&amp;gt;MSN&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt; title属性是选择性的，链接名称可以用字母、数字和空格，但是不分大小写： I start my morning with a cup of coffee and [The New York Times][NY Times]. [ny times]: http://www.nytimes.com/ 输出 HTML 为： &amp;lt;p&amp;gt;I start my morning with a cup of coffee and &amp;lt;a href=&amp;quot;http://www.nytimes.com/&amp;quot;&amp;gt;The New York Times&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt; 5 图片 图片的语法和链接很像。 行内形式（title是选择性的）： ![alt text](/path/to/img.jpg &amp;quot;Title&amp;quot;) 参考形式： ![alt text][id] [id]: /path/to/img.jpg &amp;quot;Title&amp;quot; 上面两种方法都会输出 HTML 为： &amp;lt;img src=&amp;quot;/path/to/img.jpg&amp;quot; alt=&amp;quot;alt text&amp;quot; title=&amp;quot;Title&amp;quot; /&amp;gt; 6 代码 在一般的段落文字中，你可以使用反引号 ` 来标记代码区段，区段内的 &amp;amp;、&amp;lt; 和 &amp;gt; 都会被自动的转换成 HTML 实体，这项特性让你可以很容易的在代码区段内插入 HTML 码： I strongly recommend against using any `&amp;lt;blink&amp;gt;` tags. I wish SmartyPants used named entities like `&amp;amp;mdash;` instead of decimal-encoded entites like `&amp;amp;#8212;`. 输出 HTML 为： &amp;lt;p&amp;gt;I strongly recommend against using any &amp;lt;code&amp;gt;&amp;amp;lt;blink&amp;amp;gt;&amp;lt;/code&amp;gt; tags.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;I wish SmartyPants used named entities like &amp;lt;code&amp;gt;&amp;amp;amp;mdash;&amp;lt;/code&amp;gt; instead of decimal-encoded entites like &amp;lt;code&amp;gt;&amp;amp;amp;#8212;&amp;lt;/code&amp;gt;.&amp;lt;/p&amp;gt; 如果要建立一个已经格式化好的代码区块，只要每行都缩进 4 个空格或是一个 tab 就可以了，而 &amp;amp;、&amp;lt; 和 &amp;gt; 也一样会自动转成 HTML 实体。 Markdown 语法: If you want your page to validate under XHTML 1.0 Strict, you&amp;#39;ve got to put paragraph tags in your blockquotes: &amp;lt;blockquote&amp;gt; &amp;lt;p&amp;gt;For example.&amp;lt;/p&amp;gt; &amp;lt;/blockquote&amp;gt; 输出 HTML 为： &amp;lt;p&amp;gt;If you want your page to validate under XHTML 1.0 Strict, you&amp;#39;ve got to put paragraph tags in your blockquotes:&amp;lt;/p&amp;gt; &amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;&amp;amp;lt;blockquote&amp;amp;gt; &amp;amp;lt;p&amp;amp;gt;For example.&amp;amp;lt;/p&amp;amp;gt; &amp;amp;lt;/blockquote&amp;amp;gt; &amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Markdown 语法说明 z]]></title>
    	<url>/tech/2011/11/06/markdown/</url>
		<content type="text"><![CDATA[[1 概述 1.1 宗旨 1.2 兼容 HTML 1.3 特殊字符自动转换 2 区块元素 2.1 段落和换行 2.2 标题 2.3 区块引用 Blockquotes 2.4 列表 2.5 代码区块 2.6 分隔线 3 区段元素 3.1 链接 3.2 强调 3.3 代码 3.4 图片 4 其 4.1 自动链接 4.2 反斜杠 5 感谢 6 Markdown 免费编辑器 NOTE: This is Simplelified Chinese Edition Document of Markdown Syntax. If you are seeking for English Edition Document. Please refer to Markdown: Syntax. 声明： 这份文档派生(fork)于繁体中文版，在此基础上进行了繁体转简体工作，并进行了适当的润色。此文档用 Markdown 语法编写，你可以到这里查看它的源文件。「繁体中文版的原始文件可以查看这里 。」–By @riku 注： 本项目托管于 GitHub 上，请通过“派生”和“合并请求”来帮忙改进本项目。 1 概述 1.1 宗旨 Markdown 的目标是实现「易读易写」。 可读性，无论如何，都是最重要的。一份使用 Markdown 格式撰写的文件应该可以直接以纯文本发布，并且看起来不会像是由许多标签或是格式指令所构成。Markdown 语法受到一些既有 text-to-HTML 格式的影响，包括 Setext、atx、Textile、reStructuredText、Grutatext 和 EtText，而最大灵感来源其实是纯文本电子邮件的格式。 总之， Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然。比如：在文字两旁加上星号，看起来就像*强调*。Markdown 的列表看起来，嗯，就是列表。Markdown 的区块引用看起来就真的像是引用一段文字，就像你曾在电子邮件中见过的那样。 1.2 兼容 HTML Markdown 语法的目标是：成为一种适用于网络的书写语言。 Markdown 不是想要取代 HTML，甚至也没有要和它相近，它的语法种类很少，只对应 HTML 标记的一小部分。Markdown 的构想不是要使得 HTML 文档更容易书写。在我看来， HTML 已经很容易写了。Markdown 的理念是，能让文档更容易读、写和随意改。HTML 是一种发布的格式，Markdown 是一种书写的格式。就这样，Markdown 的格式语法只涵盖纯文本可以涵盖的范围。 不在 Markdown 涵盖范围之内的标签，都可以直接在文档里面用 HTML 撰写。不需要额外标注这是 HTML 或是 Markdown；只要直接加标签就可以了。 要制约的只有一些 HTML 区块元素――比如 &amp;lt;div&amp;gt;、&amp;lt;table&amp;gt;、&amp;lt;pre&amp;gt;、&amp;lt;p&amp;gt; 等标签，必须在前后加上空行与其它内容区隔开，还要求它们的开始标签与结尾标签不能用制表符或空格来缩进。Markdown 的生成器有足够智能，不会在 HTML 区块标签外加上不必要的 &amp;lt;p&amp;gt; 标签。 例子如下，在 Markdown 文件里加上一段 HTML 表格： 这是一个普通段落。 &amp;lt;table&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;Foo&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/table&amp;gt; 这是另一个普通段落。 注意：在 HTML 区块标签间的 Markdown 格式语法将不会被处理。比如，你在 HTML 区块内使用 Markdown 样式的*强调*会没有效果。 HTML 的区段（行内）标签如 &amp;lt;span&amp;gt;、&amp;lt;cite&amp;gt;、&amp;lt;del&amp;gt; 可以在 Markdown 的段落、列表或是标题里随意使用。依照个人习惯，甚至可以不用 Markdown 格式，而直接采用 HTML 标签来格式化。举例说明：如果比较喜欢 HTML 的 &amp;lt;a&amp;gt; 或 &amp;lt;img&amp;gt; 标签，可以直接使用这些标签，而不用 Markdown 提供的链接或是图像标签语法。 提醒：和处在 HTML 区块标签间不同，Markdown 语法在 HTML 区段标签间是有效的。 1.3 特殊字符自动转换 在 HTML 文件中，有两个字符需要特殊处理： &amp;lt; 和 &amp;amp; 。 &amp;lt; 符号用于起始标签，&amp;amp; 符号则用于标记 HTML 实体，如果你只是想要显示这些字符的原型，你必须要使用实体的形式，像是 &amp;amp;lt; 和 &amp;amp;amp;。 &amp;amp; 字符尤其让网络文档编写者受折磨，如果你要打「AT&amp;amp;T」 ，你必须要写成「AT&amp;amp;amp;T」。而网址中的 &amp;amp; 字符也要转换。比如你要链接到： http://images.google.com/images?num=30&amp;amp;q=larry&#43;bird 你必须要把网址转换写为： http://images.google.com/images?num=30&amp;amp;amp;q=larry&#43;bird 才能放到链接标签的 href 属性里。不用说也知道这很容易忽略，这也可能是 HTML 标准检验所检查到的错误中，数量最多的。 Markdown 让你可以自然地书写字符，需要转换的由它来处理好了。如果你使用的 &amp;amp; 字符是 HTML 字符实体的一部分，它会保留原状，否则它会被转换成 &amp;amp;amp;。 所以你如果要在文档中插入一个版权符号 ©，你可以这样写： &amp;amp;copy; Markdown 会保留它不动。而若你写： AT&amp;amp;T Markdown 就会将它转为： AT&amp;amp;amp;T 类似的状况也会发生在 &amp;lt; 符号上，因为 Markdown 允许 兼容 HTML ，如果你是把 &amp;lt; 符号作为 HTML 标签的定界符使用，那 Markdown 也不会对它做任何转换，但是如果你写： 4 &amp;lt; 5 Markdown 将会把它转换为： 4 &amp;amp;lt; 5 不过需要注意的是，code 范围内，不论是行内还是区块， &amp;lt; 和 &amp;amp; 两个符号都一定会被转换成 HTML 实体，这项特性让你可以很容易地用 Markdown 写 HTML code （和 HTML 相对而言， HTML 语法中，你要把所有的 &amp;lt; 和 &amp;amp; 都转换为 HTML 实体，才能在 HTML 文件里面写出 HTML code。） 2 区块元素 2.1 段落和换行 一个 Markdown 段落是由一个或多个连续的文本行组成，它的前后要有一个以上的空行（空行的定义是显示上看起来像是空的，便会被视为空行。比方说，若某一行只包含空格和制表符，则该行也会被视为空行）。普通段落不该用空格或制表符来缩进。 「由一个或多个连续的文本行组成」这句话其实暗示了 Markdown 允许段落内的强迫换行（插入换行符），这个特性和其他大部分的 text-to-HTML 格式不一样（包括 Movable Type 的「Convert Line Breaks」选项），其它的格式会把每个换行符都转成 &amp;lt;br /&amp;gt; 标签。 如果你确实想要依赖 Markdown 来插入 &amp;lt;br /&amp;gt; 标签的话，在插入处先按入两个以上的空格然后回车。 的确，需要多费点事（多加空格）来产生 &amp;lt;br /&amp;gt; ，但是简单地「每个换行都转换为 &amp;lt;br /&amp;gt;」的方法在 Markdown 中并不适合， Markdown 中 email 式的 区块引用 和多段落的 列表 在使用换行来排版的时候，不但更好用，还更方便阅读。 2.2 标题 Markdown 支持两种标题的语法，类 Setext 和类 atx 形式。 类 Setext 形式是用底线的形式，利用 = （最高阶标题）和 - （第二阶标题），例如： This is an H1 ============= This is an H2 ------------- 任何数量的 = 和 - 都可以有效果。 类 Atx 形式则是在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如： # 这是 H1 ## 这是 H2 ###### 这是 H6 你可以选择性地「闭合」类 atx 样式的标题，这纯粹只是美观用的，若是觉得这样看起来比较舒适，你就可以在行尾加上 #，而行尾的 # 数量也不用和开头一样（行首的井字符数量决定标题的阶数）： # 这是 H1 # ## 这是 H2 ## ### 这是 H3 ###### 2.3 区块引用 Blockquotes Markdown 标记区块引用是使用类似 email 中用 &amp;gt; 的引用方式。如果你还熟悉在 email 信件中的引言部分，你就知道怎么在 Markdown 文件中建立一个区块引用，那会看起来像是你自己先断好行，然后在每行的最前面加上 &amp;gt; ： &amp;gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, &amp;gt; consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. &amp;gt; Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. &amp;gt; &amp;gt; Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse &amp;gt; id sem consectetuer libero luctus adipiscing. Markdown 也允许你偷懒只在整个段落的第一行最前面加上 &amp;gt; 1： &amp;gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. &amp;gt; Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 区块引用可以嵌套（例如：引用内的引用），只要根据层次加上不同数量的 &amp;gt; ： &amp;gt; This is the first level of quoting. &amp;gt; &amp;gt; &amp;gt; This is nested blockquote. &amp;gt; &amp;gt; Back to the first level. 引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等： &amp;gt; ## 这是一个标题。 &amp;gt; &amp;gt; 1. 这是第一行列表项。 &amp;gt; 2. 这是第二行列表项。 &amp;gt; &amp;gt; 给出一些例子代码： &amp;gt; &amp;gt; return shell_exec(&amp;quot;echo $input | $markdown_script&amp;quot;); 任何像样的文本编辑器都能轻松地建立 email 型的引用。例如在 BBEdit 中，你可以选取文字后然后从选单中选择增加引用阶层。 2.4 列表 Markdown 支持有序列表和无序列表。 无序列表使用星号、加号或是减号作为列表标记： * Red * Green * Blue 等同于： &#43; Red &#43; Green &#43; Blue 也等同于： - Red - Green - Blue 有序列表则使用数字接着一个英文句点： 1. Bird 2. McHale 3. Parish 很重要的一点是，你在列表标记上使用的数字并不会影响输出的 HTML 结果，上面的列表所产生的 HTML 标记为： &amp;lt;ol&amp;gt; &amp;lt;li&amp;gt;Bird&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;McHale&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;Parish&amp;lt;/li&amp;gt; &amp;lt;/ol&amp;gt; 如果你的列表标记写成： 1. Bird 1. McHale 1. Parish 或甚至是： 3. Bird 1. McHale 8. Parish 你都会得到完全相同的 HTML 输出。重点在于，你可以让 Markdown 文件的列表数字和输出的结果相同，或是你懒一点，你可以完全不用在意数字的正确性。 如果你使用懒惰的写法，建议第一个项目最好还是从 1. 开始，因为 Markdown 未来可能会支持有序列表的 start 属性。 列表项目标记通常是放在最左边，但是其实也可以缩进，最多 3 个空格，项目标记后面则一定要接着至少一个空格或制表符。 要让列表看起来更漂亮，你可以把内容用固定的缩进整理好： * Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. * Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 但是如果你懒，那也行： * Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. * Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 如果列表项目间用空行分开，在输出 HTML 时 Markdown 就会将项目内容用 &amp;lt;p&amp;gt; 标签包起来，举例来说： * Bird * Magic 会被转换为： &amp;lt;ul&amp;gt; &amp;lt;li&amp;gt;Bird&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;Magic&amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; 但是这个： * Bird * Magic 会被转换为： &amp;lt;ul&amp;gt; &amp;lt;li&amp;gt;&amp;lt;p&amp;gt;Bird&amp;lt;/p&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt;&amp;lt;p&amp;gt;Magic&amp;lt;/p&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; 列表项目可以包含多个段落，每个项目下的段落都必须缩进 4 个空格或是 1 个制表符： 1. This is a list item with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. Donec sit amet nisl. Aliquam semper ipsum sit amet velit. 2. Suspendisse id sem consectetuer libero luctus adipiscing. 如果你每行都有缩进，看起来会看好很多，当然，再次地，如果你很懒惰，Markdown 也允许： * This is a list item with two paragraphs. This is the second paragraph in the list item. You&amp;#39;re only required to indent the first line. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. * Another item in the same list. 如果要在列表项目内放进引用，那 &amp;gt; 就需要缩进： * A list item with a blockquote: &amp;gt; This is a blockquote &amp;gt; inside a list item. 如果要放代码区块的话，该区块就需要缩进两次，也就是 8 个空格或是 2 个制表符： * 一列表项包含一个列表区块： &amp;lt;代码写在这&amp;gt; 当然，项目列表很可能会不小心产生，像是下面这样的写法： 1986. What a great season. 换句话说，也就是在行首出现数字-句点-空白，要避免这样的状况，你可以在句点前面加上反斜杠。 1986\. What a great season. 2.5 代码区块 和程序相关的写作或是标签语言原始码通常会有已经排版好的代码区块，通常这些区块我们并不希望它以一般段落文件的方式去排版，而是照原来的样子显示，Markdown 会用 &amp;lt;pre&amp;gt; 和 &amp;lt;code&amp;gt; 标签来把代码区块包起来。 要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以，例如，下面的输入： 这是一个普通段落： 这是一个代码区块。 Markdown 会转换成： &amp;lt;p&amp;gt;这是一个普通段落：&amp;lt;/p&amp;gt; &amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;这是一个代码区块。 &amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt; 这个每行一阶的缩进（4 个空格或是 1 个制表符），都会被移除，例如： Here is an example of AppleScript: tell application &amp;quot;Foo&amp;quot; beep end tell 会被转换为： &amp;lt;p&amp;gt;Here is an example of AppleScript:&amp;lt;/p&amp;gt; &amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;tell application &amp;quot;Foo&amp;quot; beep end tell &amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt; 一个代码区块会一直持续到没有缩进的那一行（或是文件结尾）。 在代码区块里面， &amp;amp; 、 &amp;lt; 和 &amp;gt; 会自动转成 HTML 实体，这样的方式让你非常容易使用 Markdown 插入范例用的 HTML 原始码，只需要复制贴上，再加上缩进就可以了，剩下的 Markdown 都会帮你处理，例如： &amp;lt;div class=&amp;quot;footer&amp;quot;&amp;gt; &amp;amp;copy; 2004 Foo Corporation &amp;lt;/div&amp;gt; 会被转换为： &amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;&amp;amp;lt;div class=&amp;quot;footer&amp;quot;&amp;amp;gt; &amp;amp;amp;copy; 2004 Foo Corporation &amp;amp;lt;/div&amp;amp;gt; &amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt; 代码区块中，一般的 Markdown 语法不会被转换，像是星号便只是星号，这表示你可以很容易地以 Markdown 语法撰写 Markdown 语法相关的文件。 2.6 分隔线 你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线： * * * *** ***** - - - --------------------------------------- 3 区段元素 3.1 链接 Markdown 支持两种形式的链接语法： 行内式和参考式两种形式。 不管是哪一种，链接文字都是用 [方括号] 来标记。 要建立一个行内式的链接，只要在方块括号后面紧接着圆括号并插入网址链接即可，如果你还想要加上链接的 title 文字，只要在网址后面，用双引号把 title 文字包起来即可，例如： This is [an example](http://example.com/ &amp;quot;Title&amp;quot;) inline link. [This link](http://example.net/) has no title attribute. 会产生： &amp;lt;p&amp;gt;This is &amp;lt;a href=&amp;quot;http://example.com/&amp;quot; title=&amp;quot;Title&amp;quot;&amp;gt; an example&amp;lt;/a&amp;gt; inline link.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;a href=&amp;quot;http://example.net/&amp;quot;&amp;gt;This link&amp;lt;/a&amp;gt; has no title attribute.&amp;lt;/p&amp;gt; 如果你是要链接到同样主机的资源，你可以使用相对路径： See my [About](/about/) page for details. 参考式的链接是在链接文字的括号后面再接上另一个方括号，而在第二个方括号里面要填入用以辨识链接的标记： This is [an example][id] reference-style link. 你也可以选择性地在两个方括号中间加上一个空格： This is [an example] [id] reference-style link. 接着，在文件的任意处，你可以把这个标记的链接内容定义出来： [id]: http://example.com/ &amp;quot;Optional Title Here&amp;quot; 链接内容定义的形式为： 方括号（前面可以选择性地加上至多三个空格来缩进），里面输入链接文字 接着一个冒号 接着一个以上的空格或制表符 接着链接的网址 选择性地接着title内容，可以用单引号、双引号或是括弧包着 下面这三种链接的定义都是相同： [foo]: http://example.com/ &amp;quot;Optional Title Here&amp;quot; [foo]: http://example.com/ &amp;#39;Optional Title Here&amp;#39; [foo]: http://example.com/ (Optional Title Here) 注意：有一个已知的问题是 Markdown.pl 1.0.1 会忽略单引号包起来的链接 title。 链接网址也可以用尖括号包起来： [id]: &amp;lt;http://example.com/&amp;gt; &amp;quot;Optional Title Here&amp;quot; 你也可以把title属性放到下一行，也可以加一些缩进，若网址太长的话，这样会比较好看： [id]: http://example.com/longish/path/to/resource/here &amp;quot;Optional Title Here&amp;quot; 网址定义只有在产生链接的时候用到，并不会直接出现在文件之中。 链接辨别标签可以有字母、数字、空白和标点符号，但是并不区分大小写，因此下面两个链接是一样的： [link text][a] [link text][A] 隐式链接标记功能让你可以省略指定链接标记，这种情形下，链接标记会视为等同于链接文字，要用隐式链接标记只要在链接文字后面加上一个空的方括号，如果你要让 “Google” 链接到 google.com，你可以简化成： [Google][] 然后定义链接内容： [Google]: http://google.com/ 由于链接文字可能包含空白，所以这种简化型的标记内也许包含多个单词： Visit [Daring Fireball][] for more information. 然后接着定义链接： [Daring Fireball]: http://daringfireball.net/ 链接的定义可以放在文件中的任何一个地方，我比较偏好直接放在链接出现段落的后面，你也可以把它放在文件最后面，就像是注解一样。 下面是一个参考式链接的范例： I get 10 times more traffic from [Google] [1] than from [Yahoo] [2] or [MSN] [3]. [1]: http://google.com/ &amp;quot;Google&amp;quot; [2]: http://search.yahoo.com/ &amp;quot;Yahoo Search&amp;quot; [3]: http://search.msn.com/ &amp;quot;MSN Search&amp;quot; 如果改成用链接名称的方式写： I get 10 times more traffic from [Google][] than from [Yahoo][] or [MSN][]. [google]: http://google.com/ &amp;quot;Google&amp;quot; [yahoo]: http://search.yahoo.com/ &amp;quot;Yahoo Search&amp;quot; [msn]: http://search.msn.com/ &amp;quot;MSN Search&amp;quot; 上面两种写法都会产生下面的 HTML。 &amp;lt;p&amp;gt;I get 10 times more traffic from &amp;lt;a href=&amp;quot;http://google.com/&amp;quot; title=&amp;quot;Google&amp;quot;&amp;gt;Google&amp;lt;/a&amp;gt; than from &amp;lt;a href=&amp;quot;http://search.yahoo.com/&amp;quot; title=&amp;quot;Yahoo Search&amp;quot;&amp;gt;Yahoo&amp;lt;/a&amp;gt; or &amp;lt;a href=&amp;quot;http://search.msn.com/&amp;quot; title=&amp;quot;MSN Search&amp;quot;&amp;gt;MSN&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt; 下面是用行内式写的同样一段内容的 Markdown 文件，提供作为比较之用： I get 10 times more traffic from [Google](http://google.com/ &amp;quot;Google&amp;quot;) than from [Yahoo](http://search.yahoo.com/ &amp;quot;Yahoo Search&amp;quot;) or [MSN](http://search.msn.com/ &amp;quot;MSN Search&amp;quot;). 参考式的链接其实重点不在于它比较好写，而是它比较好读，比较一下上面的范例，使用参考式的文章本身只有 81 个字符，但是用行内形式的却会增加到 176 个字元，如果是用纯 HTML 格式来写，会有 234 个字元，在 HTML 格式中，标签比文本还要多。 使用 Markdown 的参考式链接，可以让文件更像是浏览器最后产生的结果，让你可以把一些标记相关的元数据移到段落文字之外，你就可以增加链接而不让文章的阅读感觉被打断。 3.2 强调 Markdown 使用星号（*）和底线（_）作为标记强调字词的符号，被 * 或 _ 包围的字词会被转成用 &amp;lt;em&amp;gt; 标签包围，用两个 * 或 _ 包起来的话，则会被转成 &amp;lt;strong&amp;gt;，例如： *single asterisks* _single underscores_ **double asterisks** __double underscores__ 会转成： &amp;lt;em&amp;gt;single asterisks&amp;lt;/em&amp;gt; &amp;lt;em&amp;gt;single underscores&amp;lt;/em&amp;gt; &amp;lt;strong&amp;gt;double asterisks&amp;lt;/strong&amp;gt; &amp;lt;strong&amp;gt;double underscores&amp;lt;/strong&amp;gt; 你可以随便用你喜欢的样式，唯一的限制是，你用什么符号开启标签，就要用什么符号结束。 强调也可以直接插在文字中间： un*frigging*believable 但是如果你的 * 和 _ 两边都有空白的话，它们就只会被当成普通的符号。 如果要在文字前后直接插入普通的星号或底线，你可以用反斜线： \*this text is surrounded by literal asterisks\* 3.3 代码 如果要标记一小段行内代码，你可以用反引号把它包起来（`），例如： Use the `printf()` function. 会产生： &amp;lt;p&amp;gt;Use the &amp;lt;code&amp;gt;printf()&amp;lt;/code&amp;gt; function.&amp;lt;/p&amp;gt; 如果要在代码区段内插入反引号，你可以用多个反引号来开启和结束代码区段： ``There is a literal backtick (`) here.`` 这段语法会产生： &amp;lt;p&amp;gt;&amp;lt;code&amp;gt;There is a literal backtick (`) here.&amp;lt;/code&amp;gt;&amp;lt;/p&amp;gt; 代码区段的起始和结束端都可以放入一个空白，起始端后面一个，结束端前面一个，这样你就可以在区段的一开始就插入反引号： A single backtick in a code span: `` ` `` A backtick-delimited string in a code span: `` `foo` `` 会产生： &amp;lt;p&amp;gt;A single backtick in a code span: &amp;lt;code&amp;gt;`&amp;lt;/code&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;A backtick-delimited string in a code span: &amp;lt;code&amp;gt;`foo`&amp;lt;/code&amp;gt;&amp;lt;/p&amp;gt; 在代码区段内，&amp;amp; 和尖括号都会被自动地转成 HTML 实体，这使得插入 HTML 原始码变得很容易，Markdown 会把下面这段： Please don&amp;#39;t use any `&amp;lt;blink&amp;gt;` tags. 转为： &amp;lt;p&amp;gt;Please don&amp;#39;t use any &amp;lt;code&amp;gt;&amp;amp;lt;blink&amp;amp;gt;&amp;lt;/code&amp;gt; tags.&amp;lt;/p&amp;gt; 你也可以这样写： `&amp;amp;#8212;` is the decimal-encoded equivalent of `&amp;amp;mdash;`. 以产生： &amp;lt;p&amp;gt;&amp;lt;code&amp;gt;&amp;amp;amp;#8212;&amp;lt;/code&amp;gt; is the decimal-encoded equivalent of &amp;lt;code&amp;gt;&amp;amp;amp;mdash;&amp;lt;/code&amp;gt;.&amp;lt;/p&amp;gt; 3.4 图片 很明显地，要在纯文字应用中设计一个「自然」的语法来插入图片是有一定难度的。 Markdown 使用一种和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式。 行内式的图片语法看起来像是： ![Alt text](/path/to/img.jpg) ![Alt text](/path/to/img.jpg &amp;quot;Optional title&amp;quot;) 详细叙述如下： 一个惊叹号 ! 接着一个方括号，里面放上图片的替代文字 接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的title文字。 参考式的图片语法则长得像这样： ![Alt text][id] 「id」是图片参考的名称，图片参考的定义方式则和链结参考一样： [id]: url/to/image &amp;quot;Optional title attribute&amp;quot; 到目前为止， Markdown 还没有办法指定图片的宽高，如果你需要的话，你可以使用普通的 &amp;lt;img&amp;gt; 标签。 4 其它 4.1 自动链接 Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用尖括号包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如： &amp;lt;http://example.com/&amp;gt; Markdown 会转为： &amp;lt;a href=&amp;quot;http://example.com/&amp;quot;&amp;gt;http://example.com/&amp;lt;/a&amp;gt; 邮址的自动链接也很类似，只是 Markdown 会先做一个编码转换的过程，把文字字符转成 16 进位码的 HTML 实体，这样的格式可以糊弄一些不好的邮址收集机器人，例如： &amp;lt;address@example.com&amp;gt; Markdown 会转成： &amp;lt;a href=&amp;quot;&amp;amp;#x6D;&amp;amp;#x61;i&amp;amp;#x6C;&amp;amp;#x74;&amp;amp;#x6F;:&amp;amp;#x61;&amp;amp;#x64;&amp;amp;#x64;&amp;amp;#x72;&amp;amp;#x65; &amp;amp;#115;&amp;amp;#115;&amp;amp;#64;&amp;amp;#101;&amp;amp;#120;&amp;amp;#x61;&amp;amp;#109;&amp;amp;#x70;&amp;amp;#x6C;e&amp;amp;#x2E;&amp;amp;#99;&amp;amp;#111; &amp;amp;#109;&amp;quot;&amp;gt;&amp;amp;#x61;&amp;amp;#x64;&amp;amp;#x64;&amp;amp;#x72;&amp;amp;#x65;&amp;amp;#115;&amp;amp;#115;&amp;amp;#64;&amp;amp;#101;&amp;amp;#120;&amp;amp;#x61; &amp;amp;#109;&amp;amp;#x70;&amp;amp;#x6C;e&amp;amp;#x2E;&amp;amp;#99;&amp;amp;#111;&amp;amp;#109;&amp;lt;/a&amp;gt; 在浏览器里面，这段字串（其实是 &amp;lt;a href=&amp;quot;mailto:address@example.com&amp;quot;&amp;gt;address@example.com&amp;lt;/a&amp;gt;）会变成一个可以点击的「address@example.com」链接。 （这种作法虽然可以糊弄不少的机器人，但并不能全部挡下来，不过总比什么都不做好些。不管怎样，公开你的信箱终究会引来广告信件的。） 4.2 反斜杠 Markdown 可以利用反斜杠来插入一些在语法中有其它意义的符号，例如：如果你想要用星号加在文字旁边的方式来做出强调效果（但不用 &amp;lt;em&amp;gt; 标签），你可以在星号的前面加上反斜杠： \*literal asterisks\* Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号： \ 反斜线 ` 反引号 * 星号 _ 底线 {} 花括号 [] 方括号 () 括弧 # 井字号 &#43; 加号 - 减号 . 英文句点 ! 惊叹号 5 感谢 感谢 leafy7382 协助翻译，hlb、Randylien 帮忙润稿，ethantw 的汉字标准格式・CSS Reset， WM 回报文字错误。 感谢 fenprace，addv。 6 Markdown 免费编辑器 Windows 平台 MarkdownPad MarkPad Linux 平台 ReText Mac 平台 Mou 在线编辑器 Markable.in Dillinger.io 浏览器插件 MaDe (Chrome) 高级应用 Sublime Text 2 &#43; MarkdownEditing / 教程 如有更好的 Markdown 免费编辑器推荐，请到这里反馈，谢谢！ Pandoc 的语法扩展情形中，这种只在第一行加 &amp;gt; 的做法不再适用。↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[listings 排源代码，如何让反斜杠高亮显示？z]]></title>
    	<url>/tech/2011/11/03/lsset-highlight-blackslash/</url>
		<content type="text"><![CDATA[[原文地址：http://bbs.chinatex.org/forum.php?mod=viewthread&amp;amp;tid=7282&amp;amp 原始出处：http://tex.stackexchange.com/questions/17774/listings-package-can-i-include-a-backslash-in-language-keyword-begin-for 如第一幅图所示，高亮的结果是有问题的，也不太好看，实际，我们需要用 texcsstyle 来设置即可，有关该指令的作用参考 listings 宏包的手册或http://www.chinatex.org/archives/620，这里的 cs 即控制序列。 \documentclass{article} \usepackage[T1]{fontenc} \usepackage[scaled=0.82]{beramono} \usepackage{listings,xcolor} \begin{document} \begin{lstlisting}[basicstyle=\small\ttfamily,language={[LaTeX]TeX}, texcsstyle=*\color{red}\bfseries, keywordstyle=\color{blue}\bfseries, morekeywords=alignat,moretexcs=intertext] \begin{alignat*}{4} y &amp;amp;= -4 &amp;amp;&#43; 3 &amp;amp;&#43;4 &amp;amp;-7 \\ y &amp;amp;= &amp;amp;&#43; 3 &amp;amp; &amp;amp;-7 \\ \intertext{Therefore} a &amp;amp;= b &amp;amp;d &amp;amp;= cccc &amp;amp;e &amp;amp;= d \\ a &amp;amp;= bbbb &amp;amp;d &amp;amp;= c &amp;amp;e &amp;amp;= d \end{alignat*} \end{lstlisting} \end{document} 正确的演示效果如下图所示：]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[CSS3：REM、行高、基线及元素间距 z]]></title>
    	<url>/tech/2011/10/27/css-rem/</url>
		<content type="text"><![CDATA[[原文地址：http://www.w3cplus.com/css3/define-font-size-with-css3-rem 在Web中使用 Font-Size: em vs. px vs. pt vs. percent这样的PK大局。不幸的是，仍然有不同的利弊，使各种技术都不太理想，但又无法不去用。真是进也难，退也难呀。 最近在学习em的相关知识的时候，无意之间让我拾得一宝，就是使用rem来设置 Web 页面的字体大小。让我一下子就来劲了，一口气看完并测试了一回，还真是爽歪歪的呀。师傅说好东西不能吃独食，于我就在这里给大家吹吹这个从没见过的REM。 在详细介绍rem之前，我们先一起来回顾一下我们常用的两种记量单位，也是备受争论的两个： PX 为单位 EM 为单位 PX 为单位 在Web页面初期制作中，我们都是使用“px”来设置我们的文本，因为他比较稳定和精确。但是这种方法存在一个问题，当用户在浏览器中浏览我们制作的Web页面时，他改变了浏览器的字体大小，这时会使用我们的Web页面布局被打破。这样对于那些关心自己网站可用性的用户来说，就是一个大问题了。因此，这时就提出了使用em来定义Web页面的字体。 em 为单位 前面也说了，使用是“px”为单位是比较方便，而又一致，但在浏览器中放大或缩放浏览页面时会存在一个问题，要解决这个问题，我们可以使用“em”单位。Richard Rutter&amp;rsquo;在《How to size text using ems》一文中有做过详细的介绍，追至早一点，Richard Rutter也在《How to Size Text in CSS》中进行过深入的剖析。 这种技术需要一个参考点，一般都是以&amp;lt;body&amp;gt;的“font-size”为基准。比如说我们使用“1em”等于“10px”来改变默认值“1em=16px”，这样一来，我们设置字体大小相当于“14px”时，只需要将其值设置为“1.4em”。 body { font-size: 62.5%; /*10 ÷ 16 × 100% = 62.5%*/ } h1 { font-size: 2.4em; /*2.4em × 10 = 24px */ } p { font-size: 1.4em; /*1.4em × 10 = 14px */ } li { font-size: 1.4em; /*1.4 × ? = 14px ? */ } 为什么“li”的“1.4em”是不是“14px”将是一个问号呢？如果你了解过“em”后，你会觉得这个问题是多问的。前面也简单的介绍过一回，在使用“em”作单位时，一定需要知道其父元素的设置，因为“em”就是一个相对值，而且是一个相对于父元素的值，其真正的计算公式是： 1 ÷ 父元素的font-size × 需要转换的像素值 = em值 这样的情况下“1.4em”可以是“14px”,也可以是“20px”，或者说是“24px”，总之是一个不确定值，那么解决这样的问题，要么你知道其父元素的值，要么呢在任何子元素中都使用“1em”。这样一来可能又不是我们所需要的方法。 这里我只是简单的介绍了一个这两个单位的使用，具体一点的大家可以参阅： Best Practices的站长Kyle的《CSS Font-Size: em vs. px vs. pt vs. percent》 Converting px into percentage and em for relative CSS font sizes Em Vs Percent Widths CSS: Units of Measurement Jennifer Kyrnin的Using Points, Pixels, Ems, or Percentages for CSS Fonts Rem为单位 CSS3的出现，他同时引进了一些新的单位，包括我们今天所说的rem。在W3C官网上是这样描述rem的——“font size of the root element”。下面我们就一起来详细的了解rem。 前面说了“em”是相对于其父元素来设置字体大小的，这样就会存在一个问题，进行任何元素设置，都有可能需要知道他父元素的大小，在我们多次使用时，就会带来无法预知的错误风险。而rem是相对于根元素，这样就意味着，我们只需要在根元素确定一个参考值，在根元素中设置多大的字体，这完全可以根据您自己的需，大家也可以参考下图： 我们来看一个简单的代码实例： html {font-size: 62.5%;/*10 ÷ 16 × 100% = 62.5%*/} body {font-size: 1.4rem;/*1.4 × 10px = 14px */} h1 { font-size: 2.4rem;/*2.4 × 10px = 24px*/} 我在根元素中定义了一个基本字体大小为62.5%（也就是10px。设置这个值主要方便计算，如果没有设置，将是以“16px”为基准 ）。从上面的计算结果，我们使用“rem”就像使用“px”一样的方便，而且同时解决了“px”和“em”两者不同之处。 浏览器的兼容性 rem是CSS3新引进来的一个度量单位，大家心里肯定会觉得心灰意冷呀，担心浏览器的支持情况。其实大家不用害怕，你可能会惊讶，支持的浏览器还是蛮多的，比如：Mozilla Firefox 3.6&#43;、Apple Safari 5&#43;、Google Chrome、IE9&#43;和Opera11&#43;。只是可怜的IE6-8无法，你们就把他们当透明了吧，我向来都是如此。 不过使用单位设置字体，可不能完全不考虑IE了，如果你想使用这个REM，但也想兼容IE下的效果，可你可考虑“px”和“rem”一起使用，用&amp;rdquo;px&amp;rdquo;来实现IE6-8下的效果，然后使用“Rem”来实现代浏览器的效果。就让IE6-8不能随文字的改变而改变吧，谁让这个Ie6-8这么老呢？哈。。。。大家不仿试试，还蛮有意思，说不定这个就是主流的度量单位了。 行高、基线、元素间距相关资料 inline-block元素vertical-align的问题分析：http://www.cnblogs.com/zxjwlh/p/6219896.html 深入理解CSS中的行高与基线：http://blog.csdn.net/lulujiajiawenwen/article/details/8245201 CSS line-height与vertical-align:baseline：http://www.cnblogs.com/cc156676/p/5676237.html 去除inline-block元素间间距的 N 种方法：http://www.zhangxinxu.com/wordpress/2012/04/inline-block-space-remove-%E5%8E%BB%E9%99%A4%E9%97%B4%E8%B7%9D/]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[输出 TeX 族命令名称 z]]></title>
    	<url>/tech/2011/07/22/typesetting-latex-cmd/</url>
		<content type="text"><![CDATA[[参考：Macro or package for typesetting “LaTeX” (the name)? lshort 或者类似的教程中都会提及如何在 LaTeX 中输出 TeX 风格的 TeX 族命令名称本身的方法，这里给出的可能更加完整一些 \LaTeX 输出 LaTeX 标志； \LaTeXe 输出 LaTeX2e 标志； \TeX 输出原始的 TeX 字样，可以用来生成其它以 TeX 字样为基础的标志； \AmSTeX, \BibTeX, \SliTeX 以及 \PlainTeX 各种名称对应的标志，它们具体定义在 doc 宏包中； \XeTeX, \XeLaTeX, \LuaTeX 以及 \LuaLaTeX 可以在 metalogo 宏我中找到； e-TeX 及其它一些标志可以在 hologo 宏包中找到； 最后，在 mflogo 宏包中可以找到 METAFONT logo； metalogo 宏包还允许自定义 \TeX, \LaTeX, \LaTeXe, \XeTeX, \XeLaTeX, \LuaTeX 以及 \LuaLaTeX 等标志，这在使用非 Computer Modern 族字体时会比较有用。 可进一步参考 http://www.tex.ac.uk/cgi-bin/texfaq2html?label=logos。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python - super() z]]></title>
    	<url>/tech/2011/05/03/super-mro/</url>
		<content type="text"><![CDATA[[原文地址：http://www.cnblogs.com/lovemo1314/archive/2011/05/03/2035005.html 问题的发现与提出 在 Python 类的方法（method）中，要调用父类的某个方法，在 Python 2.2 以前，通常的写法如下： # 代码 1 class A: def __init__(self): print &amp;quot;enter A&amp;quot; print &amp;quot;leave A&amp;quot; class B(A): def __init__(self): print &amp;quot;enter B&amp;quot; A.__init__(self) print &amp;quot;leave B&amp;quot; &amp;gt;&amp;gt;&amp;gt; b = B() enter B enter A leave A leave B 即，使用非绑定的类方法（用类名来引用的方法），并在参数列表中，引入待绑定的对象（self），从而达到调用父类的目的。这样做的缺点是，当一个子类的父类发生变化时（如类B的父类由A变为C时），必须遍历整个类定义，把所有的通过非绑定的方法的类名全部替换过来，例如 # 代码 2 class B(C): # A --&amp;gt; C def __init__(self): print &amp;quot;enter B&amp;quot; C.__init__(self) # A --&amp;gt; C print &amp;quot;leave B&amp;quot; 如果代码简单，这样的改动或许还可以接受。但如果代码量庞大，这样的修改可能是灾难性的。因此，自 Python 2.2 开始，Python 添加了一个关键字super，来解决这个问题。下面是 Python 2.3 的官方文档说明： super(type[, object-or-type]) Return the superclass of type. If the second argument is omitted the super object returned is unbound. If the second argument is an object, isinstance(obj, type) must be true. If the second argument is a type, issubclass(type2, type) must be true. super() only works for new-style classes. A typical use for calling a cooperative superclass method is(New in version 2.2.): &amp;gt; class C(B): &amp;gt; def meth(self, arg): &amp;gt; super(C, self).meth(arg) &amp;gt; ``` 从说明来看，可以把类`B`改写如下： ```python # 代码 3 class A(object): # A must be new-style class def __init__(self): print &amp;quot;enter A&amp;quot; print &amp;quot;leave A&amp;quot; class B(C): # A --&amp;gt; C def __init__(self): print &amp;quot;enter B&amp;quot; super(B, self).__init__() print &amp;quot;leave B&amp;quot; 尝试执行上面同样的代码，结果一致，但修改的代码只有一处，把代码的维护量降到最低，是一个不错的用法。因此在我们的开发过程中，super关键字被大量使用，而且一直表现良好。 在我们的印象中，对于super(B, self).__init__()是这样理解的：super(B, self)首先找到B的父类（就是类A），然后把类B的对象self转换为类A的对象（通过某种方式，一直没有考究是什么方式，惭愧），然后“被转换”的类A对象调用自己的__init__函数。考虑到super中只有指明子类的机制，因此，在多继承的类定义中，通常我们保留使用类似前面第一段代码的方法。 有一天某同事设计了一个相对复杂的类体系结构（我们先不要管这个类体系设计得是否合理，仅把这个例子作为一个题目来研究就好），代码如下 # 代码 4 class A(object): def __init__(self): print &amp;quot;enter A&amp;quot; print &amp;quot;leave A&amp;quot; class B(object): def __init__(self): print &amp;quot;enter B&amp;quot; print &amp;quot;leave B&amp;quot; class C(A): def __init__(self): print &amp;quot;enter C&amp;quot; super(C, self).__init__() print &amp;quot;leave C&amp;quot; class D(A): def __init__(self): print &amp;quot;enter D&amp;quot; super(D, self).__init__() print &amp;quot;leave D&amp;quot; class E(B, C): def __init__(self): print &amp;quot;enter E&amp;quot; B.__init__(self) C.__init__(self) print &amp;quot;leave E&amp;quot; class F(E, D): def __init__(self): print &amp;quot;enter F&amp;quot; E.__init__(self) D.__init__(self) print &amp;quot;leave F&amp;quot; f = F() result: enter F enter E enter B leave B enter C enter D enter A leave A leave D leave C leave E enter D enter A leave A leave D leave F 明显地，类A和类D的初始化函数被重复调用了 2 次，这并不是我们所期望的结果！我们所期望的结果是最多只有类A的初始化函数被调用 2 次——其实这是多继承的类体系必须面对的问题。我们把前一段的类体系画出来，如下图： object | \ | A | / | B C D | \ / | E | \ | F 按我们对super的理解，从图中可以看出，在调用类C的初始化函数时，应该是调用类A的初始化函数，但事实上却调用了类D的初始化函数。好一个诡异的问题！ 也就是说，mro中记录了一个类的所有基类的类类型序列。查看mro的记录，发觉包含 7 个元素，7 个类名分别为：F E B C D A object。 从而说明了为什么在C.__init__中使用super(C, self).__init__()会调用类D的初始化函数了。我们把代码段 4 改写为： # 代码 5 class A(object): def __init__(self): print &amp;quot;enter A&amp;quot; super(A, self).__init__() # new print &amp;quot;leave A&amp;quot; class B(object): def __init__(self): print &amp;quot;enter B&amp;quot; super(B, self).__init__() # new print &amp;quot;leave B&amp;quot; class C(A): def __init__(self): print &amp;quot;enter C&amp;quot; super(C, self).__init__() print &amp;quot;leave C&amp;quot; class D(A): def __init__(self): print &amp;quot;enter D&amp;quot; super(D, self).__init__() print &amp;quot;leave D&amp;quot; class E(B, C): def __init__(self): print &amp;quot;enter E&amp;quot; super(E, self).__init__() # change print &amp;quot;leave E&amp;quot; class F(E, D): def __init__(self): print &amp;quot;enter F&amp;quot; super(F, self).__init__() # change print &amp;quot;leave F&amp;quot; f = F() result: enter F enter E enter B enter C enter D enter A leave A leave D leave C leave B leave E leave F 明显地，F的初始化不仅完成了所有的父类的调用，而且保证了每一个父类的初始化函数只调用一次。再看类结构： object / \ / A | / \ B-1 C-2 D-2 \ / / E-1 / \ / F E-1,D-2是F的父类，其中表示E类在前，即F(E，D)。 所以初始化顺序可以从类结构图来看出 ：F－&amp;gt;E-&amp;gt;B --&amp;gt;C --&amp;gt; D --&amp;gt; A 由于C，D有同一个父类，因此会先初始化D再是A。 延续的讨论 我们再重新看上面的类体系图，如果把每一个类看作图的一个节点，每一个从子类到父类的直接继承关系看作一条有向边，那么该体系图将变为一个有向图。不能发现mro的顺序正好是该有向图的一个拓扑排序序列。 从而，我们得到了另一个结果——Python 是如何去处理多继承。支持多继承的传统的面向对象程序语言（如 C&#43;&#43;）是通过虚拟继承的方式去实现多继承中父类的构造函数被多次调用的问题，而 Python 则通过mro的方式去处理。 但这给我们一个难题：对于提供类体系的编写者来说，他不知道使用者会怎么使用他的类体系，也就是说，不正确的后续类，可能会导致原有类体系的错误，而且这样的错误非常隐蔽的，也难于发现。 小结 super并不是一个函数，是一个类名，形如super(B, self)事实上调用了super类的初始化函数，产生了一个super对象； super类的初始化函数并没有做什么特殊的操作，只是简单记录了类类型和具体实例； super(B, self).func的调用并不是用于调用当前类的父类的func函数； Python 的多继承类是通过mro的方式来保证各个父类的函数被逐一调用，而且保证每个父类函数只调用一次（如果每个类都使用super）； 混用super类和非绑定的函数是一个危险行为，这可能导致应该调用的父类函数没有调用或者一个父类函数被调用多次。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[创建 SAS 宏变量的几类方法及举例 z]]></title>
    	<url>/tech/2011/04/11/2011-04-11-sas-macro-definition/</url>
		<content type="text"><![CDATA[[原文地址：http://saslist.net/archives/122 SAS 里面除了变量，还有宏变量，其用途也非常广泛。创建宏变量的方法最早有 shiyiming 总结，翻了翻 Rick Aster的Professional SAS Programming Shortcuts – Over 1,000 Ways To Improve Your SAS Programs，发现里面并没有总结这个问题，有点失望。 这里转载并补充姚志勇的 SAS 书里面的内容，使得更加完整和充实，便于大家以后方便选择使用，一共有 4 类方法。 通过直接赋值或通过宏函数创建宏变量 - 最基本最常用 %let mv = 100; %let dsid=%sysfunc(open(sashelp.class)); %let nvars=%sysfunc(attrn(&amp;amp;dsid,nvars)); %let nobs=%sysfunc(attrn(&amp;amp;dsid,nobs)); %let dsid=%sysfunc(close(&amp;amp;dsid)); %put &amp;amp;nvars.; %put &amp;amp;nobs.; 通过 data 步接口子程序 call symputx 与 call symput - 有区别 创建单个宏变量 call symput(&#39;x&#39;, x); run; data _null_; set sashelp.class nobs=obs; call symputx(&#39;m1&#39;,obs); call symput(&#39;m2&#39;,obs); Stop; run; %put &amp;amp;m1.; %put &amp;amp;m2.; 为某变量的每个值创建一个宏变量 data _null_; set sashelp.class; suffix=put(_n_,5.); call symput(cats(‘Name’,suffix), Name); run; 为表的每个值创建一个宏变量 data _null_; set sashelp.class; suffix=put(_n_,5.); array xxx{*} _numeric_; do i =1 to dim(xxx); call symput(cats(vname(xxx),suffix),xxx); end; array yyy{*} $ _character_; do i =1 to dim(yyy); call symput(cats(vname(yyy),suffix),yyy); end; run; proc sql 方法 - 灵活 通过 SQL 过程用变量值创建一个宏变量 proc sql noprint; select distinct sex into :list_a separated by &#39;&#39; from sashelp.class; quit; %put &amp;amp;list_a.; 通过 SQL 过程创建多个宏变量 proc sql noprint; select nvar,nobs into:nvar , :nobs from dictionary.tables where libname = ‘SASHELP’ and memname = ‘CLASS’; quit; %put &amp;amp;nvar.; %put &amp;amp;nobs.; 通过 contents 和 sql 过程用变量名创建宏变量 proc contents data=sashelp.class out=con_class; run; proc sql noprint; select name,put(count(name),5.-l) into :clist separated by ‘ ‘,:charct from con_class where type=2; quit; %put &amp;amp;clist.; %put &amp;amp;charct.; d.通过SQL过程用宏变量创建宏变量列表 proc sql noprint; select name into :clist1-:clist999 from dictionary.columns where libname = ‘SASHELP’ and memname = ‘CLASS’; quit; %put &amp;amp;clist1.; %put &amp;amp;clist2.; e.通过SQL过程用变量值创建宏变量列表 proc sql noprint; select count(distinct sex) into :n from sashelp.class; select distinct sex into :type1 – :type%left(&amp;amp;n) from sashelp.class; quit; %put &amp;amp;n.; %put &amp;amp;type1.; 使用 call set - 处理对照表数据最强，灵活方便，性能最优 %macro doit; %let id=%sysfunc(open(sashelp.class)); %let NObs=%sysfunc(attrn(&amp;amp;amp;id,NOBS)); %syscall set(id); %do i=1 %to &amp;amp;amp;NObs; %let rc=%sysfunc(fetchobs(&amp;amp;amp;id,&amp;amp;amp;i)); %put # # # Processing &amp;amp;amp;Height # # #; %end; %let id=sysfunc(close(&amp;amp;amp;id)); %mend; 参考 SAS 中文论坛. 几种给宏变量赋值的方法, shiyiming. 姚志勇. SAS 编程与数据挖掘商业案例, 2010, p171-173.]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[鸡]]></title>
    	<url>/cn/2011/03/17/hen/</url>
		<content type="text"><![CDATA[[过年回家的时候妹妹说了一件事，家里喂的那只洋鸡，跳不高，妈说它特别会找吃的。特别会生蛋，连冬天也是一天一个，但就是感觉有点笨笨的，并且生蛋以 说这事的时候妹妹并不显得对那只鸡特别可怜，可我觉得心里有些不开心。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python 自省（反射）指南 z]]></title>
    	<url>/tech/2011/03/10/reflection/</url>
		<content type="text"><![CDATA[[原文地址：http://blog.csdn.net/langqing12345/article/details/46318503 在笔者，也就 首先通过一个例子来看一下本文中可能用到的对象和相关概念。 #coding: UTF-8 import sys # 模块，sys指向这个模块对象 import inspect def foo(): pass # 函数，foo指向这个函数对象 class Cat(object): # 类，Cat指向这个类对象 def __init__(self, name=&#39;kitty&#39;): self.name = name def sayHi(self): # 实例方法，sayHi指向这个方法对象，使用类或实例.sayHi访问 print self.name, &#39;says Hi!&#39; # 访问名为name的字段，使用实例.name访问 cat = Cat() # cat是Cat类的实例对象 print Cat.sayHi # 使用类名访问实例方法时，方法是未绑定的(unbound) print cat.sayHi # 使用实例访问实例方法时，方法是绑定的(bound) 有时候我们会碰到这样的需求，需要执行对象的某个方法，或是需要对对象的某个字段赋值，而方法名或是字段名在编码代码时并不能确定，需要通过参数传递字符串的形式输入。举个具体的例子：当我们需要实现一个通用的 DBM 框架时，可能需要对数据对象的字段赋值，但我们无法预知用到这个框架的数据对象都有些什么字段，换言之，我们在写框架的时候需要通过某种机制访问未知的属性。 这个机制被称为反射（反过来让对象告诉我们他是什么），或是自省（让对象自己告诉我们他是什么，好吧我承认括号里是我瞎掰的），用于实现在运行时获取未知对象的信息。反射是个很吓唬人的名词，听起来高深莫测，在一般的编程语言里反射相对其他概念来说稍显复杂，一般来说都是作为高级主题来讲；但在 Python 中反射非常简单，用起来几乎感觉不到与其他的代码有区别，使用反射获取到的函数和方法可以像平常一样加上括号直接调用，获取到类后可以直接构造实例；不过获取到的字段不能直接赋值，因为拿到的其实是另一个指向同一个地方的引用，赋值只能改变当前的这个引用而已。 访问对象的属性 以下列出了几个内建方法，可以用来检查或是访问对象的属性。这些方法可以用于任意对象而不仅仅是例子中的Cat实例对象；Python 中一切都是对象。 cat = Cat(&#39;kitty&#39;) print cat.name # 访问实例属性 cat.sayHi() # 调用实例方法 print dir(cat) # 获取实例的属性名，以列表形式返回 if hasattr(cat, &#39;name&#39;): # 检查实例是否有这个属性 setattr(cat, &#39;name&#39;, &#39;tiger&#39;) # same as: a.name = &#39;tiger&#39; print getattr(cat, &#39;name&#39;) # same as: print a.name getattr(cat, &#39;sayHi&#39;)() # same as: cat.sayHi() dir([obj]): 调用这个方法将返回包含obj大多数属性名的列表（会有一些特殊的属性不包含在内）。obj的默认值是当前的模块对象。 hasattr(obj, attr): 这个方法用于检查obj是否有一个名为attr的值的属性，返回一个布尔值。 getattr(obj, attr): 调用这个方法将返回obj中名为attr值的属性的值，例如如果attr为&#39;bar&#39;，则返回obj.bar。 setattr(obj, attr, val): 调用这个方法将给obj的名为attr的值的属性赋值为val。例如如果attr为&#39;bar&#39;，则相当于obj.bar = val。 访问对象的元数据 当你对一个你构造的对象使用dir()时，可能会发现列表中的很多属性并不是你定义的。这些属性一般保存了对象的元数据，比如类的__name__属性保存了类名。大部分这些属性都可以修改，不过改动它们意义并不是很大；修改其中某些属性如function.func_code还可能导致很难发现的问题，所以改改name什么的就好了，其他的属性不要在不了解后果的情况下修改。 接下来列出特定对象的一些特殊属性。另外，Python 的文档中有提到部分属性不一定会一直提供，下文中将以绿色的星号*标记，使用前你可以先打开解释器确认一下。 准备工作：确定对象的类型 在types模块中定义了全部的 Python 内置类型，结合内置方法isinstance()就可以确定对象的具体类型了。 isinstance(object, classinfo): 检查object是不是classinfo中列举出的类型，返回布尔值。classinfo可以是一个具体的类型，也可以是多个类型的元组或列表。 types模块中仅仅定义了类型，而inspect模块中封装了很多检查类型的方法，比直接使用types模块更为轻松，所以这里不给出关于types的更多介绍，如有需要可以直接查看types模块的文档说明。本文第 3 节中介绍了inspect模块。 模块(module) __doc__: 文档字符串。如果模块没有文档，这个值是None。 *__name__: 始终是定义时的模块名；即使你使用import .. as为它取了别名，或是赋值给了另一个变量名。 *__dict__: 包含了模块里可用的属性名-属性的字典；也就是可以使用模块名.属性名访问的对象。 __file__: 包含了该模块的文件路径。需要注意的是内建的模块没有这个属性，访问它会抛出异常！ import fnmatch as m print m.__doc__.splitlines()[0] # Filename matching with shell patterns. print m.__name__ # fnmatch print m.__file__ # /usr/lib/python2.6/fnmatch.pyc print m.__dict__.items()[0] # (&#39;fnmatchcase&#39;, ) 类(class) __doc__: 文档字符串。如果类没有文档，这个值是None。 *__name__: 始终是定义时的类名。 *__dict__: 包含了类里可用的属性名-属性的字典；也就是可以使用类名.属性名访问的对象。 __module__: 包含该类的定义的模块名；需要注意，是字符串形式的模块名而不是模块对象。 *__bases__: 直接父类对象的元组；但不包含继承树更上层的其他类，比如父类的父类。 print Cat.__doc__ # None print Cat.__name__ # Cat print Cat.__module__ # __main__ print Cat.__bases__ # (,) print Cat.__dict__ # {&#39;__module__&#39;: &#39;__main__&#39;, ...} 实例(instance) 实例是指类实例化以后的对象。 *__dict__: 包含了可用的属性名-属性字典。 *__class__: 该实例的类对象。对于类Cat，cat.__class__ == Cat为True。 print cat.__dict__ print cat.__class__ print cat.__class__ == Cat # True 内建函数和方法(built-in functions and methods) 根据定义，内建的(built-in)模块是指使用C写的模块，可以通过sys模块的builtin_module_names字段查看都有哪些模块是内建的。这些模块中的函数和方法可以使用的属性比较少，不过一般也不需要在代码中查看它们的信息。 __doc__: 函数或方法的文档。 __name__: 函数或方法定义时的名字。 __self__: 仅方法可用，如果是绑定的(bound)，则指向调用该方法的类（如果是类方法）或实例（如果是实例方法），否则为None。 *__module__: 函数或方法所在的模块名。 函数(function) 这里特指非内建的函数。注意，在类中使用def定义的是方法，方法与函数虽然有相似的行为，但它们是不同的概念。 __doc__: 函数的文档；另外也可以用属性名func_doc。 __name__: 函数定义时的函数名；另外也可以用属性名func_name。 *__module__: 包含该函数定义的模块名；同样注意，是模块名而不是模块对象。 *__dict__: 函数的可用属性；另外也可以用属性名func_dict。 不要忘了函数也是对象，可以使用函数.属性名访问属性（赋值时如果属性不存在将新增一个），或使用内置函数has/get/setattr()访问。不过，在函数中保存属性的意义并不大。 func_defaults: 这个属性保存了函数的参数默认值元组；因为默认值总是靠后的参数才有，所以不使用字典的形式也是可以与参数对应上的。 func_code: 这个属性指向一个该函数对应的code对象，code对象中定义了其他的一些特殊属性，将在下文中另外介绍。 func_globals: 这个属性指向定义函数时的全局命名空间。 *func_closure: 这个属性仅当函数是一个闭包时有效，指向一个保存了所引用到的外部函数的变量cell的元组，如果该函数不是一个内部函数，则始终为None。这个属性也是只读的。 下面的代码演示了func_closure： #coding: UTF-8 def foo(): n = 1 def bar(): print n # 引用非全局的外部变量n，构造一个闭包 n = 2 return bar closure = foo() print closure.func_closure # 使用dir()得知cell对象有一个cell_contents属性可以获得值 print closure.func_closure[0].cell_contents # 2 由这个例子可以看到，遇到未知的对象使用dir()是一个很好的主意。 方法(method) 方法虽然不是函数，但可以理解为在函数外面加了一层外壳；拿到方法里实际的函数以后，就可以使用 2.5 节的属性了。 __doc__: 与函数相同。 __name__: 与函数相同。 **__module__: 与函数相同。 im_func: 使用这个属性可以拿到方法里实际的函数对象的引用。另外如果是 2.6 以上的版本，还可以使用属性名__func__。 im_self: 如果是绑定的(bound)，则指向调用该方法的类（如果是类方法）或实例（如果是实例方法），否则为None。如果是 2.6 以上的版本，还可以使用属性名__self__。 im_class: 实际调用该方法的类，或实际调用该方法的实例的类。注意不是方法的定义所在的类，如果有继承关系的话。 im = cat.sayHi print im.im_func print im.im_self # cat print im.im_class # Cat 这里讨论的是一般的实例方法，另外还有两种特殊的方法分别是类方法(class method)和静态方法(static method)。类方法还是方法，不过因为需要使用类名调用，所以他始终是绑定的；而静态方法可以看成是在类的命名空间里的函数（需要使用类名调用的函数），它只能使用函数的属性，不能使用方法的属性。 生成器(generator) 生成器是调用一个生成器函数(generator function)返回的对象，多用于集合对象的迭代。 __iter__: 仅仅是一个可迭代的标记。 gi_code: 生成器对应的code对象。 gi_frame: 生成器对应的frame对象。 gi_running: 生成器函数是否在执行。生成器函数在yield以后、执行yield的下一行代码前处于frozen状态，此时这个属性的值为0。 next|close|send|throw: 这是几个可调用的方法，并不包含元数据信息，如何使用可以查看生成器的相关文档。 def gen(): for n in xrange(5): yield n g = gen() print g # &amp;lt;generator object gen at 0x...&amp;gt; print g.gi_code # &amp;lt;code object gen at 0x...&amp;gt; print g.gi_frame # &amp;lt;frame object at 0x...&amp;gt; print g.gi_running # 0 print g.next() # 0 print g.next() # 1 for n in g: print n, # 2 3 4 接下来讨论的是几个不常用到的内置对象类型。这些类型在正常的编码过程中应该很少接触，除非你正在自己实现一个解释器或开发环境之类。所以这里只列出一部分属性，如果需要一份完整的属性表或想进一步了解，可以查看文末列出的参考文档。 代码块(code) 代码块可以由类源代码、函数源代码或是一个简单的语句代码编译得到。这里我们只考虑它指代一个函数时的情况；2.5 节中我们曾提到可以使用函数的func_code属性获取到它。code的属性全部是只读的。 co_argcount: 普通参数的总数，不包括*参数和**参数。 co_names: 所有的参数名（包括*参数和**参数）和局部变量名的元组。 co_varnames: 所有的局部变量名的元组。 co_filename: 源代码所在的文件名。 co_flags: 这是一个数值，每一个二进制位都包含了特定信息。较关注的是0b100(0x4)和0b1000(0x8)，如果co_flags &amp;amp; 0b100 != 0，说明使用了*args参数；如果co_flags &amp;amp; 0b1000 != 0，说明使用了**kwargs参数。另外，如果co_flags &amp;amp; 0b100000(0x20) != 0，则说明这是一个生成器函数(generator function)。 co = cat.sayHi.func_code print co.co_argcount # 1 print co.co_names # (&#39;name&#39;,) print co.co_varnames # (&#39;self&#39;,) print co.co_flags &amp;amp; 0b100 # 0 栈帧(frame) 栈帧表示程序运行时函数调用栈中的某一帧。函数没有属性可以获取它，因为它在函数调用时才会产生，而生成器则是由函数调用返回的，所以有属性指向栈帧。想要获得某个函数相关的栈帧，则必须在调用这个函数且这个函数尚未返回时获取。你可以使用sys模块的_getframe()函数、或inspect模块的currentframe()函数获取当前栈帧。这里列出来的属性全部是只读的。 f_back: 调用栈的前一帧。 f_code: 栈帧对应的code对象。 f_locals: 用在当前栈帧时与内建函数locals()相同，但你可以先获取其他帧然后使用这个属性获取那个帧的locals()。 f_globals: 用在当前栈帧时与内建函数globals()相同，但你可以先获取其他帧……。 def add(x, y=1): f = inspect.currentframe() print f.f_locals # same as locals() print f.f_back # &amp;lt;frame object at 0x...&amp;gt; return x&#43;y add(2) 追踪(traceback) 追踪是在出现异常时用于回溯的对象，与栈帧相反。由于异常时才会构建，而异常未捕获时会一直向外层栈帧抛出，所以需要使用try才能见到这个对象。你可以使用sys模块的exc_info()函数获得它，这个函数返回一个元组，元素分别是异常类型、异常对象、追踪。traceback的属性全部是只读的。 tb_next: 追踪的下一个追踪对象。 tb_frame: 当前追踪对应的栈帧。 tb_lineno: 当前追踪的行号。 def div(x, y): try: return x/y except: tb = sys.exc_info()[2] # return (exc_type, exc_value, traceback) print tb print tb.tb_lineno # &amp;quot;return x/y&amp;quot; 的行号 div(1, 0) 使用 inspect 模块 inspect模块提供了一系列函数用于帮助使用自省。下面仅列出较常用的一些函数，想获得全部的函数资料可以查看inspect模块的文档。 检查对象类型 is{module|class|function|method|builtin}(obj): 检查对象是否为模块、类、函数、方法、内建函数或方法。 isroutine(obj): 用于检查对象是否为函数、方法、内建函数或方法等等可调用类型。用这个方法会比多个is*()更方便，不过它的实现仍然是用了多个is*()。 im = cat.sayHi if inspect.isroutine(im): im() 对于实现了__call__的类实例，这个方法会返回False。如果目的是只要可以直接调用就需要是True的话，不妨使用isinstance(obj, collections.Callable)这种形式。我也不知道为什么Callable会在collections模块中，抱歉！我猜大概是因为collections模块中包含了很多其他的 ABC(Abstract Base Class)的缘故吧。 获取对象信息 getmembers(object[, predicate]): 这个方法是dir()的扩展版，它会将dir()找到的名字对应的属性一并返回，形如[(name, value), ...]。另外，predicate是一个方法的引用，如果指定，则应当接受value作为参数并返回一个布尔值，如果为False，相应的属性将不会返回。使用is*作为第二个参数可以过滤出指定类型的属性。 getmodule(object): 还在为第2节中的__module__属性只返回字符串而遗憾吗？这个方法一定可以满足你，它返回object的定义所在的模块对象。 get{file|sourcefile}(object): 获取object的定义所在的模块的文件名|源代码文件名（如果没有则返回None）。用于内建的对象（内建模块、类、函数、方法）上时会抛出TypeError异常。 get{source|sourcelines}(object): 获取object的定义的源代码，以字符串|字符串列表返回。代码无法访问时会抛出IOError异常。只能用于module/class/function/method/code/frame/traceack对象。 getargspec(func): 仅用于方法，获取方法声明的参数，返回元组，分别是(普通参数名的列表, *参数名, **参数名, 默认值元组)。如果没有值，将是空列表和 3 个None。如果是 2.6 以上版本，将返回一个命名元组(Named Tuple)，即除了索引外还可以使用属性名访问元组中的元素。 def add(x, y=1, *z): return x &#43; y &#43; sum(z) print inspect.getargspec(add) #ArgSpec(args=[&#39;x&#39;, &#39;y&#39;], varargs=&#39;z&#39;, keywords=None, defaults=(1,)) getargvalues(frame): 仅用于栈帧，获取栈帧中保存的该次函数调用的参数值，返回元组，分别是(普通参数名的列表, *参数名, **参数名, 帧的locals())。如果是 2.6 以上版本，将返回一个命名元组(Named Tuple)，即除了索引外还可以使用属性名访问元组中的元素。 def add(x, y=1, *z): print inspect.getargvalues(inspect.currentframe()) return x &#43; y &#43; sum(z) add(2) #ArgInfo(args=[&#39;x&#39;, &#39;y&#39;], varargs=&#39;z&#39;, keywords=None, locals={&#39;y&#39;: 1, &#39;x&#39;: 2, &#39;z&#39;: ()}) getcallargs(func[, *args][, **kwds]): 返回使用args和kwds调用该方法时各参数对应的值的字典。这个方法仅在 2.7 版本中才有。 getmro(cls): 返回一个类型元组，查找类属性时按照这个元组中的顺序。如果是新式类，与cls.__mro__结果一样。但旧式类没有__mro__这个属性，直接使用这个属性会报异常，所以这个方法还是有它的价值的。 print inspect.getmro(Cat) #(&amp;lt;class &#39;__main__.Cat&#39;&amp;gt;, &amp;lt;type &#39;object&#39;&amp;gt;) print Cat.__mro__ #(&amp;lt;class &#39;__main__.Cat&#39;&amp;gt;, &amp;lt;type &#39;object&#39;&amp;gt;) class Dog: pass print inspect.getmro(Dog) #(&amp;lt;class __main__.Dog at 0x...&amp;gt;,) print Dog.__mro__ # AttributeError currentframe(): 返回当前的栈帧对象。 其他的操作frame和traceback的函数请查阅inspect模块的文档，用的比较少，这里就不多介绍了。 参考资料： The standard type hierarchy inspect — Inspect live objects]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[使用 Windows 优化大师导致 Win7 出现的两个问题]]></title>
    	<url>/tech/2011/02/26/windows-opti-master/</url>
		<content type="text"><![CDATA[[“其他用户” 该问题是在使用Windows优化大师进行注册表清理的时候导致的问题。目前官方没有给出相应的解决方法。该问题的描述见如下地址：ht Ever since my initial install of Windows 7, there has been one thing nagging me. The Windows welcome screen only ever displayed the avatar for the last logged on user and a blank image with the label “Other users”. When the latter was clicked, two text fields would appear prompting for username and password. I have tried numerous solutions, but none have worked until very recently when a user with the nickname “SaySay” came up with the following solution: Legal disclaimer: Modifying REGISTRY settings incorrectly can cause serious problems that may prevent your computer from booting properly. Neither I nor Microsoft can guarantee that any problems resulting from the configuring of REGISTRY settings can be solved. Modifications of these settings are at your own risk Open regedit： Press Windows&#43;R Type regedit &#43; enter Navigate to [HKEYLOCALMACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\ProfileList] You will probably want to right-click on “ProfileList” and click export to save the entire subtree in case something goes wrong. You will find several subfolders or “keys” named something like “S-1-x-xx…”, open them one at the time Each should contain at least the three value-sets, “Flags”, “ProfileImagePath” and “State”, some will contain more Look at the end of ProfileImagePath for the name of the user represented by the key You will usually have one for each user on the system, and one for each of the three system entries ‘systemprofile’, ‘LocalService’ and ’NetworkService’ Delete any key (i.e. the whole “S-1-x-xx” folder) that does not contain at least those three values The welcome screen should now work as expected, showing the avatar for all registered users; enjoy! “Computer Management Snapin Launcher 已停止工作” 这个是在安装了最新版的 Windows 优化大师后产生的问题。Uninstall 该软件之后问题就解决。 说明：现在的 Windows 优化大师已经不如从前，可以选择的优化软件也比较多。个人不推荐随意用各种软件来修改系统的设置。其实不经济进行软件的各种安装与反安装的话，系统也不怎么需要进行优化。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[杂技]]></title>
    	<url>/cn/2010/12/21/acrobatics/</url>
		<content type="text"><![CDATA[[从医院出来的时候，已经是晚上九点。来往的行人颇多，掩盖不住的是夜色渐浓。经过天桥下面时，我的目光又一次移向那个拐角的位置，莫名的心痛再次涌出 一个手中捧着一只铁碗，向路过的人不断作揖要钱。被欺骗后气愤的感情的积累，慢慢令我从心里非常厌烦向过路人要钱骗人的这些人。面对这个冬天里衣着单薄的小姑娘，我脑海里想起的是昨天将钱放到她碗里时，那声稚嫩又略显熟练的“谢谢”，令人心中感叹不已。也许对成佳节又重阳人来说，她所做的事情的难度在某种意义上并不算高，可是如果这样从早到晚，从昨天到今天，再到明天，并且不知到哪天呢？并且她们，她们本应是在快乐的童年的。 另一个则蹲在地上，目光里尽是茫然，呆呆的看着面前地上置着的简陋的杂技道具。那道具可令她在用嘴咬住手柄的时候，使头部和腹部弯曲到相反的方向，整个人形成一个圆弧，然后全身绕着嘴上咬住的手柄来旋转。每次我见她这样时，都觉得心中难受。谁家的父母会忍心让自己年幼的子女在这寒风里，穿着如此之少，做这样的事情呢。下午都已经走出离她有一段距离时，我还是忍不住又一次将钱拿出来，放在她面前的小盆之中。我心里很想帮她，却又担心这样的做法会让她幼小的心中留下自己这样做是正确的观念。这个小姑娘眼光没有一丝移向我的动作，仍如平时一般茫然。也许她早已被生活的艰辛、人情冷暖与世态炎凉填满，再容不下其它事情；又或者她早已被恶毒的幕后人做了不人道的事情，根本无力来思考自己动作之外的任何东西。不管怎样，她的熟练的转动与木然的反应都使我心里莫名的难过，有种不吐不快的感觉。 我很愤怒，高居庙堂之上的人，早已被追名逐利的想法填满了内心，无休止的拆拆建建只照到了华丽耀眼的GDP，宽阔却拥挤的道路与高过蓝天的楼房掩盖了民众的疾苦。人们要的是什么，是柴米油盐与立身之所；我很无语，远在象牙塔中的人，心中想着的是经世济国，统计数字与货币发行的公式将个人的生死与福祉都推到了学科之外。华丽的数据与美妙的结论摭不住渐渐空虚的菜篮。一家的欢乐与欣喜被偶尔下调几毛钱的白菜与短暂不用出钱的免费地铁相连；我很无奈，我们自得于刚刚推导的公式，但满纸的原理、众多的模型与计量，却解答不了眼前的这个现实又心酸的习题。我们玩弄的恰如一场游戏，自己能做的，最多是经己济家罢了。 尽管不是完美，毕竟我已拥有了许多。学了许久，仍然觉得心中少了一份什么。这一刻我才能体会邹极力推崇萨克斯的原因。耳边想起的是一句歌词：只要人人都献出一点爱，世界将变成美好的人间。也许这个要求太过苛刻，那么是否人心中都能存一丝悲悯的心呢？连敬畏都没有的地方，还可以谈些什么？ 已经很晚了，我已好久没写过什么，可是今晚，我很想把它全部写出来，不吐不快。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[第玖味 z]]></title>
    	<url>/cn/2010/11/21/9th-tastes/</url>
		<content type="text"><![CDATA[[作者：徐国能 酸、甜、苦、辣、咸、涩、腥、冲…… 我的父亲常说：“吃是为己，穿是为人。”这话有时想来的确有些意思，吃在里长在身上，自是一点肥不了别人，但穿在身上，漂亮一番，往往取悦了别人而折腾了自己。父亲作菜时这么说，吃菜时这么说，看我们穿新衣时也这么说，我一度以为这是父亲的人生体会，但后来才知道我的父亲并不是这个哲学的始作俑者，而是当时我们“健乐园”大厨曾先生的口头禅。 一般我们对于厨房里的师傅多称呼某厨，如刘厨王厨之类，老一辈或小一辈的帮手则以老李小张称之，唯独曾先生大家都喊声“先生”，这是一种尊敬，有别于一般厨房里的人物。 曾先生矮，但矮得很精神，头发已略花白而眼角无一丝皱纹，从来也看不出曾先生有多大岁数。我从未见过曾先生穿着一般厨师的围裙高帽，天热时他只是一件麻纱水青斜衫，冬寒时经常是月白长袍，干干净净，不染一般膳房的油腻腌臜。不识他的人看他一脸清癯，而眉眼间总带着一股凛然之色，恐怕以为他是个不世出的画家诗人之类，或是笑傲世事的某某教授之流。 曾先生从不动手作菜，只吃菜，即使再怎么忙，曾先生都是一派闲气地坐在柜台后读他的中央日报。据说他酷爱唐鲁孙先生的文章，虽然门派不同（曾先生是湘川菜而唐鲁孙属北方口味儿），但曾先生说：“天下的吃到底都是一个样的，不过是一根舌头九样味。”那时我年方十岁，不喜读书，从来就在厨房窜进窜出，我只知酸甜苦辣咸涩腥冲八味，至于第九味，曾先生说：“小子你才几岁就想尝遍天下，滚你的蛋去。”据父亲说，曾先生是花了大钱请了人物套交情才聘来的，否则当时“健乐园”怎能高过“新爱群”一个等级呢？花钱请人来光吃而不做事，我怎么看都是不合算的。 我从小命好，有得吃。 母亲的手艺绝佳，比如包粽子吧！不过就是酱油糯米加猪肉，我小学庄老师的婆婆就是一口气多吃了两个送去医院的。老师打电话来问秘诀，母亲想了半天，说：竹叶两张要一青一黄，酱油须拌匀，猪肉不可太肥太瘦，蒸完要沥干……如果这也算“秘诀”。 但父亲对母亲的厨艺是鄙薄的，母亲是浙江人，我们家有道经常上桌的家常菜，名曰：“冬瓜蒸火腿”，作法极简，将火腿（台湾多以家乡肉替代）切成薄片，冬瓜取中段一截，削皮后切成梯形块，一块冬瓜一片火腿放好，蒸熟即可食。须知此菜的奥妙在于蒸熟的过程冬瓜会吸干火腿之蜜汁，所以上桌后火腿已淡乎寡味，而冬瓜则具有瓜蔬的清苦之风与火腿的华贵之气，心软边硬，汁甜而不腻，令人倾倒。但父亲总嫌母亲切菜时肉片厚薄不一，瓜块大小不匀，因此味道上有些太浓而有些太淡，只能“凑合凑合”。父亲在买菜切菜炒菜调味上颇有功夫，一片冬瓜切得硬是像量角器般精准，这刀工自是大有来头，因与本文无关暂且按下不表。话说父亲虽有一手绝艺，但每每感叹他只是个“二厨”的料，真正的大厨，只有曾先生。 稍具规模的餐厅都有大厨，有些名气高的厨师身兼数家“大厨”，谓之“通灶”，曾先生不是“通灶”，但绝不表示他名气不高。“健乐园”的席有分数种价位，凡是挂曾先生排席的，往往要贵上许多。外行人常以为曾先生排席就是请曾先生亲自设计一桌从冷盘到甜汤的筵席，其实大非，菜色与菜序排不排席谁来排席其实都是差不多的，差别只在上菜前曾先生是不是亲口尝过。从来我见曾先生都是一尝即可，从来没有打过回票，有时甚至只是看一眼就“派司”，有人以为这只是个形式或是排场而已，这当然又是外行话了。 要知道在厨房经年累月的师傅，大多熟能生巧，经常喜欢苛扣菜色，中饱私囊，或是变些魔术，譬如鲍鱼海参排翅之类，成色不同自有些价差，即使冬菇笋片大蒜，也是失之毫厘差之千里。而大厨的功用就是在此，他是一个餐厅信誉的保证，有大厨排席的菜色，厨师们便不敢装神弄鬼，大厨的舌头是老天赏来人间享口福的，禁不起一点假，你不要想瞒混过关，味精充鸡汤，稍经察觉，即使你是国家鉴定的厨师也很难再立足厨界，从此江湖上没了这号人物。有这层顾忌，曾先生的席便没人敢滑头，自是顺利稳当。据父亲说，现下的厨界十分混乱，那些“通灶”有时兼南北各地之大厨，一晚多少筵席，哪个人能如孙悟空分身千万，所以一般餐厅多是马马虎虎，“凑合凑合”，言下有不胜唏嘘之意。 曾先生和我有缘，这是掌杓的赵胖子说的。每回放学，我必往餐厅逛去，将书包往那幅金光闪闪的“乐游园歌”下一丢，闪进厨房找吃的。这时的曾先生多半在看中央日报，经常有一香吉士果汁杯的高粱，早年白金龙算是好酒，曾先生的酒是自己带的，他从不开餐厅的酒，不像赵胖子他们常常“干喝”。 赵胖子喜欢叫曾先生“师父”，但曾先生从没答理过。曾先生特爱和我讲故事，说南道北，尤其半醉之际。曾先生嗜辣，说这是百味之王，正因为是王者之味，所以他味不易亲近，有些菜中酸甜咸涩交杂，曾先生谓之“风尘味”，没有意思。辣之于味最高最纯，不与他味相混，是王者气象，有君子自重之道在其中，曾先生说用辣宜猛，否则便是昏君庸主，纲纪凌迟，人人可欺，国焉有不亡之理？而甜则是后妃之味，最解辣，最宜人，如秋月春风，但用甜则尚淡，才是淑女之德，过腻之甜最令人反感，是露骨的谄媚。曾先生常对我讲这些， 我也似懂非懂，赵胖子他们则是在一旁暗笑，哥儿们几岁懂些什么呢？父亲则抄抄写写地勤作笔记。 有一次父亲问起咸辣两味之理，曾先生说道：咸最俗而苦最高，常人日不可无咸但苦不可兼日，况且苦味要等众味散尽方才知觉，是味之隐逸者，如晚秋之菊，冬雪之梅；而咸则最易化舌，入口便觉，看似最寻常不过，但很奇怪，咸到极致反而是苦，所以寻常之中，往往有最不寻常之处，旧时王谢堂前燕，就看你怎么尝它，怎么用它。 曾先生从不阻止父亲作笔记，但他常说烹调之道要自出机杼，得于心而忘于形，记记笔记不过是纸上的工夫，与真正的吃是不可同日而语的。 “健乐园”结束于民瑞脑消金兽国七十年间，从此我们家再没人谈起吃的事，似乎有点儿感伤。 “健乐园”的结束与曾先生的离去有很密切的关系。 曾先生好赌，有时常一连几天不见人影，有人说他去豪赌，有人说他去躲债，谁也不知道，但经常急死大家，许多次赵胖子私下建议父亲，曾先生似乎不大可靠，不如另请高明，但总被父亲一句“刀三火五吃一生”给回绝，意谓刀工三年或可以成，而火候的精准则需时间稍长，但真正能吃出真味，非用一辈子去追求，不是一般遇得上的，父亲对曾先生既敬且妒自不在话下。 据父亲回忆，那回罗中将嫁女儿，“健乐园”与“新爱群”都想接下这笔生意，结果罗中将卖曾先生一个面子，点的是曾先生排的席，有百桌之余，这在当时算是桩大生意，而罗中将又是同乡名人，父亲与赵胖子摩拳擦掌准备了一番，但曾先生当晚却不见人影。一阵鸡飞狗跳，本来父亲要退罗中将的钱，但赵胖子硬说不可，一来没有大厨排席的酒筵对罗中将面子上不好看，二来这笔钱数目实在不小，对当时已是危机重重的“健乐园”来说是救命仙丹，赵胖子发誓一定好好做，不会有差池。 这赵胖子莫看他一脸肥相，如弥勒转世，论厨艺却是博大精深，他纵横厨界也有二三十年，是独当一面的人物。那天看他挥汗如雨，如八臂金刚将铲杓使得风雨不透。本来宴会进行得十分顺利，一道一道菜流水般地上，就在最后关头，罗中将半醺之际竟拿起酒杯，要敬曾先生一杯，场面一时僵住。事情揭穿后，罗中将铁青着脸，匡当一声扔下酒杯，最后竟有点不欢而散。几个月后“健乐园”都没再接到大生意，卫生局又经常上门噪啰，清廉得不寻常。父亲本不善经营，负债累累下终于宣布倒闭。 曾先生从那晚起没有再出现过，那个月的薪俸也没有拿，只留下半瓶白金龙高粱酒，被赵胖子砸了个稀烂。 长大后我问父亲关于曾先生的事，父亲说曾先生是湘乡人，似乎是曾涤生家的远亲，与我们算是小同乡，据说是清朝皇帝曾赏给曾涤生家一位厨子，这位御厨没有儿子，将本事传给了女婿，而这女婿，就是曾先生的师父了。对于这种稗官野史我只好将信将疑，不过父亲说，要真正吃过点好东西，才是当大厨的命，曾先生大约是有些背景的，而他自己一生穷苦，是命不如曾先生。父亲又说：曾先生这种人，吃尽了天地精华，往往没有好下场，不是带着病根，就是有一门恶习。其实这些年来，父亲一直知道曾先生在躲道上兄弟的债，没得过一天好日子，所以父亲说：平凡人有其平凡乐趣，自有其甘醇的真味。 “健乐园”结束后，赔赔卖卖，父亲只拿回来几个账房用的算盘，小学的珠算课我惊奇地发现我那上二下五的算盘与老师同学的大不相同，同学争看我这酷似连续剧中武林高手用的奇门武器，但没有人会打这种东西，我只好假装上下各少一颗珠子地“凑合凑合”。 从学校毕业后，我被分发至澎湖当装甲兵，在军中我沉默寡言，朋友极少，放假又无亲戚家可去，往往一个人在街上乱逛。有一回在文化中心看完了书报杂志，正打算好好吃一顿，转入附近的巷子，一爿低矮的小店歪歪斜斜地写着“九味牛肉面”，我心中一动，进到店中，简单的陈设与极少的几种选择，不禁使我有些失望，一个肥胖的女人帮我点单下面后，自顾自的忙了起来，我这才发现暗暝的店中还有一桌有人，一个秃头的老人沈浸在电视新闻的巨大声量中，好熟悉的背影，尤其桌上一份中央日报，与那早已满渍油水的唐鲁孙的《天下味》。曾先生，我大声唤了几次，他都没有回头，“我们老板姓吴”，胖女人端面来的时候说。 “不！我姓曾。”曾先生在我面前坐下。 我们聊起了许多往事，曾先生依然精神，但眼角已有一些落寞与沧桑之感，满身厨房的气味，磨破的袖口油渍斑斑，想来常常抹桌下面之类。 我们谈到了吃，曾先生说：一般人好吃，但大多食之无味，要能粗辨味者，始可言吃，但真正能入味之人，又不在乎吃了，像那些大和尚，一杯水也能喝出许多道理来。我指着招牌问他“九味”的意思，曾先生说：辣甜咸苦是四主味，属正；酸涩腥冲是四宾味，属偏。偏不能胜正而宾不能夺主，主菜必以正味出之，而小菜则多偏味，是以好的筵席应以正奇相生而始，正奇相克而终……突然我觉得彷佛又回到了“健乐园”的厨房，满鼻子菜香酒香，爆肉的哔啵声，剁碎的笃笃声，赵胖子在一旁暗笑，而父亲正勤作笔记。我无端想起了“健乐园”穿堂口的一幅字：“乐游古园崒森爽，烟绵碧草萋萋长。公子华筵势最高，秦川对酒平如掌……” 那逝去的像流水，像云烟，多少繁华的盛宴聚了又散散了又聚，多少人事在其中，而没有一样是留得住的。曾先生谈兴极好，用香吉士的果汁杯倒满了白金龙，颤抖地举起，我们的眼中都有了泪光，“却忆年年人醉时，只今未醉已先悲”，我记得“乐游园歌”是这么说的，我们一直喝到夜阑人静。 之后几个星期连上忙着装备检查，都没放假，再次去找曾先生时门上贴了今日休息的红纸，一直到我退伍。我知道我再也找不到他了，心中不免惘然。有时想想，那会是一个梦吗？我对父亲说起这件事，父亲并没有讶异的表情，只是淡淡地说：劳碌一生，没人的时候急死，有人的时候忙死……我不懂这话在说什么。 如今我重新拾起书本，觉得天地间充满了学问，一啄一饮都是一种宽慰。有时我会翻出“乐游园歌”吟哦一番，有时我会想起曾先生话中的趣味，曾先生一直没有告诉我那第九味的真义究竟是什么，也许是连他自己也不清楚；也许是因为他相信，我很快就会明白。 徐国能：东海大学中文系毕业，现就读于台湾师大国文所。曾获全国学生文学奖、台北市文学奖等。入选八十八年年度诗选。获现代诗学会颁赠八十九年度优秀青年诗人奖。 这篇文章好象是 2000 年台湾大学生作文比赛的第一名。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[洋媳妇教育孩子的方法，令中国婆婆大开眼界 z]]></title>
    	<url>/cn/2010/09/09/baby-education/</url>
		<content type="text"><![CDATA[[儿子去美国留学，毕业后定居美国。还给我找了个洋媳妇苏珊。如今，小孙子托比已经3岁了。今年夏天，儿子为我申请了探亲签证。在美国待了三个月，洋媳妇苏珊教育孩子的方法，令我这个中国婆婆大开眼界。 不吃饭就饿着 每天早上，托比醒来后，苏珊把早餐往餐桌上一放，就自顾自地忙去了。托比会自己爬上凳子，喝牛奶，吃面包片。吃饱后，他回自己的房间，在衣柜里找衣服、鞋子，再自己穿上。毕竟托比只有3岁，还搞不清楚裤子的正反面，分不清鞋子的左右脚。有一次托比又把裤子穿反了，我赶紧上前想帮他换，却被苏珊制止了。她说，如果他觉得不舒服，会自己脱下来，重新穿好；如果他没觉得有什么不舒服，那就随他的便。那一整天，托比反穿着裤子跑来跑去，苏姗像没看见一样。 又一次，托比出去和邻居家的小朋友玩，没多大会就气喘吁吁地跑回家，对苏珊说：“妈妈，露西说我的裤子穿反了，真的吗？”露西是邻居家的小姑娘，今年5岁。苏姗笑着说：“是的，你要不要换回来？”托比点点头，自己脱下裤子，仔细看了看，重新穿上了。从那以后，托比再也没穿反过裤子。 我不禁想起，我的外孙女五六岁时不会用筷子，上小学时不会系鞋带。如今在上寄宿制初中的她，每个周末都要带回家一大堆脏衣服呢。 一天中午，托比闹情绪，不肯吃饭。苏珊说了他几句，愤怒地小托比一把将盘子推到了地上，盘子里的食物洒了一地。苏姗看着托比，认真地说：“看来你确实不想吃饭！记住，从现在到明天早上，你什么都不能吃。”托比点点头，坚定地回答：“Yes!”我在心里暗笑，这母子俩，还都挺倔！ 下午，苏珊和我商量，晚上由我做中国菜。我心领神会，托比特别爱吃中国菜，一定是苏珊觉得托比中午没好好吃饭，想让他晚上多吃点儿。 那天晚上我施展厨艺，做了托比最爱吃的糖醋里脊、油闷大虾，还用意大利面做了中国式的凉面。托比最喜欢吃那种凉面，小小的人可以吃满满一大盘。 开始吃晚饭了，托比欢天喜地地爬上凳子。苏珊却走过来，拿走了他的盘子和刀叉，说：“我们已经约好了，今天你不能吃饭，你自己也答应了的。”托比看着面容严肃的妈妈，“哇”地一声在哭起来，边哭边说：“妈妈，我饿，我要吃饭。”“不行，说过的话要算数。”苏珊毫不心软。 我心疼了，想替托比求情，说点好话，却见儿子对我使眼色。想起我刚到美国时，儿子就跟我说，在美国，父母教育孩子时，别人千万不要插手，即使是长辈也不例外。无奈，我只好保持沉默。 那顿饭，从始至终，可怜的小托比一直坐在玩具车里，眼巴巴地看着我们三个大人狼吞虎咽。我这才明白苏珊让我做中餐的真正用意。我相信，下一次，托比想发脾气扔饭碗时，一定会想起自己饿着肚子看爸爸妈妈和奶奶享用美食的经历。饿着肚子的滋味不好受，况且还是面对自己最喜爱的食物。 临睡前，我和苏珊一起去向托比道晚安。托比小心翼翼地问：“妈妈，我很饿，现在我能吃中国面吗？”苏珊微笑着摇摇头，坚决地说：“不！”托比叹了口气，又问：“那等我睡完觉睁开眼睛时，可以吃吗？”“当然可以。”苏珊温柔地回答。托比甜甜地笑了。 大部分情况下，托比吃饭都很积极，他不想因为“罢吃”而错过食物，再受饿肚子的苦。每当看到托比埋头大口大口地吃饭，嘴上脸上粘的都是食物时，我就想起外孙女。她像托比这么大时，为了哄她吃饭，几个大人端着饭碗跟在她屁股后面跑，她还不买账，还要谈条件：吃完这碗买一个玩具，再吃一碗买一个玩具…… 以其人之道，还治其人之身 有一天，我们带托比去公园玩。很快，托比就和两个女孩儿玩起了厨房游戏。塑料小锅、小铲子、小盘子、小碗摆了一地。忽然，淘气的托比拿起小锅，使劲在一个女孩儿头上敲了一下，女孩儿愣了一下，放声大哭。另一个女孩儿年纪更小一些，见些情形，也被吓得大哭起来。大概托比没想到会有这么严重的后果，站在一旁，愣住了。 苏珊走上前，开清了事情的来龙去脉后，她一声不吭，拿起小锅，使劲敲到托比的头上，托比没防备，一下子跌坐在草地上，哇哇大哭起来。苏珊问托比：“疼吗？下次还这样吗？”托比一边哭，一边拼命摇头。我相信他以后再也不会这么做了。 托比的舅舅送了他一辆浅蓝色的小自行车，托比非常喜欢，当成宝贝，不许别人碰。邻居小姑娘露西是托比的好朋友，央求托比好几次，要骑他的小车，托比都没答应。 一次，几个孩子一起玩时，露西趁托比不注意，偷偷骑上小车，扬长而去。托比发现后，气愤地跑来向苏珊告状。苏珊正和几个孩子的母亲一起聊天喝咖啡，便微笑着说：“你们的事情自己解决，妈妈可管不了。”托比无奈地走了。 过了一小会儿，露西骑着小车回来了。托比看到露西，一把将她推倒在地，抢过了小车。露西坐在地上大哭起来。苏珊抱起露西，安抚了她一会儿。很快，露西就和别的小朋友兴高采烈地玩了起来。 托比自己骑了会车，觉得有些无聊，看到那几个孩子玩得那么高兴，他想加入，又觉得有些不好意思。他蹭到苏珊身边，嘟囔道：“妈妈，我想跟露西他们一起玩。”苏珊不动声色地说：“那你自己去找他们啦！”“妈妈，你陪我一起去。”托比恳求道。“那可不行，刚才是你把露西弄哭的，现在你又想和大家玩，就得自己去解决问题。” 托比骑着小车慢慢靠近露西，快到她身边时，又掉头回来。来回好几次，不知道从什么时候开始，托比和露西又笑逐颜开，闹成了一团。 管教孩子是父母的事 苏珊的父母住在加利福尼亚州，听说我来了，两人开车来探望我们。家里来了客人，托比很兴奋，跑上跑下地乱窜。他把玩沙子用的小桶装满了水，提着小桶在屋里四处转悠。苏珊警告了她好几次，不要把水洒到地板上，托比置若罔闻。最后，托比还是把水桶弄倒了，水洒了一地。兴奋的小托比不觉得自己做错了事，还得意地光着脚丫踩水玩，把裤子全弄湿了。我连忙找出拖把准备拖地。苏珊从我手中抢过拖把交给托比，对他说：“把地拖干，把湿衣服脱下来，自己洗干净。”托比不愿意，又哭又闹。苏珊二话不说，直接把他拉到贮藏室，关了禁闭。听到托比在里面发出惊天动地的哭喊，我心疼坏了，想进去把他抱出来。托比的外婆却拦住我，说：“这是苏珊的事。” 过了一会儿，托比不哭了，他在贮藏室里大声喊：“妈妈，我错了。”苏珊站在门外，问：“那你知道该怎么做了吗？”“我知道。”苏珊打开门，托比从贮藏室走出来，脸上还挂着两行泪珠。他拿起有他两个高的拖把吃力地把地上的水拖干净。然后，他脱下裤子，拎在手上，光着屁股走进洗手间，稀里哗啦地洗起衣服来。 托比的外公外婆看着表情惊异的我，意味深长地笑了。这件事让我感触颇深。在很多中国家庭，父母管教孩子时，常常会引起“世界大战”，往往是外婆外公护，爷爷奶奶拦，夫妻吵架，鸡飞狗跳。 后来，我和托比的外公外婆聊天时，提到这件事，托比的外公说了一段话，让我印象深刻。他说，孩子是父母的孩子，首先要尊重父母对孩子的教育方式。孩子虽然小，却是天生的外交家，当他看到家庭成员之间出现分歧时，他会很聪明地钻空子。这不仅对改善他的行为毫无益处，反而会导致问题越来越严重，甚至带来更多别的问题。而且，家庭成员之间发生冲突，不和谐的家庭氛围会带给孩子更多的不安全感，对孩子的心理发展产生不利影响。所以，无论是父辈与祖辈在教育孩子的问题上发生分歧，还是夫妻两人的教育观念有差异，都不能在孩子面前发生冲突。 托比的外公外婆在家里住了一周，准备回加利福尼亚了。临走前两天，托比的外公郑重地问女儿：“托比想要一辆玩具挖掘机，我可以买给他吗？”苏珊想了想，说：“你们这次来，已经送给他一双旱冰鞋作为礼物了，到圣诞节时，再买玩具挖掘机当礼物送给他吧！” 我不知道托比的外公是怎么告诉小家伙的，后来我带托比去超市，他指着玩具挖掘机说：“外公说，圣诞节时，给我买这个当礼物。”语气里满是欣喜和期待。 虽然苏珊对托比如此严格，托比去却对妈妈爱得不得了。他在外面玩时，会采集一些好看的小花或者他认为漂亮的叶子，郑重其事地送给妈妈；别人送给他礼物，他会叫妈妈和他一起拆开；有什么好吃的，也总要留一半给妈妈。 想到很多中国孩子对父母的漠视与冷淡，我不得不佩服我的洋媳妇。在我看来，在教育孩子的问题上，美国妈妈有很多值得中国妈妈学习的地方。 请好好的想想 你希望你的孩子成为什么样的人？你能把你的孩子教育成为什么样的人？教育，不是天真就可以畅通无阻！别用孩子的一生为你的无知和固执买账！]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[湘女多情]]></title>
    	<url>/cn/2010/08/20/a-hunan-girl-who-is-full-of-affection/</url>
		<content type="text"><![CDATA[[关正杰和关菊英的那首合唱到至今仍是我最喜欢的歌曲之一。单以《湘女多情》而言，粤语版却少见的不及国语版，也许黄露仪的声音实在太动听吧。当然，关 视频地址：http://www.tudou.com/programs/view/n-khvRECvG0/ 82版《天龙八部》插曲《湘女多情》(国语版) 《湘女多情》是绿衣阿碧在江中划船时唱的，让人想到江南的秀丽风光，颇懂风雅的段誉当时便看痴了，她唱的是多情女儿，多情山水，多情公子，好像是从笛子里吹出来的一样。我很是怜爱阿碧这个女子，原先只以为她是一个忠心的丫鬟，俏皮的甜妞，殊不知她心里藏着那么深的感情，燕子坞、姑苏慕容、早已在她心底扎根，她爱的最深，却始终默默。 http://www.yyfc.com/play.aspx?reg_id=1370064&amp;amp;song_id=2323975 湘女多情(国语版) 黄莺莺 曲：梁漢銘 詞：杜雲娜 万缕垂杨 柳浪莺鸣 绿野上蝴蝶诉情 盛放红菱 吐艳水上 湖上把鸳鸯订 慢诉静听 伴侣低诉互倾 悦耳动听尽数湘女是柔情 愿你静听 欢乐声 绰约丰姿相辉映 共舞霓裳美妙轻盈 月下双双倩影 互爱定情 结下关睢咏 缘份乃三生早定 慢诉静听 伴侣低诉互倾 悦耳动听尽数湘女是柔情 愿你静听 欢乐声 绰约丰姿相辉映 共舞霓裳美妙轻盈 月下双双倩影 互爱定情 结下关睢咏 缘份乃三生早定 Over&amp;hellip;.]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[语法高亮：给部分代码添加背景颜色]]></title>
    	<url>/tech/2010/02/24/highlight-chunk/</url>
		<content type="text"><![CDATA[[原文地址：http://stackoverflow.com/questions/1304315/highlighting-a-chunk-o 基本思路：在lstlisting环境中使用 escape character，同时用\colorbox生成背景颜色1。 在导言区中添加如下代码： \usepackage{color} \definecolor{light-gray}{gray}{0.80} 之后在正文中按如下方法给部分代码添加背景颜色2： \begin{lstlisting}[escapechar=!] def mult(m: Matrix[Int], n: Matrix[Int]) { val p = !\colorbox{light-gray}{new MatrixInt}!(m.rows, n.cols) } \end{lstlisting} realboxes包提供的是Colorbox，这个地址给出了一个具体的示例。 ↩ 注意在!之间的部分。 ↩]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[I Hope]]></title>
    	<url>/cn/2009/05/12/i-hope/</url>
		<content type="text"><![CDATA[[It&amp;rsquo;s hoped that I can do something for my great motherland. But the only thing I can do now is to be rational, to play my role well and to pray for the victims. The earthquake has no mercy, but we Chinese do. Whenever, wherever, we are together, fighting against the natural disaster. Once the Chinese are united, miracle happens.]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[renewcommand partname 时的问题]]></title>
    	<url>/tech/2009/03/24/renewcommand-partname/</url>
		<content type="text"><![CDATA[[在 documentclass 为 book 时解决方法如下： \documentclass[a4paper]{book} \usepackage{ctex,CJKnumb} \renewcommand\thepart{}\renewcommand\partname{第\CJKnumbe \begin{document} \begin{CJK*}{GBK}{xihei} \part[asfd]{test asdf} asdf asdfasfd \end{CJK*} \end{document} 在documentclass为beamer时解决方法如下(该方法由黄正华老师提供，但要注意选择不同的 theme 时，要根据实际情况改变 partpage 中 beamercolorbox 的具体参数，换句话说，这个解决方法与上面的方法不同，它不具有通用性)： \documentclass[CJK]{beamer} \usepackage{CJK,CJKnumb} \usetheme{Madrid} \begin{document} \begin{CJK*}{GBK}{kai} \renewcommand\partname{第\CJKnumber{\value{part}}部分} \defbeamertemplate*{part page}{mypartpage}[1][] { \begin{centering} { \usebeamerfont{part name}\usebeamercolor[fg]{part name}\partname} \vskip1em\par \begin{beamercolorbox}[rounded=true,shadow=true,sep=8pt,center,#1]{part title} \usebeamerfont{part title}\insertpart\par \end{beamercolorbox} \end{centering} } \setbeamertemplate{part page}[mypartpage][] \title{test} \date{} \part[asfd]{test asdf} \frame{\partpage} \end{CJK*} \end{document}]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[无法删除文件或文件夹的处理]]></title>
    	<url>/tech/2009/03/03/delete-file-and-folders/</url>
		<content type="text"><![CDATA[[在删除文件或文件夹时，Windows系统错误提示：无法删除****，找不到指定文件，请确定指定的路径及文件名是否正确的解决方法…… 经常有人会 360 文件粉碎，优化大师粉碎也不行，但这一个方法需非常有效，一试就成功，收藏下来。 具体方法如下，请一步步进行，一定可以解决你的难题。 启用一个 cmd； 到要删除的文件(夹)的上一层目录下； 运行命令：dir /x，然后记下要删除的文件(夹)对应行的第三列(记作 比如：alixixi~com)； 如果是文件，输入：del alixixi~com(就是刚才记得东西)，是文件夹就输入：rd alixixi~com； 大功告成！]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Aurora]]></title>
    	<url>/tech/2008/07/26/aurora/</url>
		<content type="text"><![CDATA[[下载地址：http://elevatorlady.ca/ 破解：http://ssfighter.blog.com.cn/archives/2 演示： 懒人的最佳选择。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[心然·仙剑]]></title>
    	<url>/cn/2008/02/09/songs-from-lengend-of-a-chinese-swordman/</url>
		<content type="text"><![CDATA[[回忆从前痴迷的玩仙剑的时光 回想黄昏暮色里乡间小路上听着歌曲的日子 听着心然的歌曲 仿佛自己还能回到小的时候 现在她封麦了 感觉特别失落 我也没了以前的 不知不觉中 是我们有了新的生活 还是我们已经被新的生活打败了 这又有谁知道呢 都说怀念是人变老的向征 既然如此 就让怀念变成忧伤 在时间的流逝里慢慢被遗忘吧]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[黄依依]]></title>
    	<url>/cn/2008/02/03/huang-yiyi/</url>
		<content type="text"><![CDATA[[电视上偶尔又看到《暗算》，这部片其实已经看过好几遍，但是每次重新仍然觉得特别精彩。柳云龙演过的每部片都觉得特别好看，从《公安局长》到《知情者 老树栖栖，黄草依依；山梅漫漫，白雪袭袭。 湖水壑壑，梢棹息息；瘦伞茕茕，归履及及。 宇之渺渺，时之淅淅；风也切切，月也亟亟。 过眼茫茫，回首希希；屋檐浥浥，飞燕啼啼。 向来不愿意看悲剧，更不愿意看爱情悲剧。看《暗算》的时候万万没有想到其中有爱情，更没有想到是爱情悲剧。黄依依是一个为爱痴狂的女生的名字，可是，她无人可依。就像春天下了雪，把爱情冻结在雪里飘落下来，化作眼泪……]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[倚桐]]></title>
    	<url>/cn/2008/01/26/yitong/</url>
		<content type="text"><![CDATA[[旧梦不需记 演唱：雷安娜 曲/词：黄沾 旧梦不须记 逝去种种昨日经已死 从前人渺随梦境失掉 莫忆风里泪流怨别离 旧事也不须记 事过境迁以后不再提起 从前情爱何 万千恩怨让我尽还你 此后人生漫漫长路 自寻路向天际分飞 他日与君倘有未了缘 始终都会海角重遇你 因此旧梦不须记 亦不必苦与悲 缘来缘去前事的喜与泪 在今天里让我尽还你 也许都不算认识她，当初如果不是她指责我大量下载盗版歌曲，也许都不会知道这人。也正是因为她的无故出现，让自己记住了这个特别的名字，也让我慢慢觉得这人很值得尊重，毕竟省吃俭用只为了买某人的正版专辑，在好些人看来是不太好理解的事情，对她的尊重正缘于她的执着。再后来偶尔也会关注她发过的贴子，也总能从那里找寻到很好听的歌，也因此认识了许多这个年代已经不再被人认识的真正歌手。 我总觉得那个星座的人不应该是她表现出来的状态，可是她每列出来的每首歌又都证明了她确实是如此，我想也许人都会有例外吧。 听过一遍再一遍，这样的歌总让人会莫名感伤，容易想一些在这年代里已经不再现实的事情，也许我们都根本不应该却喜欢这样类型的歌，究竟是因为人的心情与性格喜欢了这样的歌，还是因为这样的歌让人的心情与性格变成了这样，又有谁说得清楚呢。 沒哭聲的女子，光是为这名字，我想我也一定会记得这人。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Outlook 解决找不到 Outlook.pst z]]></title>
    	<url>/tech/2007/12/18/outlook-pst/</url>
		<content type="text"><![CDATA[[找不到文件C:\Documents and Settings\Administrator\Local Settings\Application Data\Microsoft\Outlook\Outlook.pst，要求定位该文件： 开始 –&amp;gt; 运行 –&amp;gt; cmd（我建议用“附件”中的“命令提示行”，因为“cmd”下可能不支持长文件名和空格路径 在DOS下，用 &amp;quot;cd xxx&amp;quot; 切换到 Outlook.exe 可执行文件目录下（安装目录如：C:\Program Files\Microsoft Office\OFFICE12） 使用命令 outlook /importprf .\.prf 进行初始化 Outlook 数据文件。 初始化安装 Outlook 启动时，需要新建一电子邮件帐户界面[能到这里来的基本都不打算恢复邮件了吧？呵呵- -！] PS：该命令即：outlook_/importprf_.\.prf 其中“_”即空格键！ 我在安装 Outlook Connector 时，由于网络原因，下载新版本的 Connector（OLC.msi）失败，导致 Outlook 无法打开，使用上面的方法重新创建outlook.pst文件时，提示msncon.dll找不到，解决方法就是找到最新版的 Outlook Connector（如果你的Office不是正版，用迅雷搜 Outlook Connector试试）安装后就好。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[我眼里的 blog]]></title>
    	<url>/cn/2007/12/02/blog/</url>
		<content type="text"><![CDATA[[最先对 blog 有概念是从 MSN Spaces 开始的，那时候特别慢，也就一直没用过，那时候国内还没几人对 blog 这个词有听过的。 后来真正开始用是在 UUZone 上面，那是所见过的编辑器功能最开放的一家，连 Javascript 都不禁用，其它就更别提了，基本上是想怎样便怎样，自由度超高，实在是爽的不得了，再加上丰富的外观和主题，别提有多喜欢它，很可惜由于经费问题停了差不多一年，于是就再也不在那里写。 慢慢 QQ 也开始提供 blog 功能，就是 QQ Zone，也曾经尝试过在上面来写，但从一开始就受不了。QQ Zone 的编辑器只提供最简单的 UBB 语法，即使这样，提供的编辑器的交互窗口也有够差劲，反正就是全身用的不自在。我特别好奇为什么还有那么多人用这么差劲的地方来写东西，肯定有人会反对，会有人说 QQ 那么多人使用，QQ Zone 里面的主题多棒之类。 讲到编辑器，对我来说，编辑器中见过最棒的要数 eWebeditor 和 CuteEditor，只可惜前者不支持纯粹的 xhtml，后者则对路径支持不如前者，常常在想如果 CuteEditor 中有 eWebeditor 里面那么强的路径支持那会多棒，光为这个理由我也愿意花费时间把 MSIL 搞定了然后破解一份，再加上 ComponentArt 的各种界面支持，那一定是编辑器中最无敌的组合吧。至于 Javascript 和 html、css 就不提了，编辑器怎么可能会不支持呢。现在用来，似乎国内各大 blog Provider 中，Tencent 是最小气最无聊的，就丫不提供 html 支持！再往里处想时，也许 Tencent 觉得 QQ Zone 只在众多 QQ 用户里面用的最多，而直接从 IE 访问 QQ Zone 的人不多，也许不支持 html 是不是能避免一些安全上的问题，不过我还是觉得 Tencent 用这产的理由太牵强了些。 但也不是说 QQ Zone 换上了我想像中的那么超级 Editor，我就会来用这个写，为什么，因为主题，有人会问 QQ Zone 的主题怎么了，它的主题你看多优秀，什么效果都有，多漂亮，但我总觉得所有的主题都只是将一个 blog 纳入了框架，束缚感太强，对于 blog 我只想要有最高的自由，何况QQ Zone的主题整个俗不可耐，要说主题，Sohu 和 163 的主题都清新的多，也多符合国内对文字的感觉，QQ Zone 主题给人的感觉就是一个打扮的很太妹的小女生，然后还觉得自己打扮的特别美，这能不让人呕吐么？所以纯论主题而言，觉得 Sohu 可能会是最棒的。 再加一条，不加不爽：QQ Zone 一个超级无敌的设计就是在 QQ 中要转到 QQ Zone 时，转到的主页面是所有 QQ 空间用户内容提炼出来的首页，在首页的左上角有一个 Iframe，通过其中的图片 HyperLink 进入个人主页，可就是这个地方经常无法正常显示，即使对 iframe 进行 refresh 也没用，唯一的方法就是尝试对整个页面 refresh，除此之外就是自己想办法在 address bar 中输入个人地址，但这会丢失登陆信息，这个设计的给我们带来的麻烦简直无与伦比！巧合的是，Tencent 想到用抄袭 MSN 的方法来制作那个什么 TM，以为换个皮就是 QQ 式的 MSN 了，可是，乌龟脱了马甲也仍然是乌龟，并不会变成其它。所以相册我们用 Flick，邮箱大部用 163、gmail、hotmail，视频在 Youtube、Tudou，Search Agent 用 google 或者 baidu，简而方之，Tencent 什么都插了一脚，可是也就是插了一脚，连鞋都不曾湿一下，它并不是民族的希望，它只是用民族当幌子然后骗了一群小朋友口袋中的钱而已。认清这一事实的人们，早已投向免费和自由的怀抱。 有一个功能想提下，就是对 TeX 的支持，没有对 TeX 的支持，这实在是一件很不爽的事情，想打几个公式就如做恶梦一般，但好像现在只有 WordPress 可以做到对 TeX 的支持，也有另外极少数的系统对数学公式有一定支持，但没有哪家大厂商提供 WordPress 版的 blog，所以没机会用，唉！ 现在暂时用着 Live Spaces，最早我就只认识它，可是却没用它，后来我开始用它，时间长了，才发现其实很不错，也就不想换了。 MSN(live) Spaces：整体而言非常错，但编辑器功能还有待提高（缺 xhtml、TeX 支持，还有插入图片很不舒服），总体外观效果比较让人满意，但由于国际化的原因，本土风格化不如 sohu blog。 WordPress：如果自己有服务器，不用说了，就选它吧，爱怎么玩怎么玩，想怎么改怎么改。 Sohu blog、163 blog：整体不错，不过自由度和布局没 Live Spaces 强，但速度没得说（我不用主要是因为 MSN 的原因，至于 Sina blog，我不想用，所有页面没有静态页面地址，而且连客户端缓存也关了，有次为了抓某人写的 blog 还被迫写了个小东西去抓源文件，所以看着很不顺眼）。 QQ Zone：能不用就不用，不想再骂一次。 其它：比如 blogcn 或者 MySpace，没用过，也没想过去用，没资格评论。 其实我打些字上来原本就没准备给熟悉的人去看，或者就是给自己看看就好，这样而言似乎对 QQ Zone 有些过于敌视，管它的，想到什么就是什么，不用隐藏着。 有了 GitHub 之后，想要有什么样的功能都不是太大的问题。可以考虑 Jekyll, Hexo, Hego，通过一些特殊工具还可以非常轻松的将文档转换为不同格式，编辑器已经完全不再是关注的点。可以用 Mathjax 实现公式编辑，可以用别人已经做好的主题，也可以改一个自己喜欢的效果，可以实现不同客户端的良好支持。各种工具总体上来讲使用越来越方便，速度也越来越快。唯一缺点需要足够的整合能力让 blog 变成自己想要的模板。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[杂博]]></title>
    	<url>/cn/2007/12/02/micro-blog/</url>
		<content type="text"><![CDATA[[2010-12-30：昨晚做梦的时候，梦见在家里办事，然后怎么着就到了祖爷爷的坟前，他居然显身来，祖爷爷对自己说很高兴自己来找他。 2010-12-26：昨晚做梦的时候，梦见汤来找自己，我想也许她今天晚上生了吧。 2010-12-6：小区门口的那棵树开花了，一片红霞在冬天里更显耀眼和温暖。 2010-04-14：Endnote X2 导致 Word2007 速度变慢问题的简单解决方案：进入 Word：选项 - 校对 - 在word中更正和拼写和语法时，不选键入时标记语法错误。 2009-04-08：当你说“我是不可知论者”，你就犯了一个语义学错误或逻辑错误。一个不可知论者，不可能知道任何事情，包括“不可知”本身。 面试结束，就像陈安之的演讲一般，成功固然好，失败也没可惜的，谁知会不会有更好的机会留给自己呢。如同马云说的那样“等我们老的时候，我们后悔的往往不是做过某事，而是没做过某事”，何况经历过这次，才发现关心自己的人原来这么多。 这么长时间努力着做一件事，突然闲下来，才发现有些不习惯。到了时间，会自然的醒；到了时间，会不自觉的再背上包去图书馆发呆；自觉与不自觉中，才发现从前的事都已经距离自己很远很远，一切如何做了个梦，梦醒的时候，都恍忽记得不再太清楚。 马云说如果想自己出来创业，应该等个三五年，如果那时候还想创业的话，再试着去做不迟，那么读书做学问是不是也会如此呢。无心之中才发现好些地方都想借用马云的话，不知道我是不是也成了他的fans呀。 昨晚聚会的时候吃东西很晚，那会只有烧烤可以解决基本问题，顺带吃了一份油燃面。面条感觉并不太辣，只是我吃的时候顺带喝了些冰啤酒。回来的路上嗓子就难受的不行，晚上就感觉怎么也睡不踏实，今天早上起来也感觉不太舒服。看来刺激性的食物还是少吃为妙。 下午收到了 Dr. Lee 的回信，觉得有些感动，听说他平时特别忙的，为我这么个素不相识的人回信，等哪天自己可以给别人一些指导的时候，我也要一样把这样的精神发扬下去。 以前习惯不怎么好，总喜欢用一些模糊的话语表达自己的想法，这样多少带有欺骗的性质在里面。在她面前，我心中不愿意隐瞒任何的想法，可是又不知道如何说出来才好。就这么一直拖着也不是个办法，还是哪天鼓起勇气解决问题的好。此时此刻，才能体会决断力是多么重要。 对一个人好，即使非常真诚，也应该注意方式和方法，否则很容易适得其反，所谓久负大恩反成仇，应该是其中最典型的一个例子吧。互助与交流的平等地位往往比施恩或者给予的级别和层次更让人尊重。 客观来讲，男人而言刘凯瑞应该是个非常好的，女人则凌雪伤最是不错，但问题是客观和主观总会存在差异，导致的结果就是简佳选择做小航的红拂女，而段天狼心里面永远只有苏荷一个人。人与人之间的交流，好与坏的标准，除了客观实际外，主观的心理自然也非常紧要。 看过一个片，里面有句话：“女人的世界是男人，男人的世界是世界”，觉得非常经典。人活在世上，可以摆脱客观规律的奴隶，但无法不受客观规律的支配。若想不受规律的限制，要么欺骗自己，要么欺骗别人。 哲学这个东西，对大部分人来说，只在生活非常如意或者生活非常不如意的时候，你才会想起来。所以现在也越来越不被主流社会普遍接受，不能解决具体问题的学问，也慢慢走向了没落。但愿你我都只是在无病呻吟时才想起哲学，而不是在失意落寞时拿来安慰自己。 历来我都不是追星族，其实我也喜欢看戏，也会喜欢许多戏中的角色，但并不代表我也喜欢角色对应的演员。戏子最让人诟病的地方，便是明明是卖身或者无德，却说自己是为艺术献身，用个专业的词叫做没有职业操守。我不怎么喜欢张国荣这种类型的艺人，但还是尊重他，一个对生活态度认真的人，即使他的态度比较特别，但总还是值得让人尊重的，虽然不是尊敬。 十年了，也没看过一次樱花，连自己都觉得有些可怜。既然看不成樱花，就看桃花和梅花吧，再不然梨花也成。 最近状态不错，能吃能睡，看书的效率也还不差，要是能够一辈子都能这样，那该多好。 最近越来越觉得 mac 的好，简直可以说是迷恋，特别是那款 macbook pro，爱死了，做梦梦见的都是它呀，让人手痒的不得了！ 窗外风刮的特别大，呜咽声从耳边传过时，更让人感觉到房间的空洞，天气也似乎特别冷，我突然想起星楠 QQ 里面的一句：寂寞摧残的人如此可怜。这样一句沧桑的话语从一个还没长大的姑娘心中说出来，让人有些感慨。 先是《色·戒》，然后是《投名状》，觉得很不爽，我一直喜欢看喜剧或者比较优雅些的片子，不明白这世界已经很丑恶了，这些主流的所谓文化却还一直在宣传着这些丑恶的东西，是想让人们学会保护自己，还是让他们变得更加糟糕呢。对于《投名状》，我已尽力从另一个角度看另一些细节，可是能让自己记住只是苏州城里人埋人的场景：今天埋人，它日被埋。相比之下，原来《命运呼叫转移》虽然俗套，却似乎还好看一些。无怪深秋会觉得，作为女人，她宁可看张爱玲笔下的王佳芝，而不是李安电影中的那个可怜的女子。我则宁愿再看一遍《Roma Holiday》或者阿拉丁这类的小儿片。 初听水晶唱着《我只在乎你》这首歌时，就想原来这世上竟真有这样的人。再看她最后放弃选择时，突然觉得很安慰，原来世界上还有一样类似心境的人，即使是在游戏里。 别轻易说出伤害某人的话，如果是对爱你的人，那会伤了对方的心，对恨你的人，则让他更想除你而后快。但是如果是称赞某人的话，不妨多说一些，其实称赞的人多人，也许就真的会有脱胎换骨的功效的。 从没见下过那么大的雪，可我一点也不讨厌这种下法，尽管许多人都认为这是一种灾难，可是当茫茫大雪盖住了大地上的每一寸土地时，也盖住了种种的丑陋，我们可以在这种白色的世界里肆无忌惮的玩耍，仿佛儿时的每滴欢乐又回到了眼前。 明知道鸦片吸过以后会有飘飘欲仙的感觉，但我想没几人会真这么去做。所以我们一脸的神往，谈论或者怀念某事，并不代表我们会回去过去或者选择什么，感慨而已，不用当真。 所有的电视媒体无一例外的都将关注的焦点放在火车站与汽车站，中央部委在背后则特别关注着能源的调配问题，但很奇怪很少目光会集中在农村的留守人群里，他们生活的怎样，有什么样的困难，他们会是怎样的心情，没人知道；所有人都知道官兵们在这时都紧张的忙碌着，为这个社会的困难付出着，但却没人关注他们是否也会想家，他们是否也会想回家与亲人团聚。 朋友问我说你究竟喜欢什么样的女子，我好难回答，那一瞬间我要先猜测她究竟心中在想些什么，该如何回答才能让大家的对话不至于尴尬；其实那时我真的不知道该怎么形容，我说也许是气质吧，由琴棋书画而来的气质，可是没有料到的是这样的回答在对方的追问之下瞬间就被击溃了，因为我根本找不出现实中的某人或者某角色的例子来。晚上听到一些歌时，居然发现真有这样人存在，欣喜。 电影是柴米油盐，各种情景的交织让你感受现实的错综复杂，是生活的压缩与提炼版；文字是风花雪月，笔墨玑珠的运转让你体验思维的天马行空，是浪漫的源泉与缔造者。我们是生活在这世上的有精神和思想的动物，所以我们既要电影，也要文字，真实的感受生活，烂漫的追求梦想。 “我望着你，你望着我，千言万语化作沉默”，无意中看到的一句歌词，觉得非常不错，和另外一首歌中的一句“眼中带着爱意，目光带出了心痴，轻轻对望已传情”，好生相似。刚开始听后面这几句时，觉得实在妙，心想恐怕再难找到这么好感觉的句子了吧，谁知文字这东西，真是不可思意，竟然就真的能再找到一般感觉的话来，实在回味。 古人云“三十而立”，那么如果不立，是不是会便不会到三十？ 收到准考证，上面印着那个熟悉而陌生的名字，那个唯一可以寄托的名字，我将之竭尽全力的名字。 万家灯火 千点残星 一梦之间 十年以前 对任何人,事,物都不寄于希望,也就不会失望!从某个角度来说,是一种豁达.然,想深一层,悲凉倍感! 走了，轻轻的走过岁月的每一天，没有挫折的失落，没有成功的喜悦，只是平平淡淡的活着，从从容容地享受着世界给予我们的美好生活，坦坦荡荡地把握每一次幸福的机会，人生本应如此，生命应当如此。 縱然失望，別用冷漠把心收藏 已经不再笑了，彷徨于人生的得失、梦想的沉沦。 只喜欢黑色，喜欢去固定的地方，喜欢一个人呆着。 伴随着自己老去的，只有那同样孤独的灵魂，在人群之中，在繁华之中。那么，当你仰望星空，又有多少的迷惘在那无垠的寂寥以外呢？ 納須彌於微塵芥子 憐蒼生戲煙火俗世 儒家学说不是好东西。所谓“明知不可为而为之”或者“头悬梁、锥刺股；凿壁偷光、萤囊映雪”在大部分时间里实在是很糟糕的人生哲学。 我没目标，幻想倒有一个，哪天哪个老和尚瞧上我收我做他徒弟我就开心啦，我天天给他偷狗肉来吃，我自己一点也不偷吃，我只当前师傅的面光明正大的吃（开玩笑）。 喜欢怎样便怎样，何必一本正经做人，太累！ 看日出要你打破了平日的规律，要么不睡，要么太早起，很不自然；看夕阳就没这样的问题，多么随意，所以要找的那个人，应该是在一起默默看夕阳，而不是去看日出的。 我多想小时候那些原野和小溪，不用是怎样的美景，自然就行，仿佛阿炳的桑园一般（暗算-听风），那里才是他的栖息之所。 以往每年总得有次大些的感冒，今年快过完了，还没感冒过，我心里有些不安，不知是好是坏。 很多男人喜欢张曼玉，很多女人喜欢梁朝伟，不过这两人我都鄙视。一个人离婚重新再找一个也许不好说什么，赵雅芝就没人会觉得她有甚问题。但换了一个又一个的时候，恐怕问题在她身上居多，在我看来，张的人生其实和《济公》里面的那个九世妓女差不多，这样来看的话，也许古代那些妓女还可以被人可怜，毕竟她们本心不愿意如此；梁就不用说了，当年无线五虎离开的那一刹那，梁其实就已经是个被兄弟所不齿的人了，即使他演技再好，眼神再迷人，这个事实也无法被否定，更何况他还是一个演三级片（色·戒）的家伙呢？这样类型的艺人，连用戏子称呼他们也太瞧得起他们啦。 《血色浪漫》很久就看完了，真是一部很好看的片，里面的信天游唱的太好听啦，里面的主演钟跃民实在个性鲜明，是个非常让人喜欢的角色。但我觉得如果在现实中，对于女人来说，这种人也许做恋人或者情人不错，但不可以是结婚的对象，结婚还得找张海洋，除非和钟跃民一样，女的也是一个喜欢自由（做事随意，不会付任何责任也不喜欢别人对自己负责）的人，那可就成了“天造地设”的一对，所以才会有秦岭这个人的出现。只是现实中，我们都为生活奔走，又多少人可以如此随性呢，所以钟跃民在事实是最后只是一个，也是唯一一个一事无成的人。这样的人，我们可以对他传说，却不能模仿。 金庸的小说大家喜欢，除过每个主角是那么经典外，其实配角也好不错。我最喜欢李芫芷，余鱼同同学前面的做法实在是“太不识抬举”啦，相较之下，好些主角似乎也不如小李同学，实在是非常不错。 难为Maskin，要回答那个白痴主持人提出的各种白痴问题；让人郁闷的是五哈，像小学生一样回答主持人提出的各种问题；让我郁闷的是老邹同志连面也不给露一面，看来《对话》不看也罢。 我非常讨厌喝酒，非常讨厌……，厌恶一个喷着酒气的人对自己的面说话，厌恶自己身上沾染的每一丝酒气。 我改了网名，便是以前几年的总结，从此便是新的开始。 出血太多，果然口特别渴，其实嘴唇并未卷起，可是还是想再多喝一些会否好点。也会如武侠片中一般犯困，只想一闭眼就睡着不起来了，什么事情都不再管。晚上的风就像刚刚涨过工资的职员一般情绪高昂，不遗余力的吹过来时，还免费的携带着些雨水，冰冷潮湿的感觉钻到身上包裹的最严实的位置，让人不由自主想要到个阳光明媚的荷塘边，逃避这凄风苦雨，心中更会想或者干脆下场雪也好受点。只是这风也没能将人从迷糊中吹醒，潜意识里仍然想停下来歇息个够，但有个声音却不断提醒自己不能闭眼。迷糊与清醒之间，我就像是站在了宇宙的边缘，却一直都找不到人生的归宿。 大家喜欢引用鲁迅一句话是“走的多了，自然也就成了路”，现在行不通，路得靠修。何况鲁迅也不一定就是什么好鸟，他对自己原本和兄弟的态度便可见一斑。 感觉某人特别喜欢某事物，可能是错的，这也许是因为他的爱憎分明所致，也可能是宣传效应的结果。 蔑视一切，要先蔑视自己；尊重一切，也要尊重自己。 暮色茫茫有时比黑暗还让人泄气，黑暗中还会坚定信念的摸索，可暮色中却总是希望能够把眼睛再睁大一些，然而却还是无济于事。 提出二进制的人很牛，可是光有01 还不足有现在电脑的绚丽多彩的应用，那么从这个角度来说，为 01 创造应用的人就更让人值得尊重。也是因为这样，人们才需要将原本简单的事情变得很复杂，因为不这样就不足以满足需求的多样性。 鄙视lenovo，以后宁可买 Sony 甚至 Apple，也不再买 Thinkpad，联想果然是连想也不能想！ 待哪日厌倦了，便找个桃红柳绿的乡间，回忆往事。 吃完原告吃被告，如有不服，原告罚十万，被告关三年！ Not every couple is a pair! 要打电话给某人却找不到理由时，就直接打过去，然后说“你是不是打过电话给我”就行！ 在整个过程中，某件事需要划分成多个步骤来完成，每个步骤有多于一个的选择，那么是否在每一步中选择相对当时条件下最好的那一个，整个结局也是最好的呢？]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[挥之不去的武侠情节]]></title>
    	<url>/cn/2007/11/25/martial-arts-novel/</url>
		<content type="text"><![CDATA[[看过一本叫《仙篮奇剑传》的小说，附在一期《今古传奇》上，书有点白话文的意思，那时候我还很小，连字也认识不多，可即使是这样，还是断断续续看过好几遍。此前童话书或许看过不少，但那么长的小说估计还是头一回看，也许再加上书中情节很吸引人的缘故，现在快二十年过去了，一直没忘记过。这本书网上流传的很少，根本找不到全本，当年的《今古传奇》也已经无处可寻。这本书应该是能记得的最早读过的武侠吧，有次意外在网路上又看到这本书的名字，好生感慨。 有次父亲回家的时候带了两本《蜀山剑侠传》，白话的感觉就更明显了，并且非常非常长。那么长的小说、那么离奇的情节、那么宏大的气势立刻便将我吸引了去，那些日子里，我总在被窝中拿着手电筒翻了又翻，为主角的每个奇遇和每个惊险的情节起伏澎湃。这套书自然也就陪着自己走过了小学、中学里最美好的时光。只可惜，当时父亲带回家的只是前二集，所以后面是什么内容在很长的时间里都是个问号，这种遗憾直到读大学时才得到些弥补。但时间总是在走，内心也在变化着，后来我再看当年没看到的剩余部分时，已经没有了小时候的神往，这应该是对外面世界的实现多了了解吧，生活中的这样或者那样的诱惑已经让自己变得不再单纯，当然也就不如小时那样充满对小说中每个情节的激情了。这套书在很长时间里留给我印象最深的，反而是书背后引用华罗庚的一句话：“武侠小说是成年人的童话。” 小时候周围的大人小孩们都在大谈特谈《射雕英雄传》或者《神雕侠侣》的时候，我根本不懂是些什么。所谓郭靖或者黄蓉，在我的想像中无非就是可以开坦克去和人打架并且总会赢的一方。由于方言口音的原因，甚至根本不知道“雕”怎么会“下雨”，也无法理解那些日子里一大帮人聚在一起看某个电视到很晚是为了什么。以后再加上《蜀山剑侠传》的原因，对金庸小说当然就不是很熟。 直到高中的时候，才慢慢开始接触一些金庸的小说。觉得很喜欢，看了其中的几本，包括《神雕侠侣》、《碧血剑》和《连城诀》。有趣的是在高二的时候，有天上晚自习，正看《书剑恩仇录》到紧要处，班主任来了，正看的入神的时候，我的书就已经被收走了，之后还被教育“不可沉迷在其中，一切均应以学业为重”之类云云。现在想来也讽刺，当时老师教育自己的理由是这样或会早想些什么情啊爱啊之类，说明白了就是导致早恋，自然就再不能一心学习。那时的我，喜欢这些小说的原因更多是因为男孩子对江湖之类名词的向往，至于情爱一说，肯定是子虚乌有的事。那时候很单纯，经此一事，后来就再也没看过了，所以《书剑恩仇录》也就成了自己高中时看过的最后一本武侠小说。 高二寒假的的时候，地方台里有个频道放着刘、陈版的《神雕侠侣》，那时已经看得懂了。我非常喜欢那部片，连电视剧的主题曲也熟悉的不行。张德兰唱的这首《何日再相见》，不算是最早熟悉的粤语歌。当时正是大家热爱Beyond和陈百强这些明星到不行的时候，可那时我就是只对这首歌印象最深也最喜欢。现在我仍然最喜欢粤语歌，尤衷顾嘉辉和黄沾的作品，并且几乎对张德兰的歌每首必会，别人看来偏激，在我而言，应是对当年爱好的一种延续吧。有次去唱歌时，茂名的同学阿盛告诉我说我的发音听不出一点问题，这对从没去过广东也没说过粤语的人来说，应该不是太容易吧。光靠唱些歌到这样的程度，足见当时那部电视剧对自己影响多深！要提一下的是班主任对我的教导并没任何效果，那种强压下的教育方式让我吃尽了苦头，令自己遗憾难过至今，可以说没考上个好大学，很大程度是因为压力过度！我想如果这辈子如果不能考上一流大学，去读个自己喜欢的专业的话，那么对自己来说，再快乐恐怕也将会是残缺的。只可惜没人可以理解我的这种想法，还不够讽刺么。我不是怪班主任，我怪这种糟糕的教育制度，再进一层，就是对儒家所谓“明之不可为而为之”的理念深恶痛绝！ 开始读大学后，一次偶然的机会，在租书的小店中见到了《蜀山》的全集，欣喜之余便赶紧租了来看，可是经过那么些年，人已经变得浮躁了许多，没什么兴趣再坚持看完全书。想起来也确实无奈，现在每次我在向朋友推荐这套书时，朋友一听字数上千万还没结尾便毫无兴趣了，与我那时相比又有什么本质区别呢？《蜀山》没怎么看，反而其他几个人的武侠小说看了不少，但很奇怪，看来看去，装在心里面的却只有金庸的小说。当时不以为意，现在想来，便是因为他的小说中多了个字，而偏等我们长大了以后，经历了许多，才慢慢能感受得到。一直以来，对金庸的小说开始最喜欢《神雕侠侣》，后来是《碧血剑》，接着是《连城诀》，大部分人喜欢的比如《鹿鼎记》、《天龙八部》之类，这些反而没多少感觉。现在喜欢《白马啸西风》和《笑傲江湖》居多，尤其《白马啸西风》，每隔一年半年再来看一遍时，总还是同样的感觉。 网络开始走进了生活，这真是个神奇的东西。这几年的网络迅速发展，让自己以前连想也不曾想的可能都变成了现实。我也头一次在网上找到了最全的还珠小说集，为了整理这些小说，专门学了一段时间的正则表达式，专门写了小工具来进行整理、排版，而且还在几年里慢慢读完了整个蜀山系列的小说。不得不佩服还珠，那么长的小说，写了那么长时间，却原来连高潮部分都还没到，换了别人，早写得连上吊的心都有了。偶尔我也会想，如果当年他去了台湾或者香港，也许这部书就不至于如此结局吧。后来也有人尝试着去写续集，但总没人能写出那些味道，甚至连文风也无法相合。毕竟写小说的人里，又有谁可能有还珠那样的才气呢。于是只能安慰自己维纳斯要不少条胳膊她也没那么出名吧。 总有书报上或者某人说谁谁是武侠小说的什么什么，每每听到，都会哑然失笑。甚至现在有人在网上问《诛仙》在前还是《蜀山》在前这样的问题，我没看过诛仙，也从未想过去看，对还珠的这种绝对信心是以他的文字做基础的，根本没什么可置疑。光以文字水平而言，金大侠恐怕只能甘拜下风，其它些人更是不值一提。以前是这样，现在是这样，以后恐怕就更得是这样了。现在除了两三本以外，还珠的书都还可以找到，《剧孟》、《游侠郭解》只在书店见过一次，后来想买的时候，已经没有了，有些遗憾，《血滴子大侠甘凤池》连见也没见过，更是无法。不管怎么说，我还是该谢谢父亲才对，不然我也不会知道蜀山的存在。也许有天，应该买到一套全集送给父亲，不知他会作何感想？ 现在我已经很少再专门读武侠小说，毕竟已没那多时间与精力。回头再看这些年的经历时，偶尔会问自己：究竟喜欢还珠的书多一些还是金庸的书多一些。那时不好回答，想想一个离奇惊险，一个扣人心弦，教人无法取舍。现在看来，应该选金庸的可能多些吧，毕竟我们除了要看最华丽的文字之外，也不能忘记了文字的原本目的，我们还要看文字里面包含的浓浓的人情味，这一点却是还珠的小说里没有的。当然这么说并不代表我不够喜欢还珠的书，我其实也非常喜欢。举个例子，我没看过步非烟的书，可是当我看到她在自己blog中提到还珠的那几个句子的时候，我便凭空对她添了无数好感，所谓爱屋及乌，应该就是这个意思吧。只是不知道再过二十年时，我又会作何感想？ 也请您来感受当年顾氏风格的“風塵淚”，十年之后再唱，尤胜当年。 張德蘭：風塵淚（http://www.youtube.com/watch?v=A4Q-28eyl-w） youtube flv capturing（http://www.savevideodownload.com/download.php） 如果想看，建议用上面的抓取工具先分析得到视频地址，然后用flashget或者迅雷之类的工具下载后再看（现在访问youtube的速度实在糟糕）]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[金婚]]></title>
    	<url>/cn/2007/11/22/goldenwedding/</url>
		<content type="text"><![CDATA[[见过好些次介绍 片子实在是太长了 整个片子都唠唠叨叨吵个没完 那么长的电视剧要看完不是个容易的事儿 无意中听了听片尾曲 有些感触 只可惜网上没找个现成的 贴张图片算那么回事吧 就和片的壳子后面说的 哪天非得完完整整的看一遍才好]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[奇怪的梦]]></title>
    	<url>/cn/2007/11/01/dream/</url>
		<content type="text"><![CDATA[[做了段好奇怪的梦，夢里面626的人又都回到了一起，在那個最后一年的小宿舍里面，然后不知怎么所有的人都在沒有注意到的時間里不見了，變成我在某個 等講完出來的時候，發覺自己在一個很寧靜的小鎮里面，東面是雜草叢生的出口，一個人快速的向出口跑著，后面跟著一只如狗一樣大小的兔子。中間是一片水塘和沼澤地，我看著西邊的時候，一排三四棟西方式的建筑，中間還有一間教常，面對著中間的沼澤荷田。 于是就和朋友商量趁著還沒離開前的空闲到教堂那邊去看看。我們順著土路從南邊繞至西邊，西南拐角處還看到一片荷田，已經深秋，但荷葉還是翠綠無比。時令的關系，居然發現還有一朵黃綠色的荷花開著，好讓人心情愉悅。 西邊路的右手邊是荷田，左邊也有一片，離我們稍遠一些，更遠些的地方則是一片非常大的清澈得讓人羡慕的游泳池，雖然天氣已經轉涼，但人還是好多，主要都是當地小鎮上的孩子們。看著這些，心中開始幻想是否可以在這世外桃源一直呆下去不走才好。 我們到教堂后，好些人擠著便進去了。由于人多，大門便關上，我等下一批進去方才入內。里面很奇怪，一大間的屋子分成了許多間，中間走廊相連，供奉的并非西式諸神，而是秦時的名臣名將。西邊單間里面神像旁邊甚至還提供了香火供大家參拜。我見同學拿著三柱香從西邊到正廳，便也學了他樣去西邊單間中取三枝香來，卻不知為何到正廳后熄了一支，更奇怪是并非只余兩只，而是變作了五支一起，最后還都好好燒著。 西邊隔間中靠北還有一小間，里面賣各式的新潮衣物和飾品，女子用的居多，所以我就退了出來，等出大門再看時，自己所在變成了一片很大的空地，如同渡假村一般，左右停著許多車。回頭時才發現大門已經變成了坐北朝南。我見大家還沒出來，再進行去，那些神像都在，但感覺距離自己有些遠，走近時還會變作其它居間，根本不見神像，而是一些白紗隔著的內間而已，空無一人。 等退出內間時，夢便醒了。我還回味著夢中的情景，這蓮花世界里的小鎮感覺是許久以來最美妙的夢。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[好不好]]></title>
    	<url>/cn/2007/10/27/good-bad/</url>
		<content type="text"><![CDATA[[如佛：好就是不好，不好就是好； 如儒：好当然得好，不好也得好； 如道：好当然是好，不好也是好；]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[香港，民国风流的一段哀伤传承香港 z]]></title>
    	<url>/cn/2007/09/24/bufeiyan-hongkong/</url>
		<content type="text"><![CDATA[[步非烟 我向来觉得，就某种意义而言，小说发展到民国，是一个高峰。这不仅是因为那时的小说种类繁多，体裁具备，精品叠出，更是因为创作者众多，可以说是超出了任何一个时代。民国时期的读者热情也是特别高涨，大抵是因为当时救国救亡，睹物伤情，所以要从文字中找些慰藉。 所以那时的创作是无物不可入小说，也促使了小说进行细分，世情小说，爱情小说，侠义小说，神魔小说，历史小说……等等等等，不一而足。但就我们熟悉的武侠小说来讲，也由于无物不可入小说这一特点而决定着它与别的体裁边缘化，产生了历史武侠、言情武侠、市井武侠、神魔武侠等等。这是一个群星荟萃的时代，文学不仅仅是武器，是照耀民间的火炬，也是消遣的精神食粮，是谋生的手段。乱世往往也是最纸醉金迷的时代，因此，洋场才子，也成了当时一个特定的名词。民国文学的特点是复杂的，一面慷慨激昂，一面颓废破败，两者都无比鲜明。 文学的特征都基于时代，因此，当新中国建立时，民国的一切风卷残云般消失了。我们迎来了十七年文学，伤痕文学，以及打开门五光十色都吸纳来的先锋文学。我们并没有继承民国文学，这造成了一段隔离，使我们不熟悉民国文学，现在阅读跟学习起来总有些隔膜。仿佛是一杯香醇的美酒，但却被我们永远地搁置起来了。这无疑是种遗憾。 圣人有句很聪明的话叫做礼散而求诸野，香港台湾自然不能算野，却幸运地起到了这种作用，跟民国的文学一脉相承。或许其原因是当时的许多作家都移居此二地吧，另一个很重要的原因，我以为，是港台两地之于民国，并没有像新中国那样有着截然不同的变化。政治体系还是那个体系，人还是那群人，而忧患苦闷之心情，恐怕也没有一下子消除。反映在文学上，港台文学是继续循着民国文学的路子，继续走着，生根发芽，繁荣昌盛，有颇长的一段时间，令我们相当惊诧。 小说仍在写着，一直写到现在。我觉得，港台文学，特别是香港文学，实在与民国文学有着极其相似的地方。举几个例子。倪匡之还珠楼主，李碧华之于张爱玲。 还珠楼主于1932年连载《蜀山剑侠传》以来，便一发而不可收拾，《青城十九侠》《云海争奇记》等小说不断问世，一坑未平一坑又起，这一点大有现代网络作家之风范。但还珠楼主对传统文化之浸淫实在非同小可，不但表现在精雅的文笔上，而且见识广博，涉猎繁多，三教九流，古往今来，称得上是广闻博记。他的书中好写到苗疆民族，无论服饰习性还是风俗都娓娓道来，如数家珍。他的小说绝非仅仅只是神魔小说，可当作游记美文来看，可当作风物志来看，可当作志怪传奇看，可当作想象力大博览来看，难怪当时有人会守在印报机之前等他的小说。 倪匡也是以想象力著称，更自诩为写汉字最多的作家，作品之多，据说无出其右。他的小说当真是天马行空，早就脱出了这个地球，大写外星人，神脑超人云云。看的时候云烟乱舞，每每追着最新情节，看完之后却有些遗憾——原来最新情节与以前的情节也差不太多。看多了就有些腻。他所写之事并非不奇，却不足让人拍案叫绝，他所写作品并非不多，却总有些自己重复自己的感觉。 李碧华是个很有自己风格的小女人，她自言要“过上等生活，付中等劳力，享下等情欲”。只此一句话，就让我无比欣赏。这一句话，道尽了她所有的风情，那么率真，那么尖锐，那么熟知世情，那么洒脱。她的文字也是这样，有一双别人所没有的冷眼、媚眼，静中看出这个世界的分寸。《青蛇》中云：“每个男人都希望他生命中有两个女人：白蛇和青蛇。每个女人，也希望她生命中有两个男人：许仙和法海。”这恰似张爱玲的红玫瑰与白玫瑰，但李碧华的世界里，淡化了男人，只剩下了白蛇和青蛇，不再像张爱玲那样，无论白玫瑰还是红玫瑰，她们都需围绕着“男人”。《霸王别姬》中就更为露骨，已不再有女人，恩怨情仇，都只是男人。这种改变让人无比欢喜，她毕竟在自己跨过的峰顶，又前进了一步。 张爱玲是无法学习的，如果李碧华只是学习张爱玲，就永远不可能写出如今的文字来。张爱玲有着一双别人所没有的眼睛，她有着尖锐的细腻，以及同样一句就能刺穿你温柔心的文字。她不是在记叙这个时代，她是在预言，她积攒着自己的生命，预言出一段段悲欢离合。所以，只学习她的文字，或者学习她的情节，是没有用的，这只会导致一事无成。每个作者都有她的符号，张爱玲的是“乱世”，而李碧华的是“孽缘”。然而李碧华的文字常常让我觉得破碎断续，张爱玲的也不拿腔调，却流露自然的精致。 这大抵就是传承的代价吧。时代渐渐在变，我们的感慨越来越软弱。我们操着南腔北调的洋话时，是写不出精致尖锐的中国文字的。文学从未曾变过，改变的是时代，是我们审视文学时，那挑剔而轻蔑的眼睛。我们夸夸其谈，我们侃侃而谈，我们高谈阔论，我们嘻笑怒骂。我们的交流越来越只是“话”，而不再是“文字”。 无论倪匡还是李碧华，还是在谈论与讥刺着的我们，在香港与北京之中，都缺失了水分，踉踉跄跄地行走着。这传承是如此哀伤，但毕竟还在传承着。 我们呢？我们的传承在哪里？]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[万语千言]]></title>
    	<url>/cn/2007/09/20/songslist/</url>
		<content type="text"><![CDATA[[现在是人都在拍金氏片，可惜没人想过拍了多少部连首可以留下来的歌都没有(天下无双实在一般)，又怎么可能获得大家的好评呢，导演不用功，却以为花钱 万语千言, YouTube 紫水晶 相識也是緣份 情义两心坚(刘德华总唱不出味道来) 風塵淚 鮮花滿月樓 何日再相見 願君心記取 一水隔天涯 舊夢不須記 - 雷安娜 希望找到祁美云的《还有明天》。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[无雪的冬天]]></title>
    	<url>/cn/2007/09/20/winter/</url>
		<content type="text"><![CDATA[[几年前由于一次操作不慎，硬盘上的数据全部丢了，再也找不到这首歌。当初在网上找到这首歌已经非常困难，中间这几年断续翻了很多页面，并未找回。偶然 还有一首，柳云龙唱的《知情者》的片尾曲《阳光快回来》，也是自己录制的，不过那首现在已经放到柳云龙客栈了，就不想多理会。毕竟听这些歌的人不多。想听的人有这些线索，也就足够去找到歌。 《无雪的冬天》歌词、主题曲 陈琳 又是这样一个冬天 怀念雪 怀念你 还有走过太远太远的从前 又是这样一个夜晚 不想听 不想看 让黑夜慢慢浸入双眼 人纷纷退却 雨纷纷堵截 命运流转成线 吹乱了恩怨 人纷纷退却 雨纷纷堵截 命运流转成线 吹乱了恩怨 天哭不出来忘了下雪 情被人们分解 虚耗了姻缘 天哭不出来 霓虹飞泻 你和我沉没有都市边缘 天哭不出来忘了下雪 情被人们分解虚耗了姻缘 天哭不出来霓虹飞泻 你和我沉没有都市边缘 歌曲下载：地址 陈琳吧：地址]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[菱角]]></title>
    	<url>/cn/2007/08/21/water-chestnut/</url>
		<content type="text"><![CDATA[[下午做完功课，买了半公斤菱角煮来吃，心里唱着我喜欢的某个歌星的《采红菱》 这里的人不怎么吃菱角，所以非常便宜，两三块就是一公斤，很划算，刚好便 从小在水边长大，对这些东西感觉很亲近，心里隐约想吃出童年的回忆吧 已经煮了两次，但不知道怎么回事，就是煮不出心里面想要的那种味道 自己做事总在有意无意之间和“事不过三”这样的原则挂勾 既然我前两次都不太成功，就不想再试了，也许哪天我看别人真正煮的过程了，会再煮吧 想起了某个城市的蜂蜜炒板栗，还有那家铁路边破旧的板栗摊 每次在其它地方见到糖炒板栗，都会自然的想起那个我常光顾的地方 可是事实上，我在那里吃着板栗，却总想起的是家里的菱角 等我吃着菱角的时候，却又想着异地的板栗 想起一句歌词来“人在远地 万语千言 人在对面 那有一句话” 人总是这么矛盾，但慢慢回味的时候，却又感觉很是自然…… 该如何就是如何，菱角也好，板栗也好，其实都已经成了我生命里多重要的记忆，怎么抹都抹不去的 一点也不矛盾的，该是怎样还是怎样 几年前我偏离了自己的轨道，现在呢，我又慢慢在最初的轨道上了 今天在公车上听到消息说陈美龄中秋那天在人民大会堂开演唱，不知这辈子我有没有机会听她的现场]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[七七]]></title>
    	<url>/cn/2007/08/19/the-lunar-77/</url>
		<content type="text"><![CDATA[[忙，没有时间的概念 手机突然响过，告诉我今天是什么日子，恍然大悟 年轮在不经意间又多了一圈 去年今天，天气多阴沉，我假装忙碌走在林荫道上 那时多兴奋 今年今天，天气蛮好，只是忙碌是真实的 特别的日子当然应该有祝愿，祝每个人都能开心 觉得自己能活到长生不老的，祝你和牛郎织女一样 觉得自己人生苦短的，就祝你天天如今天一样 想听神话故事的，葡萄藤和月光就恐怕只有自己去找啦 当然可以的话，也请祝福一下我 祝什么呢，就祝我能在明年今天，在计划着现在我最期望的那个打算吧 最近阅读和写作其实训练蛮多的，有些地方倒是已经学会该怎么处理 可是还是没学会直接，看来哪天学会直接表达自己的想法，估计我这个月的功课就做足了]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[状态]]></title>
    	<url>/cn/2007/08/14/state/</url>
		<content type="text"><![CDATA[[每天骑车上下坡，我发现上坡感觉很不爽，相反下坡感觉很爽，并且上坡要的时间比下坡多多了。 我还记得大一下学期很颓废，以至于后来一段时间都很害怕微 每次我下决心要想办法闯过去，可是每次看完第一节就懒得看了。 某天，听某个老大讲课三小时，回来自己干了两小时，然后觉得巨简单。短短五个小时让我体会到了下坡的感觉。 最近比较忙，每天干13—14个小时，还想再多点时间，还是算了，俺现在还没给资本家打工，保持革命本钱吧。 不过话说回来，现在的心情对比以前读高中时要这么干时的心情，一个天一个地。 这几天中午不睡午觉也觉得不怎么难过了，不容易呀！ 我想即使是挣钱我也没这么卖命过，看来对我来说，合胃口与胃口本来就比较大相比，谁更重要真不好说。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[忘我]]></title>
    	<url>/cn/2007/08/04/forget/</url>
		<content type="text"><![CDATA[[今天去上课的地方看看，问起来才知道自己一直都弄错了时间，那会觉得很开心 这样做起事来不知天日的状态，期盼了许久，现在在不经意间却已经做到 回来的 仿佛每个路人都一改往日冰冷的色彩，似乎笑脸产生的惬意影响了这城市的每个人 我就被这种气氛包围着，好不愉悦 原来我们要求降低的时候，幸福的感觉就这么随手可得 如果这时候有谁在一大群人前喊着热烈的口号或者标语，我定也会和大家一般心潮澎湃 我现在时间很宝贵，恨不得一分钟能够变成五六分钟来用 可是很矛盾，不可以像大部分人那样去争分夺秒，挤时间 甚至还需要空一些时间来休息或者玩耍，但也许这样，更能让自己珍惜拥有的每时每刻 感谢我的貔貅，我真的觉得你给我带来了好运]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[latex&#43;dvipdfmx 编译 pgf 图形无效的解决方法 z]]></title>
    	<url>/tech/2007/07/18/tikz-dvipdfm-not-work/</url>
		<content type="text"><![CDATA[[原文地址：http://bbs.ctex.org/forum.php?mod=viewthread&amp;amp;tid=76106&amp;amp;ex 最好的方法是升级expl3宏包 \documentclass[cs4size,openany,twoside,UTF8]{ctexbook} \usepackage[]{graphicx} \def\pgfsysdriver{pgfsys-dvipdfm.def} \usepackage{tikz} \begin{document} \begin{tikzpicture} \draw (2,2) circle (10ex); \end{tikzpicture} \end{document}]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[八卦一把]]></title>
    	<url>/cn/2007/07/18/miscell/</url>
		<content type="text"><![CDATA[[我以前不怎么喜欢下雨天，现在却觉得下雨只要不大，其实是一件非常不错的事情。 有些年觉得下雪怎么那么让人难过，比如读大学那会每到寒假回家，下了火 看来时间和环境在慢慢变化的时候，原来有些我们期望的和不希望的事情都很变着。 最近我对自己的了解好像又不同了一些，也在想自己和周围人的差距，和同学、同事还有同龄人之间的不同，多少也有了些体会。你看这几个词：medicate、mediate、meditate，大脑闪了一下，还是晕了一下？ 想到什么了，对了，告诉你我想到的：差之毫厘，谬以千里。形象吧，我们老祖先真是厉害，多么经典的词啊！借用火柴MM的一个词：鬼斧神工，怎么，不够？再借一个：偷天换日，该满足了吧。 看来哪天觉得自己瞧不起或者看不上或者觉得不比自己优秀的某某混得比自己好了，先得瘪嘴，要么淡然一笑，要么就分析分析自己的问题吧，也许差距就在某个不经意的小地方躲着。管你是日心说还是地心说的那一帮子的人，也不管你是什么样的身份，反正地球可不会以你或者我为中心来转。别做梦啦，接受现实吧。 晚上在某个店擦鞋，有个对面饭馆的小服务员过来想和老板娘换零钱。这种事哪天都能见到，或许你我自己就曾经有过这样的经历。我就试过一次在某个我常去的小店去换，然后人家就告诉我没有，我再也没去那店买过东西。可是这次这家的店主的回答让我学到了很生动的一招，她说：“哎呀我这儿的零钱前两天都让人家换走啦，100多块，现在我这里也没有了，真不好意思，下次，下次我一定帮你攒着”。 天，这是我见过最好的回答，看来说话确实是一门艺术，原本可能让大家都难做的事，虽然就这样轻松化解，厉害！ 当然，如果是老顾客的话，偶尔就帮人家换一换了，不同的对话和应付方式因人而异还是有必要的。要不时间久了次数多了，自然也就没意思。奇兵和奇思妙想可不能用多，用多就成下三滥了！ 今天有点八卦了，不过马上就闭嘴，这辈子还没去过北方，俺去也！]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[几个正则表达式]]></title>
    	<url>/tech/2007/07/18/regexp/</url>
		<content type="text"><![CDATA[[网络上的小说总是格式比较糟糕，需要进行整理才看起来舒服一些。 整理的过程中也遇到一些问题，我用的是 VS2005、C#，C# 对正则的支持很到位， Perl 所没有的特殊功能，下面是相关表达式： 删除空行 [\s ]*(?=\r\n)，要注意这个表达式中包含了一个全角的空格。在 Editplus 中还推荐了另一种写法，不过没有采用，是这样的：^[ \t]*\n（注意 \t 前有一个空格，当然愿意的话也可以和第一种一样包含一个全角空格），这种写法用C#的正则表达式测试器来测试的时候并不有效。而前一种写法在 Editplus 中则好像不能够支持，另外 Editplus 好像对?=、?!、?&amp;lt;=、?&amp;lt;!这几种高级部分内容的支持也不存在。 当然Editplus的最大好处之一就是能够同时替换打开的多个文件，这点真的比较有用。 双引号问题 在小说中经常由于排版原因会出现““这样两个连续的起始直角双引号，这时候要注意表达式的写法，如果直接用(“.&#43;)“这样写法会导致匹配出现错误，本来想非贪婪是否能够解决，似乎没有什么作用，只好取个巧，用下面的表达式完成：(“[^“]&#43;)“，这个表达式 Editplus 也支持，所以效果还是不错的。 此外，Frontpage2003 对于正则也提供简单支持，只是它的语法与传统的正则语法有些出入，提供的功能也比较简单，但对于简单的工作它还是有很大作用的，配合它提供的“html代码格式重新设置”功能，可以简化许多通过程序也比较不容易完成的任务。 Notepad&#43;&#43; 用 Notepad&#43;&#43; 打开(注：^ 代表行首，$ 代表行尾) 去除行尾空格和空白行：按CTRL&#43;H选择正则表达式 - 查找目标：\s&#43;$ 替换为空； 去除行首空格：按CTRL&#43;H选择正则表达式 - 查找目标：^\s&#43;替换为空； 空行：\r\n，^\r\n，或者安装 TextFX 插件；]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[多段文本分段]]></title>
    	<url>/tech/2007/07/18/regexp1/</url>
		<content type="text"><![CDATA[[参考资料：http://www.geany.org/manual/gtk/glib/glib-regex-syntax.html 说明，只是大 表达式：&amp;lt;h1&amp;gt;(.(?!&amp;lt;h1&amp;gt;))&#43;\r\n，注意环境为.NET 2.0，设置匹配的 Options 为SingleLine。 文本 &amp;lt;h1&amp;gt;第一百八十一章 正文&amp;lt;/h1&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;h1&amp;gt;第一百八十三章 正文&amp;lt;/h1&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;h1&amp;gt;第一百八十二章 正文&amp;lt;/h1&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; 结果 (&amp;lt;h1&amp;gt;第一百八十一章 正文&amp;lt;/h1&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; ) (&amp;gt;) ------- NEXT MATCH ------- (&amp;lt;h1&amp;gt;第一百八十三章 正文&amp;lt;/h1&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; ) (&amp;gt;) ------- NEXT MATCH ------- (&amp;lt;h1&amp;gt;第一百八十二章 正文&amp;lt;/h1&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;asdf&amp;lt;/p&amp;gt; ) (&amp;gt;) ------- NEXT MATCH ------- 参考表达式 string strReg=@&amp;quot;((?&amp;lt;!深圳市).(?!深圳市))&#43;&amp;quot;; 参考资料 http://www.geany.org/manual/gtk/glib/glib-regex-syntax.html An assertion is a test on the characters following or preceding the current matching point that does not actually consume any characters. The simple assertions coded as \b, \B, \A, \Z, \z, ^ and $ are described above. More complicated assertions are coded as subpatterns. There are two kinds: those that look ahead of the current position in the subject string, and those that look behind it. An assertion subpattern is matched in the normal way, except that it does not cause the current matching position to be changed. Lookahead assertions start with (?= for positive assertions and (?! for negative assertions. For example, \w&#43;(?=;) matches a word followed by a semicolon, but does not include the semicolon in the match, and foo(?!bar) matches any occurrence of &amp;ldquo;foo&amp;rdquo; that is not followed by &amp;ldquo;bar&amp;rdquo;. Note that the apparently similar pattern (?!foo)bar does not find an occurrence of &amp;ldquo;bar&amp;rdquo; that is preceded by something other than &amp;ldquo;foo&amp;rdquo;; it finds any occurrence of &amp;ldquo;bar&amp;rdquo; whatsoever, because the assertion (?!foo) is always TRUE when the next three characters are &amp;ldquo;bar&amp;rdquo;. A lookbehind assertion is needed to achieve this effect. Lookbehind assertions start with (?&amp;lt;= for positive assertions and (?&amp;lt;! for negative assertions. For example, (?&amp;lt;!foo)bar does find an occurrence of &amp;ldquo;bar&amp;rdquo; that is not preceded by &amp;ldquo;foo&amp;rdquo;. The contents of a lookbehind assertion are restricted such that all the strings it matches must have a fixed length. However, if there are several alternatives, they do not all have to have the same fixed length. Thus (?&amp;lt;=bullock|donkey) is permitted, but (?&amp;lt;!dogs?|cats?) causes an error at compile time. Branches that match different length strings are permitted only at the top level of a lookbehind assertion. This is an extension compared with Perl 5.005, which requires all branches to match the same length of string. An assertion such as (?&amp;lt;=ab(c|de)) is not permitted, because its single top-level branch can match two different lengths, but it is acceptable if rewritten to use two top-level branches: (?&amp;lt;=abc|abde) The implementation of lookbehind assertions is, for each alternative, to temporarily move the current position back by the fixed width and then try to match. If there are insufficient characters before the current position, the match is deemed to fail. Lookbehinds in conjunction with once-only subpatterns can be particularly useful for matching at the ends of strings; an example is given at the end of the section on once-only subpatterns. Several assertions (of any sort) may occur in succession. For example, (?&amp;lt;=\d{3})(?&amp;lt;!999)foo matches &amp;ldquo;foo&amp;rdquo; preceded by three digits that are not &amp;ldquo;999&amp;rdquo;. Notice that each of the assertions is applied independently at the same point in the subject string. First there is a check that the previous three characters are all digits, then there is a check that the same three characters are not &amp;ldquo;999&amp;rdquo;. This pattern does not match &amp;ldquo;foo&amp;rdquo; preceded by six characters, the first of which are digits and the last three of which are not &amp;ldquo;999&amp;rdquo;. For example, it doesn&amp;rsquo;t match &amp;ldquo;123abcfoo&amp;rdquo;. A pattern to do that is (?&amp;lt;=\d{3}...)(?&amp;lt;!999)foo This time the first assertion looks at the preceding six characters, checking that the first three are digits, and then the second assertion checks that the preceding three characters are not &amp;ldquo;999&amp;rdquo;. Assertions can be nested in any combination. For example, (?&amp;lt;=(?&amp;lt;!foo)bar)baz matches an occurrence of &amp;ldquo;baz&amp;rdquo; that is preceded by &amp;ldquo;bar&amp;rdquo; which in turn is not preceded by &amp;ldquo;foo&amp;rdquo;, while (?&amp;lt;=\d{3}(?!999)...)foo is another pattern which matches &amp;ldquo;foo&amp;rdquo; preceded by three digits and any three characters that are not &amp;ldquo;999&amp;rdquo;. Assertion subpatterns are not capturing subpatterns, and may not be repeated, because it makes no sense to assert the same thing several times. If any kind of assertion contains capturing subpatterns within it, these are counted for the purposes of numbering the capturing subpatterns in the whole pattern. However, substring capturing is carried out only for positive assertions, because it does not make sense for negative assertions.]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[A dying cat]]></title>
    	<url>/cn/2007/07/05/a-dying-cat/</url>
		<content type="text"><![CDATA[[回来的路上，见到一只小猫躺在地上 嘴角流着血，和白色的毛映衬着 不停的抽动，似乎还想努力的爬起来 可是我想它再也爬不起来了 想到这里，开始觉得很心痛 街边的人也见到，都在惊讶或者叹息 其实我不怎么喜欢小动物的 在我眼里，动物和植物都会散发出它们自己的气味 我觉得一般动物或许会更喜欢植物的气味一些 而植物也许就更喜欢动物的气味一些吧 所以我想我基于天性，我就不怎么喜欢动物而是喜欢植物也 所以我也没养过小动物 可是不管怎样，当我见到这样的血的画面时，我还是很难过 一条生命就这样要消失了 而这样的消失方式，这样的事实总是让我不能接受 每日我冷冷的看着这个世界的时候，总是在想我是不是真的很冷酷 可是这时，我才意识到 原来我的血液中，仍然还流着怜悯的分子]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[专辑：天若有情]]></title>
    	<url>/cn/2007/06/27/god-full-of-feelings/</url>
		<content type="text"><![CDATA[[八年前某段日子，几乎天天听到这张专辑，当时就非常喜欢。可是那时根本听不懂粤语，所有对粤语的认识都来自 Beyond 和《射雕英雄传》。那时网络也没今天发达，大部分人甚至还没听过网络的概念，更不可能听说过 mp3 机是何物，所以对这张专辑的全部印象都只能以声乐的方式留在脑中，可是今天终于找到了。 回想起这几年，经典电视连续剧主题曲近三百首，《scaborought fair》（Sara Brightman）、《阳光快回来》（柳云龙）、《无雪的冬天》（陈琳）、《情歌》（李玲玉、唐彪），再加上张德兰福传中的那几首，再加上今天这张专辑，我想从今以后，我对歌的从前的遗憾就基本没有了。至于能不能收齐辉乐对我来说就无所谓啦，呵呵。 袁凤英 天若有情(追梦人粤语版) 巫启贤 心酸的情歌(心酸的情歌粤语版) 彭家丽 昨天 今天 下雨天 冯博娟 分手也是时候(想你想到梦里头粤语版) 孙耀威 爱的故事(上) 孙耀威 爱的故事(下) 彭家丽 何故 何苦 何必 (难以抗拒你容颜粤语版) 鲍翠薇 梦里几番哀 鲍翠薇 今天开心笑 陈美龄 愿君真爱不相欺 薰妮 相逢有如在梦中 张德兰 愿君心记取 陈美玲 爱的 Hamany 黄宝欣 两地情(绿岛小夜曲粤语版) 马俊伟 幸运就是遇到你(花心粤语版) 关菊英 狂潮 杜德伟 忘情号 《追梦人》是老版《雪山飞狐》的片尾曲，《天若有情》是我早找到的一首，大约在五年前； 《爱的故事》（下）是在三年前找到的； 《梦里几番哀》在半年前找到； 大概是五月下旬的时候，在 MP3 中突然听到一段很熟悉的旋律，那时特别激动，原来我在下载陈美龄的全集的时候，其中就包含了一首《愿君真爱不相欺》，这段时间比较忙，一直没什么时间上网，昨天晚上搜索了一下，发现原来李丽蕊也唱过，晚上临睡前突然想到了一个方法来查找这张专辑，今天刚好有点时间，谁知真的找到！ 感激陈美龄，如果不是她唱过这首歌，恐怕就没那么容易找到这个专辑了。以后俺就以陈美龄为二号俺像了，呵呵。 《爱的 Hamany》、《相逢有如在梦中》一直都是我的遗憾，虽然今天已经没时间再去找这两首歌，但我终于知道名字也，没想到幸福来的如此之快！ 今天能让自己高兴的事还有好几件，不知何故，也不知什么时候，我发现自己能够很容易就变得开心和快乐，满足和幸福的感觉时时都伴随自己的感觉真很美妙咯。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[遗憾不是年轻]]></title>
    	<url>/cn/2007/06/24/hope-young/</url>
		<content type="text"><![CDATA[[倒一杯咖啡，习惯的翻开某本已经很熟悉的杂志 一遍又一遍，重复着这样单调的习惯 画面很美丽，也许是你我一生的情节 问题很古老，也许是你我一生的思考 是 我不顾一切的 每个画面，每段音乐，每个情节 心动的、激动的、向往的、期待的、唏嘘的、怀念的 为什么年轻的时候，不可以，尝试 如窗上玻璃，变成镜子 反射而来的夕阳云彩，与玻璃后真实的高楼 层层叠叠，交融一起，无法分离 如同遗憾，令人唏嘘，被人传诵 明明存在，却不可捉摸 总是希冀，但难有侥幸 见到了铁树开花 一次两棵，同开同谢 不过并非像传说中一样，难以相逢]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[头发]]></title>
    	<url>/cn/2007/06/22/hair/</url>
		<content type="text"><![CDATA[[国人一向都很看重自己的头发，与之相关的词也很多，比如“黄童、绿鬓”，比如“乌丝、白发”，这样类似的词能找出很多。也许是某人年龄的代称，也许是 也难怪，从某人的头发总能看出一些这人的近况。春风得意的人的头发，经常比那些事事不顺的人的看着要光泽发亮些；生活讲究的人的头发，每每也比生活不居小节的人的看起来要整齐干净。 可能有一小群人对此并不关心，因为他们往往“断尽三千烦恼丝”。对生活和自己生活的环境没了激情，已经绝望时，就把看得和头一样重的头发剃了，来表达自己的无力的抗挣，他们无所谓。但大部分人恐怕不行，一般大家不会欢迎“聪明绝顶”或者“白了少年头”这类的句子用到自己身上，不信你在我们某某有所成就的领导身边多提几次这样的事情，看看领导的反应也差不多能明白一二。 生活过得不错，无忧无虑的人一般不用对自己的头发操心。往往开始担心自己头发的，总是那些多少不如果或者压力很大的人。武侠小说中比如《白发魔女传》中的那个女子，因为感情不如意，所以一夜白头；《射雕英雄传》中的瑛姑，儿子快死了，自己没办法救，所以也一夜白头。可怜未老头先白，顾影自怜愁肠断。写书的人真正聪明，实在找不到词来形容谁的伤心到底多深了，就安排他（她）来个造型上的大手术，用身体某个部位的变化在无语中把一切的一切都刻画出来。 传说中的一夜白头我没见过，所以我也没办法真正体会那是种怎样的伤心。但我确实又见到好几位恐怕对自己头发表示担心的人。 有个朋友现在在某个部门工作，压力特别大。往往一大早就自然惊醒，然后匆匆赶到单位做事；中午别说休息，连吃饭的时间也经常不能保证；半夜三更才能回家也是家常便饭。不仅如此，周末还经常需要加班，苦不堪言。有次对我说“现在他吃什么这药那药都没用啦，甚至连短发都不敢再留了，早上起来头发一把一把的掉”，听完着实让人觉得心惊，毕竟朋友连三十都还不到！所以大家往往取聚会时就笑其“干资本主义的活、拿社会主义的钱、受封建主义的气”，他听完也是无奈之极。我很替他担心，因为看得出来，他还是很看重自己的头发的。 毕竟上帝特别照顾的是少数人，你看那些在资本家银行打工的那些人，比如Zimo Zeng，累到连睡觉的时间都没有，要换常人，早就受不了了，但人家还是一头漂亮的黑发。可是这样的人能有多少呢？ 某日曾经在公车上看到一位穿制服的年轻女子，一丛丛的白头发夹在黑发中，看上去没有了自然的光泽，显得杂乱无力。虽然素不相识，可是看着仍然让自己有种很心痛的感觉。那时脑海中就立刻想到了一个词“红颜白发”，我就在想，是不是她过得很不快乐呢，是不是有很多心事呢……。如我所想，我觉得也许她满头白发了也许好些。也许你会认为我这种想法是不是疯了，哪个年轻的女子能接受自己的头发是全白的，那还不如别叫她活了！可我想的是也许如果真是全白了，也就无所谓、不在乎了吧，也许这样她能活得轻松一些。当然这只是自己的想法，当不得真，但其实世事总是如此，我们往往自 觉不自觉的就会在意别人的目光与感觉。 比如头发，长在自己头上，却总是担心别人看着会指指点点，真是无可奈何。所以如果你看某人的头发很好，就毫无保留的妒嫉吧，因为那不仅是外表的光鲜，也许还是她内心的优越。当然，希望你也是让大家妒嫉的人群中的某位。 PS：这两天自己的睡眠不好，昨天回来的时候觉得头痛得要命，让人按摩了一会，头发掉了好多，觉得有些害怕，没想到这样的担心居然就应到自己身上来了。以前妹妹总是戏谑我说“哥哥所有营养都全给头发了”（我比较瘦，但头发还可以），一直以来，似乎对头发的担心都不应该在自己身上，但是究竟会不会有天我也变成一个秃顶的老头呢，呵呵。这和中国的股市一样，猜不出来，就让时间去验证吧！]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[我在河西]]></title>
    	<url>/cn/2007/06/19/side/</url>
		<content type="text"><![CDATA[[三十年河东，三十年河西 站在河西，看着河东 脚下，是流淌不息的水 水中，一支芦苇在盘旋 请别诧异 我不是达摩，又怎会一苇渡江 人老了 饱经世间沧桑，尝遍人 也就凡事看开，心如古井中的水一般，不见波澜 不以物喜 其实如果有高兴的事情，或者笑人的话语 我还会开心一笑 不以已悲 似乎原来总会容易影响自己心情的事 现在发生也，也仿佛不曾发生 现在的我 喜欢老歌，喜欢怀念小时 喜欢淡淡看着周围发生的事 似乎连朋友、家人在眼中，也清淡如白水 如果是这样，那么我是不是老了 我现在仍然喜欢童话，相信神话 喜欢远离成人世界的简]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Tecsun PL550]]></title>
    	<url>/cn/2007/06/04/2007-06-tecsun-pl550/</url>
		<content type="text"><![CDATA[[订了一个 Tecsun PL550 想像有一天 背着黑色的包 浪迹天涯 陪伴我的 是包中黑色的砖头一样的你 也许我再追逐 我最期望的也还是离我那么远 可是 又有什么关系呢 至少这是我 还不够么]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[我多想]]></title>
    	<url>/cn/2007/06/04/i-hope/</url>
		<content type="text"><![CDATA[[虽然不能够相信没有确定过的事情 可是我也并不怀疑 比如来世 如果今生能够再来 我希望从我生命的最后减去5年 当然如果可以 8年我也愿意 一切重新开始的时候 我愿意没有这几年 如果有来世 我希望能够在 IVY 之一的某地方 丛丛容容的过着我今生期望的日子 再不济 我愿意是一棵草]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[第三个十年]]></title>
    	<url>/cn/2007/06/04/3rd-decade/</url>
		<content type="text"><![CDATA[[在我们避而不谈的时候 在我们视而不见的时候 第三个 十年之约 不期而至 这个疯狂的社会和时代 知识受到了前所未有的轻视 不该和不会还有不懂股市的人 全来了 现 他们可以骂政府的不讲信用 可是我觉得 换一个角度 这本身 就是对他们盲目和无知的 惩罚]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[我还想]]></title>
    	<url>/cn/2007/06/01/i-hope/</url>
		<content type="text"><![CDATA[[处处都是稚嫩的童声，处处可见穿着漂亮衣服的小朋友，在车里的，在路上的，和爸爸妈妈、老师、同学们一起，有说有笑。拍照的、追逐嬉戏的，抱着玩具的，吃着东西的，一脸的幸福与快乐。各种儿童用品痁和快餐痁门口也都成了欢乐的海洋。仿佛盛大的节日一样，这种快乐的气氛在整个城市弥漫，丝毫不亚于任何一个喜庆的传统节日。今年的六一还非常的特别，因为不仅能够在节日当天过足自己的节日，而且还能够接着过一个轻松愉快的周末，正所谓福有双至，这份额外的礼物其实是让大人们也心动羡慕不已。 回来的时候，公车停到某个站点时，上来了一群刚刚参加完表演的小朋友。马上非常平常的车厢里就充满了叽叽喳喳的声音。带队的老师看着他们在拥挤的人群中钻来钻去，一脸的无可奈何，他们哪里会舍得甚至是多说一句严厉的话呢，所以只能一遍遍的重复告诉大家注意安全，一会叫叫这个，一会喊喊那个，忙了个不亦乐乎。连面带匆忙的乘客们也仿佛被他们的天真可爱的情绪感染了，平常冰冷的神色也慢慢融化，面带微笑看这些祖国未来的小花朵。在天真无邪面前，久经人情世故的人们都怎么可能不被打动呢。 其实不仅是小朋友，爸爸妈妈们其实也都非常的快乐，看着自己的孩子已经渐渐长大，也许自己曾经的辛苦和劳累又算得了什么呢，下一代的茁壮成长带给父母的幸福感是无可替代的。 我的朋友中结婚的只有一对，他们去年年末生了一个宝宝，现在两人成天围着宝宝转，总是忙的晕头转向，可是从言谈中又透露出无比的自豪和幸福。 今天两人轮流给我发一些特别搞笑的六一小短信。我就问他们怎么那么默契，因为除了他俩之外其它人可是一条不发的。朋友回信说“六一了当然要发好玩的短信给小朋友啦”，虽然一下子让自己哭笑不得，怎么自己就变成小朋友了，可是还是非常高兴。也许有时这是一种还不成熟的表现，可是有什么关系呢，能够保持一份这样的心情可不是谁都可以做到，更何况还有那么多大人总希望有一天可以回到自己的童年，所以更多的时候我会对各种鄙夷的语言和神色视若无睹。 甚至直到现在，自己的床头也仍然会放着一两本童话选集，在睡觉之前翻翻，那可是一件非常惬意的事情。最喜欢也是最早拥有的一本书《三百六十五夜》，多么熟悉的字眼，多么美好的童年。这其中包含的，浪漫多于现实，理想多过生活，那书中的情节，直到现在都还能想起很多。有人说童话是给孩子看的，浪漫，也很幼稚，那又怎样呢，我就是喜欢那样天真与美丽的故事。 直到现在我也没有看过所谓的四大名著，也很少会留意《安徒生童话》或者其它。我也并不喜欢，这些要么被加上了政治的色彩，要么太过悲伤，直到现在看到这些书，我也会怀疑在小小的心灵里，怎么可能去看懂那字里行间的各种“道理”。当年我只是个孩子，不可能看到其中包含的深意，在我心中，只是想留下一个美丽的影子，一个好听的故事，那种教化多过趣味的书又怎能在心中占据一席之位。 唠唠叨叨的话一堆，连个顺序也没有，写出这些来，就是将自己的心情表达出来。无非一个意思：我还想是小时候的我，一个无忧无虑的我。 有一首歌《门》，歌词，是对美好童年生活的向往，虽然那可往不可及；我从其中听到的，却是另一种忧伤，对逝去童年的希冀。 还记得风中飘着炊烟 他离家时推开门的声音 孩子站在门沿睁大着眼睛 满天风沙掩没依靠的身影 常常望着天空白云 想着船长爸爸吐出的烟圈 他常说四望无际的岁月 漂泊只为了想忘记 THOSE FOOLIS HDAYS, FOOLISH DAYS 浴室里水滴的声音总伴着有人轻轻的叹息 看星星变成忧郁的萤火虫 孩子躲在床边想海上的父亲 在小小的心灵里孩子不明白 他们大人口里的无奈与舍弃 只知道蝴蝶和花该在一起 就像爸爸妈妈应该永不分离永不分离 巷子的风中又飘着炊烟 依稀听见有人推门的声音 孩子站在门沿睁大着眼睛 满天风沙掩没依靠的]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[意想不到]]></title>
    	<url>/cn/2007/05/30/unexpected/</url>
		<content type="text"><![CDATA[[昨天晚上在外面听到一首歌，觉得蛮不错，就记了一两句歌词 当时想着等回来的时候，马上去找来听 谁知道等回来想找时，却已经把记住的内容忘得一干二净 我 毕竟好听的歌那么多，如果连一首擦肩而过的歌都缠住自己，不是活得太辛苦了么 所以今天甚至已经不记得这事了 中午吃完饭的时候，拿出 mp3 来听 平常一直都不注意录音功能的 今天朋友也想买一个，而她觉得我这款还不错，打电话就让我先给点意见，试下录音 我打开试的时候，才发现，在录音的目录中，居然就有昨天听到的那首歌 我才想起，原来有次晚上听电台的时候，无意中听到这首歌 那时觉得不错，就录下了一小段 看文件创建时间，已经有快半年了 这期间其实已经更新过 mp3 中的内容好几次 甚至前段 copy 了一些单位的读音在其中，导致好多歌都已经被冲掉 这段录音却并没有受到影响 于是我昨天的遗憾，虽然今天已经忘记了的遗憾，就不再是遗憾了 我想这是什么呢，也许是奇迹吧 冥冥中早已注定 我们会拥有什么]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[素天心]]></title>
    	<url>/cn/2007/05/29/tianxin-su/</url>
		<content type="text"><![CDATA[[不是宿命的导演，所有情节只是别人早已编写好的剧本 不是宿命的主演，所有的心事也自然不会被人明白 只是宿命的见证，身在其中，又不在其中 一个人，二百 活着别人的嘱托，也活着自己的辛苦 死去的时候，有人看懂了你的忧愁 可那人如你一样，化作了一缕孤魂 活着的人，只是在质疑你的所为 谁，又记得你曾经的存在和无奈 活着，为了别人无法完成的心愿 死去，为了别人不能确定的命运 没有人可以倾诉 所以，死去，未必不是一种解脱…… 剑是莫邪剑，但心却是素天心 干将死，莫邪剑消失 莫邪消失，天心心不复存 干将与莫邪，是悲剧 你，又何尝不是]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[缅桂]]></title>
    	<url>/cn/2007/05/25/white-michelia/</url>
		<content type="text"><![CDATA[[晴空碧月下，和风细雨中，明明清清总是我，芳华依旧在 翠玉流珠转，绿丝银钗缠，千千静静独守候，幽香总不减]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Roma Holiday]]></title>
    	<url>/cn/2007/05/24/roma-holiday/</url>
		<content type="text"><![CDATA[[Roma Holiday(1953) is a delightful, captivating fairy-tale romance shot entirely on location in Rome. The film&#39;s bittersweet story is a charming romantic-comedy, a kind of Cinderella tale in reverse (with an April-October romance). A runaway princess (Hepburn) rebels against her royal obligations and escapes the insulated confines of her royal prison to find a &#39;Prince Charming&#39; commoner - an American reporter (Peck) covering the royal tour in Rome. The story was reportedly based on the real-life Italian adventures of British Princess Margaret. I have to leave you now. I&#39;m going to that corner there and turning. You stay in the car and drive away. Promise not to watch me go beyond the corner. Just drive away and leave me. As I leave you. …… Joe Bradley, American News Service. ——So happy, thanks. ……]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[闲暇的时候，你搜索谁]]></title>
    	<url>/cn/2007/05/19/who-do-you-search-in-spare-time/</url>
		<content type="text"><![CDATA[[偶尔想起看过一小段文字，名字叫《你的 MSN 为谁隐身》 当时蛮触动，虽然其实这样类似的问题很多 比如喜欢什么人的什么歌，喜欢什么结局的电影，喜欢什么样 我想到的是搜索引擎，平时大部分时间是拿来查找资料的 Google 或者 Baidu 我在想我们会不会偶尔也做一些奇怪的事情，比如搜索某人的名字 你会么，又或许这样类似的习惯你又会坚持多久 然后我发现我的确会这么做，甚至已经无法记起这种习惯已经有多久 也许很久，我静默的注视着每次搜索到的同样的页面 也许偶尔，我涟漪的凝望着突然显现出的不同的条目 也许，我的这种习惯，随着我 从多情的南方到热情的北方，从古老的东方到遥远的西方 用一种属于这个世纪文明的方式来表达对从前的怀念 那么，屏幕背后无尽的线上，你又会搜索谁呢]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[心灯]]></title>
    	<url>/cn/2007/05/01/heart-light/</url>
		<content type="text"><![CDATA[[如果還在相信無須相信的事 是因為還沒有失去信心 不是稻草 稻草在漂於茫茫海上時基本無用 不是明燈 明燈總能在黑暗中照亮前方 想期待的 是滄海君 可是從沒滄海 至少因為自己 畢竟沒有人能預計未來 畢竟人力無法逾越 逾越了 換回的 也許付出的是比逾越本身更高的代價 相信 是因為還有心 是因為還存僥倖 比如心燈 有了心燈 謝山才成寒月 如遠處微弱的火光 在沒有死前 接近它 證明已經看到的 不是眼花 但求心光 笑看風雲]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[北海·潿洲島]]></title>
    	<url>/cn/2007/04/27/weizhou-island/</url>
		<content type="text"><![CDATA[[一直我都想完成這次作業，即使沒有人會檢查 但一直我都沒有完成，等想起來的時候，已經完成的也都化成了記憶，再找不到 我想其實現在我仍然有能力回憶， 但是已經沒有了心情，不想再在深夜兩點的時候，還在繼續我激動的一路向往和張望 所以全部的眷戀和回憶都只能在我心中偶爾翻過，不再和誰去分享或者回味 自私不是一個好的詞，但怎麽偶爾也代表著無奈，無奈回首空 裂縫已經在的時候，即使靜下來了，時間也會如引力般慢慢將它拉大，大到如黑洞般不見底 For him, there was no upside. If he won, he upset Gutfreund. No good came of this. But if he lost, he was out of pocket a million bucks. This was worse than upsetting the boss. 也許我也曾經如 Meriwether 一樣左右為難 所以當 Meriwether 面對 Gutfreund 的為難，以他專業經化險為夷 &amp;ldquo;No, John,&amp;rdquo; he said, &amp;ldquo;if we&amp;rsquo;re going to play for those kind of numbers, I&amp;rsquo;d rather play for real money. Ten million dollars. No tears.&amp;rdquo; 於是 Meriwether 就不用再上下不安，左右為難 我也學到了他的方式，所以雖然原則曾經帶給我上下不安，可是它現在又確實讓我成功逃脫 &amp;ldquo;No, thought Meriwether, just very, very good.&amp;rdquo; 看到 Meriwether 這麽想的時候，其實我也已經學會了 原來我們完全可以不用執著於別人或者自己給自己下的圈套，如果根本沒有束縛，就不用為束縛發愁 在兩種選擇外還有第三種選擇，這就如同太極一樣在轉輾之間將重擔卸下 小時候聽過一首歌《外婆的澎湖彎》，裏面那句“陽光、沙灘、海浪仙人掌，還有一位老船長”讓我心生幾多向往 於是我輕鬆做完了一次新作業，即使作業沒人能夠看懂]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[吃早点]]></title>
    	<url>/cn/2007/04/27/breakfast/</url>
		<content type="text"><![CDATA[[起床之后随意收洗一番，去吃早点。 经过铁路，这铁路的历史很长，还在解放以前就有，是滇中商人自己建立起来的一条。本地人叫它“米轨”，是因为它比一般的铁路在要窄一些，只有一米宽。在“米轨”之上跑的是小火车，这名称还有些由来，当年为了防日本人直接将火车开到滇中来，就特意将铁轨做得窄一些，能由当时的商人自己定造的小火车往来。到现在这条铁轨还使用的好好的，只是城中的部分由于城市建设的原因较之前往北移动了大约三五十米。沿着铁路一直可以到河口，那里一处是和越南交界的边境口岸。 铁路前八米左右是一家网吧，昆明很不喜欢的一点就是网吧多，特别是住处更多，大大小小将近二十家，由于大学就在附近，机器也很多，来往的人更是复杂。自己是从来不进的，觉得里面的人大都是些没有事情可做或者觉得无事可做之人，加以浑浊的空气和满室的烟味，还有诸多茫然和不恭的脸孔，那不是每个人都喜欢的场合，但又确实有很多人喜欢。 网吧还没有完全打开，毕竟过于明显的和zf对着来也不是很好。网吧外面有很多人在整理今天准备卖出的报纸，大都是四川口音，那么早就开始忙碌的在昆明除了天还未亮之前的环卫人员外，应该就是他们了。这时候连路上行人也还很少，只有少数赶远去上班骑着自行车的人，还有早班的公车的身影。很是佩服四川人，他们特别能够适应生活，从高到低，从南到北，都可以见到他们的身影。现在他们就在为了自己的生活在努力着。 绕一个弯去报亭看一下新的杂志，然后又回到马路这边，过路的时候，有个老者从学校出来，穿一件T恤，T恤上印着投行的字样。看面容年轻的时候应该是很帅的，呵呵，可能刚刚早锻炼完去吃早点吧。 到早点的小店门口，要一碗稀饭，加点榨菜，慢慢吃起来，起早了就不想吃味道太重的东西，就没有要其它什么，觉得有些不好意思，但也不管了。 对面忽然有人打招呼，原来是平时经常去吃的一家东北水饺店的老板，正陪儿子吃早点，老板和他妻子例外，他们的口音并不重，这让我觉得很轻松，我们打个招呼，又随意聊过几句。 老板带着他儿子去上学了，又注意一下桌子对面，刚才遇到的老者也来了，开始吃起来，旁边又来一们，土生土长的老昆明，老者便让了座，接着吃。新来的老昆明也开始边吃边与一起来的人聊天。 自己吃得很慢，一边又听他们说话玩，不知为什么，很喜欢这样的场合听他们的各种说话，虽然不像北京胡同里的南侃北聊，听着也还是蛮有味道的。 我对面还有一个小姑娘，才七八岁光景，也只要了一碗稀饭，听口音应该是附近民工的子女，她父母可能就是刚才所见那些整理报纸的人中的某一位吧，他们的父母为这个城市做了那么多事情，可是却从来没有让自己能够觉得有融入这个城市的感觉，连子女上学也很困难，唉。 小姑娘长吃得很高兴，声音蛮大的，不过这次自己并没有感觉难受，换作平时，如果谁吃东西声音太大，会感觉特别难过。记得有次也是来吃早点，有人吃得声音差点没让自己就过去提意见。见小姑娘吃完，给她一张纸，她没要，不过还是觉得自己应该这么做，呵呵。 旁边的人也还在吃，自己已经吃完，就付帐回去，路上的人已经很多了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[瑛姑]]></title>
    	<url>/cn/2007/04/21/ying-gu/</url>
		<content type="text"><![CDATA[[晚上偶然又看到《射雕英雄傳》，正好播到瑛姑在周伯通在大理國一段。大意是說段皇爺為了怕裘千仞來找麻煩，所以請了天下第一高手王重陽來教自己《九陰真經》的功夫。王道長師弟周伯通是個好玩的人，想參觀參觀皇宮，所以也就一塊跟來了。他們到了大理國之後，段皇爺當然就開始跟著王老師勤修苦練。誰知由於自己的冷落，王妃瑛姑就在種種巧合之下稀裏糊塗的和周伯通同志攪到了一起，以至於這事到最後一發不可收拾。王道長自覺丟臉，含羞離去；周伯通內心有愧，溜之大吉；王妃娘娘原本守在簾後，安安靜靜的寂寞，這下變成了未老白頭，淒淒慘慘的等待；皇爺倒還是皇爺，不過就是和皇爺對自己的稱呼一樣，皇爺成了真正的孤家寡人，這事換了別人，非得殺了那兩男女不可，可是即便是皇爺寬宏大量也沒能挽回娘娘的心，於是皇爺一個想不通就乾脆出家去當了和尚，成了一盞青燈。 其實長大後，很長時間裏陪我的，是還珠的全集，它的風格和內容都迥異於金氏作品。字裏行間講究的是修身養性、除魔衛道、成仙成佛，剛好俺小時候受還珠的影響太大，所以對金先生這種現代江湖恩怨情仇式的武俠故事，記的當然就不如蜀山中的紫、青雙劍清楚啦。 不過這樣都好，這樣自己才饒有興趣的回味了一下當年的經典。對於靖哥哥、蓉兒之類的情節，早就已經不感有趣，金庸太偏心，整部小說中得到善終的人都不多。全部的焦點都毫無保留的集中到了兩個人身上，其他人要麽死、要麽傷，再慘一點妻離子散，又或者黃卷青燈。自己一向喜歡將同情心放到弱者身上，所以這種心態影響之下，即使瑛姑做了這些多少可以用“以爛為爛”來形容的事之後，俺還是覺得她蠻可憐的。 瑛姑的扮演者也是選美出身，比起大陸後來拍的而言，覺得曾慶瑜要演的好的多。不管是在大理皇宮身著華服，還是後來布衣荊衩，瑛姑身上的雍容氣質都不曾有變。可是做皇妃做到瑛姑那樣，真的很鬱悶，你看她出場到終場的表情就能見到，除了和周伯通同志在樹林裏嬉戲的小段時間裏，偶爾表現的一段小兒女的害羞外，始終都是眉頭深鎖。皇爺練功練了一年，當妻子的想探一探都不行，心情糟糕到什麽程度也可想像。後來曾慶瑜去臺灣唱歌後，比較經典的一張專輯《一網情深》，裏面的歌，那些歌詞和歌聲，聽聽總會覺得她還留在瑛如的感覺沒走出來。 所以周伯通式的嬉皮讓她留下一生的眷念，我不知道別的讀者怎樣想，但兒子死後，萬念俱灰時，《四張機》的聲音，特別是最後的“可憐未老頭先白”在耳邊想起時，我能感覺得到她確實是受害者之一。當然我也能理解段皇爺其實心裏也很苦。比起瑛姑，段皇爺這個角色甚至從頭到尾讓人感覺不到一絲皇帝應有的霸氣，早年為仇困，彼於練武；後來為情困，醉於消沉；再後來為心困，黃卷青燈。哪裏還像是一國君王。這兩個人，用華仔一首不太出名的歌《真愛是苦味》來形容，還是蠻貼切的。整個事情下來，畢竟不是每個人都能像周伯通同志那樣瀟灑，拍拍屁股就走，所以大家的傷心也就再所難免。 《四机张》 鸳鸯织就欲双飞 可怜未老头先白 春波碧草 晓寒深处 相对浴红衣 不過不管怎麽說，瑛姑也算沒看走眼，雖然經歷了那麽多傷心事，受過各種打擊和心理折磨，甚至足足熬了兩代人的時光，才算是等來了自己的幸福。比起楊鐵心和包惜弱的含恨而終，她在兩部小說中位列幸運佩角也無可厚非。可以猜想，也許金先生在寫這個角色的時候，應該也是憐惜多過鄙薄。前面說了，金先生和還珠樓主不同，他不是那種為了一個並不重要的支線情節就肯花幾萬字甚至十幾萬字去敘說的主。以瑛姑在小說中的角色地位，《神雕俠侶》，甚至《射雕》都沒有必要去花那麽多的筆墨去寫這個曾經苦命的女子。 其實除了這些，金庸在寫這部小說的時候都有意無意的將另一個事實敷衍了事。《射雕英雄傳》的背景時間是南宋，那時陳、朱理學已經一統天下，甚至“三綱五常”也已經成為了當時的主流思想道德標準。大理這樣的小國，即使偏遠一些，民風不如中原那般拘謹，但以段智興當時和中原之間的密切來往而言，“倫理”二字不應該淡薄到甚至讓人不曾想起。可是從小說到電視劇，特別是83版，我很難感覺得到這一點。瑛姑在周伯通走了以後，仍然住在皇宮，甚至生了一個兒子，如果按照當時的倫常來看，她非但不能住在皇宮，恐怕早就如《神雕俠侶》（劉德華、陳玉蓮版）中的人一般，被唾罵至死了。要知道，楊過和小龍女才是師生戀就受到了全江湖人的反對啊，更何況瑛姑的所做所為。在兒子被蒙面人打成重傷後，她也不管不顧的去求救於段皇爺；在那一晚上漫長的等待後一夜白發，連受害人之一段皇爺都不勉心酸被打動；再後來十年如一日苦練武功，想方設法去報仇，所有這一切只有一個解釋：為了兒子，為了兒子她完全可以不顧全部人的冷熱嘲諷。由此可見她內心對兒子有多看重。她為了兒子如此瘋狂，瘋狂到裘千仞同志都頭疼不已。她也苦學河科洛書，為的是去桃花島救老周。所以當黃蓉說不能陪她住滿一年，但是可以幫她找到周伯通的時候，她連想也不想就將與郭、黃二人的怨恨也一筆勾銷，這又說明周伯通在她心中的位置有多重。也許正是這種刻骨銘心的癡纏，和近乎瘋狂的執著，讓金先生都不忍心再在她身上加上過重的有違倫常的帽子吧。 曾慶瑜演過的片其實不多，雖然曾經也踏足歌壇並有所成就，畢竟不曾是一線紅星，所以基本也沒人記得此人此角，網上對她介紹也不多，後來的生活更是少有轉載，她應該还好吧? 我想比起另一個類似的配角周秀蘭（雙兒）來……]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[春逝]]></title>
    	<url>/cn/2007/04/20/faring-spring/</url>
		<content type="text"><![CDATA[[清時已經過去半月 雨也漸漸下得少了 絲絲的傷感也隨著天氣的好轉慢慢逝去 還記得曾經收到過的一條短信 清時到 雀兒噪 踏青節裏春光俏 柳葉長 杏花鬧 卻是綠野晴 放風箏 也嬉鬧 且看人倚秋天笑 探鶯花 春醉倒 願你幸福樂逍遙 童年的回憶當然不會這般詩情畫意 畢竟這都是才子佳人式的出遊 是童麗《煙花三月》式的歌聲 在自己的記憶中的春天 是春雨過後門前屋後長出來的小小的新芽 是路兩旁、樹梢上 一不留意冒出來的淺淺的綠 是草長鶯飛 是柳絮紛揚 是處處貼於樹上 灑滿一地的桃紅梨白 是和小夥伴們奔跑在油菜花叢中 在草地中打滾 在牆角隙縫捉蜜蜂 在路旁的野花叢裏捕蝴蝶 是風中飄過的至今猶如在昨天的鳥語花香…… 昨天是三月三 相傳是黃帝建國立都的日子 很多地方在祭拜這位傳說中中華民族的始祖 我家鄉沒有這樣的風俗 只是在這天 從前 小孩子們會去野外地裏摘一種叫“細米草”的小草 回來讓媽媽將草與雞蛋放在一起煮過 然後兄弟姊妹們在滿臉幸福享受著只屬於這個季節的味道 過完這天 春天就要遠去 取而代之的 是火熱的夏天…… 很多年後 我們的生活好起來後 甚至連河兩邊都已經是路燈和垂柳的時候 可是 我們童年的記憶 還有春天 去哪里了呢]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[记住气味，还是记住动作 z]]></title>
    	<url>/cn/2007/04/19/memory/</url>
		<content type="text"><![CDATA[[一直在家呆着呢，暂时，还想不到去做怎样适合自己的事 我想起来前几天在楼下的花园遇到同楼邻居家的那条狗 那真的是已经很老很老的狗了 我小学的时候，它 我用脚去蹭它的腹部，结果它就安静下来了，似乎很享受 然后后来每回看到我它就摇着尾巴绕着我转圈咯 后来我一直都知道它喜欢那种安抚方式 我好像空白了之间对它的记忆，因为也只是照面 后来是在前几天，我遇到它，气喘吁吁的，我问爷爷它生病了吗？或者它也觉得天冷 爷爷说，它老了，和他一样，我看到爷爷花白的发 奇怪，它对我不热情了 它过来闻闻我的脚，好久好久，然后走开了，也没摇尾巴 我有些难受，我想，原来牲畜也是会忘记人的 突然它又回过头来在我面前趴下 我看了会，突然想起做那个动作，然后它就很激动地跳起来抱我的腿了哦 我看到它眼睛里有白色的东西，爷爷说它眼睛也不好了 它很努力地直着身抱我的腿 那时候我有些泪楚 我知道我是过于敏感了，可是我还是会想到 有一天，你视线模糊了，气喘得走路都累，或者你鼻子也不甚灵敏了，耳朵好像也不好了 可是你会不会用一种动作记住一个人 我在想唐伯虎点秋香的时候，也许秋香也露出了什么小动作吧]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[留]]></title>
    	<url>/cn/2007/04/15/keeping/</url>
		<content type="text"><![CDATA[[回來的時候路過區政府時遠遠看見樓前綠地的花壇邊坐著一個六七歲的小姑娘相對的是一位大約三十左右的女子也許是小姑娘的孃孃或者媽媽兩人正在玩著大約 想起另外一次在蘇寧門口的另一幕也是一名年輕女子和一個小姑娘簡單的拳語一蹦一跳、你進一步我退一步那洋溢著天真爛漫與清澈的表情令自己至今難忘 羡慕小朋友的無憂無慮但讓自己感動的是大人身上散發出來的那種幾分稚趣中又分外明現的明清與悠然 我們大部分人在生活壓力下忙碌著偶爾才會有時間抱怨現實懷念已經不在的童年似乎我們都想不起原來我們還可以留住童心]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[日記]]></title>
    	<url>/cn/2007/04/12/diary/</url>
		<content type="text"><![CDATA[[偶然拿起以前的日記本 黑色的軟膠皮本中 記載著舊日的喜怒哀樂 沒有歷歷在目的感覺 仿佛都是記憶深處 很久以前的點點滴滴 遙遠、模糊又淡淡的 那時的夢想、追 一如那年的年齡 埋在了心靈深處 只在閒暇時 擦了灰塵 慢慢回味]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[我想了半天]]></title>
    	<url>/cn/2007/04/11/meditations/</url>
		<content type="text"><![CDATA[[收拾完发现有些凉 才想起已经下了一星期的雨 记得曾经有一段时间并不喜欢这样的季节 谁知道呢 也许是因为那心情不好的时候正在下雨 不经不觉过了很久 连爱好 走在清新带着湿润的树下 花和草的香味多新鲜 那时还是白天 现在已经是深夜 我开了个空间 叫什么名字好呢 就突然看见了一首歌 《我想了半天》]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[懷念的 80 年代歌謠之聲東洋篇 z]]></title>
    	<url>/cn/2007/04/02/japansongs/</url>
		<content type="text"><![CDATA[[注入大陸的日語歌曲其實很多，喜歡的人也不少；部分來自港台歌手當年的翻唱，那一輩的歌手比如張德蘭、徐小鳳、鄧麗君、張學友、李克勤、劉若英等等， 單是歌曲，東京愛情故事、鏡中的裙子、碧綠色的兔子、夢的點滴、像花一樣……，數也數不清。歌手比如松隆子，當年甚至跑遍全城去找她的專輯，回想那時的癡迷，都多感慨…… 日本80年代超级偶像 - http://blog.xuite.net/pec2000/ray 這裏面有一張專輯，裏面有好幾首，比如星之所在、水果籃子等等 - http://stargirl.15150.com/ bbs - http://bbs.javaws.com/index.php 這首橋幸夫——子連れ狼(天涯孤客原曲) ，是否有或曾熟悉的感覺？ - http://blog.xuite.net/pec2000/ray/10440939 其實許多日本歌手，男的女的，他（她）們的聲音都比較符合自己的口味，最喜歡的一點還是因為大部分能留下印象的歌，除了聲音很動聽外，更重要是歌中連直接的情或者愛都很少，一陣風、一朵花、一段前緣都能夠是一首曲，反而看現在的歌，無病呻吟、句度不准、吐字不清的實在太多，往往一首歌今年流行了，明年就再也不會有人提起；一張唱片還沒有出，各種媒體的花邊新聞就開始想方設法去引人關注，唱下來的水平不敢恭維。 經典是要流傳下去代代口述的……]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[About CPA]]></title>
    	<url>/cn/2007/01/26/about-cpa/</url>
		<content type="text"><![CDATA[[经过亚太的时候，看见门口非常热闹，才想起来今天是 cpa 报名的日子 一起的学友，报考的工作人员，更多的是不认识的人，一派忙碌 现在 cpa 到底还能值多少钱， 当初教自己的老师，当年五科一次通过，那年是计划经济调整为市场经济的改革年 今年刚好是会计和审计准则调整的又一个重大年 时间过得真快，转眼就十几年了，但那位老师并没有离开他的工作单位，在单位他也是主审 在街上张望，不经意中才发现 和周围的大楼比起来，那幢属于亚太的楼实在矮小，cpa 真有传说中那么好么]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[冷、暖]]></title>
    	<url>/cn/2007/01/26/warm-cold/</url>
		<content type="text"><![CDATA[[刮了一早上的湿凉的风 天渐渐的越来越冷 呼出的气也变成白色 要下雪了 坐在车里看窗外 昏黄的街灯 思绪在漫天飞舞 从一个冷的地方 到另一个更冷的地方 心中怀念]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[烧饼]]></title>
    	<url>/cn/2007/01/24/pan-pie/</url>
		<content type="text"><![CDATA[[想起星期五 朋友一起玩时吃的烧饼 名字很土 叫土掉渣 不过味道很好 很好吃 像小时候吃的一种早点 叫猪油锅魁 那时候不会吃 现在会了 却早已经没人再做 这几日晚上 总是小时候留在心中家的印象 回家感觉蛮好]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[看《創世紀》]]></title>
    	<url>/cn/2007/01/22/at-the-threshold-of-an-ear/</url>
		<content type="text"><![CDATA[[沒天沒夜的日子連時間也似乎變快一些 記不清有多久 終於看完了《創世紀》的前半部分 從前看喜劇更多 突然改看這樣的片 心也隨著情節沈重失落 在黑夜裏 一個人 等待又一個天明]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[Python - 资源汇总]]></title>
    	<url>/src/python/</url>
		<content type="text"><![CDATA[[Packages THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包。他们最近发布了 THULAC 分词 Python 版本，已经可以试用了。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[R - 资源汇总]]></title>
    	<url>/src/r/</url>
		<content type="text"><![CDATA[[Packages testthat 是一个可以让你将现有的非正式的检验转化为正式自动检验的 R 包，这样让你运行起来更容易和更快速，更多详细信息请参见此处。 wbstats 提供了世界银行数据 weightr 是一个元分析（meta-analysis）的新包 ，同时该包添加了启动 Shiny app 的功能。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[机器学习 - 资源汇总]]></title>
    	<url>/src/machine-learning/</url>
		<content type="text"><![CDATA[[学习资料 非平衡样本的分类问题是机器学习的经典问题之一，困扰着不少童鞋。这篇博文提供在 R 中解决该问题的实用指南。 CMU 机器学习系著名教授 Alex Smola 在 Quora 对 Machine Learning》的建议：Alex 推荐了不少关于线性代数、优化、系统、和统计领域的经典教材和资料。 Papers 机器学习顶级会议 ICML2016 论文赏析：deep reinforcement learning benchmarking，代码下载。]]]></content>
    </entry>
    <entry>
    	<title><![CDATA[深度学习 - 资源汇总]]></title>
    	<url>/src/deep-learning/</url>
		<content type="text"><![CDATA[[学习资料 Yann LeCun 是卷积神经网络的发明人，Facebook 人工智能研究院的负责人。下文的 150 张 PPT，是 LeCun 对深度学习领域的全面而细致的思考。LeCu 非常坚定看好无监督学习，认为无监督学习是能够提供足够信息去训练数以十亿计的神经网络的唯一学习形式。但 LeCun 也认为，这要做好非常难，毕竟世界是不可理解的。我们来看看 LeCun 在这 150 张 PPT 中，究竟给我们带来什么样的惊喜。]]]></content>
    </entry>
</search>